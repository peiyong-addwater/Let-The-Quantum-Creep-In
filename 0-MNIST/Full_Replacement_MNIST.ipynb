{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:37.033986Z",
     "iopub.status.busy": "2024-04-02T16:10:37.033383Z",
     "iopub.status.idle": "2024-04-02T16:10:43.549089Z",
     "shell.execute_reply": "2024-04-02T16:10:43.548169Z",
     "shell.execute_reply.started": "2024-04-02T16:10:37.033958Z"
    },
    "id": "5ebnFsErY0EM",
    "outputId": "e8932b66-715c-467f-912a-8b449c518214",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pennylane in /usr/local/lib/python3.9/dist-packages (0.35.1)\n",
      "Requirement already satisfied: cotengra in /usr/local/lib/python3.9/dist-packages (0.5.6)\n",
      "Requirement already satisfied: quimb in /usr/local/lib/python3.9/dist-packages (1.7.3)\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (1.3.2)\n",
      "Requirement already satisfied: rustworkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.14.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.23.4)\n",
      "Requirement already satisfied: autograd in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.6.2)\n",
      "Requirement already satisfied: pennylane-lightning>=0.35 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.35.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pennylane) (4.10.0)\n",
      "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.6.9)\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\n",
      "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.10.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (3.0)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.59.1)\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.12.3)\n",
      "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.2.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (0.11.2)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.39->quimb) (0.42.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (3.9.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2023.1.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.99)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!pip install torch torchvision torchaudio --upgrade\n",
    "!pip install pennylane cotengra quimb torchmetrics --upgrade\n",
    "#!pip install -U \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:43.550874Z",
     "iopub.status.busy": "2024-04-02T16:10:43.550642Z",
     "iopub.status.idle": "2024-04-02T16:10:47.679639Z",
     "shell.execute_reply": "2024-04-02T16:10:47.678406Z",
     "shell.execute_reply.started": "2024-04-02T16:10:43.550852Z"
    },
    "id": "VN2wD-U7bGse",
    "outputId": "efc73948-0832-402c-eafc-b0e6adce4a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "\n",
    "\n",
    "#import pennylane as qml\n",
    "#import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "#prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "#torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WmQPQuLbSFw"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:47.680869Z",
     "iopub.status.busy": "2024-04-02T16:10:47.680416Z",
     "iopub.status.idle": "2024-04-02T16:10:47.809475Z",
     "shell.execute_reply": "2024-04-02T16:10:47.808255Z",
     "shell.execute_reply.started": "2024-04-02T16:10:47.680826Z"
    },
    "id": "rUhQUP2dbTja",
    "outputId": "0d3641db-a2ff-4689-8b7e-32e0e8b313d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 32, 32])\n",
      "torch.Size([32])\n",
      "tensor([7, 7, 5, 7, 4, 7, 7, 7, 5, 4, 7, 2, 7, 8, 1, 1, 1, 6, 4, 1, 5, 1, 2, 7,\n",
      "        8, 0, 8, 6, 1, 8, 8, 7])\n",
      "tensor([-1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j,  0.2863+0.j,  0.7098+0.j, -0.7333+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j])\n"
     ]
    }
   ],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQI8WWY6byQq"
   },
   "source": [
    "# Some Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:47.812079Z",
     "iopub.status.busy": "2024-04-02T16:10:47.811853Z",
     "iopub.status.idle": "2024-04-02T16:10:49.208648Z",
     "shell.execute_reply": "2024-04-02T16:10:49.207311Z",
     "shell.execute_reply.started": "2024-04-02T16:10:47.812056Z"
    },
    "id": "7B4DTncWbwQl",
    "outputId": "38975b7c-66c4-49f5-e5fd-158e55028cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-0.j, 0.+0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j,  0.+0.j, -1.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-0.j, 0.+0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j,  0.+0.j, -1.+0.j],\n",
      "        [ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.-0.j, 0.-1.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  0.-0.j,  0.-0.j, -1.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j,  1.-0.j,  0.-0.j],\n",
      "        [ 0.+0.j,  1.-0.j,  0.+0.j,  0.-0.j],\n",
      "        [-1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, -0.+0.j, 0.-0.j, 0.+1.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, -0.-1.j, 0.+0.j, -0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -0.+0.j, -1.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -0.+0.j, -1.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-0.j, -0.+0.j, 0.+1.j],\n",
      "        [0.+0.j, 0.+0.j, -0.-1.j, -0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j, -0.+0.j,  1.-0.j]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "ket = {\n",
    "    '0':torch.tensor([1.,0.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '1':torch.tensor([0.,1.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '+':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) + torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device)),\n",
    "    '-':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) - torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device))\n",
    "}\n",
    "\n",
    "pauli = {\n",
    "    'I':torch.tensor([[1.,0.],[0.,1.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'X':torch.tensor([[0.,1.],[1.,0.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Y':torch.tensor([[0., -1.j],[1.j, 0]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Z':torch.tensor([[1.,0.],[0.,-1.]], dtype = COMPLEX_DTYPE, device=device)\n",
    "}\n",
    "\n",
    "def tensor_product(*args):\n",
    "  input_list = [a for a in args]\n",
    "  return functools.reduce(torch.kron, input_list)\n",
    "\n",
    "def multi_qubit_identity(n_qubits:int):\n",
    "  assert n_qubits>0\n",
    "  if n_qubits == 1:\n",
    "    return pauli['I']\n",
    "  else:\n",
    "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
    "\n",
    "pauli_words_su4 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
    "      #print(key1, key2)\n",
    "      #print(pauli[key1])\n",
    "      #print(pauli[key2])\n",
    "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
    "      #print(pauli_words_su4[key1+key2])\n",
    "\n",
    "pauli_words_su8 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      if not key1+key2+key3 == 'III':\n",
    "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
    "\n",
    "pauli_words_su16 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        if not key1+key2+key3+key4 == 'IIII':\n",
    "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
    "              pauli[key1],\n",
    "              pauli[key2],\n",
    "              pauli[key3],\n",
    "              pauli[key4]\n",
    "          )\n",
    "\n",
    "pauli_words_su32 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        for key5 in pauli.keys():\n",
    "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
    "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
    "                pauli[key1],\n",
    "                pauli[key2],\n",
    "                pauli[key3],\n",
    "                pauli[key4],\n",
    "                pauli[key5]\n",
    "            )\n",
    "\n",
    "def su32_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = list(pauli_words_su32.values())\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, torch.stack(dict_values))\n",
    "  #print(generator.shape)\n",
    "  return torch.matrix_exp(1j*generator)\n",
    "\n",
    "def su4_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = torch.stack(list(pauli_words_su4.values()))\n",
    "  #print(dict_values.shape)\n",
    "  generator = torch.einsum(\"i, ijk -> jk\", params, dict_values)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "def pauli_dict_func(key):\n",
    "    return pauli[key]\n",
    "\n",
    "def pauli_dict_func_multiple_keys(keys):\n",
    "    return list(map(pauli_dict_func, keys))\n",
    "\n",
    "def pauli_string_tensor_prod(pauli_string:str):\n",
    "    paulis_char = list(pauli_string)\n",
    "    paulis_mat = pauli_dict_func_multiple_keys(paulis_char)\n",
    "    return tensor_product(*paulis_mat)\n",
    "\n",
    "def generate_nqubit_pauli_strings(n_qubits:int):\n",
    "    assert n_qubits>0\n",
    "    pauli_labels = ['I', 'X', 'Y', 'Z']\n",
    "    pauli_strings = []\n",
    "    for labels in itertools.product(pauli_labels, repeat=n_qubits):\n",
    "        pauli_str = \"\".join(labels)\n",
    "        if pauli_str != 'I'*n_qubits:\n",
    "            pauli_strings.append(pauli_str)\n",
    "    return pauli_strings\n",
    "\n",
    "def generate_pauli_tensor_list(pauli_strings:list):\n",
    "    return list(map(pauli_string_tensor_prod, pauli_strings))\n",
    "\n",
    "\n",
    "print(\n",
    "    generate_pauli_tensor_list(generate_nqubit_pauli_strings(2))\n",
    ")\n",
    "\n",
    "\n",
    "#test_params = torch.randn(4**2-1,  device=device).type(COMPLEX_DTYPE)\n",
    "#print(test_params.shape)\n",
    "#test_op = su4_op(test_params)\n",
    "#print(test_op)\n",
    "#print(qml.is_unitary(qml.QubitUnitary(test_op.cpu().numpy(), wires=[0,1]))) # will be false if not torch.cdouble\n",
    "\n",
    "#rx = torch.linalg.matrix_exp(1j*torch.pi*pauli[\"X\"]/2)\n",
    "#print(qml.is_unitary(qml.QubitUnitary(rx.cpu().numpy(), wires=[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:49.210092Z",
     "iopub.status.busy": "2024-04-02T16:10:49.209878Z",
     "iopub.status.idle": "2024-04-02T16:10:49.219820Z",
     "shell.execute_reply": "2024-04-02T16:10:49.219262Z",
     "shell.execute_reply.started": "2024-04-02T16:10:49.210072Z"
    },
    "id": "Xs0c2F1eBnGc"
   },
   "outputs": [],
   "source": [
    "def measure_sv(\n",
    "    state,\n",
    "    observable\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Measure a statevector with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not\n",
    "  \"\"\"\n",
    "  expectation_value = torch.conj(state)@observable@state\n",
    "  return torch.real(expectation_value)\n",
    "\n",
    "def measure_dm(\n",
    "    rho,\n",
    "    observable\n",
    "):\n",
    "  \"\"\"\n",
    "  Measure a density matrix with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not.\n",
    "  \"\"\"\n",
    "  product = torch.matmul(rho, observable)\n",
    "\n",
    "  # Calculate the trace, which is the sum of diagonal elements\n",
    "  trace = torch.trace(product)\n",
    "\n",
    "  # The expectation value should be real for physical observables\n",
    "  return torch.real(trace)\n",
    "\n",
    "vmap_measure_sv = torch.vmap(measure_sv, in_dims=(None, 0), out_dims=0)\n",
    "vmap_measure_dm = torch.vmap(measure_dm, in_dims=(None, 0), out_dims=0)\n",
    "\n",
    "# assuming the input patch observables (hermitianized) has shape (batchsize,n_patches, c, h, w)\n",
    "# assuming the input set statevectors has shape (c, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_sv_batched_ob = torch.vmap(measure_sv, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "# assuming the input set desnity matrices has shape (batchsize, n_patches, c, 2**n, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_dm_batched_ob = torch.vmap(measure_dm, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "def bitstring_to_state(bitstring:str):\n",
    "  \"\"\"\n",
    "  Convert a bit string, like '0101001' or '+-+-101'\n",
    "  to a statevector. Each character in the bitstring must be among\n",
    "  0, 1, + and -\n",
    "  \"\"\"\n",
    "  assert len(bitstring)>0\n",
    "  for c in bitstring:\n",
    "    assert c in ['0', '1', '+', '-']\n",
    "  single_qubit_states = [ket[c] for c in bitstring]\n",
    "  return tensor_product(*single_qubit_states)\n",
    "\n",
    "\n",
    "#test_patch = torch.randn(size=[5, 2, 3, 4,4], dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_herm_patch = (torch.einsum(\"...jk->...kj\", test_patch)+test_patch)/2\n",
    "#print(test_herm_patch)\n",
    "#print(\"all patches shape\", test_herm_patch.shape)\n",
    "#test_sv = torch.stack([bitstring_to_state('++')]*3, axis = 0)\n",
    "#print(test_sv)\n",
    "#print(\"all sv shape\", test_sv.shape)\n",
    "#res = vmap_measure_channel_sv_batched_ob(test_sv, test_herm_patch)\n",
    "#print(res)\n",
    "#print(\"res shape\",res.shape)\n",
    "\n",
    "#for batchid in range(test_patch.shape[0]):\n",
    "#  batchitem = test_patch[batchid]\n",
    "#  for patch_idx in range(batchitem.shape[0]):\n",
    "#    patch = batchitem[patch_idx]\n",
    "#    for ch_idx in range(patch.shape[0]):\n",
    "#      print(f\"batchid={batchid}; patchid = {patch_idx}; chid = {ch_idx}\")\n",
    "#      ch = patch[ch_idx]\n",
    "#      #print(ch.shape)\n",
    "#      sv_ch = test_sv[ch_idx]\n",
    "#      #print(sv_ch.shape)\n",
    "#      print(measure_sv(sv_ch, ch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFZqT6bpElaL"
   },
   "source": [
    "# Flipped Quanvolution Kernel\n",
    "\n",
    "First, let's define the function to extract patches for a single kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:49.221071Z",
     "iopub.status.busy": "2024-04-02T16:10:49.220885Z",
     "iopub.status.idle": "2024-04-02T16:10:49.226832Z",
     "shell.execute_reply": "2024-04-02T16:10:49.226163Z",
     "shell.execute_reply.started": "2024-04-02T16:10:49.221053Z"
    },
    "id": "He4HdMRHC7T6"
   },
   "outputs": [],
   "source": [
    "def extract_patches(image, patch_size, stride, padding=None):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image with multiple input channels and optional custom padding.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (in_channels, height, width).\n",
    "        patch_size (int): Size of the square patches to extract.\n",
    "        stride (int): Stride between patches.\n",
    "        padding (tuple): Padding value(s) for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n",
    "    pad_l, pad_r, pad_t, pad_b = padding if padding is not None else (0,0,0,0)\n",
    "\n",
    "    if padding is not None:\n",
    "        image = torch.nn.functional.pad(image, (pad_l, pad_r, pad_t, pad_b), mode='constant')\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    height, width = image.shape[-2],  image.shape[-1]\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = image[..., i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    patches = torch.stack(patches, dim=-4)\n",
    "    return patches # has shape (batchsize, n_patches, channel, h, w)\n",
    "\n",
    "\n",
    "#single_img = torch.stack([torch.arange(32*32*1,dtype = torch.double, device=device).reshape((32,32)).type(torch.cdouble)]*3)\n",
    "\n",
    "#test_img = torch.stack([single_img]*2)\n",
    "#print(test_img[0][...,:4,:4])\n",
    "#patches = extract_patches(test_img, patch_size=4, stride=2,padding=(0,0,0,0))\n",
    "#print(patches.shape)\n",
    "#print(patches[0].shape)\n",
    "#print(patches[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRqflooEqKN"
   },
   "source": [
    "## A Three-by-Three Quanv Kernel Function\n",
    "\n",
    "For 3 by 3 patches, we only need a 2-qubit parameterised state, generated by the SU4 gate applied to the $|00\\rangle$ state. The first thing to do is to pad the 3 by 3 patches to 4 by 4. However, for input with size\n",
    "\n",
    "$$\n",
    "D_1\\times H_1 \\times W_1\n",
    "$$\n",
    "\n",
    "where $D_1$ is the number of input channels, we will need $D_1$ different two-qubit states, generated by $D_1$ different parameterised circuits. These circuits will produce $D_1$ expectation values. Suppose each circuit is parameterised by $\\theta_i, i\\in \\{1,2,\\cdots, D_1\\}$, the parameterised (pure) state can be written as\n",
    "\n",
    "$$\n",
    "|\\psi (\\theta_i)⟩ = SU4(\\theta_i)|00⟩\n",
    "$$\n",
    "\n",
    "For a channel $M_i$ of patch $M$ with shape $4\\times 4$, the expectation value is calculated by\n",
    "\n",
    "$$\n",
    "⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$\n",
    "\n",
    "Then, a single pixel value of the output channel corresponding this single kernel is\n",
    "\n",
    "$$\n",
    "\\frac{1}{D_1}\\sum_{i=1}^{D_1}⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:49.227850Z",
     "iopub.status.busy": "2024-04-02T16:10:49.227670Z",
     "iopub.status.idle": "2024-04-02T16:10:49.233341Z",
     "shell.execute_reply": "2024-04-02T16:10:49.232708Z",
     "shell.execute_reply.started": "2024-04-02T16:10:49.227837Z"
    },
    "id": "Yzn4KEt5ErG7"
   },
   "outputs": [],
   "source": [
    "def generate_2q_param_state(theta):\n",
    "  state = bitstring_to_state('00')\n",
    "  state = torch.matmul(su4_op(theta), state)\n",
    "  return state\n",
    "\n",
    "vmap_generate_2q_param_state = torch.vmap(generate_2q_param_state, in_dims=0, out_dims = 0)\n",
    "\n",
    "def single_kernel_op_over_batched_patches(thetas, patch):\n",
    "  # patch has shape (c_in, h, w)\n",
    "  # thetas has shape (c_in, 4^2-1) for SU4 gates\n",
    "  n_theta = thetas.shape[-2]\n",
    "  n_channel = patch.shape[-3]\n",
    "  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n",
    "  states = vmap_generate_2q_param_state(thetas)\n",
    "  #print(\"States shape\", states.shape)\n",
    "  patch = torch.nn.functional.pad(patch, (0, 1, 0, 1), mode='constant')\n",
    "  patch_t = torch.einsum(\"...jk->...kj\", patch)\n",
    "  herm_patch = (patch_t+patch)/2 # has dim (batchsize, num_patches, c, h, w)\n",
    "  #print(\"Herm patch shape\", herm_patch.shape)\n",
    "  channel_out = vmap_measure_channel_sv_batched_ob(states, herm_patch) # has dim (batchsize,n_patches, c)\n",
    "  return torch.sum(channel_out, axis = -1)/n_theta # has dim (batchszie, n_patches)\n",
    "\n",
    "\n",
    "#test_params = torch.randn((1,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_img = dummy_x.to(device)#.type(torch.cdouble)\n",
    "#print(test_img.shape)\n",
    "#test_patches = extract_patches(test_img, patch_size=3, stride=1,padding=(1,1,1,1))\n",
    "#print(test_patches.shape)\n",
    "#print(test_params.shape)\n",
    "#print(vmap_generate_2q_param_state(test_params))\n",
    "#test_out = single_kernel_op_over_batched_patches(test_params, test_patches)\n",
    "#print(test_out.shape)\n",
    "#h = (32-3+1*2)//1 +1\n",
    "#h**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4BXEKd8I67_"
   },
   "source": [
    "## Multiple Output Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:49.234207Z",
     "iopub.status.busy": "2024-04-02T16:10:49.234022Z",
     "iopub.status.idle": "2024-04-02T16:10:49.237565Z",
     "shell.execute_reply": "2024-04-02T16:10:49.237013Z",
     "shell.execute_reply.started": "2024-04-02T16:10:49.234191Z"
    },
    "id": "72vkHV_BI80l"
   },
   "outputs": [],
   "source": [
    "# For multiple channel output\n",
    "# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n",
    "vmap_vmap_single_kernel_op_through_extracted_patches = torch.vmap(single_kernel_op_over_batched_patches, in_dims=(0, None), out_dims=-2) # output has dim (batchsize, c_out, n_patches)\n",
    "\n",
    "#c_in = 1\n",
    "#c_out = 2\n",
    "\n",
    "#test_params2 = torch.randn((c_out, c_in,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_out2 = vmap_vmap_single_kernel_op_through_extracted_patches(test_params2, test_patches)\n",
    "#print(test_out2.shape)\n",
    "#test_out_2_features = test_out2.reshape((-1,c_out, 32, 32))\n",
    "#print(test_out_2_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuZ6G5-sKK_J"
   },
   "source": [
    "## Pytorch Module for Three-by-Three Flipped Quanv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:49.238726Z",
     "iopub.status.busy": "2024-04-02T16:10:49.238539Z",
     "iopub.status.idle": "2024-04-02T16:10:49.246127Z",
     "shell.execute_reply": "2024-04-02T16:10:49.245545Z",
     "shell.execute_reply.started": "2024-04-02T16:10:49.238711Z"
    },
    "id": "Gww_XdJ5KPJt"
   },
   "outputs": [],
   "source": [
    "class FlippedQuanv3x3(torch.nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, padding):\n",
    "    super(FlippedQuanv3x3, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.pad_l, self.pad_r, self.pad_t, self.pad_b = padding if padding is not None else (0,0,0,0)\n",
    "    self.pad = (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n",
    "    self.weight= torch.nn.Parameter(torch.randn((out_channels, in_channels, 4**2-1)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn((out_channels, 1)).type(COMPLEX_DTYPE))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batchsize ,c_in, h, w)\n",
    "    # weight has shape (c_out, c_in, 15)\n",
    "    # bias has shape (c_out, 1)\n",
    "    x = x.type(COMPLEX_DTYPE)\n",
    "    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n",
    "    h_out = (h_in-3+(self.pad_t+self.pad_b))//self.stride +1\n",
    "    w_out = (w_in-3+(self.pad_l+self.pad_r))//self.stride +1\n",
    "\n",
    "    #print(self.weight.shape)\n",
    "    #print(patches.shape)\n",
    "\n",
    "    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n",
    "\n",
    "\n",
    "    out = out + self.bias\n",
    "    return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_channels={self.in_channels}, out_channels={self.out_channels}, stride={self.stride}, padding={self.padding}'\n",
    "\n",
    "#test_module = FlippedQuanv3x3(in_channels=1, out_channels=2, stride=1, padding=(1,1,1,1)).to(device)\n",
    "#print(dummy_x.shape)\n",
    "#test_out = test_module(dummy_x.to(device))\n",
    "#print(test_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jY9sQZ7Ynla"
   },
   "source": [
    "# Linear Layer\n",
    "\n",
    "For input dimension $D$, the quantum linear layer is a $n = \\lceil \\log_4(D+1)⌉$-qubit quantum circuit. Both the data encoding and parameterised circuit is achieved via the $SU(2^n)$ unitary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:49.248151Z",
     "iopub.status.busy": "2024-04-02T16:10:49.247958Z",
     "iopub.status.idle": "2024-04-02T16:10:50.659602Z",
     "shell.execute_reply": "2024-04-02T16:10:50.658944Z",
     "shell.execute_reply.started": "2024-04-02T16:10:49.248134Z"
    },
    "id": "AXxNIObFYnPW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0847, 0.6649, 0.0675, 0.1829], device='cuda:0')\n",
      "tensor(1.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def data_encode_unitary(padded_data, t):\n",
    "  original_dim = padded_data.shape[-1]\n",
    "  new_dim = torch.sqrt(torch.tensor(original_dim)).type(torch.int)\n",
    "  data = torch.reshape(padded_data, (new_dim, new_dim))\n",
    "  generator = (data + torch.einsum(\"...jk->...kj\", data))/2\n",
    "  return torch.linalg.matrix_exp(1.0j*generator*t)\n",
    "\n",
    "def su_n(params, pauli_string_tensor_list):\n",
    "  # params has dim 4**n-1\n",
    "  paulis = torch.stack(pauli_string_tensor_list)\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, paulis)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "def linear_layer_func(padded_data, params, pauli_string_tensor_list, observables, n_qubits):\n",
    "  n_rep = params.shape[0]\n",
    "  state = bitstring_to_state(\"+\"*n_qubits)\n",
    "  for i in range(n_rep):\n",
    "    data_unitary = data_encode_unitary(padded_data, 1/n_rep)\n",
    "    #print(data_unitary.shape)\n",
    "    #print(state.shape)\n",
    "    state = torch.matmul(\n",
    "      data_unitary,\n",
    "      state\n",
    "    )\n",
    "    sun_gate = su_n(params[i], pauli_string_tensor_list)\n",
    "    #print(sun_gate.shape)\n",
    "    state = torch.matmul(\n",
    "      sun_gate,\n",
    "      state\n",
    "    )\n",
    "  return vmap_measure_sv(state, observables)\n",
    "\n",
    "test_params = torch.randn((3, 4**2-1),device=device).type(COMPLEX_DTYPE)\n",
    "test_data = torch.randn(4**2, device=device).type(COMPLEX_DTYPE)\n",
    "test_obs = torch.stack([torch.outer(bitstring_to_state('00'), bitstring_to_state('00')),\n",
    "                        torch.outer(bitstring_to_state('01'), bitstring_to_state('01')),\n",
    "                        torch.outer(bitstring_to_state('10'), bitstring_to_state('10')),\n",
    "                         torch.outer(bitstring_to_state('11'), bitstring_to_state('11'))])\n",
    "test_pauli_string_tensor_list = generate_pauli_tensor_list(generate_nqubit_pauli_strings(2))\n",
    "\n",
    "\n",
    "test_out = linear_layer_func(test_data, test_params, test_pauli_string_tensor_list, test_obs, 2)\n",
    "print(test_out)\n",
    "print(torch.sum(test_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:50.660727Z",
     "iopub.status.busy": "2024-04-02T16:10:50.660497Z",
     "iopub.status.idle": "2024-04-02T16:10:50.673338Z",
     "shell.execute_reply": "2024-04-02T16:10:50.672720Z",
     "shell.execute_reply.started": "2024-04-02T16:10:50.660708Z"
    },
    "id": "2F4_SBgIYnMv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3414, 0.4328, 0.0295, 0.1964],\n",
      "        [0.4175, 0.1313, 0.2182, 0.2330],\n",
      "        [0.1889, 0.5187, 0.1613, 0.1311]], device='cuda:0')\n",
      "tensor(3.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "vmap_batch_linear_layer_func = torch.vmap(linear_layer_func, in_dims=(0, None, None, None, None), out_dims=0)\n",
    "\n",
    "test_data = torch.randn((3, 4**2), device=device).type(COMPLEX_DTYPE)\n",
    "test_out = vmap_batch_linear_layer_func(test_data, test_params, test_pauli_string_tensor_list, test_obs, 2)\n",
    "print(test_out)\n",
    "print(torch.sum(test_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryROGtnC_3po"
   },
   "source": [
    "## PyTorch Module for the Quantum Version of Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:50.674334Z",
     "iopub.status.busy": "2024-04-02T16:10:50.674133Z",
     "iopub.status.idle": "2024-04-02T16:10:50.704785Z",
     "shell.execute_reply": "2024-04-02T16:10:50.704215Z",
     "shell.execute_reply.started": "2024-04-02T16:10:50.674316Z"
    },
    "id": "RlTC952w_8VW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 45])\n",
      "torch.Size([16, 6])\n",
      "DataReUploadingLinear(in_dim=45, out_dim=6, n_qubits=3, n_reps=10)\n"
     ]
    }
   ],
   "source": [
    "class DataReUploadingLinear(torch.nn.Module):\n",
    "  def __init__(self, in_dim, out_dim, n_qubits, n_reps):\n",
    "    super(DataReUploadingLinear, self).__init__()\n",
    "    assert 2**n_qubits >= out_dim\n",
    "    assert 4**n_qubits >= in_dim\n",
    "    self.in_dim = in_dim\n",
    "    self.out_dim = out_dim\n",
    "    self.n_qubits = n_qubits\n",
    "    self.n_reps = n_reps\n",
    "    self.pauli_string_tensor_list = generate_pauli_tensor_list(generate_nqubit_pauli_strings(n_qubits))\n",
    "    self.param_dim = 4**n_qubits-1\n",
    "    self.params = torch.nn.Parameter(torch.randn((self.n_reps,self.param_dim)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn(out_dim).type(REAL_DTYPE))\n",
    "    self.observables = self.generate_observables()\n",
    "    self.pad_size = 4**n_qubits-self.in_dim\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has size (batchsize, in_dim)\n",
    "    # pad x\n",
    "    x = torch.nn.functional.pad(x, (0, self.pad_size)).type(COMPLEX_DTYPE)\n",
    "    out = vmap_batch_linear_layer_func(x, self.params, self.pauli_string_tensor_list, self.observables, self.n_qubits)\n",
    "    out = out.type(REAL_DTYPE) + self.bias\n",
    "    return out\n",
    "\n",
    "  def generate_observables(self):\n",
    "    observables = []\n",
    "    for i in range(self.out_dim):\n",
    "      temp_bitstring = '{0:b}'.format(i).zfill(self.n_qubits)\n",
    "      ob = torch.outer(bitstring_to_state(temp_bitstring), bitstring_to_state(temp_bitstring))\n",
    "      observables.append(ob)\n",
    "    return torch.stack(observables)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_dim={self.in_dim}, out_dim={self.out_dim}, n_qubits={self.n_qubits}, n_reps={self.n_reps}'\n",
    "\n",
    "test_linear_module = DataReUploadingLinear(in_dim=45, out_dim=6, n_qubits=3, n_reps = 10).to(device)\n",
    "test_data = torch.randn((16, 45), device=device).type(COMPLEX_DTYPE)\n",
    "print(test_data.shape)\n",
    "test_out = test_linear_module(test_data.to(device))\n",
    "print(test_out.shape)\n",
    "print(test_linear_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yumZTjDVu63"
   },
   "source": [
    "# Replacing Classical Layers with Quantum Layers\n",
    "\n",
    "Let's replace `Conv2d` with `FlippedQuanv3x3`, and `Linear` with `QuantLinear`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:50.705862Z",
     "iopub.status.busy": "2024-04-02T16:10:50.705674Z",
     "iopub.status.idle": "2024-04-02T16:10:52.137407Z",
     "shell.execute_reply": "2024-04-02T16:10:52.136754Z",
     "shell.execute_reply.started": "2024-04-02T16:10:50.705842Z"
    },
    "id": "W4RC5ou-VzbE",
    "outputId": "9eb0b35d-ed18-431b-8a53-ba5216d3fd06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 32, 32])\n",
      "HybridNet(\n",
      "  (layers): Sequential(\n",
      "    (0): FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None)\n",
      "    (1): FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): DataReUploadingLinear(in_dim=12544, out_dim=10, n_qubits=7, n_reps=10)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_469/4168075707.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n",
      "/tmp/ipykernel_469/1684779817.py:30: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:301.)\n",
      "  return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "class HybridNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HybridNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None), # 32->30\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None), # 30->28\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        DataReUploadingLinear(16*28*28, 10, 7, 10)\n",
    "        #torch.nn.Linear(32*14*14, 10).type(REAL_DTYPE)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "\n",
    "net = HybridNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:52.140051Z",
     "iopub.status.busy": "2024-04-02T16:10:52.139814Z",
     "iopub.status.idle": "2024-04-03T00:55:24.432378Z",
     "shell.execute_reply": "2024-04-03T00:55:24.431260Z",
     "shell.execute_reply.started": "2024-04-02T16:10:52.140031Z"
    },
    "id": "x4ZY59q1WS4x",
    "outputId": "6532aea1-47a3-49f3-fe36-b774aee1905f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 300, Number of test batches = 50\n",
      "Print every train batch = 30, Print every test batch = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_469/4168075707.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at step=0, batch=0, train loss = 2.6262035369873047, train acc = 0.10000000149011612, time = 1.0056102275848389\n",
      "Training at step=0, batch=30, train loss = 2.647953510284424, train acc = 0.10999999940395355, time = 0.9500279426574707\n",
      "Training at step=0, batch=60, train loss = 2.695133686065674, train acc = 0.07999999821186066, time = 0.9525797367095947\n",
      "Training at step=0, batch=90, train loss = 2.60817813873291, train acc = 0.10499999672174454, time = 0.9510729312896729\n",
      "Training at step=0, batch=120, train loss = 2.7170846462249756, train acc = 0.07999999821186066, time = 0.9501872062683105\n",
      "Training at step=0, batch=150, train loss = 2.7266244888305664, train acc = 0.08500000089406967, time = 0.9551002979278564\n",
      "Training at step=0, batch=180, train loss = 2.524245023727417, train acc = 0.13500000536441803, time = 0.9530143737792969\n",
      "Training at step=0, batch=210, train loss = 2.539255380630493, train acc = 0.10000000149011612, time = 0.9499902725219727\n",
      "Training at step=0, batch=240, train loss = 2.48237681388855, train acc = 0.07999999821186066, time = 0.9477486610412598\n",
      "Training at step=0, batch=270, train loss = 2.2584633827209473, train acc = 0.1599999964237213, time = 0.9510147571563721\n",
      "Testing at step=0, batch=0, test loss = 2.1013617515563965, test acc = 0.33000001311302185, time = 0.35616111755371094\n",
      "Testing at step=0, batch=5, test loss = 2.1060094833374023, test acc = 0.30000001192092896, time = 0.35504770278930664\n",
      "Testing at step=0, batch=10, test loss = 2.0407419204711914, test acc = 0.3400000035762787, time = 0.3556232452392578\n",
      "Testing at step=0, batch=15, test loss = 2.059937000274658, test acc = 0.3400000035762787, time = 0.3533358573913574\n",
      "Testing at step=0, batch=20, test loss = 2.0375983715057373, test acc = 0.36000001430511475, time = 0.35323023796081543\n",
      "Testing at step=0, batch=25, test loss = 2.0582046508789062, test acc = 0.3400000035762787, time = 0.3538053035736084\n",
      "Testing at step=0, batch=30, test loss = 2.0109472274780273, test acc = 0.3700000047683716, time = 0.3551909923553467\n",
      "Testing at step=0, batch=35, test loss = 2.067034959793091, test acc = 0.3400000035762787, time = 0.352832555770874\n",
      "Testing at step=0, batch=40, test loss = 2.0901384353637695, test acc = 0.3100000023841858, time = 0.3538365364074707\n",
      "Testing at step=0, batch=45, test loss = 2.0128960609436035, test acc = 0.3400000035762787, time = 0.35500049591064453\n",
      "Step 0 finished in 314.9282269477844, Train loss = 2.5492526284853616, Test loss = 2.06086642742157; Train Acc = 0.11693333347638447, Test Acc = 0.34190000116825103\n",
      "Training at step=1, batch=0, train loss = 2.074751377105713, train acc = 0.32499998807907104, time = 0.9523661136627197\n",
      "Training at step=1, batch=30, train loss = 1.7602241039276123, train acc = 0.5649999976158142, time = 0.95074462890625\n",
      "Training at step=1, batch=60, train loss = 0.8862211108207703, train acc = 0.7149999737739563, time = 0.9521710872650146\n",
      "Training at step=1, batch=90, train loss = 0.6942176818847656, train acc = 0.800000011920929, time = 0.9495847225189209\n",
      "Training at step=1, batch=120, train loss = 0.5613515377044678, train acc = 0.8050000071525574, time = 0.9480085372924805\n",
      "Training at step=1, batch=150, train loss = 0.5006600022315979, train acc = 0.8399999737739563, time = 0.9507975578308105\n",
      "Training at step=1, batch=180, train loss = 0.6407527923583984, train acc = 0.7850000262260437, time = 0.9492645263671875\n",
      "Training at step=1, batch=210, train loss = 0.5253095626831055, train acc = 0.8500000238418579, time = 0.9477803707122803\n",
      "Training at step=1, batch=240, train loss = 0.4481050968170166, train acc = 0.8550000190734863, time = 0.9476807117462158\n",
      "Training at step=1, batch=270, train loss = 0.44765013456344604, train acc = 0.8500000238418579, time = 0.9491794109344482\n",
      "Testing at step=1, batch=0, test loss = 0.39059412479400635, test acc = 0.8700000047683716, time = 0.3577697277069092\n",
      "Testing at step=1, batch=5, test loss = 0.31877046823501587, test acc = 0.9200000166893005, time = 0.35025501251220703\n",
      "Testing at step=1, batch=10, test loss = 0.2718200981616974, test acc = 0.925000011920929, time = 0.34995031356811523\n",
      "Testing at step=1, batch=15, test loss = 0.29844072461128235, test acc = 0.9300000071525574, time = 0.3503153324127197\n",
      "Testing at step=1, batch=20, test loss = 0.4207988977432251, test acc = 0.875, time = 0.3501243591308594\n",
      "Testing at step=1, batch=25, test loss = 0.3506962060928345, test acc = 0.8849999904632568, time = 0.35271787643432617\n",
      "Testing at step=1, batch=30, test loss = 0.22314617037773132, test acc = 0.9350000023841858, time = 0.3539435863494873\n",
      "Testing at step=1, batch=35, test loss = 0.3061339259147644, test acc = 0.9150000214576721, time = 0.35282373428344727\n",
      "Testing at step=1, batch=40, test loss = 0.5184500813484192, test acc = 0.8399999737739563, time = 0.3517720699310303\n",
      "Testing at step=1, batch=45, test loss = 0.37950053811073303, test acc = 0.875, time = 0.35144615173339844\n",
      "Step 1 finished in 314.7161054611206, Train loss = 0.7414793339371681, Test loss = 0.3618190085887909; Train Acc = 0.7736999989549319, Test Acc = 0.8912999987602234\n",
      "Training at step=2, batch=0, train loss = 0.42325305938720703, train acc = 0.8849999904632568, time = 0.9489645957946777\n",
      "Training at step=2, batch=30, train loss = 0.3867584466934204, train acc = 0.8849999904632568, time = 0.947472333908081\n",
      "Training at step=2, batch=60, train loss = 0.3377794921398163, train acc = 0.8949999809265137, time = 0.9524729251861572\n",
      "Training at step=2, batch=90, train loss = 0.37891632318496704, train acc = 0.8500000238418579, time = 0.9512100219726562\n",
      "Training at step=2, batch=120, train loss = 0.3784426748752594, train acc = 0.8650000095367432, time = 0.9511168003082275\n",
      "Training at step=2, batch=150, train loss = 0.3845149576663971, train acc = 0.875, time = 0.9497458934783936\n",
      "Training at step=2, batch=180, train loss = 0.2608403265476227, train acc = 0.9200000166893005, time = 0.9494781494140625\n",
      "Training at step=2, batch=210, train loss = 0.4643225073814392, train acc = 0.8600000143051147, time = 0.9580051898956299\n",
      "Training at step=2, batch=240, train loss = 0.2262866199016571, train acc = 0.9300000071525574, time = 0.9527885913848877\n",
      "Training at step=2, batch=270, train loss = 0.40468552708625793, train acc = 0.8650000095367432, time = 0.9532296657562256\n",
      "Testing at step=2, batch=0, test loss = 0.34447190165519714, test acc = 0.925000011920929, time = 0.35294175148010254\n",
      "Testing at step=2, batch=5, test loss = 0.20782466232776642, test acc = 0.9649999737739563, time = 0.3526132106781006\n",
      "Testing at step=2, batch=10, test loss = 0.34945279359817505, test acc = 0.9100000262260437, time = 0.3537306785583496\n",
      "Testing at step=2, batch=15, test loss = 0.2470441460609436, test acc = 0.925000011920929, time = 0.35286784172058105\n",
      "Testing at step=2, batch=20, test loss = 0.30444303154945374, test acc = 0.8999999761581421, time = 0.3529050350189209\n",
      "Testing at step=2, batch=25, test loss = 0.22841964662075043, test acc = 0.9350000023841858, time = 0.35112476348876953\n",
      "Testing at step=2, batch=30, test loss = 0.2762007415294647, test acc = 0.9200000166893005, time = 0.35286545753479004\n",
      "Testing at step=2, batch=35, test loss = 0.3158486485481262, test acc = 0.9150000214576721, time = 0.3541381359100342\n",
      "Testing at step=2, batch=40, test loss = 0.37857550382614136, test acc = 0.875, time = 0.35459113121032715\n",
      "Testing at step=2, batch=45, test loss = 0.2811392545700073, test acc = 0.9300000071525574, time = 0.3529078960418701\n",
      "Step 2 finished in 314.7400019168854, Train loss = 0.34626099015275635, Test loss = 0.2850732463598251; Train Acc = 0.894066665371259, Test Acc = 0.9137999975681305\n",
      "Training at step=3, batch=0, train loss = 0.3089476525783539, train acc = 0.9150000214576721, time = 0.950610876083374\n",
      "Training at step=3, batch=30, train loss = 0.2943645715713501, train acc = 0.9200000166893005, time = 0.948357343673706\n",
      "Training at step=3, batch=60, train loss = 0.4432311952114105, train acc = 0.8849999904632568, time = 0.9495201110839844\n",
      "Training at step=3, batch=90, train loss = 0.34929144382476807, train acc = 0.8949999809265137, time = 0.9490597248077393\n",
      "Training at step=3, batch=120, train loss = 0.23549692332744598, train acc = 0.9399999976158142, time = 0.9501791000366211\n",
      "Training at step=3, batch=150, train loss = 0.2373361587524414, train acc = 0.9350000023841858, time = 0.9524657726287842\n",
      "Training at step=3, batch=180, train loss = 0.2646532356739044, train acc = 0.8949999809265137, time = 0.9553489685058594\n",
      "Training at step=3, batch=210, train loss = 0.3175707757472992, train acc = 0.8899999856948853, time = 0.9474709033966064\n",
      "Training at step=3, batch=240, train loss = 0.18486928939819336, train acc = 0.949999988079071, time = 0.9501633644104004\n",
      "Training at step=3, batch=270, train loss = 0.17396599054336548, train acc = 0.9599999785423279, time = 0.9499938488006592\n",
      "Testing at step=3, batch=0, test loss = 0.3330296277999878, test acc = 0.9399999976158142, time = 0.35235071182250977\n",
      "Testing at step=3, batch=5, test loss = 0.22602713108062744, test acc = 0.9200000166893005, time = 0.34981727600097656\n",
      "Testing at step=3, batch=10, test loss = 0.179409459233284, test acc = 0.925000011920929, time = 0.35156893730163574\n",
      "Testing at step=3, batch=15, test loss = 0.2615317404270172, test acc = 0.9399999976158142, time = 0.35134243965148926\n",
      "Testing at step=3, batch=20, test loss = 0.15801650285720825, test acc = 0.9350000023841858, time = 0.3511691093444824\n",
      "Testing at step=3, batch=25, test loss = 0.23846542835235596, test acc = 0.9399999976158142, time = 0.3494985103607178\n",
      "Testing at step=3, batch=30, test loss = 0.26138344407081604, test acc = 0.9300000071525574, time = 0.3541679382324219\n",
      "Testing at step=3, batch=35, test loss = 0.19188612699508667, test acc = 0.9599999785423279, time = 0.35196590423583984\n",
      "Testing at step=3, batch=40, test loss = 0.1671605259180069, test acc = 0.9350000023841858, time = 0.34976911544799805\n",
      "Testing at step=3, batch=45, test loss = 0.2933027446269989, test acc = 0.9200000166893005, time = 0.3495368957519531\n",
      "Step 3 finished in 314.6059265136719, Train loss = 0.2857857917745908, Test loss = 0.23534125298261643; Train Acc = 0.9146833348274231, Test Acc = 0.9309999990463257\n",
      "Training at step=4, batch=0, train loss = 0.2516617178916931, train acc = 0.9449999928474426, time = 0.9463756084442139\n",
      "Training at step=4, batch=30, train loss = 0.2598750591278076, train acc = 0.9200000166893005, time = 0.9487025737762451\n",
      "Training at step=4, batch=60, train loss = 0.24656742811203003, train acc = 0.925000011920929, time = 0.9504053592681885\n",
      "Training at step=4, batch=90, train loss = 0.19185741245746613, train acc = 0.9649999737739563, time = 0.9480290412902832\n",
      "Training at step=4, batch=120, train loss = 0.3275562524795532, train acc = 0.9200000166893005, time = 0.9481046199798584\n",
      "Training at step=4, batch=150, train loss = 0.2560991644859314, train acc = 0.9150000214576721, time = 0.9466276168823242\n",
      "Training at step=4, batch=180, train loss = 0.22587017714977264, train acc = 0.9399999976158142, time = 0.9503381252288818\n",
      "Training at step=4, batch=210, train loss = 0.2453368753194809, train acc = 0.9100000262260437, time = 0.9504179954528809\n",
      "Training at step=4, batch=240, train loss = 0.28852972388267517, train acc = 0.9200000166893005, time = 0.9481887817382812\n",
      "Training at step=4, batch=270, train loss = 0.1568741500377655, train acc = 0.9399999976158142, time = 0.9498617649078369\n",
      "Testing at step=4, batch=0, test loss = 0.19993790984153748, test acc = 0.9350000023841858, time = 0.35339999198913574\n",
      "Testing at step=4, batch=5, test loss = 0.2190309315919876, test acc = 0.9399999976158142, time = 0.35285067558288574\n",
      "Testing at step=4, batch=10, test loss = 0.13685786724090576, test acc = 0.9549999833106995, time = 0.3536190986633301\n",
      "Testing at step=4, batch=15, test loss = 0.15322710573673248, test acc = 0.949999988079071, time = 0.35381364822387695\n",
      "Testing at step=4, batch=20, test loss = 0.18034975230693817, test acc = 0.949999988079071, time = 0.3532998561859131\n",
      "Testing at step=4, batch=25, test loss = 0.22104261815547943, test acc = 0.9549999833106995, time = 0.3529362678527832\n",
      "Testing at step=4, batch=30, test loss = 0.18719236552715302, test acc = 0.925000011920929, time = 0.35249781608581543\n",
      "Testing at step=4, batch=35, test loss = 0.2752297520637512, test acc = 0.925000011920929, time = 0.35172557830810547\n",
      "Testing at step=4, batch=40, test loss = 0.15717363357543945, test acc = 0.9350000023841858, time = 0.35388946533203125\n",
      "Testing at step=4, batch=45, test loss = 0.2518872916698456, test acc = 0.8999999761581421, time = 0.3540916442871094\n",
      "Step 4 finished in 314.40026330947876, Train loss = 0.24641475550830363, Test loss = 0.20655474707484245; Train Acc = 0.9264666680494944, Test Acc = 0.9372999954223633\n",
      "Training at step=5, batch=0, train loss = 0.1579340100288391, train acc = 0.9549999833106995, time = 0.9515628814697266\n",
      "Training at step=5, batch=30, train loss = 0.30154722929000854, train acc = 0.9399999976158142, time = 0.9498417377471924\n",
      "Training at step=5, batch=60, train loss = 0.23209594190120697, train acc = 0.9300000071525574, time = 0.952415943145752\n",
      "Training at step=5, batch=90, train loss = 0.23387236893177032, train acc = 0.925000011920929, time = 0.9484362602233887\n",
      "Training at step=5, batch=120, train loss = 0.20440804958343506, train acc = 0.9300000071525574, time = 0.9511170387268066\n",
      "Training at step=5, batch=150, train loss = 0.16386456787586212, train acc = 0.9200000166893005, time = 0.9489743709564209\n",
      "Training at step=5, batch=180, train loss = 0.18201813101768494, train acc = 0.9399999976158142, time = 0.9520716667175293\n",
      "Training at step=5, batch=210, train loss = 0.22982697188854218, train acc = 0.9200000166893005, time = 0.9484524726867676\n",
      "Training at step=5, batch=240, train loss = 0.1676415055990219, train acc = 0.9449999928474426, time = 0.9474599361419678\n",
      "Training at step=5, batch=270, train loss = 0.21748442947864532, train acc = 0.9449999928474426, time = 0.9480679035186768\n",
      "Testing at step=5, batch=0, test loss = 0.16636791825294495, test acc = 0.9549999833106995, time = 0.3518080711364746\n",
      "Testing at step=5, batch=5, test loss = 0.21387772262096405, test acc = 0.9350000023841858, time = 0.35137462615966797\n",
      "Testing at step=5, batch=10, test loss = 0.16777735948562622, test acc = 0.9449999928474426, time = 0.3516674041748047\n",
      "Testing at step=5, batch=15, test loss = 0.1925976574420929, test acc = 0.9350000023841858, time = 0.3529164791107178\n",
      "Testing at step=5, batch=20, test loss = 0.159044548869133, test acc = 0.9599999785423279, time = 0.3531925678253174\n",
      "Testing at step=5, batch=25, test loss = 0.10149938613176346, test acc = 0.9599999785423279, time = 0.35361647605895996\n",
      "Testing at step=5, batch=30, test loss = 0.25143006443977356, test acc = 0.9150000214576721, time = 0.35610270500183105\n",
      "Testing at step=5, batch=35, test loss = 0.14495186507701874, test acc = 0.9549999833106995, time = 0.35274767875671387\n",
      "Testing at step=5, batch=40, test loss = 0.13935193419456482, test acc = 0.9700000286102295, time = 0.35409045219421387\n",
      "Testing at step=5, batch=45, test loss = 0.2105211317539215, test acc = 0.9449999928474426, time = 0.35411572456359863\n",
      "Step 5 finished in 314.33192253112793, Train loss = 0.22186608696977297, Test loss = 0.1884727005660534; Train Acc = 0.9342000003655752, Test Acc = 0.9444999945163727\n",
      "Training at step=6, batch=0, train loss = 0.28917717933654785, train acc = 0.9300000071525574, time = 0.9497504234313965\n",
      "Training at step=6, batch=30, train loss = 0.27084270119667053, train acc = 0.9100000262260437, time = 0.9502685070037842\n",
      "Training at step=6, batch=60, train loss = 0.2878173887729645, train acc = 0.8999999761581421, time = 0.9503016471862793\n",
      "Training at step=6, batch=90, train loss = 0.2824704647064209, train acc = 0.9399999976158142, time = 0.9491641521453857\n",
      "Training at step=6, batch=120, train loss = 0.22651754319667816, train acc = 0.9200000166893005, time = 0.9492173194885254\n",
      "Training at step=6, batch=150, train loss = 0.14912518858909607, train acc = 0.9549999833106995, time = 0.9519162178039551\n",
      "Training at step=6, batch=180, train loss = 0.24106402695178986, train acc = 0.9399999976158142, time = 0.9484429359436035\n",
      "Training at step=6, batch=210, train loss = 0.25527307391166687, train acc = 0.9100000262260437, time = 0.9487895965576172\n",
      "Training at step=6, batch=240, train loss = 0.20716440677642822, train acc = 0.9399999976158142, time = 0.950080156326294\n",
      "Training at step=6, batch=270, train loss = 0.13772372901439667, train acc = 0.9599999785423279, time = 0.9500517845153809\n",
      "Testing at step=6, batch=0, test loss = 0.2861660420894623, test acc = 0.9150000214576721, time = 0.3596334457397461\n",
      "Testing at step=6, batch=5, test loss = 0.13574029505252838, test acc = 0.9549999833106995, time = 0.3503074645996094\n",
      "Testing at step=6, batch=10, test loss = 0.206853985786438, test acc = 0.9350000023841858, time = 0.3512113094329834\n",
      "Testing at step=6, batch=15, test loss = 0.1120365783572197, test acc = 0.9700000286102295, time = 0.3502480983734131\n",
      "Testing at step=6, batch=20, test loss = 0.1250137835741043, test acc = 0.949999988079071, time = 0.3516721725463867\n",
      "Testing at step=6, batch=25, test loss = 0.21362191438674927, test acc = 0.9350000023841858, time = 0.3533663749694824\n",
      "Testing at step=6, batch=30, test loss = 0.22283300757408142, test acc = 0.9399999976158142, time = 0.3494279384613037\n",
      "Testing at step=6, batch=35, test loss = 0.1388615220785141, test acc = 0.949999988079071, time = 0.3522357940673828\n",
      "Testing at step=6, batch=40, test loss = 0.15426544845104218, test acc = 0.9549999833106995, time = 0.3522679805755615\n",
      "Testing at step=6, batch=45, test loss = 0.13583055138587952, test acc = 0.9599999785423279, time = 0.367781400680542\n",
      "Step 6 finished in 314.5176594257355, Train loss = 0.20008125628034273, Test loss = 0.195435122102499; Train Acc = 0.9386999978621801, Test Acc = 0.9404999983310699\n",
      "Training at step=7, batch=0, train loss = 0.09603603184223175, train acc = 0.9800000190734863, time = 0.9484586715698242\n",
      "Training at step=7, batch=30, train loss = 0.23355531692504883, train acc = 0.949999988079071, time = 0.9679396152496338\n",
      "Training at step=7, batch=60, train loss = 0.16578169167041779, train acc = 0.9449999928474426, time = 0.9500176906585693\n",
      "Training at step=7, batch=90, train loss = 0.23800896108150482, train acc = 0.9150000214576721, time = 0.948765754699707\n",
      "Training at step=7, batch=120, train loss = 0.27669379115104675, train acc = 0.9350000023841858, time = 0.9480068683624268\n",
      "Training at step=7, batch=150, train loss = 0.16352783143520355, train acc = 0.9399999976158142, time = 0.949739933013916\n",
      "Training at step=7, batch=180, train loss = 0.13299454748630524, train acc = 0.9649999737739563, time = 0.9495391845703125\n",
      "Training at step=7, batch=210, train loss = 0.1697351336479187, train acc = 0.9399999976158142, time = 0.9466609954833984\n",
      "Training at step=7, batch=240, train loss = 0.15926621854305267, train acc = 0.9599999785423279, time = 0.949378490447998\n",
      "Training at step=7, batch=270, train loss = 0.22828106582164764, train acc = 0.9350000023841858, time = 0.9489970207214355\n",
      "Testing at step=7, batch=0, test loss = 0.1958228498697281, test acc = 0.9300000071525574, time = 0.36040306091308594\n",
      "Testing at step=7, batch=5, test loss = 0.10281293094158173, test acc = 0.9649999737739563, time = 0.3511946201324463\n",
      "Testing at step=7, batch=10, test loss = 0.28564217686653137, test acc = 0.9300000071525574, time = 0.3519623279571533\n",
      "Testing at step=7, batch=15, test loss = 0.1775284707546234, test acc = 0.9449999928474426, time = 0.3526315689086914\n",
      "Testing at step=7, batch=20, test loss = 0.2016054391860962, test acc = 0.9549999833106995, time = 0.34964585304260254\n",
      "Testing at step=7, batch=25, test loss = 0.1228569969534874, test acc = 0.9599999785423279, time = 0.3499472141265869\n",
      "Testing at step=7, batch=30, test loss = 0.2661038339138031, test acc = 0.925000011920929, time = 0.3504056930541992\n",
      "Testing at step=7, batch=35, test loss = 0.15446248650550842, test acc = 0.9599999785423279, time = 0.35157227516174316\n",
      "Testing at step=7, batch=40, test loss = 0.14917992055416107, test acc = 0.9649999737739563, time = 0.3500325679779053\n",
      "Testing at step=7, batch=45, test loss = 0.11320409178733826, test acc = 0.9599999785423279, time = 0.35093116760253906\n",
      "Step 7 finished in 314.26803851127625, Train loss = 0.1804075198372205, Test loss = 0.17035206273198128; Train Acc = 0.9453166627883911, Test Acc = 0.9477999925613403\n",
      "Training at step=8, batch=0, train loss = 0.17035724222660065, train acc = 0.9449999928474426, time = 0.9486596584320068\n",
      "Training at step=8, batch=30, train loss = 0.21002238988876343, train acc = 0.949999988079071, time = 0.9516391754150391\n",
      "Training at step=8, batch=60, train loss = 0.1991485059261322, train acc = 0.9549999833106995, time = 0.9499402046203613\n",
      "Training at step=8, batch=90, train loss = 0.23641495406627655, train acc = 0.9399999976158142, time = 0.9487190246582031\n",
      "Training at step=8, batch=120, train loss = 0.20035409927368164, train acc = 0.9300000071525574, time = 0.9473106861114502\n",
      "Training at step=8, batch=150, train loss = 0.18494397401809692, train acc = 0.9549999833106995, time = 0.9479732513427734\n",
      "Training at step=8, batch=180, train loss = 0.1432051956653595, train acc = 0.9599999785423279, time = 0.9521596431732178\n",
      "Training at step=8, batch=210, train loss = 0.292572021484375, train acc = 0.9150000214576721, time = 0.9486958980560303\n",
      "Training at step=8, batch=240, train loss = 0.23029060661792755, train acc = 0.9449999928474426, time = 0.9496791362762451\n",
      "Training at step=8, batch=270, train loss = 0.13226187229156494, train acc = 0.9449999928474426, time = 0.9516386985778809\n",
      "Testing at step=8, batch=0, test loss = 0.16772569715976715, test acc = 0.9399999976158142, time = 0.35313868522644043\n",
      "Testing at step=8, batch=5, test loss = 0.16715465486049652, test acc = 0.9300000071525574, time = 0.3544445037841797\n",
      "Testing at step=8, batch=10, test loss = 0.08629205822944641, test acc = 0.9700000286102295, time = 0.3557112216949463\n",
      "Testing at step=8, batch=15, test loss = 0.24229218065738678, test acc = 0.9150000214576721, time = 0.35470151901245117\n",
      "Testing at step=8, batch=20, test loss = 0.17396485805511475, test acc = 0.949999988079071, time = 0.3529531955718994\n",
      "Testing at step=8, batch=25, test loss = 0.147267147898674, test acc = 0.9599999785423279, time = 0.3524329662322998\n",
      "Testing at step=8, batch=30, test loss = 0.17380553483963013, test acc = 0.949999988079071, time = 0.35204052925109863\n",
      "Testing at step=8, batch=35, test loss = 0.21107667684555054, test acc = 0.925000011920929, time = 0.3538546562194824\n",
      "Testing at step=8, batch=40, test loss = 0.0944935530424118, test acc = 0.9700000286102295, time = 0.3541691303253174\n",
      "Testing at step=8, batch=45, test loss = 0.10185948014259338, test acc = 0.9700000286102295, time = 0.3519759178161621\n",
      "Step 8 finished in 314.6064009666443, Train loss = 0.17220996563633284, Test loss = 0.1620102322101593; Train Acc = 0.9482833282152812, Test Acc = 0.9508000004291535\n",
      "Training at step=9, batch=0, train loss = 0.17799554765224457, train acc = 0.9300000071525574, time = 0.9496500492095947\n",
      "Training at step=9, batch=30, train loss = 0.17260178923606873, train acc = 0.9599999785423279, time = 0.9528837203979492\n",
      "Training at step=9, batch=60, train loss = 0.17569543421268463, train acc = 0.9449999928474426, time = 0.9525613784790039\n",
      "Training at step=9, batch=90, train loss = 0.22541701793670654, train acc = 0.9350000023841858, time = 0.9528355598449707\n",
      "Training at step=9, batch=120, train loss = 0.08373315632343292, train acc = 0.9750000238418579, time = 0.9495120048522949\n",
      "Training at step=9, batch=150, train loss = 0.08487457036972046, train acc = 0.9850000143051147, time = 0.9486675262451172\n",
      "Training at step=9, batch=180, train loss = 0.13366514444351196, train acc = 0.9599999785423279, time = 0.9488580226898193\n",
      "Training at step=9, batch=210, train loss = 0.20947274565696716, train acc = 0.949999988079071, time = 0.9495730400085449\n",
      "Training at step=9, batch=240, train loss = 0.1324516236782074, train acc = 0.9549999833106995, time = 0.9503750801086426\n",
      "Training at step=9, batch=270, train loss = 0.14071248471736908, train acc = 0.9599999785423279, time = 0.9474034309387207\n",
      "Testing at step=9, batch=0, test loss = 0.18010137975215912, test acc = 0.9449999928474426, time = 0.3518548011779785\n",
      "Testing at step=9, batch=5, test loss = 0.10188040882349014, test acc = 0.9599999785423279, time = 0.35497546195983887\n",
      "Testing at step=9, batch=10, test loss = 0.16013474762439728, test acc = 0.9399999976158142, time = 0.35300517082214355\n",
      "Testing at step=9, batch=15, test loss = 0.15708118677139282, test acc = 0.9700000286102295, time = 0.3520526885986328\n",
      "Testing at step=9, batch=20, test loss = 0.21773283183574677, test acc = 0.9399999976158142, time = 0.35291171073913574\n",
      "Testing at step=9, batch=25, test loss = 0.1850002408027649, test acc = 0.9449999928474426, time = 0.3515608310699463\n",
      "Testing at step=9, batch=30, test loss = 0.23162022233009338, test acc = 0.925000011920929, time = 0.35391950607299805\n",
      "Testing at step=9, batch=35, test loss = 0.1597834676504135, test acc = 0.9649999737739563, time = 0.352736234664917\n",
      "Testing at step=9, batch=40, test loss = 0.2103947401046753, test acc = 0.925000011920929, time = 0.3502845764160156\n",
      "Testing at step=9, batch=45, test loss = 0.12534309923648834, test acc = 0.9649999737739563, time = 0.34995269775390625\n",
      "Step 9 finished in 314.74717950820923, Train loss = 0.15819930605590343, Test loss = 0.15548401638865472; Train Acc = 0.9528166613976161, Test Acc = 0.9530999994277954\n",
      "Training at step=10, batch=0, train loss = 0.16184672713279724, train acc = 0.9399999976158142, time = 0.9525582790374756\n",
      "Training at step=10, batch=30, train loss = 0.14919660985469818, train acc = 0.9599999785423279, time = 0.948204755783081\n",
      "Training at step=10, batch=60, train loss = 0.1556711494922638, train acc = 0.9599999785423279, time = 0.9490725994110107\n",
      "Training at step=10, batch=90, train loss = 0.1460307538509369, train acc = 0.9449999928474426, time = 0.9499843120574951\n",
      "Training at step=10, batch=120, train loss = 0.14551810920238495, train acc = 0.9599999785423279, time = 0.9506418704986572\n",
      "Training at step=10, batch=150, train loss = 0.12465353310108185, train acc = 0.9549999833106995, time = 0.9497947692871094\n",
      "Training at step=10, batch=180, train loss = 0.09346681833267212, train acc = 0.9700000286102295, time = 0.9509871006011963\n",
      "Training at step=10, batch=210, train loss = 0.09359573572874069, train acc = 0.9599999785423279, time = 0.9495093822479248\n",
      "Training at step=10, batch=240, train loss = 0.09526943415403366, train acc = 0.9549999833106995, time = 0.9473667144775391\n",
      "Training at step=10, batch=270, train loss = 0.1269395649433136, train acc = 0.9449999928474426, time = 0.9476027488708496\n",
      "Testing at step=10, batch=0, test loss = 0.14738325774669647, test acc = 0.9599999785423279, time = 0.35086989402770996\n",
      "Testing at step=10, batch=5, test loss = 0.10935533791780472, test acc = 0.9549999833106995, time = 0.3520655632019043\n",
      "Testing at step=10, batch=10, test loss = 0.09439995884895325, test acc = 0.9649999737739563, time = 0.349916934967041\n",
      "Testing at step=10, batch=15, test loss = 0.17781640589237213, test acc = 0.949999988079071, time = 0.351515531539917\n",
      "Testing at step=10, batch=20, test loss = 0.07879070937633514, test acc = 0.9750000238418579, time = 0.3502781391143799\n",
      "Testing at step=10, batch=25, test loss = 0.1469288468360901, test acc = 0.949999988079071, time = 0.3515620231628418\n",
      "Testing at step=10, batch=30, test loss = 0.08789388835430145, test acc = 0.9850000143051147, time = 0.353804349899292\n",
      "Testing at step=10, batch=35, test loss = 0.1409708708524704, test acc = 0.9649999737739563, time = 0.3489704132080078\n",
      "Testing at step=10, batch=40, test loss = 0.14247453212738037, test acc = 0.9549999833106995, time = 0.3511238098144531\n",
      "Testing at step=10, batch=45, test loss = 0.10140668600797653, test acc = 0.9549999833106995, time = 0.351377010345459\n",
      "Step 10 finished in 314.4955406188965, Train loss = 0.14815355802575747, Test loss = 0.13828004285693168; Train Acc = 0.9544833296537399, Test Acc = 0.9571999943256378\n",
      "Training at step=11, batch=0, train loss = 0.13864949345588684, train acc = 0.9449999928474426, time = 0.9519755840301514\n",
      "Training at step=11, batch=30, train loss = 0.14688676595687866, train acc = 0.9549999833106995, time = 0.9509918689727783\n",
      "Training at step=11, batch=60, train loss = 0.09843002259731293, train acc = 0.9599999785423279, time = 0.951061487197876\n",
      "Training at step=11, batch=90, train loss = 0.14036916196346283, train acc = 0.9599999785423279, time = 0.9509739875793457\n",
      "Training at step=11, batch=120, train loss = 0.11904501169919968, train acc = 0.9700000286102295, time = 0.950448751449585\n",
      "Training at step=11, batch=150, train loss = 0.13603860139846802, train acc = 0.9449999928474426, time = 0.9517486095428467\n",
      "Training at step=11, batch=180, train loss = 0.14328768849372864, train acc = 0.9599999785423279, time = 0.9532468318939209\n",
      "Training at step=11, batch=210, train loss = 0.05723433196544647, train acc = 0.9750000238418579, time = 0.9523751735687256\n",
      "Training at step=11, batch=240, train loss = 0.21507690846920013, train acc = 0.925000011920929, time = 0.9521982669830322\n",
      "Training at step=11, batch=270, train loss = 0.16533346474170685, train acc = 0.9700000286102295, time = 0.9500007629394531\n",
      "Testing at step=11, batch=0, test loss = 0.20235611498355865, test acc = 0.9549999833106995, time = 0.3525688648223877\n",
      "Testing at step=11, batch=5, test loss = 0.14357033371925354, test acc = 0.9549999833106995, time = 0.3508412837982178\n",
      "Testing at step=11, batch=10, test loss = 0.13508754968643188, test acc = 0.949999988079071, time = 0.35079431533813477\n",
      "Testing at step=11, batch=15, test loss = 0.15274539589881897, test acc = 0.9549999833106995, time = 0.35226869583129883\n",
      "Testing at step=11, batch=20, test loss = 0.10240394622087479, test acc = 0.9649999737739563, time = 0.3520336151123047\n",
      "Testing at step=11, batch=25, test loss = 0.05431415140628815, test acc = 0.9900000095367432, time = 0.3543739318847656\n",
      "Testing at step=11, batch=30, test loss = 0.1324254870414734, test acc = 0.9599999785423279, time = 0.3532850742340088\n",
      "Testing at step=11, batch=35, test loss = 0.10321566462516785, test acc = 0.9750000238418579, time = 0.35048413276672363\n",
      "Testing at step=11, batch=40, test loss = 0.13923263549804688, test acc = 0.9549999833106995, time = 0.35231637954711914\n",
      "Testing at step=11, batch=45, test loss = 0.12133242934942245, test acc = 0.9449999928474426, time = 0.35048723220825195\n",
      "Step 11 finished in 314.7425711154938, Train loss = 0.1415729829793175, Test loss = 0.12760586909949778; Train Acc = 0.956416662534078, Test Acc = 0.9602999985218048\n",
      "Training at step=12, batch=0, train loss = 0.08352203667163849, train acc = 0.9750000238418579, time = 0.9498136043548584\n",
      "Training at step=12, batch=30, train loss = 0.13334476947784424, train acc = 0.9549999833106995, time = 0.9501276016235352\n",
      "Training at step=12, batch=60, train loss = 0.12637385725975037, train acc = 0.9649999737739563, time = 0.9548108577728271\n",
      "Training at step=12, batch=90, train loss = 0.15103359520435333, train acc = 0.949999988079071, time = 0.9521162509918213\n",
      "Training at step=12, batch=120, train loss = 0.11175810545682907, train acc = 0.9700000286102295, time = 0.952749490737915\n",
      "Training at step=12, batch=150, train loss = 0.12234949320554733, train acc = 0.9549999833106995, time = 0.9509561061859131\n",
      "Training at step=12, batch=180, train loss = 0.16846628487110138, train acc = 0.9449999928474426, time = 0.9538459777832031\n",
      "Training at step=12, batch=210, train loss = 0.15730920433998108, train acc = 0.949999988079071, time = 0.9501657485961914\n",
      "Training at step=12, batch=240, train loss = 0.13577310740947723, train acc = 0.9599999785423279, time = 0.9471168518066406\n",
      "Training at step=12, batch=270, train loss = 0.1420803815126419, train acc = 0.9549999833106995, time = 0.9541769027709961\n",
      "Testing at step=12, batch=0, test loss = 0.16498824954032898, test acc = 0.9449999928474426, time = 0.35314059257507324\n",
      "Testing at step=12, batch=5, test loss = 0.11452272534370422, test acc = 0.9649999737739563, time = 0.3517732620239258\n",
      "Testing at step=12, batch=10, test loss = 0.14128029346466064, test acc = 0.9449999928474426, time = 0.35553455352783203\n",
      "Testing at step=12, batch=15, test loss = 0.09033588320016861, test acc = 0.9700000286102295, time = 0.35317373275756836\n",
      "Testing at step=12, batch=20, test loss = 0.07832686603069305, test acc = 0.9599999785423279, time = 0.3549461364746094\n",
      "Testing at step=12, batch=25, test loss = 0.12008717656135559, test acc = 0.9649999737739563, time = 0.350513219833374\n",
      "Testing at step=12, batch=30, test loss = 0.11381245404481888, test acc = 0.9649999737739563, time = 0.3528451919555664\n",
      "Testing at step=12, batch=35, test loss = 0.09993413090705872, test acc = 0.9649999737739563, time = 0.35100722312927246\n",
      "Testing at step=12, batch=40, test loss = 0.1476227045059204, test acc = 0.9549999833106995, time = 0.352266788482666\n",
      "Testing at step=12, batch=45, test loss = 0.12577250599861145, test acc = 0.9449999928474426, time = 0.35597872734069824\n",
      "Step 12 finished in 315.10226225852966, Train loss = 0.1335452144717177, Test loss = 0.1306190375983715; Train Acc = 0.959766664703687, Test Acc = 0.9602999949455261\n",
      "Training at step=13, batch=0, train loss = 0.1525873988866806, train acc = 0.949999988079071, time = 0.9492487907409668\n",
      "Training at step=13, batch=30, train loss = 0.21046026051044464, train acc = 0.9449999928474426, time = 0.94889235496521\n",
      "Training at step=13, batch=60, train loss = 0.09781122207641602, train acc = 0.9649999737739563, time = 0.9489529132843018\n",
      "Training at step=13, batch=90, train loss = 0.13675396144390106, train acc = 0.9599999785423279, time = 0.9517810344696045\n",
      "Training at step=13, batch=120, train loss = 0.10049722343683243, train acc = 0.9800000190734863, time = 0.9505152702331543\n",
      "Training at step=13, batch=150, train loss = 0.1305256485939026, train acc = 0.9649999737739563, time = 0.949941873550415\n",
      "Training at step=13, batch=180, train loss = 0.0815311074256897, train acc = 0.9599999785423279, time = 0.9546017646789551\n",
      "Training at step=13, batch=210, train loss = 0.09291461855173111, train acc = 0.9649999737739563, time = 0.951714038848877\n",
      "Training at step=13, batch=240, train loss = 0.179081991314888, train acc = 0.9300000071525574, time = 0.9535689353942871\n",
      "Training at step=13, batch=270, train loss = 0.13423942029476166, train acc = 0.9700000286102295, time = 0.9513678550720215\n",
      "Testing at step=13, batch=0, test loss = 0.06386739760637283, test acc = 0.9700000286102295, time = 0.3516874313354492\n",
      "Testing at step=13, batch=5, test loss = 0.11995008587837219, test acc = 0.9750000238418579, time = 0.35442018508911133\n",
      "Testing at step=13, batch=10, test loss = 0.11414336413145065, test acc = 0.9700000286102295, time = 0.35167908668518066\n",
      "Testing at step=13, batch=15, test loss = 0.10779394209384918, test acc = 0.9599999785423279, time = 0.35101795196533203\n",
      "Testing at step=13, batch=20, test loss = 0.05773167684674263, test acc = 0.9800000190734863, time = 0.3525218963623047\n",
      "Testing at step=13, batch=25, test loss = 0.18064697086811066, test acc = 0.9449999928474426, time = 0.3516693115234375\n",
      "Testing at step=13, batch=30, test loss = 0.1411568969488144, test acc = 0.9549999833106995, time = 0.3503844738006592\n",
      "Testing at step=13, batch=35, test loss = 0.13746756315231323, test acc = 0.9599999785423279, time = 0.3530566692352295\n",
      "Testing at step=13, batch=40, test loss = 0.11896573752164841, test acc = 0.9649999737739563, time = 0.3520801067352295\n",
      "Testing at step=13, batch=45, test loss = 0.19939059019088745, test acc = 0.9300000071525574, time = 0.35053420066833496\n",
      "Step 13 finished in 314.9908516407013, Train loss = 0.1267562053973476, Test loss = 0.12416076466441155; Train Acc = 0.9619166644414266, Test Acc = 0.9608999991416931\n",
      "Training at step=14, batch=0, train loss = 0.11581219732761383, train acc = 0.9700000286102295, time = 0.9503216743469238\n",
      "Training at step=14, batch=30, train loss = 0.14740429818630219, train acc = 0.949999988079071, time = 0.9482541084289551\n",
      "Training at step=14, batch=60, train loss = 0.1105119064450264, train acc = 0.9549999833106995, time = 0.947913408279419\n",
      "Training at step=14, batch=90, train loss = 0.09489353001117706, train acc = 0.9700000286102295, time = 0.9713635444641113\n",
      "Training at step=14, batch=120, train loss = 0.12542419135570526, train acc = 0.9549999833106995, time = 0.9487731456756592\n",
      "Training at step=14, batch=150, train loss = 0.1301993876695633, train acc = 0.949999988079071, time = 0.9497213363647461\n",
      "Training at step=14, batch=180, train loss = 0.13935397565364838, train acc = 0.9599999785423279, time = 0.9507269859313965\n",
      "Training at step=14, batch=210, train loss = 0.13035979866981506, train acc = 0.949999988079071, time = 0.9469010829925537\n",
      "Training at step=14, batch=240, train loss = 0.11444132775068283, train acc = 0.9599999785423279, time = 0.9475767612457275\n",
      "Training at step=14, batch=270, train loss = 0.1121501624584198, train acc = 0.9599999785423279, time = 0.9511489868164062\n",
      "Testing at step=14, batch=0, test loss = 0.1315910667181015, test acc = 0.9649999737739563, time = 0.35234904289245605\n",
      "Testing at step=14, batch=5, test loss = 0.12341424822807312, test acc = 0.9549999833106995, time = 0.35111093521118164\n",
      "Testing at step=14, batch=10, test loss = 0.123757503926754, test acc = 0.949999988079071, time = 0.3525264263153076\n",
      "Testing at step=14, batch=15, test loss = 0.07950542122125626, test acc = 0.9800000190734863, time = 0.35251784324645996\n",
      "Testing at step=14, batch=20, test loss = 0.10787905007600784, test acc = 0.949999988079071, time = 0.354048490524292\n",
      "Testing at step=14, batch=25, test loss = 0.12397654354572296, test acc = 0.9700000286102295, time = 0.3527848720550537\n",
      "Testing at step=14, batch=30, test loss = 0.13864028453826904, test acc = 0.9649999737739563, time = 0.3519868850708008\n",
      "Testing at step=14, batch=35, test loss = 0.11166384816169739, test acc = 0.9599999785423279, time = 0.35248351097106934\n",
      "Testing at step=14, batch=40, test loss = 0.12433239072561264, test acc = 0.949999988079071, time = 0.35230302810668945\n",
      "Testing at step=14, batch=45, test loss = 0.1347176879644394, test acc = 0.9599999785423279, time = 0.35096192359924316\n",
      "Step 14 finished in 314.293790102005, Train loss = 0.1220159654940168, Test loss = 0.14106071278452872; Train Acc = 0.9625833334525427, Test Acc = 0.9548999905586243\n",
      "Training at step=15, batch=0, train loss = 0.11612687259912491, train acc = 0.949999988079071, time = 0.9494411945343018\n",
      "Training at step=15, batch=30, train loss = 0.11641205102205276, train acc = 0.949999988079071, time = 0.9475252628326416\n",
      "Training at step=15, batch=60, train loss = 0.08406001329421997, train acc = 0.9700000286102295, time = 0.9512710571289062\n",
      "Training at step=15, batch=90, train loss = 0.0720188096165657, train acc = 0.9750000238418579, time = 0.9515960216522217\n",
      "Training at step=15, batch=120, train loss = 0.05858824774622917, train acc = 0.9850000143051147, time = 0.9472682476043701\n",
      "Training at step=15, batch=150, train loss = 0.060412079095840454, train acc = 0.9850000143051147, time = 0.9485585689544678\n",
      "Training at step=15, batch=180, train loss = 0.10940153151750565, train acc = 0.9599999785423279, time = 0.9514145851135254\n",
      "Training at step=15, batch=210, train loss = 0.11555549502372742, train acc = 0.9750000238418579, time = 0.9503214359283447\n",
      "Training at step=15, batch=240, train loss = 0.09720791131258011, train acc = 0.9649999737739563, time = 0.949770450592041\n",
      "Training at step=15, batch=270, train loss = 0.09552856534719467, train acc = 0.9649999737739563, time = 0.9515683650970459\n",
      "Testing at step=15, batch=0, test loss = 0.07321908324956894, test acc = 0.9700000286102295, time = 0.35516929626464844\n",
      "Testing at step=15, batch=5, test loss = 0.03967774286866188, test acc = 0.9850000143051147, time = 0.356168270111084\n",
      "Testing at step=15, batch=10, test loss = 0.09334851056337357, test acc = 0.9850000143051147, time = 0.3511788845062256\n",
      "Testing at step=15, batch=15, test loss = 0.07619526237249374, test acc = 0.9750000238418579, time = 0.3522355556488037\n",
      "Testing at step=15, batch=20, test loss = 0.06876517087221146, test acc = 0.9850000143051147, time = 0.3546741008758545\n",
      "Testing at step=15, batch=25, test loss = 0.25703418254852295, test acc = 0.9649999737739563, time = 0.3522367477416992\n",
      "Testing at step=15, batch=30, test loss = 0.16650082170963287, test acc = 0.9449999928474426, time = 0.35335803031921387\n",
      "Testing at step=15, batch=35, test loss = 0.09154820442199707, test acc = 0.9599999785423279, time = 0.35414576530456543\n",
      "Testing at step=15, batch=40, test loss = 0.14816510677337646, test acc = 0.9549999833106995, time = 0.35274291038513184\n",
      "Testing at step=15, batch=45, test loss = 0.06052123010158539, test acc = 0.9750000238418579, time = 0.3543508052825928\n",
      "Step 15 finished in 314.56821298599243, Train loss = 0.11544457340613007, Test loss = 0.12150770872831344; Train Acc = 0.9653500000635783, Test Acc = 0.9646999967098236\n",
      "Training at step=16, batch=0, train loss = 0.1302403062582016, train acc = 0.949999988079071, time = 0.953913688659668\n",
      "Training at step=16, batch=30, train loss = 0.07212162762880325, train acc = 0.9850000143051147, time = 0.9487576484680176\n",
      "Training at step=16, batch=60, train loss = 0.06622402369976044, train acc = 0.9750000238418579, time = 0.9497127532958984\n",
      "Training at step=16, batch=90, train loss = 0.13718196749687195, train acc = 0.9549999833106995, time = 0.9489619731903076\n",
      "Training at step=16, batch=120, train loss = 0.16019372642040253, train acc = 0.9549999833106995, time = 0.9480555057525635\n",
      "Training at step=16, batch=150, train loss = 0.1461893767118454, train acc = 0.9449999928474426, time = 0.94927978515625\n",
      "Training at step=16, batch=180, train loss = 0.06298325955867767, train acc = 0.9800000190734863, time = 0.9612832069396973\n",
      "Training at step=16, batch=210, train loss = 0.0658838152885437, train acc = 0.9800000190734863, time = 0.9475409984588623\n",
      "Training at step=16, batch=240, train loss = 0.09386830031871796, train acc = 0.9649999737739563, time = 0.9455633163452148\n",
      "Training at step=16, batch=270, train loss = 0.08019180595874786, train acc = 0.9700000286102295, time = 0.951995849609375\n",
      "Testing at step=16, batch=0, test loss = 0.10039720684289932, test acc = 0.9800000190734863, time = 0.35198259353637695\n",
      "Testing at step=16, batch=5, test loss = 0.10876481980085373, test acc = 0.9599999785423279, time = 0.35030317306518555\n",
      "Testing at step=16, batch=10, test loss = 0.09227306395769119, test acc = 0.9750000238418579, time = 0.35559606552124023\n",
      "Testing at step=16, batch=15, test loss = 0.1705469936132431, test acc = 0.9449999928474426, time = 0.35149192810058594\n",
      "Testing at step=16, batch=20, test loss = 0.15294507145881653, test acc = 0.9549999833106995, time = 0.35125041007995605\n",
      "Testing at step=16, batch=25, test loss = 0.07436371594667435, test acc = 0.9750000238418579, time = 0.3504672050476074\n",
      "Testing at step=16, batch=30, test loss = 0.15647754073143005, test acc = 0.9649999737739563, time = 0.35184693336486816\n",
      "Testing at step=16, batch=35, test loss = 0.05958665907382965, test acc = 0.9850000143051147, time = 0.35385847091674805\n",
      "Testing at step=16, batch=40, test loss = 0.11453771591186523, test acc = 0.9549999833106995, time = 0.3512296676635742\n",
      "Testing at step=16, batch=45, test loss = 0.10358967632055283, test acc = 0.9700000286102295, time = 0.35248470306396484\n",
      "Step 16 finished in 314.59397411346436, Train loss = 0.11024573463946581, Test loss = 0.11161871038377286; Train Acc = 0.966350000500679, Test Acc = 0.966300002336502\n",
      "Training at step=17, batch=0, train loss = 0.08844296634197235, train acc = 0.9750000238418579, time = 0.9479687213897705\n",
      "Training at step=17, batch=30, train loss = 0.08553517609834671, train acc = 0.9700000286102295, time = 0.9479663372039795\n",
      "Training at step=17, batch=60, train loss = 0.09091595560312271, train acc = 0.9549999833106995, time = 0.9472246170043945\n",
      "Training at step=17, batch=90, train loss = 0.12920840084552765, train acc = 0.9700000286102295, time = 0.9509873390197754\n",
      "Training at step=17, batch=120, train loss = 0.10943184792995453, train acc = 0.9700000286102295, time = 0.9521520137786865\n",
      "Training at step=17, batch=150, train loss = 0.12446537613868713, train acc = 0.9649999737739563, time = 0.9493458271026611\n",
      "Training at step=17, batch=180, train loss = 0.12634140253067017, train acc = 0.9549999833106995, time = 0.9509589672088623\n",
      "Training at step=17, batch=210, train loss = 0.14785976707935333, train acc = 0.9649999737739563, time = 0.9510433673858643\n",
      "Training at step=17, batch=240, train loss = 0.11282134801149368, train acc = 0.9649999737739563, time = 0.951352596282959\n",
      "Training at step=17, batch=270, train loss = 0.13081392645835876, train acc = 0.9549999833106995, time = 0.9524059295654297\n",
      "Testing at step=17, batch=0, test loss = 0.08044054359197617, test acc = 0.9700000286102295, time = 0.3530592918395996\n",
      "Testing at step=17, batch=5, test loss = 0.05244915187358856, test acc = 0.9750000238418579, time = 0.352750301361084\n",
      "Testing at step=17, batch=10, test loss = 0.11852161586284637, test acc = 0.9599999785423279, time = 0.3519597053527832\n",
      "Testing at step=17, batch=15, test loss = 0.06434869021177292, test acc = 0.9800000190734863, time = 0.35082054138183594\n",
      "Testing at step=17, batch=20, test loss = 0.08008331060409546, test acc = 0.9700000286102295, time = 0.3518848419189453\n",
      "Testing at step=17, batch=25, test loss = 0.1738581508398056, test acc = 0.9649999737739563, time = 0.3513369560241699\n",
      "Testing at step=17, batch=30, test loss = 0.1685018539428711, test acc = 0.9449999928474426, time = 0.35343337059020996\n",
      "Testing at step=17, batch=35, test loss = 0.03542354702949524, test acc = 0.9850000143051147, time = 0.35160303115844727\n",
      "Testing at step=17, batch=40, test loss = 0.08678688108921051, test acc = 0.9700000286102295, time = 0.3511362075805664\n",
      "Testing at step=17, batch=45, test loss = 0.11414292454719543, test acc = 0.9700000286102295, time = 0.35231757164001465\n",
      "Step 17 finished in 314.5996286869049, Train loss = 0.10679733715330561, Test loss = 0.10857868395745754; Train Acc = 0.9674333357810974, Test Acc = 0.9692000031471253\n",
      "Training at step=18, batch=0, train loss = 0.049425192177295685, train acc = 0.9800000190734863, time = 0.9542315006256104\n",
      "Training at step=18, batch=30, train loss = 0.12482953816652298, train acc = 0.9750000238418579, time = 0.9496145248413086\n",
      "Training at step=18, batch=60, train loss = 0.0836799368262291, train acc = 0.9800000190734863, time = 0.9516570568084717\n",
      "Training at step=18, batch=90, train loss = 0.15746377408504486, train acc = 0.9449999928474426, time = 0.9499788284301758\n",
      "Training at step=18, batch=120, train loss = 0.07911224663257599, train acc = 0.9800000190734863, time = 0.9509899616241455\n",
      "Training at step=18, batch=150, train loss = 0.12939277291297913, train acc = 0.9649999737739563, time = 0.9519696235656738\n",
      "Training at step=18, batch=180, train loss = 0.09180878847837448, train acc = 0.9700000286102295, time = 0.9507591724395752\n",
      "Training at step=18, batch=210, train loss = 0.10801166296005249, train acc = 0.9750000238418579, time = 0.9487249851226807\n",
      "Training at step=18, batch=240, train loss = 0.0757586881518364, train acc = 0.9900000095367432, time = 0.9481046199798584\n",
      "Training at step=18, batch=270, train loss = 0.1702793836593628, train acc = 0.9399999976158142, time = 0.9520649909973145\n",
      "Testing at step=18, batch=0, test loss = 0.06979327648878098, test acc = 0.9700000286102295, time = 0.35477471351623535\n",
      "Testing at step=18, batch=5, test loss = 0.10207807272672653, test acc = 0.9649999737739563, time = 0.3542780876159668\n",
      "Testing at step=18, batch=10, test loss = 0.10210072249174118, test acc = 0.9700000286102295, time = 0.35394859313964844\n",
      "Testing at step=18, batch=15, test loss = 0.12828978896141052, test acc = 0.9649999737739563, time = 0.35400986671447754\n",
      "Testing at step=18, batch=20, test loss = 0.08257089555263519, test acc = 0.9750000238418579, time = 0.3517873287200928\n",
      "Testing at step=18, batch=25, test loss = 0.10322573035955429, test acc = 0.9700000286102295, time = 0.35508251190185547\n",
      "Testing at step=18, batch=30, test loss = 0.08426567167043686, test acc = 0.9750000238418579, time = 0.3526265621185303\n",
      "Testing at step=18, batch=35, test loss = 0.0990683063864708, test acc = 0.9750000238418579, time = 0.3535196781158447\n",
      "Testing at step=18, batch=40, test loss = 0.1428515613079071, test acc = 0.949999988079071, time = 0.3530914783477783\n",
      "Testing at step=18, batch=45, test loss = 0.15931107103824615, test acc = 0.9549999833106995, time = 0.3541135787963867\n",
      "Step 18 finished in 314.8143689632416, Train loss = 0.10269585471600294, Test loss = 0.11148929215967655; Train Acc = 0.9685500023762385, Test Acc = 0.9657000041007996\n",
      "Training at step=19, batch=0, train loss = 0.10106651484966278, train acc = 0.9700000286102295, time = 0.9546504020690918\n",
      "Training at step=19, batch=30, train loss = 0.06340405344963074, train acc = 0.9800000190734863, time = 0.9521148204803467\n",
      "Training at step=19, batch=60, train loss = 0.08860785514116287, train acc = 0.9700000286102295, time = 0.9499540328979492\n",
      "Training at step=19, batch=90, train loss = 0.10768333077430725, train acc = 0.9700000286102295, time = 0.9490277767181396\n",
      "Training at step=19, batch=120, train loss = 0.04546258598566055, train acc = 0.9850000143051147, time = 0.9638547897338867\n",
      "Training at step=19, batch=150, train loss = 0.14901046454906464, train acc = 0.9449999928474426, time = 0.9489507675170898\n",
      "Training at step=19, batch=180, train loss = 0.1141899898648262, train acc = 0.9649999737739563, time = 0.9496657848358154\n",
      "Training at step=19, batch=210, train loss = 0.07866348326206207, train acc = 0.9599999785423279, time = 0.9520108699798584\n",
      "Training at step=19, batch=240, train loss = 0.15909916162490845, train acc = 0.9350000023841858, time = 0.9533934593200684\n",
      "Training at step=19, batch=270, train loss = 0.15193289518356323, train acc = 0.9599999785423279, time = 0.9505527019500732\n",
      "Testing at step=19, batch=0, test loss = 0.1395205706357956, test acc = 0.949999988079071, time = 0.35172152519226074\n",
      "Testing at step=19, batch=5, test loss = 0.07740369439125061, test acc = 0.9800000190734863, time = 0.35215067863464355\n",
      "Testing at step=19, batch=10, test loss = 0.15140344202518463, test acc = 0.9599999785423279, time = 0.35408544540405273\n",
      "Testing at step=19, batch=15, test loss = 0.1553061604499817, test acc = 0.9399999976158142, time = 0.35118842124938965\n",
      "Testing at step=19, batch=20, test loss = 0.08722179383039474, test acc = 0.9850000143051147, time = 0.3523421287536621\n",
      "Testing at step=19, batch=25, test loss = 0.09856986999511719, test acc = 0.9700000286102295, time = 0.3531181812286377\n",
      "Testing at step=19, batch=30, test loss = 0.0681072473526001, test acc = 0.9800000190734863, time = 0.3548915386199951\n",
      "Testing at step=19, batch=35, test loss = 0.0806419774889946, test acc = 0.9649999737739563, time = 0.3538331985473633\n",
      "Testing at step=19, batch=40, test loss = 0.08013136684894562, test acc = 0.9700000286102295, time = 0.35320591926574707\n",
      "Testing at step=19, batch=45, test loss = 0.1535613238811493, test acc = 0.9549999833106995, time = 0.3541531562805176\n",
      "Step 19 finished in 314.83204555511475, Train loss = 0.09887810116633773, Test loss = 0.10756517745554448; Train Acc = 0.9687833368778229, Test Acc = 0.9665000021457673\n",
      "Training at step=20, batch=0, train loss = 0.06576976180076599, train acc = 0.9800000190734863, time = 0.9532582759857178\n",
      "Training at step=20, batch=30, train loss = 0.10461612045764923, train acc = 0.9649999737739563, time = 0.9486241340637207\n",
      "Training at step=20, batch=60, train loss = 0.07258061319589615, train acc = 0.9750000238418579, time = 0.9529435634613037\n",
      "Training at step=20, batch=90, train loss = 0.14389343559741974, train acc = 0.9549999833106995, time = 0.948775053024292\n",
      "Training at step=20, batch=120, train loss = 0.11129740625619888, train acc = 0.9750000238418579, time = 0.9517767429351807\n",
      "Training at step=20, batch=150, train loss = 0.04689789190888405, train acc = 0.9800000190734863, time = 0.949885368347168\n",
      "Training at step=20, batch=180, train loss = 0.16118766367435455, train acc = 0.9549999833106995, time = 0.9488825798034668\n",
      "Training at step=20, batch=210, train loss = 0.07426213473081589, train acc = 0.9750000238418579, time = 0.9468386173248291\n",
      "Training at step=20, batch=240, train loss = 0.10224583745002747, train acc = 0.9549999833106995, time = 0.9550576210021973\n",
      "Training at step=20, batch=270, train loss = 0.11670958250761032, train acc = 0.9750000238418579, time = 0.9516205787658691\n",
      "Testing at step=20, batch=0, test loss = 0.045381974428892136, test acc = 0.9950000047683716, time = 0.3543996810913086\n",
      "Testing at step=20, batch=5, test loss = 0.11685841530561447, test acc = 0.9449999928474426, time = 0.35256123542785645\n",
      "Testing at step=20, batch=10, test loss = 0.07243449240922928, test acc = 0.9750000238418579, time = 0.3529477119445801\n",
      "Testing at step=20, batch=15, test loss = 0.046904824674129486, test acc = 0.9950000047683716, time = 0.35491323471069336\n",
      "Testing at step=20, batch=20, test loss = 0.13755826652050018, test acc = 0.9599999785423279, time = 0.35460996627807617\n",
      "Testing at step=20, batch=25, test loss = 0.1969112902879715, test acc = 0.9449999928474426, time = 0.3535492420196533\n",
      "Testing at step=20, batch=30, test loss = 0.053174156695604324, test acc = 0.9800000190734863, time = 0.3528568744659424\n",
      "Testing at step=20, batch=35, test loss = 0.16136816143989563, test acc = 0.9399999976158142, time = 0.35387492179870605\n",
      "Testing at step=20, batch=40, test loss = 0.06843379139900208, test acc = 0.9800000190734863, time = 0.3527522087097168\n",
      "Testing at step=20, batch=45, test loss = 0.12516796588897705, test acc = 0.9649999737739563, time = 0.3522946834564209\n",
      "Step 20 finished in 314.7906882762909, Train loss = 0.09389244050408403, Test loss = 0.09827900409698487; Train Acc = 0.9717666727304458, Test Acc = 0.9699000060558319\n",
      "Training at step=21, batch=0, train loss = 0.11534957587718964, train acc = 0.9800000190734863, time = 0.9540753364562988\n",
      "Training at step=21, batch=30, train loss = 0.0740090161561966, train acc = 0.9700000286102295, time = 0.9511642456054688\n",
      "Training at step=21, batch=60, train loss = 0.08395829796791077, train acc = 0.9750000238418579, time = 0.9487814903259277\n",
      "Training at step=21, batch=90, train loss = 0.09011644124984741, train acc = 0.9649999737739563, time = 0.951263427734375\n",
      "Training at step=21, batch=120, train loss = 0.09643769264221191, train acc = 0.949999988079071, time = 0.9502720832824707\n",
      "Training at step=21, batch=150, train loss = 0.07891248911619186, train acc = 0.9599999785423279, time = 0.9512593746185303\n",
      "Training at step=21, batch=180, train loss = 0.13110540807247162, train acc = 0.9599999785423279, time = 0.9518356323242188\n",
      "Training at step=21, batch=210, train loss = 0.10661637037992477, train acc = 0.949999988079071, time = 0.9520509243011475\n",
      "Training at step=21, batch=240, train loss = 0.06370387971401215, train acc = 0.9800000190734863, time = 0.950690746307373\n",
      "Training at step=21, batch=270, train loss = 0.04860273376107216, train acc = 0.9850000143051147, time = 0.9535114765167236\n",
      "Testing at step=21, batch=0, test loss = 0.08801599591970444, test acc = 0.9800000190734863, time = 0.35382723808288574\n",
      "Testing at step=21, batch=5, test loss = 0.13937263190746307, test acc = 0.9649999737739563, time = 0.3528449535369873\n",
      "Testing at step=21, batch=10, test loss = 0.07774534076452255, test acc = 0.9750000238418579, time = 0.35359811782836914\n",
      "Testing at step=21, batch=15, test loss = 0.10173559933900833, test acc = 0.9649999737739563, time = 0.35228967666625977\n",
      "Testing at step=21, batch=20, test loss = 0.07529334723949432, test acc = 0.9750000238418579, time = 0.3524470329284668\n",
      "Testing at step=21, batch=25, test loss = 0.12840406596660614, test acc = 0.9649999737739563, time = 0.3515927791595459\n",
      "Testing at step=21, batch=30, test loss = 0.10278521478176117, test acc = 0.9800000190734863, time = 0.3517875671386719\n",
      "Testing at step=21, batch=35, test loss = 0.06961134076118469, test acc = 0.9850000143051147, time = 0.35205698013305664\n",
      "Testing at step=21, batch=40, test loss = 0.12176910042762756, test acc = 0.9599999785423279, time = 0.35162949562072754\n",
      "Testing at step=21, batch=45, test loss = 0.035019420087337494, test acc = 0.9900000095367432, time = 0.35265350341796875\n",
      "Step 21 finished in 314.83305191993713, Train loss = 0.0920340688712895, Test loss = 0.100390207991004; Train Acc = 0.9711333398024241, Test Acc = 0.970600004196167\n",
      "Training at step=22, batch=0, train loss = 0.04032078757882118, train acc = 0.9900000095367432, time = 0.9503145217895508\n",
      "Training at step=22, batch=30, train loss = 0.08720096945762634, train acc = 0.9700000286102295, time = 0.9513936042785645\n",
      "Training at step=22, batch=60, train loss = 0.16796556115150452, train acc = 0.9449999928474426, time = 0.9494812488555908\n",
      "Training at step=22, batch=90, train loss = 0.051954761147499084, train acc = 0.9850000143051147, time = 0.9534599781036377\n",
      "Training at step=22, batch=120, train loss = 0.08323650062084198, train acc = 0.9850000143051147, time = 0.9736673831939697\n",
      "Training at step=22, batch=150, train loss = 0.0722413957118988, train acc = 0.9649999737739563, time = 0.9510960578918457\n",
      "Training at step=22, batch=180, train loss = 0.07657879590988159, train acc = 0.9900000095367432, time = 0.9518828392028809\n",
      "Training at step=22, batch=210, train loss = 0.13165183365345, train acc = 0.9599999785423279, time = 0.9533109664916992\n",
      "Training at step=22, batch=240, train loss = 0.10667095333337784, train acc = 0.9750000238418579, time = 0.9502794742584229\n",
      "Training at step=22, batch=270, train loss = 0.05847137048840523, train acc = 0.9850000143051147, time = 0.9518744945526123\n",
      "Testing at step=22, batch=0, test loss = 0.13794678449630737, test acc = 0.9800000190734863, time = 0.3507981300354004\n",
      "Testing at step=22, batch=5, test loss = 0.0867416113615036, test acc = 0.9599999785423279, time = 0.35184621810913086\n",
      "Testing at step=22, batch=10, test loss = 0.04571732506155968, test acc = 0.9850000143051147, time = 0.3536107540130615\n",
      "Testing at step=22, batch=15, test loss = 0.14151063561439514, test acc = 0.9599999785423279, time = 0.356691837310791\n",
      "Testing at step=22, batch=20, test loss = 0.21385696530342102, test acc = 0.9300000071525574, time = 0.35221171379089355\n",
      "Testing at step=22, batch=25, test loss = 0.03054811805486679, test acc = 0.9900000095367432, time = 0.35052061080932617\n",
      "Testing at step=22, batch=30, test loss = 0.0857231542468071, test acc = 0.9800000190734863, time = 0.34974002838134766\n",
      "Testing at step=22, batch=35, test loss = 0.06758729368448257, test acc = 0.9700000286102295, time = 0.35361266136169434\n",
      "Testing at step=22, batch=40, test loss = 0.11028008162975311, test acc = 0.9599999785423279, time = 0.3508756160736084\n",
      "Testing at step=22, batch=45, test loss = 0.13973642885684967, test acc = 0.9549999833106995, time = 0.35069942474365234\n",
      "Step 22 finished in 315.03191447257996, Train loss = 0.08862410519272089, Test loss = 0.0988839902728796; Train Acc = 0.9721166737874349, Test Acc = 0.9686000049114227\n",
      "Training at step=23, batch=0, train loss = 0.12334997206926346, train acc = 0.9700000286102295, time = 0.9517130851745605\n",
      "Training at step=23, batch=30, train loss = 0.07273305207490921, train acc = 0.9750000238418579, time = 0.9492542743682861\n",
      "Training at step=23, batch=60, train loss = 0.11410669982433319, train acc = 0.9700000286102295, time = 0.948375940322876\n",
      "Training at step=23, batch=90, train loss = 0.07606902718544006, train acc = 0.9700000286102295, time = 0.9516146183013916\n",
      "Training at step=23, batch=120, train loss = 0.06337536126375198, train acc = 0.9700000286102295, time = 0.9538624286651611\n",
      "Training at step=23, batch=150, train loss = 0.09527923911809921, train acc = 0.9750000238418579, time = 0.9513332843780518\n",
      "Training at step=23, batch=180, train loss = 0.106513112783432, train acc = 0.9700000286102295, time = 0.9516828060150146\n",
      "Training at step=23, batch=210, train loss = 0.09637109935283661, train acc = 0.9700000286102295, time = 0.9489803314208984\n",
      "Training at step=23, batch=240, train loss = 0.09579259902238846, train acc = 0.9700000286102295, time = 0.9492154121398926\n",
      "Training at step=23, batch=270, train loss = 0.08769029378890991, train acc = 0.9850000143051147, time = 0.9489552974700928\n",
      "Testing at step=23, batch=0, test loss = 0.08591489493846893, test acc = 0.9800000190734863, time = 0.3527672290802002\n",
      "Testing at step=23, batch=5, test loss = 0.11146590113639832, test acc = 0.9599999785423279, time = 0.3530395030975342\n",
      "Testing at step=23, batch=10, test loss = 0.0882784053683281, test acc = 0.9700000286102295, time = 0.35324573516845703\n",
      "Testing at step=23, batch=15, test loss = 0.09990180283784866, test acc = 0.9800000190734863, time = 0.3508472442626953\n",
      "Testing at step=23, batch=20, test loss = 0.07134097069501877, test acc = 0.9700000286102295, time = 0.35069894790649414\n",
      "Testing at step=23, batch=25, test loss = 0.08877429366111755, test acc = 0.9700000286102295, time = 0.35117578506469727\n",
      "Testing at step=23, batch=30, test loss = 0.12411108613014221, test acc = 0.949999988079071, time = 0.3524153232574463\n",
      "Testing at step=23, batch=35, test loss = 0.11525961011648178, test acc = 0.9649999737739563, time = 0.3634307384490967\n",
      "Testing at step=23, batch=40, test loss = 0.13434460759162903, test acc = 0.9700000286102295, time = 0.3520669937133789\n",
      "Testing at step=23, batch=45, test loss = 0.1201171875, test acc = 0.9700000286102295, time = 0.3499562740325928\n",
      "Step 23 finished in 314.8687708377838, Train loss = 0.08556849020843704, Test loss = 0.10058357656002044; Train Acc = 0.974183341662089, Test Acc = 0.9694000113010407\n",
      "Training at step=24, batch=0, train loss = 0.16206146776676178, train acc = 0.949999988079071, time = 0.948988676071167\n",
      "Training at step=24, batch=30, train loss = 0.13282270729541779, train acc = 0.9649999737739563, time = 0.951012134552002\n",
      "Training at step=24, batch=60, train loss = 0.10866869986057281, train acc = 0.9599999785423279, time = 0.9483821392059326\n",
      "Training at step=24, batch=90, train loss = 0.08151085674762726, train acc = 0.9649999737739563, time = 0.9492924213409424\n",
      "Training at step=24, batch=120, train loss = 0.03869306296110153, train acc = 0.9950000047683716, time = 0.9500486850738525\n",
      "Training at step=24, batch=150, train loss = 0.03638685494661331, train acc = 0.9900000095367432, time = 0.9475405216217041\n",
      "Training at step=24, batch=180, train loss = 0.08185861259698868, train acc = 0.9800000190734863, time = 0.9537544250488281\n",
      "Training at step=24, batch=210, train loss = 0.08521667122840881, train acc = 0.9850000143051147, time = 0.9502847194671631\n",
      "Training at step=24, batch=240, train loss = 0.10372205823659897, train acc = 0.9750000238418579, time = 0.9546668529510498\n",
      "Training at step=24, batch=270, train loss = 0.12489034980535507, train acc = 0.9649999737739563, time = 0.9524340629577637\n",
      "Testing at step=24, batch=0, test loss = 0.14672599732875824, test acc = 0.9599999785423279, time = 0.35627055168151855\n",
      "Testing at step=24, batch=5, test loss = 0.026914717629551888, test acc = 0.9950000047683716, time = 0.35207557678222656\n",
      "Testing at step=24, batch=10, test loss = 0.13672155141830444, test acc = 0.9599999785423279, time = 0.35366296768188477\n",
      "Testing at step=24, batch=15, test loss = 0.05915728956460953, test acc = 0.9800000190734863, time = 0.35335445404052734\n",
      "Testing at step=24, batch=20, test loss = 0.04186234623193741, test acc = 0.9900000095367432, time = 0.35527992248535156\n",
      "Testing at step=24, batch=25, test loss = 0.10657354444265366, test acc = 0.9750000238418579, time = 0.35396742820739746\n",
      "Testing at step=24, batch=30, test loss = 0.0932827740907669, test acc = 0.9700000286102295, time = 0.3535575866699219\n",
      "Testing at step=24, batch=35, test loss = 0.12920799851417542, test acc = 0.949999988079071, time = 0.35459351539611816\n",
      "Testing at step=24, batch=40, test loss = 0.05076614394783974, test acc = 0.9850000143051147, time = 0.3528711795806885\n",
      "Testing at step=24, batch=45, test loss = 0.10851683467626572, test acc = 0.9750000238418579, time = 0.35253214836120605\n",
      "Step 24 finished in 314.6274437904358, Train loss = 0.08763366834570964, Test loss = 0.09875640448182821; Train Acc = 0.973083340326945, Test Acc = 0.9692000007629394\n",
      "Training at step=25, batch=0, train loss = 0.05594920739531517, train acc = 0.9900000095367432, time = 0.9552645683288574\n",
      "Training at step=25, batch=30, train loss = 0.04183584824204445, train acc = 0.9850000143051147, time = 0.9495255947113037\n",
      "Training at step=25, batch=60, train loss = 0.06398998945951462, train acc = 0.9700000286102295, time = 0.9492659568786621\n",
      "Training at step=25, batch=90, train loss = 0.1730099469423294, train acc = 0.9649999737739563, time = 0.9510595798492432\n",
      "Training at step=25, batch=120, train loss = 0.11061504483222961, train acc = 0.9750000238418579, time = 0.9534115791320801\n",
      "Training at step=25, batch=150, train loss = 0.07661041617393494, train acc = 0.9850000143051147, time = 0.9500141143798828\n",
      "Training at step=25, batch=180, train loss = 0.07866299152374268, train acc = 0.9750000238418579, time = 0.9501094818115234\n",
      "Training at step=25, batch=210, train loss = 0.1414368897676468, train acc = 0.9649999737739563, time = 0.9515891075134277\n",
      "Training at step=25, batch=240, train loss = 0.12720461189746857, train acc = 0.9649999737739563, time = 0.9490094184875488\n",
      "Training at step=25, batch=270, train loss = 0.09808503836393356, train acc = 0.9599999785423279, time = 0.9492807388305664\n",
      "Testing at step=25, batch=0, test loss = 0.16906848549842834, test acc = 0.9599999785423279, time = 0.3552696704864502\n",
      "Testing at step=25, batch=5, test loss = 0.10800351947546005, test acc = 0.9850000143051147, time = 0.35326504707336426\n",
      "Testing at step=25, batch=10, test loss = 0.13405251502990723, test acc = 0.9649999737739563, time = 0.3534245491027832\n",
      "Testing at step=25, batch=15, test loss = 0.07909899950027466, test acc = 0.9800000190734863, time = 0.35108065605163574\n",
      "Testing at step=25, batch=20, test loss = 0.08519013226032257, test acc = 0.9750000238418579, time = 0.3510100841522217\n",
      "Testing at step=25, batch=25, test loss = 0.11766072362661362, test acc = 0.9700000286102295, time = 0.3521909713745117\n",
      "Testing at step=25, batch=30, test loss = 0.06805600970983505, test acc = 0.9700000286102295, time = 0.3544001579284668\n",
      "Testing at step=25, batch=35, test loss = 0.04970903322100639, test acc = 0.9750000238418579, time = 0.3532750606536865\n",
      "Testing at step=25, batch=40, test loss = 0.06211079657077789, test acc = 0.9850000143051147, time = 0.35149669647216797\n",
      "Testing at step=25, batch=45, test loss = 0.061157673597335815, test acc = 0.9850000143051147, time = 0.35349345207214355\n",
      "Step 25 finished in 314.8438584804535, Train loss = 0.08290062608197331, Test loss = 0.09616243008524179; Train Acc = 0.9745666750272115, Test Acc = 0.9697000026702881\n",
      "Training at step=26, batch=0, train loss = 0.12136353552341461, train acc = 0.9750000238418579, time = 0.9518616199493408\n",
      "Training at step=26, batch=30, train loss = 0.1433827430009842, train acc = 0.9549999833106995, time = 1.0451416969299316\n",
      "Training at step=26, batch=60, train loss = 0.11709244549274445, train acc = 0.9649999737739563, time = 0.950667142868042\n",
      "Training at step=26, batch=90, train loss = 0.08372310549020767, train acc = 0.9800000190734863, time = 0.9534153938293457\n",
      "Training at step=26, batch=120, train loss = 0.06108323484659195, train acc = 0.9800000190734863, time = 0.9499185085296631\n",
      "Training at step=26, batch=150, train loss = 0.05731848627328873, train acc = 0.9750000238418579, time = 0.9500653743743896\n",
      "Training at step=26, batch=180, train loss = 0.10506543517112732, train acc = 0.9750000238418579, time = 0.9518563747406006\n",
      "Training at step=26, batch=210, train loss = 0.06708335131406784, train acc = 0.9750000238418579, time = 0.9502582550048828\n",
      "Training at step=26, batch=240, train loss = 0.12736809253692627, train acc = 0.9599999785423279, time = 0.9505813121795654\n",
      "Training at step=26, batch=270, train loss = 0.07488084584474564, train acc = 0.9700000286102295, time = 0.9510142803192139\n",
      "Testing at step=26, batch=0, test loss = 0.19022002816200256, test acc = 0.949999988079071, time = 0.35146307945251465\n",
      "Testing at step=26, batch=5, test loss = 0.04607919231057167, test acc = 0.9800000190734863, time = 0.35306644439697266\n",
      "Testing at step=26, batch=10, test loss = 0.03665603697299957, test acc = 0.9900000095367432, time = 0.3519704341888428\n",
      "Testing at step=26, batch=15, test loss = 0.10761890560388565, test acc = 0.9750000238418579, time = 0.35258984565734863\n",
      "Testing at step=26, batch=20, test loss = 0.10257986187934875, test acc = 0.9649999737739563, time = 0.3539106845855713\n",
      "Testing at step=26, batch=25, test loss = 0.07628509402275085, test acc = 0.9649999737739563, time = 0.35181474685668945\n",
      "Testing at step=26, batch=30, test loss = 0.10497179627418518, test acc = 0.9700000286102295, time = 0.35007214546203613\n",
      "Testing at step=26, batch=35, test loss = 0.06519798934459686, test acc = 0.9800000190734863, time = 0.3520810604095459\n",
      "Testing at step=26, batch=40, test loss = 0.04528379812836647, test acc = 0.9850000143051147, time = 0.35083794593811035\n",
      "Testing at step=26, batch=45, test loss = 0.047805801033973694, test acc = 0.9850000143051147, time = 0.35066723823547363\n",
      "Step 26 finished in 314.84208512306213, Train loss = 0.08021754244963328, Test loss = 0.09673744790256024; Train Acc = 0.9743500103553137, Test Acc = 0.9702000021934509\n",
      "Training at step=27, batch=0, train loss = 0.04717046394944191, train acc = 0.9850000143051147, time = 0.9527699947357178\n",
      "Training at step=27, batch=30, train loss = 0.11274436861276627, train acc = 0.9649999737739563, time = 0.9505727291107178\n",
      "Training at step=27, batch=60, train loss = 0.04760768264532089, train acc = 0.9850000143051147, time = 0.9491817951202393\n",
      "Training at step=27, batch=90, train loss = 0.0470006950199604, train acc = 0.9800000190734863, time = 0.9501805305480957\n",
      "Training at step=27, batch=120, train loss = 0.04691338539123535, train acc = 0.9850000143051147, time = 0.9530627727508545\n",
      "Training at step=27, batch=150, train loss = 0.03292106091976166, train acc = 0.9900000095367432, time = 0.9478065967559814\n",
      "Training at step=27, batch=180, train loss = 0.19449199736118317, train acc = 0.9599999785423279, time = 0.9491465091705322\n",
      "Training at step=27, batch=210, train loss = 0.04694916680455208, train acc = 0.9850000143051147, time = 0.947228193283081\n",
      "Training at step=27, batch=240, train loss = 0.0891990214586258, train acc = 0.9700000286102295, time = 0.9477317333221436\n",
      "Training at step=27, batch=270, train loss = 0.07605041563510895, train acc = 0.9750000238418579, time = 0.9506564140319824\n",
      "Testing at step=27, batch=0, test loss = 0.15537817776203156, test acc = 0.949999988079071, time = 0.3536698818206787\n",
      "Testing at step=27, batch=5, test loss = 0.0919356718659401, test acc = 0.9750000238418579, time = 0.372791051864624\n",
      "Testing at step=27, batch=10, test loss = 0.048921164125204086, test acc = 0.9900000095367432, time = 0.3527674674987793\n",
      "Testing at step=27, batch=15, test loss = 0.14852456748485565, test acc = 0.9449999928474426, time = 0.3500545024871826\n",
      "Testing at step=27, batch=20, test loss = 0.09267517924308777, test acc = 0.9649999737739563, time = 0.34975361824035645\n",
      "Testing at step=27, batch=25, test loss = 0.07613382488489151, test acc = 0.9850000143051147, time = 0.3505401611328125\n",
      "Testing at step=27, batch=30, test loss = 0.12950830161571503, test acc = 0.9599999785423279, time = 0.35277318954467773\n",
      "Testing at step=27, batch=35, test loss = 0.1548479050397873, test acc = 0.9700000286102295, time = 0.35070204734802246\n",
      "Testing at step=27, batch=40, test loss = 0.03525545820593834, test acc = 0.9800000190734863, time = 0.3520522117614746\n",
      "Testing at step=27, batch=45, test loss = 0.07117626070976257, test acc = 0.9850000143051147, time = 0.35014820098876953\n",
      "Step 27 finished in 314.56922602653503, Train loss = 0.07867304858441154, Test loss = 0.08786565531045198; Train Acc = 0.9758833432197571, Test Acc = 0.9739000082015992\n",
      "Training at step=28, batch=0, train loss = 0.12185406684875488, train acc = 0.9700000286102295, time = 0.9484870433807373\n",
      "Training at step=28, batch=30, train loss = 0.05493055284023285, train acc = 0.9800000190734863, time = 0.9477627277374268\n",
      "Training at step=28, batch=60, train loss = 0.024758651852607727, train acc = 0.9900000095367432, time = 0.9490683078765869\n",
      "Training at step=28, batch=90, train loss = 0.056112971156835556, train acc = 0.9750000238418579, time = 0.9515273571014404\n",
      "Training at step=28, batch=120, train loss = 0.0602748766541481, train acc = 0.9800000190734863, time = 0.9486629962921143\n",
      "Training at step=28, batch=150, train loss = 0.07414703071117401, train acc = 0.9750000238418579, time = 0.9496898651123047\n",
      "Training at step=28, batch=180, train loss = 0.0970255583524704, train acc = 0.9750000238418579, time = 0.9526288509368896\n",
      "Training at step=28, batch=210, train loss = 0.07870958000421524, train acc = 0.9649999737739563, time = 0.952239990234375\n",
      "Training at step=28, batch=240, train loss = 0.05796985328197479, train acc = 0.9700000286102295, time = 0.9559926986694336\n",
      "Training at step=28, batch=270, train loss = 0.0664864107966423, train acc = 0.9750000238418579, time = 0.9500939846038818\n",
      "Testing at step=28, batch=0, test loss = 0.12065467238426208, test acc = 0.9599999785423279, time = 0.3521888256072998\n",
      "Testing at step=28, batch=5, test loss = 0.10389258712530136, test acc = 0.9549999833106995, time = 0.3565044403076172\n",
      "Testing at step=28, batch=10, test loss = 0.05887806788086891, test acc = 0.9800000190734863, time = 0.3561995029449463\n",
      "Testing at step=28, batch=15, test loss = 0.07256920635700226, test acc = 0.9750000238418579, time = 0.355104923248291\n",
      "Testing at step=28, batch=20, test loss = 0.11000032722949982, test acc = 0.9750000238418579, time = 0.3509960174560547\n",
      "Testing at step=28, batch=25, test loss = 0.08438777178525925, test acc = 0.9700000286102295, time = 0.3500063419342041\n",
      "Testing at step=28, batch=30, test loss = 0.10212388634681702, test acc = 0.9599999785423279, time = 0.3502354621887207\n",
      "Testing at step=28, batch=35, test loss = 0.10263130068778992, test acc = 0.9599999785423279, time = 0.3524436950683594\n",
      "Testing at step=28, batch=40, test loss = 0.06781145930290222, test acc = 0.9850000143051147, time = 0.3637981414794922\n",
      "Testing at step=28, batch=45, test loss = 0.09164203703403473, test acc = 0.9800000190734863, time = 0.3511509895324707\n",
      "Step 28 finished in 314.7913966178894, Train loss = 0.07586300576105715, Test loss = 0.08523845966905355; Train Acc = 0.9759166753292083, Test Acc = 0.9749000072479248\n",
      "Training at step=29, batch=0, train loss = 0.04548069089651108, train acc = 0.9750000238418579, time = 0.9514541625976562\n",
      "Training at step=29, batch=30, train loss = 0.10543237626552582, train acc = 0.9750000238418579, time = 0.9509775638580322\n",
      "Training at step=29, batch=60, train loss = 0.02898240089416504, train acc = 0.9850000143051147, time = 0.9540574550628662\n",
      "Training at step=29, batch=90, train loss = 0.10303786396980286, train acc = 0.9599999785423279, time = 0.950587272644043\n",
      "Training at step=29, batch=120, train loss = 0.1387987732887268, train acc = 0.9649999737739563, time = 0.9472439289093018\n",
      "Training at step=29, batch=150, train loss = 0.02153485268354416, train acc = 0.9900000095367432, time = 0.9499785900115967\n",
      "Training at step=29, batch=180, train loss = 0.053316425532102585, train acc = 0.9850000143051147, time = 0.9534034729003906\n",
      "Training at step=29, batch=210, train loss = 0.05818842723965645, train acc = 0.9800000190734863, time = 0.9492864608764648\n",
      "Training at step=29, batch=240, train loss = 0.10206517577171326, train acc = 0.949999988079071, time = 0.952336311340332\n",
      "Training at step=29, batch=270, train loss = 0.07913967221975327, train acc = 0.9750000238418579, time = 0.9490368366241455\n",
      "Testing at step=29, batch=0, test loss = 0.08375756442546844, test acc = 0.9750000238418579, time = 0.3520805835723877\n",
      "Testing at step=29, batch=5, test loss = 0.10663451999425888, test acc = 0.9649999737739563, time = 0.35140442848205566\n",
      "Testing at step=29, batch=10, test loss = 0.12623654305934906, test acc = 0.9549999833106995, time = 0.3522200584411621\n",
      "Testing at step=29, batch=15, test loss = 0.0730845183134079, test acc = 0.9700000286102295, time = 0.35253167152404785\n",
      "Testing at step=29, batch=20, test loss = 0.05648445710539818, test acc = 0.9800000190734863, time = 0.38890504837036133\n",
      "Testing at step=29, batch=25, test loss = 0.13749776780605316, test acc = 0.9599999785423279, time = 0.3536686897277832\n",
      "Testing at step=29, batch=30, test loss = 0.10595783591270447, test acc = 0.9649999737739563, time = 0.35282111167907715\n",
      "Testing at step=29, batch=35, test loss = 0.0174301415681839, test acc = 1.0, time = 0.350644588470459\n",
      "Testing at step=29, batch=40, test loss = 0.07067424058914185, test acc = 0.9700000286102295, time = 0.3522770404815674\n",
      "Testing at step=29, batch=45, test loss = 0.061784982681274414, test acc = 0.9750000238418579, time = 0.35155463218688965\n",
      "Step 29 finished in 314.8163335323334, Train loss = 0.07237513623200358, Test loss = 0.09423061542212963; Train Acc = 0.9769500106573105, Test Acc = 0.9703000044822693\n",
      "Training at step=30, batch=0, train loss = 0.06815016269683838, train acc = 0.9850000143051147, time = 0.9488449096679688\n",
      "Training at step=30, batch=30, train loss = 0.05006507784128189, train acc = 0.9800000190734863, time = 0.9644613265991211\n",
      "Training at step=30, batch=60, train loss = 0.021716222167015076, train acc = 0.9950000047683716, time = 0.9482274055480957\n",
      "Training at step=30, batch=90, train loss = 0.08188359439373016, train acc = 0.9649999737739563, time = 0.9487526416778564\n",
      "Training at step=30, batch=120, train loss = 0.03279750421643257, train acc = 0.9850000143051147, time = 0.9510772228240967\n",
      "Training at step=30, batch=150, train loss = 0.03855636343359947, train acc = 0.9850000143051147, time = 0.9499189853668213\n",
      "Training at step=30, batch=180, train loss = 0.060675088316202164, train acc = 0.9800000190734863, time = 0.9488034248352051\n",
      "Training at step=30, batch=210, train loss = 0.10117878019809723, train acc = 0.9800000190734863, time = 0.9537112712860107\n",
      "Training at step=30, batch=240, train loss = 0.0829358696937561, train acc = 0.9599999785423279, time = 0.950617790222168\n",
      "Training at step=30, batch=270, train loss = 0.03726436197757721, train acc = 0.9950000047683716, time = 0.9507846832275391\n",
      "Testing at step=30, batch=0, test loss = 0.045762743800878525, test acc = 0.9850000143051147, time = 0.35278797149658203\n",
      "Testing at step=30, batch=5, test loss = 0.04538634419441223, test acc = 0.9900000095367432, time = 0.3503134250640869\n",
      "Testing at step=30, batch=10, test loss = 0.1473776400089264, test acc = 0.9549999833106995, time = 0.3538203239440918\n",
      "Testing at step=30, batch=15, test loss = 0.09413085132837296, test acc = 0.9700000286102295, time = 0.3521153926849365\n",
      "Testing at step=30, batch=20, test loss = 0.06269144266843796, test acc = 0.9900000095367432, time = 0.35195446014404297\n",
      "Testing at step=30, batch=25, test loss = 0.1179303377866745, test acc = 0.9599999785423279, time = 0.37145519256591797\n",
      "Testing at step=30, batch=30, test loss = 0.16483788192272186, test acc = 0.949999988079071, time = 0.3511974811553955\n",
      "Testing at step=30, batch=35, test loss = 0.06183983013033867, test acc = 0.9750000238418579, time = 0.3507199287414551\n",
      "Testing at step=30, batch=40, test loss = 0.04082835093140602, test acc = 0.9750000238418579, time = 0.350327730178833\n",
      "Testing at step=30, batch=45, test loss = 0.0601494126021862, test acc = 0.9700000286102295, time = 0.35387587547302246\n",
      "Step 30 finished in 314.50083351135254, Train loss = 0.07116474566360315, Test loss = 0.09925032813102007; Train Acc = 0.9772833446661632, Test Acc = 0.9695000064373016\n",
      "Training at step=31, batch=0, train loss = 0.04310526326298714, train acc = 0.9850000143051147, time = 0.9534859657287598\n",
      "Training at step=31, batch=30, train loss = 0.07439020276069641, train acc = 0.9750000238418579, time = 0.9516682624816895\n",
      "Training at step=31, batch=60, train loss = 0.06370509415864944, train acc = 0.9700000286102295, time = 0.9516339302062988\n",
      "Training at step=31, batch=90, train loss = 0.04709404334425926, train acc = 0.9750000238418579, time = 0.9548053741455078\n",
      "Training at step=31, batch=120, train loss = 0.0889410674571991, train acc = 0.9750000238418579, time = 0.9511795043945312\n",
      "Training at step=31, batch=150, train loss = 0.1698942333459854, train acc = 0.9700000286102295, time = 0.9528684616088867\n",
      "Training at step=31, batch=180, train loss = 0.03526782989501953, train acc = 0.9850000143051147, time = 0.9507243633270264\n",
      "Training at step=31, batch=210, train loss = 0.07309023290872574, train acc = 0.9800000190734863, time = 0.9510266780853271\n",
      "Training at step=31, batch=240, train loss = 0.09327918291091919, train acc = 0.9649999737739563, time = 0.950897216796875\n",
      "Training at step=31, batch=270, train loss = 0.08497840911149979, train acc = 0.9649999737739563, time = 0.9511020183563232\n",
      "Testing at step=31, batch=0, test loss = 0.06890098750591278, test acc = 0.9850000143051147, time = 0.353954553604126\n",
      "Testing at step=31, batch=5, test loss = 0.07258381694555283, test acc = 0.9750000238418579, time = 0.3514711856842041\n",
      "Testing at step=31, batch=10, test loss = 0.14202529191970825, test acc = 0.9649999737739563, time = 0.35202598571777344\n",
      "Testing at step=31, batch=15, test loss = 0.017778150737285614, test acc = 0.9950000047683716, time = 0.35205960273742676\n",
      "Testing at step=31, batch=20, test loss = 0.07587610930204391, test acc = 0.9750000238418579, time = 0.35297489166259766\n",
      "Testing at step=31, batch=25, test loss = 0.03699096292257309, test acc = 0.9850000143051147, time = 0.3533592224121094\n",
      "Testing at step=31, batch=30, test loss = 0.05170057341456413, test acc = 0.9850000143051147, time = 0.3521416187286377\n",
      "Testing at step=31, batch=35, test loss = 0.07169292867183685, test acc = 0.9800000190734863, time = 0.35359644889831543\n",
      "Testing at step=31, batch=40, test loss = 0.07910633832216263, test acc = 0.9700000286102295, time = 0.35693836212158203\n",
      "Testing at step=31, batch=45, test loss = 0.10427412390708923, test acc = 0.9750000238418579, time = 0.3534393310546875\n",
      "Step 31 finished in 315.0956907272339, Train loss = 0.06811543559965988, Test loss = 0.08843913957476617; Train Acc = 0.9777333456277847, Test Acc = 0.973200011253357\n",
      "Training at step=32, batch=0, train loss = 0.05746643990278244, train acc = 0.9850000143051147, time = 0.9513516426086426\n",
      "Training at step=32, batch=30, train loss = 0.055304158478975296, train acc = 0.9700000286102295, time = 0.9494996070861816\n",
      "Training at step=32, batch=60, train loss = 0.05462387576699257, train acc = 0.9750000238418579, time = 0.952960729598999\n",
      "Training at step=32, batch=90, train loss = 0.049951910972595215, train acc = 0.9800000190734863, time = 0.9492483139038086\n",
      "Training at step=32, batch=120, train loss = 0.08774672448635101, train acc = 0.9750000238418579, time = 0.9488058090209961\n",
      "Training at step=32, batch=150, train loss = 0.06842736899852753, train acc = 0.9750000238418579, time = 0.9516379833221436\n",
      "Training at step=32, batch=180, train loss = 0.05106250196695328, train acc = 0.9900000095367432, time = 0.9513161182403564\n",
      "Training at step=32, batch=210, train loss = 0.047041840851306915, train acc = 0.9950000047683716, time = 0.9489612579345703\n",
      "Training at step=32, batch=240, train loss = 0.07158099859952927, train acc = 0.9850000143051147, time = 0.947986364364624\n",
      "Training at step=32, batch=270, train loss = 0.037110358476638794, train acc = 0.9850000143051147, time = 0.9482228755950928\n",
      "Testing at step=32, batch=0, test loss = 0.09129448235034943, test acc = 0.9700000286102295, time = 0.35493016242980957\n",
      "Testing at step=32, batch=5, test loss = 0.0977807268500328, test acc = 0.9649999737739563, time = 0.35381293296813965\n",
      "Testing at step=32, batch=10, test loss = 0.07732709497213364, test acc = 0.9700000286102295, time = 0.35378551483154297\n",
      "Testing at step=32, batch=15, test loss = 0.06365979462862015, test acc = 0.9850000143051147, time = 0.3539159297943115\n",
      "Testing at step=32, batch=20, test loss = 0.08905825763940811, test acc = 0.9750000238418579, time = 0.35306549072265625\n",
      "Testing at step=32, batch=25, test loss = 0.06497640162706375, test acc = 0.9850000143051147, time = 0.3540313243865967\n",
      "Testing at step=32, batch=30, test loss = 0.12018416076898575, test acc = 0.9750000238418579, time = 0.3560824394226074\n",
      "Testing at step=32, batch=35, test loss = 0.15649671852588654, test acc = 0.9549999833106995, time = 0.35252952575683594\n",
      "Testing at step=32, batch=40, test loss = 0.047455042600631714, test acc = 0.9850000143051147, time = 0.3520164489746094\n",
      "Testing at step=32, batch=45, test loss = 0.07548023015260696, test acc = 0.9700000286102295, time = 0.354464054107666\n",
      "Step 32 finished in 314.60549688339233, Train loss = 0.06920122971137364, Test loss = 0.08725658409297467; Train Acc = 0.9783500121037165, Test Acc = 0.9744000089168549\n",
      "Training at step=33, batch=0, train loss = 0.054896339774131775, train acc = 0.9800000190734863, time = 0.9612071514129639\n",
      "Training at step=33, batch=30, train loss = 0.05083567649126053, train acc = 0.9900000095367432, time = 0.947674036026001\n",
      "Training at step=33, batch=60, train loss = 0.07674732059240341, train acc = 0.9800000190734863, time = 0.948279857635498\n",
      "Training at step=33, batch=90, train loss = 0.0595121756196022, train acc = 0.9850000143051147, time = 0.9486145973205566\n",
      "Training at step=33, batch=120, train loss = 0.08662217110395432, train acc = 0.9649999737739563, time = 0.9493370056152344\n",
      "Training at step=33, batch=150, train loss = 0.06425845623016357, train acc = 0.9800000190734863, time = 0.9465932846069336\n",
      "Training at step=33, batch=180, train loss = 0.06668650358915329, train acc = 0.9850000143051147, time = 0.9491145610809326\n",
      "Training at step=33, batch=210, train loss = 0.06703737378120422, train acc = 0.9700000286102295, time = 0.9533290863037109\n",
      "Training at step=33, batch=240, train loss = 0.05052247643470764, train acc = 0.9850000143051147, time = 0.9528672695159912\n",
      "Training at step=33, batch=270, train loss = 0.04572629928588867, train acc = 0.9850000143051147, time = 0.9499857425689697\n",
      "Testing at step=33, batch=0, test loss = 0.05766182020306587, test acc = 0.9750000238418579, time = 0.35084986686706543\n",
      "Testing at step=33, batch=5, test loss = 0.057964224368333817, test acc = 0.9800000190734863, time = 0.35074472427368164\n",
      "Testing at step=33, batch=10, test loss = 0.18633070588111877, test acc = 0.949999988079071, time = 0.3508455753326416\n",
      "Testing at step=33, batch=15, test loss = 0.082033671438694, test acc = 0.9750000238418579, time = 0.35207414627075195\n",
      "Testing at step=33, batch=20, test loss = 0.10349540412425995, test acc = 0.9750000238418579, time = 0.35167956352233887\n",
      "Testing at step=33, batch=25, test loss = 0.08986620604991913, test acc = 0.9750000238418579, time = 0.3502943515777588\n",
      "Testing at step=33, batch=30, test loss = 0.07228410989046097, test acc = 0.9900000095367432, time = 0.34911561012268066\n",
      "Testing at step=33, batch=35, test loss = 0.06574691087007523, test acc = 0.9800000190734863, time = 0.34871649742126465\n",
      "Testing at step=33, batch=40, test loss = 0.11603657901287079, test acc = 0.9599999785423279, time = 0.35338425636291504\n",
      "Testing at step=33, batch=45, test loss = 0.021438295021653175, test acc = 0.9900000095367432, time = 0.34909534454345703\n",
      "Step 33 finished in 314.2394301891327, Train loss = 0.06532058997390171, Test loss = 0.08907259423285722; Train Acc = 0.9795500121514003, Test Acc = 0.9736000061035156\n",
      "Training at step=34, batch=0, train loss = 0.08455800265073776, train acc = 0.9750000238418579, time = 0.9487705230712891\n",
      "Training at step=34, batch=30, train loss = 0.039562828838825226, train acc = 0.9850000143051147, time = 0.9537465572357178\n",
      "Training at step=34, batch=60, train loss = 0.05436863377690315, train acc = 0.9750000238418579, time = 0.9647386074066162\n",
      "Training at step=34, batch=90, train loss = 0.06195877864956856, train acc = 0.9750000238418579, time = 0.9520275592803955\n",
      "Training at step=34, batch=120, train loss = 0.08947116136550903, train acc = 0.9700000286102295, time = 0.9649019241333008\n",
      "Training at step=34, batch=150, train loss = 0.08387947082519531, train acc = 0.9700000286102295, time = 0.953106164932251\n",
      "Training at step=34, batch=180, train loss = 0.07505256682634354, train acc = 0.9800000190734863, time = 0.9492127895355225\n",
      "Training at step=34, batch=210, train loss = 0.030337966978549957, train acc = 0.9850000143051147, time = 0.9487574100494385\n",
      "Training at step=34, batch=240, train loss = 0.03207904472947121, train acc = 0.9900000095367432, time = 0.9520275592803955\n",
      "Training at step=34, batch=270, train loss = 0.04859136417508125, train acc = 0.9800000190734863, time = 0.954949140548706\n",
      "Testing at step=34, batch=0, test loss = 0.027998482808470726, test acc = 0.9850000143051147, time = 0.35123133659362793\n",
      "Testing at step=34, batch=5, test loss = 0.048292942345142365, test acc = 0.9900000095367432, time = 0.3525092601776123\n",
      "Testing at step=34, batch=10, test loss = 0.06494473665952682, test acc = 0.9850000143051147, time = 0.35202527046203613\n",
      "Testing at step=34, batch=15, test loss = 0.169066920876503, test acc = 0.9750000238418579, time = 0.35320067405700684\n",
      "Testing at step=34, batch=20, test loss = 0.07108102738857269, test acc = 0.9700000286102295, time = 0.35521769523620605\n",
      "Testing at step=34, batch=25, test loss = 0.10583789646625519, test acc = 0.9800000190734863, time = 0.3516654968261719\n",
      "Testing at step=34, batch=30, test loss = 0.10364213585853577, test acc = 0.9599999785423279, time = 0.3527965545654297\n",
      "Testing at step=34, batch=35, test loss = 0.04679323360323906, test acc = 0.9800000190734863, time = 0.3520646095275879\n",
      "Testing at step=34, batch=40, test loss = 0.10155686736106873, test acc = 0.9700000286102295, time = 0.3507678508758545\n",
      "Testing at step=34, batch=45, test loss = 0.0842997282743454, test acc = 0.9850000143051147, time = 0.35234975814819336\n",
      "Step 34 finished in 314.61490392684937, Train loss = 0.06260822881789257, Test loss = 0.08645881626754999; Train Acc = 0.9797666794061661, Test Acc = 0.9736000084877015\n",
      "Training at step=35, batch=0, train loss = 0.12636040151119232, train acc = 0.9750000238418579, time = 0.9510197639465332\n",
      "Training at step=35, batch=30, train loss = 0.045824743807315826, train acc = 0.9700000286102295, time = 0.947350263595581\n",
      "Training at step=35, batch=60, train loss = 0.08125827461481094, train acc = 0.9700000286102295, time = 0.9593367576599121\n",
      "Training at step=35, batch=90, train loss = 0.04499804601073265, train acc = 0.9850000143051147, time = 0.9493496417999268\n",
      "Training at step=35, batch=120, train loss = 0.02353566698729992, train acc = 1.0, time = 0.9498469829559326\n",
      "Training at step=35, batch=150, train loss = 0.05198968946933746, train acc = 0.9800000190734863, time = 0.9512460231781006\n",
      "Training at step=35, batch=180, train loss = 0.02582368068397045, train acc = 0.9900000095367432, time = 0.952723503112793\n",
      "Training at step=35, batch=210, train loss = 0.03655974939465523, train acc = 0.9950000047683716, time = 0.9502482414245605\n",
      "Training at step=35, batch=240, train loss = 0.05162361264228821, train acc = 0.9850000143051147, time = 0.9506375789642334\n",
      "Training at step=35, batch=270, train loss = 0.07112430781126022, train acc = 0.9750000238418579, time = 0.9495294094085693\n",
      "Testing at step=35, batch=0, test loss = 0.0714036300778389, test acc = 0.9850000143051147, time = 0.4171030521392822\n",
      "Testing at step=35, batch=5, test loss = 0.12350047379732132, test acc = 0.9599999785423279, time = 0.35156846046447754\n",
      "Testing at step=35, batch=10, test loss = 0.04002392664551735, test acc = 0.9850000143051147, time = 0.35540199279785156\n",
      "Testing at step=35, batch=15, test loss = 0.16852939128875732, test acc = 0.9700000286102295, time = 0.35132622718811035\n",
      "Testing at step=35, batch=20, test loss = 0.13992062211036682, test acc = 0.949999988079071, time = 0.3510904312133789\n",
      "Testing at step=35, batch=25, test loss = 0.08513599634170532, test acc = 0.9900000095367432, time = 0.3524770736694336\n",
      "Testing at step=35, batch=30, test loss = 0.10299201309680939, test acc = 0.9800000190734863, time = 0.35287904739379883\n",
      "Testing at step=35, batch=35, test loss = 0.15224240720272064, test acc = 0.9700000286102295, time = 0.3539423942565918\n",
      "Testing at step=35, batch=40, test loss = 0.13447920978069305, test acc = 0.9549999833106995, time = 0.3527557849884033\n",
      "Testing at step=35, batch=45, test loss = 0.08256220072507858, test acc = 0.9649999737739563, time = 0.35347723960876465\n",
      "Step 35 finished in 314.57905101776123, Train loss = 0.06347945230081678, Test loss = 0.09272587433457374; Train Acc = 0.9793666785955429, Test Acc = 0.9726000118255616\n",
      "Training at step=36, batch=0, train loss = 0.03944692015647888, train acc = 0.9900000095367432, time = 0.9501931667327881\n",
      "Training at step=36, batch=30, train loss = 0.06355535984039307, train acc = 0.9800000190734863, time = 0.9492027759552002\n",
      "Training at step=36, batch=60, train loss = 0.07273588329553604, train acc = 0.9950000047683716, time = 0.9489996433258057\n",
      "Training at step=36, batch=90, train loss = 0.015959450975060463, train acc = 0.9950000047683716, time = 0.9499161243438721\n",
      "Training at step=36, batch=120, train loss = 0.07112985849380493, train acc = 0.9750000238418579, time = 0.9486830234527588\n",
      "Training at step=36, batch=150, train loss = 0.09464320540428162, train acc = 0.9750000238418579, time = 0.9480855464935303\n",
      "Training at step=36, batch=180, train loss = 0.06626174598932266, train acc = 0.9800000190734863, time = 0.9477174282073975\n",
      "Training at step=36, batch=210, train loss = 0.06448996812105179, train acc = 0.9750000238418579, time = 0.9471328258514404\n",
      "Training at step=36, batch=240, train loss = 0.02907780185341835, train acc = 0.9950000047683716, time = 0.9507715702056885\n",
      "Training at step=36, batch=270, train loss = 0.026595773175358772, train acc = 0.9900000095367432, time = 0.9495546817779541\n",
      "Testing at step=36, batch=0, test loss = 0.06765702366828918, test acc = 0.9850000143051147, time = 0.3524332046508789\n",
      "Testing at step=36, batch=5, test loss = 0.03855622932314873, test acc = 0.9900000095367432, time = 0.35154247283935547\n",
      "Testing at step=36, batch=10, test loss = 0.1288793683052063, test acc = 0.9649999737739563, time = 0.3536055088043213\n",
      "Testing at step=36, batch=15, test loss = 0.11412449181079865, test acc = 0.9750000238418579, time = 0.35090112686157227\n",
      "Testing at step=36, batch=20, test loss = 0.06499072909355164, test acc = 0.9850000143051147, time = 0.350813627243042\n",
      "Testing at step=36, batch=25, test loss = 0.0983562171459198, test acc = 0.9649999737739563, time = 0.3513832092285156\n",
      "Testing at step=36, batch=30, test loss = 0.07157078385353088, test acc = 0.9850000143051147, time = 0.352170467376709\n",
      "Testing at step=36, batch=35, test loss = 0.11228125542402267, test acc = 0.9649999737739563, time = 0.35259437561035156\n",
      "Testing at step=36, batch=40, test loss = 0.04680800810456276, test acc = 0.9900000095367432, time = 0.3538169860839844\n",
      "Testing at step=36, batch=45, test loss = 0.14458082616329193, test acc = 0.9649999737739563, time = 0.3502793312072754\n",
      "Step 36 finished in 314.5416405200958, Train loss = 0.06092489382252097, Test loss = 0.09045427853241562; Train Acc = 0.9802166791756948, Test Acc = 0.9725000023841858\n",
      "Training at step=37, batch=0, train loss = 0.03579721599817276, train acc = 0.9950000047683716, time = 0.9490764141082764\n",
      "Training at step=37, batch=30, train loss = 0.09549371898174286, train acc = 0.9649999737739563, time = 0.9521455764770508\n",
      "Training at step=37, batch=60, train loss = 0.03955744579434395, train acc = 0.9800000190734863, time = 0.9489641189575195\n",
      "Training at step=37, batch=90, train loss = 0.029309283941984177, train acc = 0.9950000047683716, time = 0.9488365650177002\n",
      "Training at step=37, batch=120, train loss = 0.023907141759991646, train acc = 0.9900000095367432, time = 0.9503049850463867\n",
      "Training at step=37, batch=150, train loss = 0.04850050061941147, train acc = 0.9850000143051147, time = 0.9511997699737549\n",
      "Training at step=37, batch=180, train loss = 0.03603051230311394, train acc = 0.9800000190734863, time = 0.9522480964660645\n",
      "Training at step=37, batch=210, train loss = 0.055556539446115494, train acc = 0.9850000143051147, time = 0.9563133716583252\n",
      "Training at step=37, batch=240, train loss = 0.09804065525531769, train acc = 0.9549999833106995, time = 0.9500100612640381\n",
      "Training at step=37, batch=270, train loss = 0.08021508902311325, train acc = 0.9700000286102295, time = 0.9519522190093994\n",
      "Testing at step=37, batch=0, test loss = 0.08299435675144196, test acc = 0.9750000238418579, time = 0.35755324363708496\n",
      "Testing at step=37, batch=5, test loss = 0.0682104304432869, test acc = 0.9800000190734863, time = 0.3550405502319336\n",
      "Testing at step=37, batch=10, test loss = 0.0677335187792778, test acc = 0.9800000190734863, time = 0.35276222229003906\n",
      "Testing at step=37, batch=15, test loss = 0.11043208837509155, test acc = 0.9750000238418579, time = 0.35378432273864746\n",
      "Testing at step=37, batch=20, test loss = 0.07267114520072937, test acc = 0.9850000143051147, time = 0.3533930778503418\n",
      "Testing at step=37, batch=25, test loss = 0.09990793466567993, test acc = 0.9700000286102295, time = 0.35266709327697754\n",
      "Testing at step=37, batch=30, test loss = 0.09912142157554626, test acc = 0.9700000286102295, time = 0.3553323745727539\n",
      "Testing at step=37, batch=35, test loss = 0.13590598106384277, test acc = 0.9549999833106995, time = 0.35175514221191406\n",
      "Testing at step=37, batch=40, test loss = 0.0713852196931839, test acc = 0.9800000190734863, time = 0.35317373275756836\n",
      "Testing at step=37, batch=45, test loss = 0.05892385169863701, test acc = 0.9850000143051147, time = 0.35087084770202637\n",
      "Step 37 finished in 314.8049418926239, Train loss = 0.06090085062819223, Test loss = 0.08724132806062698; Train Acc = 0.9802833453814188, Test Acc = 0.973400012254715\n",
      "Training at step=38, batch=0, train loss = 0.05674907565116882, train acc = 0.9750000238418579, time = 0.9483737945556641\n",
      "Training at step=38, batch=30, train loss = 0.019628887996077538, train acc = 0.9950000047683716, time = 0.9527080059051514\n",
      "Training at step=38, batch=60, train loss = 0.0647914931178093, train acc = 0.9800000190734863, time = 0.9496517181396484\n",
      "Training at step=38, batch=90, train loss = 0.07347504049539566, train acc = 0.9800000190734863, time = 0.9506807327270508\n",
      "Training at step=38, batch=120, train loss = 0.04227721691131592, train acc = 0.9850000143051147, time = 0.9495410919189453\n",
      "Training at step=38, batch=150, train loss = 0.06556344777345657, train acc = 0.9750000238418579, time = 0.9478836059570312\n",
      "Training at step=38, batch=180, train loss = 0.04874182119965553, train acc = 0.9850000143051147, time = 0.9489154815673828\n",
      "Training at step=38, batch=210, train loss = 0.03466786444187164, train acc = 0.9900000095367432, time = 0.9485867023468018\n",
      "Training at step=38, batch=240, train loss = 0.09140574187040329, train acc = 0.9700000286102295, time = 0.950932502746582\n",
      "Training at step=38, batch=270, train loss = 0.050802115350961685, train acc = 0.9700000286102295, time = 0.9500250816345215\n",
      "Testing at step=38, batch=0, test loss = 0.027070648968219757, test acc = 0.9850000143051147, time = 0.3510096073150635\n",
      "Testing at step=38, batch=5, test loss = 0.07991530001163483, test acc = 0.9649999737739563, time = 0.35341978073120117\n",
      "Testing at step=38, batch=10, test loss = 0.13877679407596588, test acc = 0.9599999785423279, time = 0.35164761543273926\n",
      "Testing at step=38, batch=15, test loss = 0.052646707743406296, test acc = 0.9800000190734863, time = 0.35285258293151855\n",
      "Testing at step=38, batch=20, test loss = 0.07719434052705765, test acc = 0.9750000238418579, time = 0.3634922504425049\n",
      "Testing at step=38, batch=25, test loss = 0.06805668026208878, test acc = 0.9750000238418579, time = 0.3494844436645508\n",
      "Testing at step=38, batch=30, test loss = 0.05904038995504379, test acc = 0.9800000190734863, time = 0.35013723373413086\n",
      "Testing at step=38, batch=35, test loss = 0.12696141004562378, test acc = 0.949999988079071, time = 0.3510305881500244\n",
      "Testing at step=38, batch=40, test loss = 0.07197120785713196, test acc = 0.9750000238418579, time = 0.35041356086730957\n",
      "Testing at step=38, batch=45, test loss = 0.11687371879816055, test acc = 0.9800000190734863, time = 0.35067296028137207\n",
      "Step 38 finished in 314.5207543373108, Train loss = 0.057924752387528616, Test loss = 0.08671195454895496; Train Acc = 0.9813000138600667, Test Acc = 0.9729000091552734\n",
      "Training at step=39, batch=0, train loss = 0.047785282135009766, train acc = 0.9850000143051147, time = 0.9478933811187744\n",
      "Training at step=39, batch=30, train loss = 0.04501956328749657, train acc = 0.9800000190734863, time = 0.9510037899017334\n",
      "Training at step=39, batch=60, train loss = 0.1268710494041443, train acc = 0.9649999737739563, time = 0.9526176452636719\n",
      "Training at step=39, batch=90, train loss = 0.09471369534730911, train acc = 0.9599999785423279, time = 0.9482409954071045\n",
      "Training at step=39, batch=120, train loss = 0.034128252416849136, train acc = 0.9850000143051147, time = 0.9549260139465332\n",
      "Training at step=39, batch=150, train loss = 0.06607576459646225, train acc = 0.9800000190734863, time = 0.9511539936065674\n",
      "Training at step=39, batch=180, train loss = 0.05321233719587326, train acc = 0.9800000190734863, time = 0.9504814147949219\n",
      "Training at step=39, batch=210, train loss = 0.07352200150489807, train acc = 0.9800000190734863, time = 0.9491486549377441\n",
      "Training at step=39, batch=240, train loss = 0.049047891050577164, train acc = 0.9800000190734863, time = 0.9485676288604736\n",
      "Training at step=39, batch=270, train loss = 0.06041315943002701, train acc = 0.9900000095367432, time = 0.9535572528839111\n",
      "Testing at step=39, batch=0, test loss = 0.03854899853467941, test acc = 0.9850000143051147, time = 0.35420680046081543\n",
      "Testing at step=39, batch=5, test loss = 0.0537833534181118, test acc = 0.9850000143051147, time = 0.35299086570739746\n",
      "Testing at step=39, batch=10, test loss = 0.08328680694103241, test acc = 0.9750000238418579, time = 0.351423978805542\n",
      "Testing at step=39, batch=15, test loss = 0.049433384090662, test acc = 0.9900000095367432, time = 0.3548097610473633\n",
      "Testing at step=39, batch=20, test loss = 0.09103474020957947, test acc = 0.9750000238418579, time = 0.35352134704589844\n",
      "Testing at step=39, batch=25, test loss = 0.10866246372461319, test acc = 0.9599999785423279, time = 0.35215258598327637\n",
      "Testing at step=39, batch=30, test loss = 0.07455351948738098, test acc = 0.9850000143051147, time = 0.3552699089050293\n",
      "Testing at step=39, batch=35, test loss = 0.06372905522584915, test acc = 0.9649999737739563, time = 0.3538336753845215\n",
      "Testing at step=39, batch=40, test loss = 0.10628757625818253, test acc = 0.9750000238418579, time = 0.352431058883667\n",
      "Testing at step=39, batch=45, test loss = 0.11335211992263794, test acc = 0.9700000286102295, time = 0.35445570945739746\n",
      "Step 39 finished in 314.89674520492554, Train loss = 0.05554218132824947, Test loss = 0.08875229075551033; Train Acc = 0.9822500133514405, Test Acc = 0.9760000133514404\n",
      "Training at step=40, batch=0, train loss = 0.038852520287036896, train acc = 0.9800000190734863, time = 0.953676700592041\n",
      "Training at step=40, batch=30, train loss = 0.06182081624865532, train acc = 0.9800000190734863, time = 0.9519305229187012\n",
      "Training at step=40, batch=60, train loss = 0.030601579695940018, train acc = 0.9900000095367432, time = 0.9541151523590088\n",
      "Training at step=40, batch=90, train loss = 0.10535482317209244, train acc = 0.9800000190734863, time = 0.9508364200592041\n",
      "Training at step=40, batch=120, train loss = 0.04672228917479515, train acc = 0.9900000095367432, time = 0.9506435394287109\n",
      "Training at step=40, batch=150, train loss = 0.04581998661160469, train acc = 0.9850000143051147, time = 0.9552257061004639\n",
      "Training at step=40, batch=180, train loss = 0.13790491223335266, train acc = 0.9700000286102295, time = 0.9504399299621582\n",
      "Training at step=40, batch=210, train loss = 0.07624604552984238, train acc = 0.9700000286102295, time = 0.949540376663208\n",
      "Training at step=40, batch=240, train loss = 0.07030855119228363, train acc = 0.9900000095367432, time = 0.9503567218780518\n",
      "Training at step=40, batch=270, train loss = 0.0781514048576355, train acc = 0.9649999737739563, time = 0.9485318660736084\n",
      "Testing at step=40, batch=0, test loss = 0.06073667109012604, test acc = 0.9850000143051147, time = 0.35300207138061523\n",
      "Testing at step=40, batch=5, test loss = 0.09209860116243362, test acc = 0.9800000190734863, time = 0.35466670989990234\n",
      "Testing at step=40, batch=10, test loss = 0.06227881461381912, test acc = 0.9850000143051147, time = 0.37697410583496094\n",
      "Testing at step=40, batch=15, test loss = 0.11502302438020706, test acc = 0.9599999785423279, time = 0.3588733673095703\n",
      "Testing at step=40, batch=20, test loss = 0.13021999597549438, test acc = 0.9649999737739563, time = 0.35179877281188965\n",
      "Testing at step=40, batch=25, test loss = 0.04950784146785736, test acc = 0.9800000190734863, time = 0.35198235511779785\n",
      "Testing at step=40, batch=30, test loss = 0.04983130469918251, test acc = 0.9750000238418579, time = 0.35284852981567383\n",
      "Testing at step=40, batch=35, test loss = 0.06779888272285461, test acc = 0.9800000190734863, time = 0.3525524139404297\n",
      "Testing at step=40, batch=40, test loss = 0.05184103548526764, test acc = 0.9850000143051147, time = 0.35059022903442383\n",
      "Testing at step=40, batch=45, test loss = 0.06951569765806198, test acc = 0.9800000190734863, time = 0.35146403312683105\n",
      "Step 40 finished in 314.9276354312897, Train loss = 0.05577064415129523, Test loss = 0.08317836306989193; Train Acc = 0.9818166806300481, Test Acc = 0.9758000123500824\n",
      "Training at step=41, batch=0, train loss = 0.02414591796696186, train acc = 0.9950000047683716, time = 0.9524896144866943\n",
      "Training at step=41, batch=30, train loss = 0.05526987090706825, train acc = 0.9850000143051147, time = 0.9503171443939209\n",
      "Training at step=41, batch=60, train loss = 0.021405158564448357, train acc = 0.9900000095367432, time = 0.950777530670166\n",
      "Training at step=41, batch=90, train loss = 0.057891640812158585, train acc = 0.9750000238418579, time = 0.9510097503662109\n",
      "Training at step=41, batch=120, train loss = 0.05940411984920502, train acc = 0.9850000143051147, time = 0.9529533386230469\n",
      "Training at step=41, batch=150, train loss = 0.07615458220243454, train acc = 0.9800000190734863, time = 0.9518077373504639\n",
      "Training at step=41, batch=180, train loss = 0.06372731178998947, train acc = 0.9700000286102295, time = 0.9541189670562744\n",
      "Training at step=41, batch=210, train loss = 0.07334718108177185, train acc = 0.9750000238418579, time = 0.9547967910766602\n",
      "Training at step=41, batch=240, train loss = 0.04918818548321724, train acc = 0.9900000095367432, time = 0.9474921226501465\n",
      "Training at step=41, batch=270, train loss = 0.04820440709590912, train acc = 0.9900000095367432, time = 0.948000431060791\n",
      "Testing at step=41, batch=0, test loss = 0.056805502623319626, test acc = 0.9800000190734863, time = 0.3538503646850586\n",
      "Testing at step=41, batch=5, test loss = 0.01960272714495659, test acc = 0.9900000095367432, time = 0.3523592948913574\n",
      "Testing at step=41, batch=10, test loss = 0.13731703162193298, test acc = 0.9900000095367432, time = 0.35236072540283203\n",
      "Testing at step=41, batch=15, test loss = 0.10222624987363815, test acc = 0.9750000238418579, time = 0.35130810737609863\n",
      "Testing at step=41, batch=20, test loss = 0.07488571852445602, test acc = 0.9750000238418579, time = 0.35758352279663086\n",
      "Testing at step=41, batch=25, test loss = 0.1433568149805069, test acc = 0.9549999833106995, time = 0.3521690368652344\n",
      "Testing at step=41, batch=30, test loss = 0.11191944032907486, test acc = 0.9750000238418579, time = 0.35266947746276855\n",
      "Testing at step=41, batch=35, test loss = 0.07255169749259949, test acc = 0.9800000190734863, time = 0.35107421875\n",
      "Testing at step=41, batch=40, test loss = 0.03809330239892006, test acc = 0.9750000238418579, time = 0.3520219326019287\n",
      "Testing at step=41, batch=45, test loss = 0.10554510354995728, test acc = 0.9649999737739563, time = 0.3519930839538574\n",
      "Step 41 finished in 314.97210216522217, Train loss = 0.05371105669376751, Test loss = 0.08788834773004055; Train Acc = 0.9831166803836823, Test Acc = 0.9749000108242035\n",
      "Training at step=42, batch=0, train loss = 0.06688797473907471, train acc = 0.9700000286102295, time = 0.9505503177642822\n",
      "Training at step=42, batch=30, train loss = 0.04202548414468765, train acc = 0.9900000095367432, time = 0.9512050151824951\n",
      "Training at step=42, batch=60, train loss = 0.037078607827425, train acc = 0.9800000190734863, time = 0.9491503238677979\n",
      "Training at step=42, batch=90, train loss = 0.08083439618349075, train acc = 0.9850000143051147, time = 0.9498677253723145\n",
      "Training at step=42, batch=120, train loss = 0.06269438564777374, train acc = 0.9800000190734863, time = 0.9506208896636963\n",
      "Training at step=42, batch=150, train loss = 0.0715869590640068, train acc = 0.9800000190734863, time = 0.9716780185699463\n",
      "Training at step=42, batch=180, train loss = 0.10614006966352463, train acc = 0.949999988079071, time = 0.9517245292663574\n",
      "Training at step=42, batch=210, train loss = 0.056525178253650665, train acc = 0.9750000238418579, time = 0.9474341869354248\n",
      "Training at step=42, batch=240, train loss = 0.029584085568785667, train acc = 0.9900000095367432, time = 0.94598388671875\n",
      "Training at step=42, batch=270, train loss = 0.0841032862663269, train acc = 0.9700000286102295, time = 0.9513070583343506\n",
      "Testing at step=42, batch=0, test loss = 0.02648903615772724, test acc = 0.9900000095367432, time = 0.3536379337310791\n",
      "Testing at step=42, batch=5, test loss = 0.11405280977487564, test acc = 0.9800000190734863, time = 0.35130763053894043\n",
      "Testing at step=42, batch=10, test loss = 0.058308571577072144, test acc = 0.9800000190734863, time = 0.3539307117462158\n",
      "Testing at step=42, batch=15, test loss = 0.05399756878614426, test acc = 0.9750000238418579, time = 0.3570077419281006\n",
      "Testing at step=42, batch=20, test loss = 0.06227605417370796, test acc = 0.9900000095367432, time = 0.35362863540649414\n",
      "Testing at step=42, batch=25, test loss = 0.13993340730667114, test acc = 0.9750000238418579, time = 0.35356879234313965\n",
      "Testing at step=42, batch=30, test loss = 0.10435601323843002, test acc = 0.9700000286102295, time = 0.35313940048217773\n",
      "Testing at step=42, batch=35, test loss = 0.12795689702033997, test acc = 0.9750000238418579, time = 0.35260772705078125\n",
      "Testing at step=42, batch=40, test loss = 0.07549739629030228, test acc = 0.9750000238418579, time = 0.3554346561431885\n",
      "Testing at step=42, batch=45, test loss = 0.0508650504052639, test acc = 0.9850000143051147, time = 0.35281968116760254\n",
      "Step 42 finished in 314.8033103942871, Train loss = 0.05134502121247351, Test loss = 0.08657433526590466; Train Acc = 0.9834666794538498, Test Acc = 0.9756000137329102\n",
      "Training at step=43, batch=0, train loss = 0.030788030475378036, train acc = 0.9850000143051147, time = 0.9502565860748291\n",
      "Training at step=43, batch=30, train loss = 0.024450959637761116, train acc = 1.0, time = 0.9486114978790283\n",
      "Training at step=43, batch=60, train loss = 0.015532681718468666, train acc = 1.0, time = 0.9488925933837891\n",
      "Training at step=43, batch=90, train loss = 0.07555004209280014, train acc = 0.9750000238418579, time = 0.9474174976348877\n",
      "Training at step=43, batch=120, train loss = 0.025486096739768982, train acc = 0.9900000095367432, time = 0.9481627941131592\n",
      "Training at step=43, batch=150, train loss = 0.06091148778796196, train acc = 0.9800000190734863, time = 0.952125072479248\n",
      "Training at step=43, batch=180, train loss = 0.06408649682998657, train acc = 0.9700000286102295, time = 0.9500308036804199\n",
      "Training at step=43, batch=210, train loss = 0.039984531700611115, train acc = 0.9950000047683716, time = 0.9483480453491211\n",
      "Training at step=43, batch=240, train loss = 0.013510633260011673, train acc = 0.9950000047683716, time = 0.9477622509002686\n",
      "Training at step=43, batch=270, train loss = 0.04772251471877098, train acc = 0.9850000143051147, time = 0.949819803237915\n",
      "Testing at step=43, batch=0, test loss = 0.037519097328186035, test acc = 0.9900000095367432, time = 0.35089850425720215\n",
      "Testing at step=43, batch=5, test loss = 0.07754236459732056, test acc = 0.9700000286102295, time = 0.35021162033081055\n",
      "Testing at step=43, batch=10, test loss = 0.0775868147611618, test acc = 0.9900000095367432, time = 0.3533303737640381\n",
      "Testing at step=43, batch=15, test loss = 0.05644575506448746, test acc = 0.9850000143051147, time = 0.34955859184265137\n",
      "Testing at step=43, batch=20, test loss = 0.10512826591730118, test acc = 0.9700000286102295, time = 0.35041022300720215\n",
      "Testing at step=43, batch=25, test loss = 0.053568027913570404, test acc = 0.9900000095367432, time = 0.34952449798583984\n",
      "Testing at step=43, batch=30, test loss = 0.11157925426959991, test acc = 0.9750000238418579, time = 0.34992480278015137\n",
      "Testing at step=43, batch=35, test loss = 0.06324628740549088, test acc = 0.9750000238418579, time = 0.35333752632141113\n",
      "Testing at step=43, batch=40, test loss = 0.03899528831243515, test acc = 0.9800000190734863, time = 0.3500993251800537\n",
      "Testing at step=43, batch=45, test loss = 0.11404750496149063, test acc = 0.9700000286102295, time = 0.35108065605163574\n",
      "Step 43 finished in 314.5245053768158, Train loss = 0.053639072158063454, Test loss = 0.08558194264769554; Train Acc = 0.9829333464304606, Test Acc = 0.974400007724762\n",
      "Training at step=44, batch=0, train loss = 0.021019194275140762, train acc = 0.9950000047683716, time = 0.9487724304199219\n",
      "Training at step=44, batch=30, train loss = 0.05836664140224457, train acc = 0.9800000190734863, time = 0.9527993202209473\n",
      "Training at step=44, batch=60, train loss = 0.024155788123607635, train acc = 0.9900000095367432, time = 0.949770450592041\n",
      "Training at step=44, batch=90, train loss = 0.03197534382343292, train acc = 0.9900000095367432, time = 0.9500792026519775\n",
      "Training at step=44, batch=120, train loss = 0.05274806171655655, train acc = 0.9950000047683716, time = 0.948479175567627\n",
      "Training at step=44, batch=150, train loss = 0.0833645835518837, train acc = 0.9850000143051147, time = 0.9490659236907959\n",
      "Training at step=44, batch=180, train loss = 0.03677494078874588, train acc = 0.9850000143051147, time = 0.949235200881958\n",
      "Training at step=44, batch=210, train loss = 0.025057075545191765, train acc = 0.9950000047683716, time = 0.9519648551940918\n",
      "Training at step=44, batch=240, train loss = 0.05050448328256607, train acc = 0.9750000238418579, time = 0.9496655464172363\n",
      "Training at step=44, batch=270, train loss = 0.033252984285354614, train acc = 0.9850000143051147, time = 0.951153039932251\n",
      "Testing at step=44, batch=0, test loss = 0.09354012459516525, test acc = 0.9649999737739563, time = 0.3539621829986572\n",
      "Testing at step=44, batch=5, test loss = 0.03123020939528942, test acc = 0.9950000047683716, time = 0.353621244430542\n",
      "Testing at step=44, batch=10, test loss = 0.08372890204191208, test acc = 0.9700000286102295, time = 0.35486865043640137\n",
      "Testing at step=44, batch=15, test loss = 0.12224683910608292, test acc = 0.9800000190734863, time = 0.354412317276001\n",
      "Testing at step=44, batch=20, test loss = 0.04251592606306076, test acc = 0.9800000190734863, time = 0.35432910919189453\n",
      "Testing at step=44, batch=25, test loss = 0.07086582481861115, test acc = 0.9850000143051147, time = 0.353351354598999\n",
      "Testing at step=44, batch=30, test loss = 0.04379923641681671, test acc = 0.9800000190734863, time = 0.35400867462158203\n",
      "Testing at step=44, batch=35, test loss = 0.11651530861854553, test acc = 0.9700000286102295, time = 0.354841947555542\n",
      "Testing at step=44, batch=40, test loss = 0.04316820576786995, test acc = 0.9900000095367432, time = 0.35579729080200195\n",
      "Testing at step=44, batch=45, test loss = 0.07193739712238312, test acc = 0.9800000190734863, time = 0.35236573219299316\n",
      "Step 44 finished in 314.9461750984192, Train loss = 0.05048119990931203, Test loss = 0.09362917587161064; Train Acc = 0.9833000129461289, Test Acc = 0.9732000088691711\n",
      "Training at step=45, batch=0, train loss = 0.023207325488328934, train acc = 0.9950000047683716, time = 0.9489984512329102\n",
      "Training at step=45, batch=30, train loss = 0.02540154941380024, train acc = 1.0, time = 0.9546267986297607\n",
      "Training at step=45, batch=60, train loss = 0.0879192054271698, train acc = 0.9549999833106995, time = 0.9530661106109619\n",
      "Training at step=45, batch=90, train loss = 0.05116437375545502, train acc = 0.9850000143051147, time = 0.9508004188537598\n",
      "Training at step=45, batch=120, train loss = 0.08910924941301346, train acc = 0.9750000238418579, time = 0.9490718841552734\n",
      "Training at step=45, batch=150, train loss = 0.09035921096801758, train acc = 0.9750000238418579, time = 0.9485256671905518\n",
      "Training at step=45, batch=180, train loss = 0.05664035677909851, train acc = 0.9850000143051147, time = 0.9505538940429688\n",
      "Training at step=45, batch=210, train loss = 0.12091013044118881, train acc = 0.9700000286102295, time = 0.9570157527923584\n",
      "Training at step=45, batch=240, train loss = 0.05556361377239227, train acc = 0.9750000238418579, time = 0.951521635055542\n",
      "Training at step=45, batch=270, train loss = 0.01751675270497799, train acc = 1.0, time = 0.9494051933288574\n",
      "Testing at step=45, batch=0, test loss = 0.18705914914608002, test acc = 0.9549999833106995, time = 0.3530724048614502\n",
      "Testing at step=45, batch=5, test loss = 0.147239089012146, test acc = 0.9549999833106995, time = 0.3514385223388672\n",
      "Testing at step=45, batch=10, test loss = 0.06068478152155876, test acc = 0.9750000238418579, time = 0.35462141036987305\n",
      "Testing at step=45, batch=15, test loss = 0.037421613931655884, test acc = 0.9800000190734863, time = 0.3509807586669922\n",
      "Testing at step=45, batch=20, test loss = 0.1207960769534111, test acc = 0.949999988079071, time = 0.35042262077331543\n",
      "Testing at step=45, batch=25, test loss = 0.12266354262828827, test acc = 0.9649999737739563, time = 0.3510313034057617\n",
      "Testing at step=45, batch=30, test loss = 0.14785709977149963, test acc = 0.9549999833106995, time = 0.35034656524658203\n",
      "Testing at step=45, batch=35, test loss = 0.1255808025598526, test acc = 0.9800000190734863, time = 0.3514866828918457\n",
      "Testing at step=45, batch=40, test loss = 0.09996344894170761, test acc = 0.9700000286102295, time = 0.3506603240966797\n",
      "Testing at step=45, batch=45, test loss = 0.05751381069421768, test acc = 0.9800000190734863, time = 0.35082411766052246\n",
      "Step 45 finished in 314.90635442733765, Train loss = 0.049952761985672015, Test loss = 0.09276331804692745; Train Acc = 0.9838500130176544, Test Acc = 0.9718000078201294\n",
      "Training at step=46, batch=0, train loss = 0.03471214696764946, train acc = 0.9900000095367432, time = 0.9503164291381836\n",
      "Training at step=46, batch=30, train loss = 0.01670885644853115, train acc = 1.0, time = 0.9537186622619629\n",
      "Training at step=46, batch=60, train loss = 0.04387297481298447, train acc = 0.9750000238418579, time = 0.9556565284729004\n",
      "Training at step=46, batch=90, train loss = 0.0769684687256813, train acc = 0.9800000190734863, time = 0.9471163749694824\n",
      "Training at step=46, batch=120, train loss = 0.05576663091778755, train acc = 0.9800000190734863, time = 0.9562675952911377\n",
      "Training at step=46, batch=150, train loss = 0.15375782549381256, train acc = 0.9700000286102295, time = 0.9512982368469238\n",
      "Training at step=46, batch=180, train loss = 0.05797995999455452, train acc = 0.9800000190734863, time = 0.9520561695098877\n",
      "Training at step=46, batch=210, train loss = 0.06776750832796097, train acc = 0.9900000095367432, time = 0.954688549041748\n",
      "Training at step=46, batch=240, train loss = 0.06319525092840195, train acc = 0.9800000190734863, time = 0.9484412670135498\n",
      "Training at step=46, batch=270, train loss = 0.053311705589294434, train acc = 0.9850000143051147, time = 0.9505205154418945\n",
      "Testing at step=46, batch=0, test loss = 0.08846081048250198, test acc = 0.9800000190734863, time = 0.35436463356018066\n",
      "Testing at step=46, batch=5, test loss = 0.1502823382616043, test acc = 0.9700000286102295, time = 0.35509157180786133\n",
      "Testing at step=46, batch=10, test loss = 0.09682794660329819, test acc = 0.9750000238418579, time = 0.3537158966064453\n",
      "Testing at step=46, batch=15, test loss = 0.050963349640369415, test acc = 0.9850000143051147, time = 0.3524203300476074\n",
      "Testing at step=46, batch=20, test loss = 0.12899130582809448, test acc = 0.9700000286102295, time = 0.35375308990478516\n",
      "Testing at step=46, batch=25, test loss = 0.10001661628484726, test acc = 0.9649999737739563, time = 0.3541102409362793\n",
      "Testing at step=46, batch=30, test loss = 0.1457507461309433, test acc = 0.9599999785423279, time = 0.35446953773498535\n",
      "Testing at step=46, batch=35, test loss = 0.10759688168764114, test acc = 0.9649999737739563, time = 0.35284972190856934\n",
      "Testing at step=46, batch=40, test loss = 0.04970557242631912, test acc = 0.9900000095367432, time = 0.3528711795806885\n",
      "Testing at step=46, batch=45, test loss = 0.0810549333691597, test acc = 0.9700000286102295, time = 0.3520162105560303\n",
      "Step 46 finished in 315.0203974246979, Train loss = 0.048508006039385994, Test loss = 0.0928046334721148; Train Acc = 0.9840500140190125, Test Acc = 0.9741000080108643\n",
      "Training at step=47, batch=0, train loss = 0.03306770697236061, train acc = 0.9900000095367432, time = 0.9508635997772217\n",
      "Training at step=47, batch=30, train loss = 0.045685455203056335, train acc = 0.9850000143051147, time = 0.9500901699066162\n",
      "Training at step=47, batch=60, train loss = 0.0619225800037384, train acc = 0.9800000190734863, time = 0.9483382701873779\n",
      "Training at step=47, batch=90, train loss = 0.060085516422986984, train acc = 0.9750000238418579, time = 0.949129581451416\n",
      "Training at step=47, batch=120, train loss = 0.04493299499154091, train acc = 0.9700000286102295, time = 0.9501326084136963\n",
      "Training at step=47, batch=150, train loss = 0.05405249446630478, train acc = 0.9800000190734863, time = 0.9475424289703369\n",
      "Training at step=47, batch=180, train loss = 0.034962788224220276, train acc = 0.9900000095367432, time = 0.9487049579620361\n",
      "Training at step=47, batch=210, train loss = 0.038268495351076126, train acc = 0.9900000095367432, time = 0.9519333839416504\n",
      "Training at step=47, batch=240, train loss = 0.03136153891682625, train acc = 0.9900000095367432, time = 0.9694373607635498\n",
      "Training at step=47, batch=270, train loss = 0.036510828882455826, train acc = 0.9850000143051147, time = 0.9508039951324463\n",
      "Testing at step=47, batch=0, test loss = 0.1135459616780281, test acc = 0.9800000190734863, time = 0.35350871086120605\n",
      "Testing at step=47, batch=5, test loss = 0.07269714772701263, test acc = 0.9750000238418579, time = 0.3597276210784912\n",
      "Testing at step=47, batch=10, test loss = 0.12766996026039124, test acc = 0.9649999737739563, time = 0.35111427307128906\n",
      "Testing at step=47, batch=15, test loss = 0.05554526299238205, test acc = 0.9900000095367432, time = 0.3539125919342041\n",
      "Testing at step=47, batch=20, test loss = 0.06624370813369751, test acc = 0.9750000238418579, time = 0.3539600372314453\n",
      "Testing at step=47, batch=25, test loss = 0.05793873220682144, test acc = 0.9900000095367432, time = 0.3532826900482178\n",
      "Testing at step=47, batch=30, test loss = 0.12589889764785767, test acc = 0.9649999737739563, time = 0.35221314430236816\n",
      "Testing at step=47, batch=35, test loss = 0.03245196491479874, test acc = 0.9900000095367432, time = 0.3528151512145996\n",
      "Testing at step=47, batch=40, test loss = 0.03153834491968155, test acc = 0.9850000143051147, time = 0.3513603210449219\n",
      "Testing at step=47, batch=45, test loss = 0.06568624824285507, test acc = 0.9700000286102295, time = 0.3535163402557373\n",
      "Step 47 finished in 314.555787563324, Train loss = 0.04683069579924146, Test loss = 0.09083348862826825; Train Acc = 0.9846500124533971, Test Acc = 0.9748000073432922\n",
      "Training at step=48, batch=0, train loss = 0.04252740368247032, train acc = 0.9900000095367432, time = 0.9509377479553223\n",
      "Training at step=48, batch=30, train loss = 0.044687896966934204, train acc = 0.9850000143051147, time = 0.9490089416503906\n",
      "Training at step=48, batch=60, train loss = 0.020953362807631493, train acc = 0.9950000047683716, time = 0.9494326114654541\n",
      "Training at step=48, batch=90, train loss = 0.011582832783460617, train acc = 1.0, time = 0.9510409832000732\n",
      "Training at step=48, batch=120, train loss = 0.07661418616771698, train acc = 0.9649999737739563, time = 0.9500758647918701\n",
      "Training at step=48, batch=150, train loss = 0.02906004711985588, train acc = 0.9900000095367432, time = 0.9651541709899902\n",
      "Training at step=48, batch=180, train loss = 0.12676255404949188, train acc = 0.9750000238418579, time = 0.9509944915771484\n",
      "Training at step=48, batch=210, train loss = 0.04886389151215553, train acc = 0.9700000286102295, time = 0.9505138397216797\n",
      "Training at step=48, batch=240, train loss = 0.021818533539772034, train acc = 0.9900000095367432, time = 0.9483089447021484\n",
      "Training at step=48, batch=270, train loss = 0.029620375484228134, train acc = 0.9900000095367432, time = 0.9493696689605713\n",
      "Testing at step=48, batch=0, test loss = 0.10648517310619354, test acc = 0.9750000238418579, time = 0.352419376373291\n",
      "Testing at step=48, batch=5, test loss = 0.029535209760069847, test acc = 0.9950000047683716, time = 0.3497195243835449\n",
      "Testing at step=48, batch=10, test loss = 0.05674533545970917, test acc = 0.9750000238418579, time = 0.35309934616088867\n",
      "Testing at step=48, batch=15, test loss = 0.1430477648973465, test acc = 0.9750000238418579, time = 0.3527648448944092\n",
      "Testing at step=48, batch=20, test loss = 0.07777352631092072, test acc = 0.9800000190734863, time = 0.3529973030090332\n",
      "Testing at step=48, batch=25, test loss = 0.053367406129837036, test acc = 0.9850000143051147, time = 0.3539469242095947\n",
      "Testing at step=48, batch=30, test loss = 0.143811896443367, test acc = 0.9649999737739563, time = 0.3516581058502197\n",
      "Testing at step=48, batch=35, test loss = 0.17844100296497345, test acc = 0.949999988079071, time = 0.35245823860168457\n",
      "Testing at step=48, batch=40, test loss = 0.07758333534002304, test acc = 0.9800000190734863, time = 0.35279107093811035\n",
      "Testing at step=48, batch=45, test loss = 0.12494461238384247, test acc = 0.9649999737739563, time = 0.3533813953399658\n",
      "Step 48 finished in 314.70583033561707, Train loss = 0.047572416610394914, Test loss = 0.0944121154025197; Train Acc = 0.9844500132401784, Test Acc = 0.975000011920929\n",
      "Training at step=49, batch=0, train loss = 0.028466852381825447, train acc = 0.9900000095367432, time = 0.9514169692993164\n",
      "Training at step=49, batch=30, train loss = 0.06954073160886765, train acc = 0.9800000190734863, time = 0.9504306316375732\n",
      "Training at step=49, batch=60, train loss = 0.01425420492887497, train acc = 0.9950000047683716, time = 0.9515042304992676\n",
      "Training at step=49, batch=90, train loss = 0.09489277750253677, train acc = 0.9750000238418579, time = 0.9510848522186279\n",
      "Training at step=49, batch=120, train loss = 0.024718843400478363, train acc = 0.9900000095367432, time = 0.9516828060150146\n",
      "Training at step=49, batch=150, train loss = 0.04243950545787811, train acc = 0.9900000095367432, time = 0.9516639709472656\n",
      "Training at step=49, batch=180, train loss = 0.08217473328113556, train acc = 0.9649999737739563, time = 0.9497559070587158\n",
      "Training at step=49, batch=210, train loss = 0.06029864773154259, train acc = 0.9850000143051147, time = 0.9492833614349365\n",
      "Training at step=49, batch=240, train loss = 0.029749274253845215, train acc = 0.9850000143051147, time = 0.948932409286499\n",
      "Training at step=49, batch=270, train loss = 0.06930001080036163, train acc = 0.9700000286102295, time = 0.950615644454956\n",
      "Testing at step=49, batch=0, test loss = 0.14455996453762054, test acc = 0.9449999928474426, time = 0.37607526779174805\n",
      "Testing at step=49, batch=5, test loss = 0.03370391204953194, test acc = 0.9950000047683716, time = 0.35215282440185547\n",
      "Testing at step=49, batch=10, test loss = 0.054157596081495285, test acc = 0.9950000047683716, time = 0.3510298728942871\n",
      "Testing at step=49, batch=15, test loss = 0.16164201498031616, test acc = 0.9800000190734863, time = 0.3508157730102539\n",
      "Testing at step=49, batch=20, test loss = 0.16056926548480988, test acc = 0.9599999785423279, time = 0.3516581058502197\n",
      "Testing at step=49, batch=25, test loss = 0.12026326358318329, test acc = 0.9700000286102295, time = 0.352264404296875\n",
      "Testing at step=49, batch=30, test loss = 0.03400963917374611, test acc = 0.9900000095367432, time = 0.35042834281921387\n",
      "Testing at step=49, batch=35, test loss = 0.1325404793024063, test acc = 0.9549999833106995, time = 0.35129404067993164\n",
      "Testing at step=49, batch=40, test loss = 0.03580879420042038, test acc = 0.9800000190734863, time = 0.352276086807251\n",
      "Testing at step=49, batch=45, test loss = 0.12282443791627884, test acc = 0.9599999785423279, time = 0.3496429920196533\n",
      "Step 49 finished in 314.6117432117462, Train loss = 0.04647124992528309, Test loss = 0.08721257165074349; Train Acc = 0.9848000113169352, Test Acc = 0.9747000074386597\n",
      "Training at step=50, batch=0, train loss = 0.046682704240083694, train acc = 0.9800000190734863, time = 0.9540116786956787\n",
      "Training at step=50, batch=30, train loss = 0.03817012906074524, train acc = 0.9800000190734863, time = 0.9489798545837402\n",
      "Training at step=50, batch=60, train loss = 0.04913894087076187, train acc = 0.9800000190734863, time = 0.9473299980163574\n",
      "Training at step=50, batch=90, train loss = 0.0235401913523674, train acc = 0.9950000047683716, time = 0.9608776569366455\n",
      "Training at step=50, batch=120, train loss = 0.05533149838447571, train acc = 0.9800000190734863, time = 0.9484014511108398\n",
      "Training at step=50, batch=150, train loss = 0.08794450759887695, train acc = 0.9649999737739563, time = 0.9490156173706055\n",
      "Training at step=50, batch=180, train loss = 0.054979875683784485, train acc = 0.9700000286102295, time = 0.9491045475006104\n",
      "Training at step=50, batch=210, train loss = 0.02567426487803459, train acc = 0.9900000095367432, time = 0.9472637176513672\n",
      "Training at step=50, batch=240, train loss = 0.06252069771289825, train acc = 0.9800000190734863, time = 0.9499540328979492\n",
      "Training at step=50, batch=270, train loss = 0.05768529325723648, train acc = 0.9850000143051147, time = 0.950352668762207\n",
      "Testing at step=50, batch=0, test loss = 0.1208537369966507, test acc = 0.9700000286102295, time = 0.3505258560180664\n",
      "Testing at step=50, batch=5, test loss = 0.05964858829975128, test acc = 0.9850000143051147, time = 0.3505086898803711\n",
      "Testing at step=50, batch=10, test loss = 0.04211295023560524, test acc = 0.9700000286102295, time = 0.3519465923309326\n",
      "Testing at step=50, batch=15, test loss = 0.09036885201931, test acc = 0.9800000190734863, time = 0.3514683246612549\n",
      "Testing at step=50, batch=20, test loss = 0.11397943645715714, test acc = 0.9649999737739563, time = 0.35216808319091797\n",
      "Testing at step=50, batch=25, test loss = 0.09550844132900238, test acc = 0.9850000143051147, time = 0.35096263885498047\n",
      "Testing at step=50, batch=30, test loss = 0.10267173498868942, test acc = 0.9599999785423279, time = 0.3511648178100586\n",
      "Testing at step=50, batch=35, test loss = 0.14579959213733673, test acc = 0.9549999833106995, time = 0.35086488723754883\n",
      "Testing at step=50, batch=40, test loss = 0.12895098328590393, test acc = 0.9700000286102295, time = 0.353466272354126\n",
      "Testing at step=50, batch=45, test loss = 0.0488542877137661, test acc = 0.9800000190734863, time = 0.35042643547058105\n",
      "Step 50 finished in 314.33592438697815, Train loss = 0.044862717548385264, Test loss = 0.08740614388138056; Train Acc = 0.9858333458503087, Test Acc = 0.9757000124454498\n",
      "Training at step=51, batch=0, train loss = 0.0835728570818901, train acc = 0.9850000143051147, time = 0.9467294216156006\n",
      "Training at step=51, batch=30, train loss = 0.017752444371581078, train acc = 0.9950000047683716, time = 0.9500086307525635\n",
      "Training at step=51, batch=60, train loss = 0.07450542598962784, train acc = 0.9750000238418579, time = 0.9512691497802734\n",
      "Training at step=51, batch=90, train loss = 0.0450487844645977, train acc = 0.9750000238418579, time = 0.9529149532318115\n",
      "Training at step=51, batch=120, train loss = 0.08511757105588913, train acc = 0.9800000190734863, time = 0.9522497653961182\n",
      "Training at step=51, batch=150, train loss = 0.03149062767624855, train acc = 0.9850000143051147, time = 0.9497182369232178\n",
      "Training at step=51, batch=180, train loss = 0.041230928152799606, train acc = 0.9900000095367432, time = 0.9554779529571533\n",
      "Training at step=51, batch=210, train loss = 0.03173507750034332, train acc = 0.9950000047683716, time = 0.950812816619873\n",
      "Training at step=51, batch=240, train loss = 0.025431470945477486, train acc = 0.9900000095367432, time = 0.95198655128479\n",
      "Training at step=51, batch=270, train loss = 0.07370701432228088, train acc = 0.9900000095367432, time = 0.9530363082885742\n",
      "Testing at step=51, batch=0, test loss = 0.15909001231193542, test acc = 0.9649999737739563, time = 0.3550844192504883\n",
      "Testing at step=51, batch=5, test loss = 0.06025051698088646, test acc = 0.9850000143051147, time = 0.35819578170776367\n",
      "Testing at step=51, batch=10, test loss = 0.12298346310853958, test acc = 0.9800000190734863, time = 0.35320258140563965\n",
      "Testing at step=51, batch=15, test loss = 0.027809614315629005, test acc = 0.9850000143051147, time = 0.35352182388305664\n",
      "Testing at step=51, batch=20, test loss = 0.06336300075054169, test acc = 0.9800000190734863, time = 0.3550112247467041\n",
      "Testing at step=51, batch=25, test loss = 0.08805976808071136, test acc = 0.9700000286102295, time = 0.3535177707672119\n",
      "Testing at step=51, batch=30, test loss = 0.07415857911109924, test acc = 0.9700000286102295, time = 0.3548722267150879\n",
      "Testing at step=51, batch=35, test loss = 0.10781785845756531, test acc = 0.9700000286102295, time = 0.4000101089477539\n",
      "Testing at step=51, batch=40, test loss = 0.10905903577804565, test acc = 0.9700000286102295, time = 0.35170555114746094\n",
      "Testing at step=51, batch=45, test loss = 0.12112089991569519, test acc = 0.9800000190734863, time = 0.35149288177490234\n",
      "Step 51 finished in 315.3406386375427, Train loss = 0.04283558370700727, Test loss = 0.0916150515154004; Train Acc = 0.985716679294904, Test Acc = 0.9740000128746032\n",
      "Training at step=52, batch=0, train loss = 0.062000516802072525, train acc = 0.9850000143051147, time = 0.9504594802856445\n",
      "Training at step=52, batch=30, train loss = 0.03534086048603058, train acc = 0.9800000190734863, time = 0.9507796764373779\n",
      "Training at step=52, batch=60, train loss = 0.014277376234531403, train acc = 0.9950000047683716, time = 0.9488263130187988\n",
      "Training at step=52, batch=90, train loss = 0.05832701176404953, train acc = 0.9900000095367432, time = 0.9526817798614502\n",
      "Training at step=52, batch=120, train loss = 0.024915141984820366, train acc = 0.9900000095367432, time = 0.9530189037322998\n",
      "Training at step=52, batch=150, train loss = 0.04336689040064812, train acc = 0.9900000095367432, time = 0.950139045715332\n",
      "Training at step=52, batch=180, train loss = 0.045284949243068695, train acc = 0.9800000190734863, time = 0.9501152038574219\n",
      "Training at step=52, batch=210, train loss = 0.08903674781322479, train acc = 0.9800000190734863, time = 0.9534366130828857\n",
      "Training at step=52, batch=240, train loss = 0.03641191124916077, train acc = 0.9900000095367432, time = 0.951866865158081\n",
      "Training at step=52, batch=270, train loss = 0.027979129925370216, train acc = 0.9900000095367432, time = 0.9497132301330566\n",
      "Testing at step=52, batch=0, test loss = 0.12830017507076263, test acc = 0.9700000286102295, time = 0.35169553756713867\n",
      "Testing at step=52, batch=5, test loss = 0.13838329911231995, test acc = 0.9700000286102295, time = 0.3519110679626465\n",
      "Testing at step=52, batch=10, test loss = 0.09857887029647827, test acc = 0.9700000286102295, time = 0.3523592948913574\n",
      "Testing at step=52, batch=15, test loss = 0.18050043284893036, test acc = 0.949999988079071, time = 0.3519151210784912\n",
      "Testing at step=52, batch=20, test loss = 0.1394186168909073, test acc = 0.9700000286102295, time = 0.35159730911254883\n",
      "Testing at step=52, batch=25, test loss = 0.08190365135669708, test acc = 0.9750000238418579, time = 0.3500094413757324\n",
      "Testing at step=52, batch=30, test loss = 0.06604503095149994, test acc = 0.9800000190734863, time = 0.35153865814208984\n",
      "Testing at step=52, batch=35, test loss = 0.11855635046958923, test acc = 0.9649999737739563, time = 0.3509700298309326\n",
      "Testing at step=52, batch=40, test loss = 0.17289236187934875, test acc = 0.9549999833106995, time = 0.35050153732299805\n",
      "Testing at step=52, batch=45, test loss = 0.1129726767539978, test acc = 0.9649999737739563, time = 0.35274434089660645\n",
      "Step 52 finished in 314.69453740119934, Train loss = 0.04274481062001238, Test loss = 0.0931179877370596; Train Acc = 0.9859166787068049, Test Acc = 0.9756000101566314\n",
      "Training at step=53, batch=0, train loss = 0.08811582624912262, train acc = 0.9750000238418579, time = 0.9494006633758545\n",
      "Training at step=53, batch=30, train loss = 0.018900273367762566, train acc = 0.9950000047683716, time = 0.9513087272644043\n",
      "Training at step=53, batch=60, train loss = 0.017974751070141792, train acc = 0.9900000095367432, time = 0.9488756656646729\n",
      "Training at step=53, batch=90, train loss = 0.03671044111251831, train acc = 0.9900000095367432, time = 0.9482519626617432\n",
      "Training at step=53, batch=120, train loss = 0.018935203552246094, train acc = 0.9950000047683716, time = 0.9624371528625488\n",
      "Training at step=53, batch=150, train loss = 0.025043204426765442, train acc = 0.9950000047683716, time = 0.9487881660461426\n",
      "Training at step=53, batch=180, train loss = 0.04913066327571869, train acc = 0.9900000095367432, time = 0.9477956295013428\n",
      "Training at step=53, batch=210, train loss = 0.039012063294649124, train acc = 0.9900000095367432, time = 0.9501688480377197\n",
      "Training at step=53, batch=240, train loss = 0.0627303496003151, train acc = 0.9750000238418579, time = 0.9500465393066406\n",
      "Training at step=53, batch=270, train loss = 0.02311338484287262, train acc = 1.0, time = 0.9487195014953613\n",
      "Testing at step=53, batch=0, test loss = 0.03969753533601761, test acc = 0.9850000143051147, time = 0.3508765697479248\n",
      "Testing at step=53, batch=5, test loss = 0.04365457594394684, test acc = 0.9850000143051147, time = 0.3496577739715576\n",
      "Testing at step=53, batch=10, test loss = 0.0286320298910141, test acc = 0.9950000047683716, time = 0.35130786895751953\n",
      "Testing at step=53, batch=15, test loss = 0.13212572038173676, test acc = 0.9750000238418579, time = 0.3491485118865967\n",
      "Testing at step=53, batch=20, test loss = 0.08157002180814743, test acc = 0.9800000190734863, time = 0.39632606506347656\n",
      "Testing at step=53, batch=25, test loss = 0.12033601105213165, test acc = 0.9750000238418579, time = 0.352078914642334\n",
      "Testing at step=53, batch=30, test loss = 0.021077096462249756, test acc = 0.9900000095367432, time = 0.3514893054962158\n",
      "Testing at step=53, batch=35, test loss = 0.16055679321289062, test acc = 0.9750000238418579, time = 0.35154104232788086\n",
      "Testing at step=53, batch=40, test loss = 0.061920490115880966, test acc = 0.9850000143051147, time = 0.3500525951385498\n",
      "Testing at step=53, batch=45, test loss = 0.0787670835852623, test acc = 0.9800000190734863, time = 0.35291218757629395\n",
      "Step 53 finished in 314.5056390762329, Train loss = 0.04222655996369819, Test loss = 0.09148193091154098; Train Acc = 0.9868000115950902, Test Acc = 0.9749000084400177\n",
      "Training at step=54, batch=0, train loss = 0.029789790511131287, train acc = 0.9950000047683716, time = 0.9495654106140137\n",
      "Training at step=54, batch=30, train loss = 0.028761988505721092, train acc = 0.9950000047683716, time = 0.9524335861206055\n",
      "Training at step=54, batch=60, train loss = 0.027775457128882408, train acc = 0.9900000095367432, time = 0.9476232528686523\n",
      "Training at step=54, batch=90, train loss = 0.01986854523420334, train acc = 0.9950000047683716, time = 0.9490010738372803\n",
      "Training at step=54, batch=120, train loss = 0.024527037516236305, train acc = 0.9900000095367432, time = 0.947340726852417\n",
      "Training at step=54, batch=150, train loss = 0.04812515899538994, train acc = 0.9900000095367432, time = 0.9494187831878662\n",
      "Training at step=54, batch=180, train loss = 0.054153647273778915, train acc = 0.9800000190734863, time = 0.9494216442108154\n",
      "Training at step=54, batch=210, train loss = 0.07681406289339066, train acc = 0.9750000238418579, time = 0.9509294033050537\n",
      "Training at step=54, batch=240, train loss = 0.02387113869190216, train acc = 0.9900000095367432, time = 0.9496173858642578\n",
      "Training at step=54, batch=270, train loss = 0.031065061688423157, train acc = 0.9950000047683716, time = 0.9491169452667236\n",
      "Testing at step=54, batch=0, test loss = 0.12941457331180573, test acc = 0.9599999785423279, time = 0.3523261547088623\n",
      "Testing at step=54, batch=5, test loss = 0.037155549973249435, test acc = 0.9850000143051147, time = 0.3526134490966797\n",
      "Testing at step=54, batch=10, test loss = 0.04445566609501839, test acc = 0.9800000190734863, time = 0.35062575340270996\n",
      "Testing at step=54, batch=15, test loss = 0.08143016695976257, test acc = 0.9900000095367432, time = 0.35091280937194824\n",
      "Testing at step=54, batch=20, test loss = 0.10143003612756729, test acc = 0.9750000238418579, time = 0.3520188331604004\n",
      "Testing at step=54, batch=25, test loss = 0.21304625272750854, test acc = 0.9599999785423279, time = 0.3513374328613281\n",
      "Testing at step=54, batch=30, test loss = 0.14739970862865448, test acc = 0.9599999785423279, time = 0.3532130718231201\n",
      "Testing at step=54, batch=35, test loss = 0.13255414366722107, test acc = 0.9599999785423279, time = 0.35562586784362793\n",
      "Testing at step=54, batch=40, test loss = 0.057522937655448914, test acc = 0.9850000143051147, time = 0.35207295417785645\n",
      "Testing at step=54, batch=45, test loss = 0.06454159319400787, test acc = 0.9649999737739563, time = 0.35207676887512207\n",
      "Step 54 finished in 314.7033951282501, Train loss = 0.04075788286398165, Test loss = 0.09706120751798153; Train Acc = 0.9870500119527181, Test Acc = 0.972400004863739\n",
      "Training at step=55, batch=0, train loss = 0.054596949368715286, train acc = 0.9900000095367432, time = 0.9529128074645996\n",
      "Training at step=55, batch=30, train loss = 0.0517440140247345, train acc = 0.9900000095367432, time = 0.9498059749603271\n",
      "Training at step=55, batch=60, train loss = 0.03354299068450928, train acc = 0.9800000190734863, time = 0.9498660564422607\n",
      "Training at step=55, batch=90, train loss = 0.04249907657504082, train acc = 0.9850000143051147, time = 0.9489185810089111\n",
      "Training at step=55, batch=120, train loss = 0.017741719260811806, train acc = 0.9950000047683716, time = 0.9484970569610596\n",
      "Training at step=55, batch=150, train loss = 0.040933553129434586, train acc = 0.9900000095367432, time = 0.9494266510009766\n",
      "Training at step=55, batch=180, train loss = 0.04106583446264267, train acc = 0.9950000047683716, time = 0.9486145973205566\n",
      "Training at step=55, batch=210, train loss = 0.015404539182782173, train acc = 1.0, time = 0.9496798515319824\n",
      "Training at step=55, batch=240, train loss = 0.07647905498743057, train acc = 0.9800000190734863, time = 0.94980788230896\n",
      "Training at step=55, batch=270, train loss = 0.04055780544877052, train acc = 0.9900000095367432, time = 0.9495031833648682\n",
      "Testing at step=55, batch=0, test loss = 0.05390729755163193, test acc = 0.9850000143051147, time = 0.3525357246398926\n",
      "Testing at step=55, batch=5, test loss = 0.07460790872573853, test acc = 0.9700000286102295, time = 0.3515598773956299\n",
      "Testing at step=55, batch=10, test loss = 0.0707380548119545, test acc = 0.9900000095367432, time = 0.35126495361328125\n",
      "Testing at step=55, batch=15, test loss = 0.007522940635681152, test acc = 1.0, time = 0.3651738166809082\n",
      "Testing at step=55, batch=20, test loss = 0.10459139198064804, test acc = 0.9700000286102295, time = 0.3563876152038574\n",
      "Testing at step=55, batch=25, test loss = 0.11188021302223206, test acc = 0.9599999785423279, time = 0.35094571113586426\n",
      "Testing at step=55, batch=30, test loss = 0.04619087651371956, test acc = 0.9850000143051147, time = 0.3511495590209961\n",
      "Testing at step=55, batch=35, test loss = 0.1054382249712944, test acc = 0.9700000286102295, time = 0.35112929344177246\n",
      "Testing at step=55, batch=40, test loss = 0.051011573523283005, test acc = 0.9900000095367432, time = 0.35186004638671875\n",
      "Testing at step=55, batch=45, test loss = 0.1262999325990677, test acc = 0.9700000286102295, time = 0.35308098793029785\n",
      "Step 55 finished in 314.87802815437317, Train loss = 0.04261737642344087, Test loss = 0.09983454436063767; Train Acc = 0.9857166783014933, Test Acc = 0.9723000061511994\n",
      "Training at step=56, batch=0, train loss = 0.05050893872976303, train acc = 0.9800000190734863, time = 1.0026874542236328\n",
      "Training at step=56, batch=30, train loss = 0.057016801089048386, train acc = 0.9900000095367432, time = 0.9492859840393066\n",
      "Training at step=56, batch=60, train loss = 0.05142321437597275, train acc = 0.9900000095367432, time = 0.9521024227142334\n",
      "Training at step=56, batch=90, train loss = 0.016668502241373062, train acc = 0.9950000047683716, time = 0.9496569633483887\n",
      "Training at step=56, batch=120, train loss = 0.1293783038854599, train acc = 0.9549999833106995, time = 0.9521164894104004\n",
      "Training at step=56, batch=150, train loss = 0.03140006959438324, train acc = 0.9900000095367432, time = 0.9495151042938232\n",
      "Training at step=56, batch=180, train loss = 0.019194910302758217, train acc = 0.9950000047683716, time = 0.9489920139312744\n",
      "Training at step=56, batch=210, train loss = 0.015239696949720383, train acc = 0.9950000047683716, time = 0.9495806694030762\n",
      "Training at step=56, batch=240, train loss = 0.028100525960326195, train acc = 0.9850000143051147, time = 0.9527356624603271\n",
      "Training at step=56, batch=270, train loss = 0.057246651500463486, train acc = 0.9800000190734863, time = 0.9468181133270264\n",
      "Testing at step=56, batch=0, test loss = 0.10704073309898376, test acc = 0.9599999785423279, time = 0.351149320602417\n",
      "Testing at step=56, batch=5, test loss = 0.07454858720302582, test acc = 0.9800000190734863, time = 0.3508589267730713\n",
      "Testing at step=56, batch=10, test loss = 0.08771178871393204, test acc = 0.9750000238418579, time = 0.35032010078430176\n",
      "Testing at step=56, batch=15, test loss = 0.10417895019054413, test acc = 0.9800000190734863, time = 0.34910106658935547\n",
      "Testing at step=56, batch=20, test loss = 0.16158099472522736, test acc = 0.9649999737739563, time = 0.34997129440307617\n",
      "Testing at step=56, batch=25, test loss = 0.06650665402412415, test acc = 0.9700000286102295, time = 0.3496677875518799\n",
      "Testing at step=56, batch=30, test loss = 0.14407789707183838, test acc = 0.9599999785423279, time = 0.35014963150024414\n",
      "Testing at step=56, batch=35, test loss = 0.05196181312203407, test acc = 0.9850000143051147, time = 0.34908175468444824\n",
      "Testing at step=56, batch=40, test loss = 0.11442145705223083, test acc = 0.9850000143051147, time = 0.3505721092224121\n",
      "Testing at step=56, batch=45, test loss = 0.07862087339162827, test acc = 0.9750000238418579, time = 0.3504643440246582\n",
      "Step 56 finished in 314.5989990234375, Train loss = 0.03810976088202248, Test loss = 0.10357915449887514; Train Acc = 0.9879000105460485, Test Acc = 0.9745000100135803\n",
      "Training at step=57, batch=0, train loss = 0.02517872117459774, train acc = 0.9950000047683716, time = 0.9494962692260742\n",
      "Training at step=57, batch=30, train loss = 0.04760146141052246, train acc = 0.9850000143051147, time = 0.9511125087738037\n",
      "Training at step=57, batch=60, train loss = 0.03669861704111099, train acc = 0.9900000095367432, time = 0.9493005275726318\n",
      "Training at step=57, batch=90, train loss = 0.03379075229167938, train acc = 0.9850000143051147, time = 0.949700117111206\n",
      "Training at step=57, batch=120, train loss = 0.02811763808131218, train acc = 0.9900000095367432, time = 0.9501910209655762\n",
      "Training at step=57, batch=150, train loss = 0.05330834910273552, train acc = 0.9800000190734863, time = 0.9518089294433594\n",
      "Training at step=57, batch=180, train loss = 0.034938737750053406, train acc = 0.9950000047683716, time = 0.9534401893615723\n",
      "Training at step=57, batch=210, train loss = 0.04097817465662956, train acc = 0.9850000143051147, time = 0.948500394821167\n",
      "Training at step=57, batch=240, train loss = 0.05437588319182396, train acc = 0.9750000238418579, time = 0.9520506858825684\n",
      "Training at step=57, batch=270, train loss = 0.013723213225603104, train acc = 0.9950000047683716, time = 0.9512834548950195\n",
      "Testing at step=57, batch=0, test loss = 0.09930498898029327, test acc = 0.9750000238418579, time = 0.3524656295776367\n",
      "Testing at step=57, batch=5, test loss = 0.2098204791545868, test acc = 0.9599999785423279, time = 0.35201478004455566\n",
      "Testing at step=57, batch=10, test loss = 0.07651302963495255, test acc = 0.9700000286102295, time = 0.3508424758911133\n",
      "Testing at step=57, batch=15, test loss = 0.05344586446881294, test acc = 0.9800000190734863, time = 0.3533210754394531\n",
      "Testing at step=57, batch=20, test loss = 0.12315309792757034, test acc = 0.9549999833106995, time = 0.35286569595336914\n",
      "Testing at step=57, batch=25, test loss = 0.12698093056678772, test acc = 0.9649999737739563, time = 0.3501739501953125\n",
      "Testing at step=57, batch=30, test loss = 0.08322349190711975, test acc = 0.9750000238418579, time = 0.35201048851013184\n",
      "Testing at step=57, batch=35, test loss = 0.06407596915960312, test acc = 0.9750000238418579, time = 0.3518519401550293\n",
      "Testing at step=57, batch=40, test loss = 0.0889592245221138, test acc = 0.9750000238418579, time = 0.35346007347106934\n",
      "Testing at step=57, batch=45, test loss = 0.07444141805171967, test acc = 0.9800000190734863, time = 0.35469722747802734\n",
      "Step 57 finished in 314.88401675224304, Train loss = 0.03962018519407138, Test loss = 0.09689698033034802; Train Acc = 0.9868666785955429, Test Acc = 0.9731000089645385\n",
      "Training at step=58, batch=0, train loss = 0.0455147884786129, train acc = 0.9800000190734863, time = 0.9506962299346924\n",
      "Training at step=58, batch=30, train loss = 0.060397449880838394, train acc = 0.9800000190734863, time = 0.9481868743896484\n",
      "Training at step=58, batch=60, train loss = 0.012885508127510548, train acc = 0.9950000047683716, time = 0.9498476982116699\n",
      "Training at step=58, batch=90, train loss = 0.012765472754836082, train acc = 1.0, time = 0.9476339817047119\n",
      "Training at step=58, batch=120, train loss = 0.039979249238967896, train acc = 0.9850000143051147, time = 0.9496779441833496\n",
      "Training at step=58, batch=150, train loss = 0.04665157198905945, train acc = 0.9750000238418579, time = 0.9473412036895752\n",
      "Training at step=58, batch=180, train loss = 0.03161603957414627, train acc = 0.9850000143051147, time = 0.9496860504150391\n",
      "Training at step=58, batch=210, train loss = 0.01206471212208271, train acc = 0.9950000047683716, time = 0.9497103691101074\n",
      "Training at step=58, batch=240, train loss = 0.05503109097480774, train acc = 0.9800000190734863, time = 0.9516808986663818\n",
      "Training at step=58, batch=270, train loss = 0.018656639382243156, train acc = 0.9900000095367432, time = 0.9502415657043457\n",
      "Testing at step=58, batch=0, test loss = 0.04167145863175392, test acc = 0.9800000190734863, time = 0.3541228771209717\n",
      "Testing at step=58, batch=5, test loss = 0.1254773885011673, test acc = 0.9700000286102295, time = 0.35597729682922363\n",
      "Testing at step=58, batch=10, test loss = 0.013894901610910892, test acc = 0.9950000047683716, time = 0.35315537452697754\n",
      "Testing at step=58, batch=15, test loss = 0.04468857869505882, test acc = 0.9800000190734863, time = 0.35237884521484375\n",
      "Testing at step=58, batch=20, test loss = 0.19269829988479614, test acc = 0.9350000023841858, time = 0.40676379203796387\n",
      "Testing at step=58, batch=25, test loss = 0.11370182782411575, test acc = 0.9800000190734863, time = 0.354478120803833\n",
      "Testing at step=58, batch=30, test loss = 0.1292484700679779, test acc = 0.9599999785423279, time = 0.35231876373291016\n",
      "Testing at step=58, batch=35, test loss = 0.04303184524178505, test acc = 0.9850000143051147, time = 0.354794979095459\n",
      "Testing at step=58, batch=40, test loss = 0.06670071929693222, test acc = 0.9750000238418579, time = 0.35669589042663574\n",
      "Testing at step=58, batch=45, test loss = 0.13125842809677124, test acc = 0.949999988079071, time = 0.35260438919067383\n",
      "Step 58 finished in 314.83152055740356, Train loss = 0.03710686067895343, Test loss = 0.0970692015066743; Train Acc = 0.9878666774431865, Test Acc = 0.9733000087738037\n",
      "Training at step=59, batch=0, train loss = 0.028866060078144073, train acc = 0.9900000095367432, time = 0.9520602226257324\n",
      "Training at step=59, batch=30, train loss = 0.023843979462981224, train acc = 0.9950000047683716, time = 0.9523932933807373\n",
      "Training at step=59, batch=60, train loss = 0.04402424395084381, train acc = 0.9800000190734863, time = 0.9520471096038818\n",
      "Training at step=59, batch=90, train loss = 0.01131351012736559, train acc = 0.9950000047683716, time = 0.9501850605010986\n",
      "Training at step=59, batch=120, train loss = 0.07078836113214493, train acc = 0.9750000238418579, time = 0.9544625282287598\n",
      "Training at step=59, batch=150, train loss = 0.03561880812048912, train acc = 0.9800000190734863, time = 0.9490361213684082\n",
      "Training at step=59, batch=180, train loss = 0.0898023247718811, train acc = 0.9850000143051147, time = 0.9506912231445312\n",
      "Training at step=59, batch=210, train loss = 0.03377172350883484, train acc = 0.9900000095367432, time = 0.9520425796508789\n",
      "Training at step=59, batch=240, train loss = 0.09449873864650726, train acc = 0.9750000238418579, time = 0.9503862857818604\n",
      "Training at step=59, batch=270, train loss = 0.01688644289970398, train acc = 0.9950000047683716, time = 0.9493899345397949\n",
      "Testing at step=59, batch=0, test loss = 0.14935123920440674, test acc = 0.9700000286102295, time = 0.3525071144104004\n",
      "Testing at step=59, batch=5, test loss = 0.03015410341322422, test acc = 0.9900000095367432, time = 0.3545830249786377\n",
      "Testing at step=59, batch=10, test loss = 0.14878316223621368, test acc = 0.9649999737739563, time = 0.35368943214416504\n",
      "Testing at step=59, batch=15, test loss = 0.23250748217105865, test acc = 0.9549999833106995, time = 0.3569796085357666\n",
      "Testing at step=59, batch=20, test loss = 0.08166196197271347, test acc = 0.9750000238418579, time = 0.351611852645874\n",
      "Testing at step=59, batch=25, test loss = 0.0773582011461258, test acc = 0.9750000238418579, time = 0.35201358795166016\n",
      "Testing at step=59, batch=30, test loss = 0.1149045079946518, test acc = 0.9649999737739563, time = 0.3544349670410156\n",
      "Testing at step=59, batch=35, test loss = 0.10272989422082901, test acc = 0.9750000238418579, time = 0.38210487365722656\n",
      "Testing at step=59, batch=40, test loss = 0.06607873737812042, test acc = 0.9700000286102295, time = 0.3505859375\n",
      "Testing at step=59, batch=45, test loss = 0.20100894570350647, test acc = 0.949999988079071, time = 0.3909878730773926\n",
      "Step 59 finished in 314.9102666378021, Train loss = 0.03793619397406777, Test loss = 0.10225170519202947; Train Acc = 0.9874833446741104, Test Acc = 0.9715000033378601\n",
      "Training at step=60, batch=0, train loss = 0.05073415860533714, train acc = 0.9750000238418579, time = 0.9527409076690674\n",
      "Training at step=60, batch=30, train loss = 0.02052980661392212, train acc = 0.9950000047683716, time = 0.9492642879486084\n",
      "Training at step=60, batch=60, train loss = 0.04615360125899315, train acc = 0.9950000047683716, time = 0.948678731918335\n",
      "Training at step=60, batch=90, train loss = 0.02038167230784893, train acc = 0.9950000047683716, time = 0.9505689144134521\n",
      "Training at step=60, batch=120, train loss = 0.0252822358161211, train acc = 0.9900000095367432, time = 0.9474799633026123\n",
      "Training at step=60, batch=150, train loss = 0.024355683475732803, train acc = 0.9900000095367432, time = 0.9491395950317383\n",
      "Training at step=60, batch=180, train loss = 0.07263760268688202, train acc = 0.9750000238418579, time = 0.9514145851135254\n",
      "Training at step=60, batch=210, train loss = 0.08910679817199707, train acc = 0.9700000286102295, time = 0.9508159160614014\n",
      "Training at step=60, batch=240, train loss = 0.03992311283946037, train acc = 0.9900000095367432, time = 0.9487838745117188\n",
      "Training at step=60, batch=270, train loss = 0.02947198413312435, train acc = 0.9850000143051147, time = 0.9486300945281982\n",
      "Testing at step=60, batch=0, test loss = 0.03932942822575569, test acc = 0.9800000190734863, time = 0.35451531410217285\n",
      "Testing at step=60, batch=5, test loss = 0.08578085899353027, test acc = 0.9800000190734863, time = 0.35403990745544434\n",
      "Testing at step=60, batch=10, test loss = 0.10522951930761337, test acc = 0.949999988079071, time = 0.36717653274536133\n",
      "Testing at step=60, batch=15, test loss = 0.1457897126674652, test acc = 0.9700000286102295, time = 0.352344274520874\n",
      "Testing at step=60, batch=20, test loss = 0.18295840919017792, test acc = 0.9599999785423279, time = 0.35432863235473633\n",
      "Testing at step=60, batch=25, test loss = 0.18679212033748627, test acc = 0.9549999833106995, time = 0.3536696434020996\n",
      "Testing at step=60, batch=30, test loss = 0.20619717240333557, test acc = 0.9649999737739563, time = 0.35502123832702637\n",
      "Testing at step=60, batch=35, test loss = 0.089180126786232, test acc = 0.9649999737739563, time = 0.3554110527038574\n",
      "Testing at step=60, batch=40, test loss = 0.07899951189756393, test acc = 0.9800000190734863, time = 0.3517336845397949\n",
      "Testing at step=60, batch=45, test loss = 0.23079660534858704, test acc = 0.949999988079071, time = 0.35288333892822266\n",
      "Step 60 finished in 314.6310706138611, Train loss = 0.03630381292508294, Test loss = 0.10492711279541254; Train Acc = 0.9877666781346003, Test Acc = 0.973500007390976\n",
      "Training at step=61, batch=0, train loss = 0.029626013711094856, train acc = 0.9900000095367432, time = 0.9493463039398193\n",
      "Training at step=61, batch=30, train loss = 0.04218338429927826, train acc = 0.9850000143051147, time = 0.9483213424682617\n",
      "Training at step=61, batch=60, train loss = 0.043306101113557816, train acc = 0.9800000190734863, time = 0.9535574913024902\n",
      "Training at step=61, batch=90, train loss = 0.046129260212183, train acc = 0.9750000238418579, time = 0.9523472785949707\n",
      "Training at step=61, batch=120, train loss = 0.004413792863488197, train acc = 1.0, time = 0.9529018402099609\n",
      "Training at step=61, batch=150, train loss = 0.07039694488048553, train acc = 0.9700000286102295, time = 0.951385498046875\n",
      "Training at step=61, batch=180, train loss = 0.027314774692058563, train acc = 0.9900000095367432, time = 0.9491746425628662\n",
      "Training at step=61, batch=210, train loss = 0.028224075213074684, train acc = 0.9950000047683716, time = 0.9495432376861572\n",
      "Training at step=61, batch=240, train loss = 0.012161743827164173, train acc = 1.0, time = 0.9480185508728027\n",
      "Training at step=61, batch=270, train loss = 0.03850129246711731, train acc = 0.9850000143051147, time = 0.9512906074523926\n",
      "Testing at step=61, batch=0, test loss = 0.0688638761639595, test acc = 0.9800000190734863, time = 0.3642456531524658\n",
      "Testing at step=61, batch=5, test loss = 0.0479247085750103, test acc = 0.9850000143051147, time = 0.3516819477081299\n",
      "Testing at step=61, batch=10, test loss = 0.1090666651725769, test acc = 0.9800000190734863, time = 0.3526430130004883\n",
      "Testing at step=61, batch=15, test loss = 0.10444759577512741, test acc = 0.9800000190734863, time = 0.3506746292114258\n",
      "Testing at step=61, batch=20, test loss = 0.033491894602775574, test acc = 0.9900000095367432, time = 0.3549227714538574\n",
      "Testing at step=61, batch=25, test loss = 0.09513165801763535, test acc = 0.9800000190734863, time = 0.35593605041503906\n",
      "Testing at step=61, batch=30, test loss = 0.13733109831809998, test acc = 0.9750000238418579, time = 0.34886789321899414\n",
      "Testing at step=61, batch=35, test loss = 0.10089080780744553, test acc = 0.9700000286102295, time = 0.3851335048675537\n",
      "Testing at step=61, batch=40, test loss = 0.09136417508125305, test acc = 0.9700000286102295, time = 0.35878896713256836\n",
      "Testing at step=61, batch=45, test loss = 0.045983728021383286, test acc = 0.9850000143051147, time = 0.35125041007995605\n",
      "Step 61 finished in 314.87664246559143, Train loss = 0.03679217264599477, Test loss = 0.09170316889882088; Train Acc = 0.9877333438396454, Test Acc = 0.9745000147819519\n",
      "Training at step=62, batch=0, train loss = 0.02184647135436535, train acc = 0.9950000047683716, time = 0.9476487636566162\n",
      "Training at step=62, batch=30, train loss = 0.0892118290066719, train acc = 0.9800000190734863, time = 0.948563814163208\n",
      "Training at step=62, batch=60, train loss = 0.009029148146510124, train acc = 1.0, time = 0.9487473964691162\n",
      "Training at step=62, batch=90, train loss = 0.044708408415317535, train acc = 0.9900000095367432, time = 0.9469337463378906\n",
      "Training at step=62, batch=120, train loss = 0.060838356614112854, train acc = 0.9850000143051147, time = 0.9466769695281982\n",
      "Training at step=62, batch=150, train loss = 0.01594405807554722, train acc = 0.9950000047683716, time = 0.953803539276123\n",
      "Training at step=62, batch=180, train loss = 0.029511068016290665, train acc = 0.9900000095367432, time = 0.9512612819671631\n",
      "Training at step=62, batch=210, train loss = 0.033940114080905914, train acc = 0.9850000143051147, time = 0.9486303329467773\n",
      "Training at step=62, batch=240, train loss = 0.08965887874364853, train acc = 0.9850000143051147, time = 0.9513657093048096\n",
      "Training at step=62, batch=270, train loss = 0.059404630213975906, train acc = 0.9800000190734863, time = 0.9511532783508301\n",
      "Testing at step=62, batch=0, test loss = 0.048029348254203796, test acc = 0.9800000190734863, time = 0.3592844009399414\n",
      "Testing at step=62, batch=5, test loss = 0.11506441980600357, test acc = 0.9700000286102295, time = 0.35001134872436523\n",
      "Testing at step=62, batch=10, test loss = 0.11824412643909454, test acc = 0.9649999737739563, time = 0.3521275520324707\n",
      "Testing at step=62, batch=15, test loss = 0.04537877067923546, test acc = 0.9850000143051147, time = 0.352741003036499\n",
      "Testing at step=62, batch=20, test loss = 0.09980769455432892, test acc = 0.9649999737739563, time = 0.35004758834838867\n",
      "Testing at step=62, batch=25, test loss = 0.1418095976114273, test acc = 0.9700000286102295, time = 0.35187268257141113\n",
      "Testing at step=62, batch=30, test loss = 0.16338157653808594, test acc = 0.9599999785423279, time = 0.353407621383667\n",
      "Testing at step=62, batch=35, test loss = 0.10536380112171173, test acc = 0.9649999737739563, time = 0.35016536712646484\n",
      "Testing at step=62, batch=40, test loss = 0.10903171449899673, test acc = 0.9850000143051147, time = 0.35074329376220703\n",
      "Testing at step=62, batch=45, test loss = 0.04981204494833946, test acc = 0.9750000238418579, time = 0.3666412830352783\n",
      "Step 62 finished in 314.4628622531891, Train loss = 0.035200212787215905, Test loss = 0.09484899584203958; Train Acc = 0.988550010919571, Test Acc = 0.9736000072956085\n",
      "Training at step=63, batch=0, train loss = 0.021014636382460594, train acc = 0.9950000047683716, time = 0.9482181072235107\n",
      "Training at step=63, batch=30, train loss = 0.037233807146549225, train acc = 0.9900000095367432, time = 0.9490289688110352\n",
      "Training at step=63, batch=60, train loss = 0.0417591817677021, train acc = 0.9850000143051147, time = 0.9494965076446533\n",
      "Training at step=63, batch=90, train loss = 0.014839164912700653, train acc = 0.9950000047683716, time = 0.9530050754547119\n",
      "Training at step=63, batch=120, train loss = 0.06377732008695602, train acc = 0.9800000190734863, time = 0.949986457824707\n",
      "Training at step=63, batch=150, train loss = 0.028319649398326874, train acc = 0.9900000095367432, time = 0.9512808322906494\n",
      "Training at step=63, batch=180, train loss = 0.0678768903017044, train acc = 0.9800000190734863, time = 0.9495618343353271\n",
      "Training at step=63, batch=210, train loss = 0.027975954115390778, train acc = 0.9900000095367432, time = 0.949108362197876\n",
      "Training at step=63, batch=240, train loss = 0.05285654217004776, train acc = 0.9800000190734863, time = 0.950742244720459\n",
      "Training at step=63, batch=270, train loss = 0.01181392278522253, train acc = 0.9950000047683716, time = 0.9509344100952148\n",
      "Testing at step=63, batch=0, test loss = 0.08648218214511871, test acc = 0.9850000143051147, time = 0.35315656661987305\n",
      "Testing at step=63, batch=5, test loss = 0.06067119166254997, test acc = 0.9750000238418579, time = 0.34995532035827637\n",
      "Testing at step=63, batch=10, test loss = 0.1304825097322464, test acc = 0.9649999737739563, time = 0.3495218753814697\n",
      "Testing at step=63, batch=15, test loss = 0.0980021059513092, test acc = 0.9700000286102295, time = 0.35117316246032715\n",
      "Testing at step=63, batch=20, test loss = 0.10241935402154922, test acc = 0.9549999833106995, time = 0.35311198234558105\n",
      "Testing at step=63, batch=25, test loss = 0.02945571020245552, test acc = 0.9950000047683716, time = 0.3505702018737793\n",
      "Testing at step=63, batch=30, test loss = 0.10716462880373001, test acc = 0.9649999737739563, time = 0.3499903678894043\n",
      "Testing at step=63, batch=35, test loss = 0.07524722814559937, test acc = 0.9800000190734863, time = 0.35109806060791016\n",
      "Testing at step=63, batch=40, test loss = 0.12855733931064606, test acc = 0.9700000286102295, time = 0.3523707389831543\n",
      "Testing at step=63, batch=45, test loss = 0.13457530736923218, test acc = 0.9649999737739563, time = 0.34966588020324707\n",
      "Step 63 finished in 314.7427303791046, Train loss = 0.03576473058744644, Test loss = 0.08921900052577257; Train Acc = 0.9880333439509074, Test Acc = 0.9765000116825103\n",
      "Training at step=64, batch=0, train loss = 0.028523987159132957, train acc = 0.9850000143051147, time = 0.9514498710632324\n",
      "Training at step=64, batch=30, train loss = 0.020280513912439346, train acc = 0.9950000047683716, time = 0.9499356746673584\n",
      "Training at step=64, batch=60, train loss = 0.020184049382805824, train acc = 0.9950000047683716, time = 0.9491066932678223\n",
      "Training at step=64, batch=90, train loss = 0.06793983280658722, train acc = 0.9800000190734863, time = 0.9495124816894531\n",
      "Training at step=64, batch=120, train loss = 0.028113558888435364, train acc = 0.9950000047683716, time = 0.9525089263916016\n",
      "Training at step=64, batch=150, train loss = 0.010300558060407639, train acc = 1.0, time = 0.9496886730194092\n",
      "Training at step=64, batch=180, train loss = 0.04623359069228172, train acc = 0.9950000047683716, time = 0.9482653141021729\n",
      "Training at step=64, batch=210, train loss = 0.011904638260602951, train acc = 1.0, time = 0.9489846229553223\n",
      "Training at step=64, batch=240, train loss = 0.013713202439248562, train acc = 0.9950000047683716, time = 0.9471309185028076\n",
      "Training at step=64, batch=270, train loss = 0.052126165479421616, train acc = 0.9750000238418579, time = 0.9506592750549316\n",
      "Testing at step=64, batch=0, test loss = 0.17392171919345856, test acc = 0.9649999737739563, time = 0.3549919128417969\n",
      "Testing at step=64, batch=5, test loss = 0.049841202795505524, test acc = 0.9850000143051147, time = 0.3521554470062256\n",
      "Testing at step=64, batch=10, test loss = 0.1324889212846756, test acc = 0.9700000286102295, time = 0.3516101837158203\n",
      "Testing at step=64, batch=15, test loss = 0.12931175529956818, test acc = 0.9700000286102295, time = 0.3525419235229492\n",
      "Testing at step=64, batch=20, test loss = 0.06148498132824898, test acc = 0.9750000238418579, time = 0.3497283458709717\n",
      "Testing at step=64, batch=25, test loss = 0.1328568011522293, test acc = 0.9649999737739563, time = 0.353135347366333\n",
      "Testing at step=64, batch=30, test loss = 0.07383006811141968, test acc = 0.9800000190734863, time = 0.3529961109161377\n",
      "Testing at step=64, batch=35, test loss = 0.08549995720386505, test acc = 0.9750000238418579, time = 0.3513157367706299\n",
      "Testing at step=64, batch=40, test loss = 0.07778725773096085, test acc = 0.9649999737739563, time = 0.3499417304992676\n",
      "Testing at step=64, batch=45, test loss = 0.12864121794700623, test acc = 0.9700000286102295, time = 0.3516993522644043\n",
      "Step 64 finished in 314.66811084747314, Train loss = 0.033741914705994226, Test loss = 0.10415920343250036; Train Acc = 0.9888500102361043, Test Acc = 0.9730000078678132\n",
      "Training at step=65, batch=0, train loss = 0.03582783043384552, train acc = 0.9800000190734863, time = 0.9554524421691895\n",
      "Training at step=65, batch=30, train loss = 0.02253354713320732, train acc = 0.9900000095367432, time = 0.9541456699371338\n",
      "Training at step=65, batch=60, train loss = 0.013310756534337997, train acc = 1.0, time = 0.9542884826660156\n",
      "Training at step=65, batch=90, train loss = 0.025711610913276672, train acc = 0.9900000095367432, time = 0.9541950225830078\n",
      "Training at step=65, batch=120, train loss = 0.05868343263864517, train acc = 0.9900000095367432, time = 0.9511198997497559\n",
      "Training at step=65, batch=150, train loss = 0.04573896527290344, train acc = 0.9950000047683716, time = 0.9488508701324463\n",
      "Training at step=65, batch=180, train loss = 0.012866092845797539, train acc = 1.0, time = 0.9514353275299072\n",
      "Training at step=65, batch=210, train loss = 0.04634786769747734, train acc = 0.9700000286102295, time = 0.9540109634399414\n",
      "Training at step=65, batch=240, train loss = 0.01938028074800968, train acc = 0.9900000095367432, time = 0.9516556262969971\n",
      "Training at step=65, batch=270, train loss = 0.041770901530981064, train acc = 0.9800000190734863, time = 0.9479877948760986\n",
      "Testing at step=65, batch=0, test loss = 0.1054784283041954, test acc = 0.9750000238418579, time = 0.37046265602111816\n",
      "Testing at step=65, batch=5, test loss = 0.1328587681055069, test acc = 0.9700000286102295, time = 0.3697371482849121\n",
      "Testing at step=65, batch=10, test loss = 0.06722436845302582, test acc = 0.9850000143051147, time = 0.3513071537017822\n",
      "Testing at step=65, batch=15, test loss = 0.08404019474983215, test acc = 0.9800000190734863, time = 0.3515655994415283\n",
      "Testing at step=65, batch=20, test loss = 0.04887160286307335, test acc = 0.9850000143051147, time = 0.3521728515625\n",
      "Testing at step=65, batch=25, test loss = 0.07723870128393173, test acc = 0.9800000190734863, time = 0.35468363761901855\n",
      "Testing at step=65, batch=30, test loss = 0.1385515332221985, test acc = 0.9750000238418579, time = 0.35126805305480957\n",
      "Testing at step=65, batch=35, test loss = 0.07088680565357208, test acc = 0.9900000095367432, time = 0.35452771186828613\n",
      "Testing at step=65, batch=40, test loss = 0.18509578704833984, test acc = 0.9549999833106995, time = 0.35297274589538574\n",
      "Testing at step=65, batch=45, test loss = 0.08861906081438065, test acc = 0.9700000286102295, time = 0.3503386974334717\n",
      "Step 65 finished in 314.9410517215729, Train loss = 0.03343835181634252, Test loss = 0.09747500989586115; Train Acc = 0.9890666768948237, Test Acc = 0.9749000132083893\n",
      "Training at step=66, batch=0, train loss = 0.060956720262765884, train acc = 0.9800000190734863, time = 0.9506118297576904\n",
      "Training at step=66, batch=30, train loss = 0.02396681345999241, train acc = 0.9900000095367432, time = 0.9471385478973389\n",
      "Training at step=66, batch=60, train loss = 0.01307904627174139, train acc = 0.9950000047683716, time = 0.9506077766418457\n",
      "Training at step=66, batch=90, train loss = 0.011060196906328201, train acc = 1.0, time = 0.951155424118042\n",
      "Training at step=66, batch=120, train loss = 0.036799587309360504, train acc = 0.9900000095367432, time = 0.9498746395111084\n",
      "Training at step=66, batch=150, train loss = 0.032673321664333344, train acc = 0.9800000190734863, time = 0.9503576755523682\n",
      "Training at step=66, batch=180, train loss = 0.02157113142311573, train acc = 0.9850000143051147, time = 0.9517202377319336\n",
      "Training at step=66, batch=210, train loss = 0.01629967987537384, train acc = 0.9950000047683716, time = 0.9523093700408936\n",
      "Training at step=66, batch=240, train loss = 0.10538097470998764, train acc = 0.9850000143051147, time = 0.950554370880127\n",
      "Training at step=66, batch=270, train loss = 0.029094712808728218, train acc = 0.9950000047683716, time = 0.9499244689941406\n",
      "Testing at step=66, batch=0, test loss = 0.07162586599588394, test acc = 0.9800000190734863, time = 0.351823091506958\n",
      "Testing at step=66, batch=5, test loss = 0.09939244389533997, test acc = 0.9750000238418579, time = 0.3517189025878906\n",
      "Testing at step=66, batch=10, test loss = 0.10147839784622192, test acc = 0.9750000238418579, time = 0.3494555950164795\n",
      "Testing at step=66, batch=15, test loss = 0.04065476357936859, test acc = 0.9950000047683716, time = 0.34972143173217773\n",
      "Testing at step=66, batch=20, test loss = 0.0394117496907711, test acc = 0.9900000095367432, time = 0.3516533374786377\n",
      "Testing at step=66, batch=25, test loss = 0.264671266078949, test acc = 0.9599999785423279, time = 0.35225439071655273\n",
      "Testing at step=66, batch=30, test loss = 0.1434081792831421, test acc = 0.9649999737739563, time = 0.3572859764099121\n",
      "Testing at step=66, batch=35, test loss = 0.11332662403583527, test acc = 0.9750000238418579, time = 0.3512427806854248\n",
      "Testing at step=66, batch=40, test loss = 0.1685507595539093, test acc = 0.9449999928474426, time = 0.3606250286102295\n",
      "Testing at step=66, batch=45, test loss = 0.06402553617954254, test acc = 0.9750000238418579, time = 0.3508930206298828\n",
      "Step 66 finished in 314.7654519081116, Train loss = 0.03250424024493744, Test loss = 0.10409714095294476; Train Acc = 0.9892833431561788, Test Acc = 0.9743000102043152\n",
      "Training at step=67, batch=0, train loss = 0.007694674655795097, train acc = 1.0, time = 0.9498622417449951\n",
      "Training at step=67, batch=30, train loss = 0.01983756572008133, train acc = 0.9900000095367432, time = 0.9498155117034912\n",
      "Training at step=67, batch=60, train loss = 0.007695310283452272, train acc = 1.0, time = 0.9490327835083008\n",
      "Training at step=67, batch=90, train loss = 0.0331856943666935, train acc = 0.9900000095367432, time = 0.9486203193664551\n",
      "Training at step=67, batch=120, train loss = 0.025803107768297195, train acc = 0.9900000095367432, time = 0.9493374824523926\n",
      "Training at step=67, batch=150, train loss = 0.03915158659219742, train acc = 0.9850000143051147, time = 0.9508538246154785\n",
      "Training at step=67, batch=180, train loss = 0.030724693089723587, train acc = 0.9850000143051147, time = 0.9500260353088379\n",
      "Training at step=67, batch=210, train loss = 0.018562927842140198, train acc = 0.9850000143051147, time = 0.9489140510559082\n",
      "Training at step=67, batch=240, train loss = 0.01174530852586031, train acc = 1.0, time = 0.9472856521606445\n",
      "Training at step=67, batch=270, train loss = 0.04030356556177139, train acc = 0.9850000143051147, time = 0.9707438945770264\n",
      "Testing at step=67, batch=0, test loss = 0.030135786160826683, test acc = 0.9850000143051147, time = 0.3538625240325928\n",
      "Testing at step=67, batch=5, test loss = 0.056061748415231705, test acc = 0.9800000190734863, time = 0.35008859634399414\n",
      "Testing at step=67, batch=10, test loss = 0.15358200669288635, test acc = 0.9649999737739563, time = 0.3499467372894287\n",
      "Testing at step=67, batch=15, test loss = 0.11162768304347992, test acc = 0.9649999737739563, time = 0.35198092460632324\n",
      "Testing at step=67, batch=20, test loss = 0.09775038808584213, test acc = 0.9750000238418579, time = 0.3509252071380615\n",
      "Testing at step=67, batch=25, test loss = 0.04987884685397148, test acc = 0.9900000095367432, time = 0.35191965103149414\n",
      "Testing at step=67, batch=30, test loss = 0.04656915366649628, test acc = 0.9750000238418579, time = 0.3551664352416992\n",
      "Testing at step=67, batch=35, test loss = 0.0884428322315216, test acc = 0.9750000238418579, time = 0.3494596481323242\n",
      "Testing at step=67, batch=40, test loss = 0.1381485015153885, test acc = 0.9599999785423279, time = 0.3507404327392578\n",
      "Testing at step=67, batch=45, test loss = 0.1326172798871994, test acc = 0.9700000286102295, time = 0.3520946502685547\n",
      "Step 67 finished in 314.3833634853363, Train loss = 0.03222495273919776, Test loss = 0.10537989240139722; Train Acc = 0.989000009894371, Test Acc = 0.9724000036716461\n",
      "Training at step=68, batch=0, train loss = 0.029226701706647873, train acc = 0.9850000143051147, time = 0.9481179714202881\n",
      "Training at step=68, batch=30, train loss = 0.04871707037091255, train acc = 0.9850000143051147, time = 0.9477939605712891\n",
      "Training at step=68, batch=60, train loss = 0.08386663347482681, train acc = 0.9700000286102295, time = 0.9532954692840576\n",
      "Training at step=68, batch=90, train loss = 0.049713049083948135, train acc = 0.9950000047683716, time = 0.9506728649139404\n",
      "Training at step=68, batch=120, train loss = 0.029093287885189056, train acc = 0.9850000143051147, time = 0.9500973224639893\n",
      "Training at step=68, batch=150, train loss = 0.02071419171988964, train acc = 0.9900000095367432, time = 0.9483332633972168\n",
      "Training at step=68, batch=180, train loss = 0.02311377041041851, train acc = 0.9950000047683716, time = 0.953589916229248\n",
      "Training at step=68, batch=210, train loss = 0.023547982797026634, train acc = 0.9900000095367432, time = 0.9509131908416748\n",
      "Training at step=68, batch=240, train loss = 0.04267437383532524, train acc = 0.9850000143051147, time = 0.9491689205169678\n",
      "Training at step=68, batch=270, train loss = 0.05096130818128586, train acc = 0.9850000143051147, time = 0.9487950801849365\n",
      "Testing at step=68, batch=0, test loss = 0.026775915175676346, test acc = 0.9900000095367432, time = 0.35164332389831543\n",
      "Testing at step=68, batch=5, test loss = 0.1583699733018875, test acc = 0.9549999833106995, time = 0.3518993854522705\n",
      "Testing at step=68, batch=10, test loss = 0.08767425268888474, test acc = 0.9700000286102295, time = 0.35024595260620117\n",
      "Testing at step=68, batch=15, test loss = 0.10603342205286026, test acc = 0.9850000143051147, time = 0.3539862632751465\n",
      "Testing at step=68, batch=20, test loss = 0.06653901189565659, test acc = 0.9850000143051147, time = 0.35294055938720703\n",
      "Testing at step=68, batch=25, test loss = 0.12910524010658264, test acc = 0.9700000286102295, time = 0.3712608814239502\n",
      "Testing at step=68, batch=30, test loss = 0.01635831966996193, test acc = 0.9950000047683716, time = 0.3519303798675537\n",
      "Testing at step=68, batch=35, test loss = 0.07904543727636337, test acc = 0.9649999737739563, time = 0.3510005474090576\n",
      "Testing at step=68, batch=40, test loss = 0.06780590862035751, test acc = 0.9800000190734863, time = 0.35349464416503906\n",
      "Testing at step=68, batch=45, test loss = 0.07623757421970367, test acc = 0.9800000190734863, time = 0.3517122268676758\n",
      "Step 68 finished in 314.6782217025757, Train loss = 0.03068277610776325, Test loss = 0.098205160908401; Train Acc = 0.99013334274292, Test Acc = 0.975500009059906\n",
      "Training at step=69, batch=0, train loss = 0.006853064522147179, train acc = 1.0, time = 0.948500394821167\n",
      "Training at step=69, batch=30, train loss = 0.023839883506298065, train acc = 0.9950000047683716, time = 0.9502553939819336\n",
      "Training at step=69, batch=60, train loss = 0.018212532624602318, train acc = 0.9900000095367432, time = 0.9509711265563965\n",
      "Training at step=69, batch=90, train loss = 0.0312969870865345, train acc = 0.9900000095367432, time = 0.949552059173584\n",
      "Training at step=69, batch=120, train loss = 0.01285528764128685, train acc = 0.9950000047683716, time = 0.9512119293212891\n",
      "Training at step=69, batch=150, train loss = 0.009181614965200424, train acc = 0.9950000047683716, time = 0.9547021389007568\n",
      "Training at step=69, batch=180, train loss = 0.016371874138712883, train acc = 0.9950000047683716, time = 0.9498336315155029\n",
      "Training at step=69, batch=210, train loss = 0.03931739926338196, train acc = 0.9950000047683716, time = 0.9500601291656494\n",
      "Training at step=69, batch=240, train loss = 0.008781904354691505, train acc = 1.0, time = 0.9505774974822998\n",
      "Training at step=69, batch=270, train loss = 0.010176843032240868, train acc = 0.9950000047683716, time = 0.9498558044433594\n",
      "Testing at step=69, batch=0, test loss = 0.05043456703424454, test acc = 0.9850000143051147, time = 0.35461854934692383\n",
      "Testing at step=69, batch=5, test loss = 0.10107941925525665, test acc = 0.9750000238418579, time = 0.354475736618042\n",
      "Testing at step=69, batch=10, test loss = 0.21004845201969147, test acc = 0.9449999928474426, time = 0.35316991806030273\n",
      "Testing at step=69, batch=15, test loss = 0.09247585386037827, test acc = 0.9750000238418579, time = 0.3533933162689209\n",
      "Testing at step=69, batch=20, test loss = 0.06937271356582642, test acc = 0.9800000190734863, time = 0.3520984649658203\n",
      "Testing at step=69, batch=25, test loss = 0.11530590057373047, test acc = 0.9649999737739563, time = 0.35442376136779785\n",
      "Testing at step=69, batch=30, test loss = 0.08482998609542847, test acc = 0.9700000286102295, time = 0.3540611267089844\n",
      "Testing at step=69, batch=35, test loss = 0.11541583389043808, test acc = 0.9649999737739563, time = 0.3531014919281006\n",
      "Testing at step=69, batch=40, test loss = 0.04500620812177658, test acc = 0.9900000095367432, time = 0.3551180362701416\n",
      "Testing at step=69, batch=45, test loss = 0.06159710884094238, test acc = 0.9800000190734863, time = 0.3526184558868408\n",
      "Step 69 finished in 314.66821479797363, Train loss = 0.029988793983745078, Test loss = 0.10017938524484635; Train Acc = 0.9904166754086813, Test Acc = 0.9749000108242035\n",
      "Training at step=70, batch=0, train loss = 0.0314151830971241, train acc = 0.9950000047683716, time = 0.951939582824707\n",
      "Training at step=70, batch=30, train loss = 0.01719539798796177, train acc = 0.9950000047683716, time = 0.9490385055541992\n",
      "Training at step=70, batch=60, train loss = 0.016141891479492188, train acc = 0.9950000047683716, time = 0.9499526023864746\n",
      "Training at step=70, batch=90, train loss = 0.012472203001379967, train acc = 0.9950000047683716, time = 0.9517734050750732\n",
      "Training at step=70, batch=120, train loss = 0.015241708606481552, train acc = 0.9950000047683716, time = 0.9519588947296143\n",
      "Training at step=70, batch=150, train loss = 0.033296916633844376, train acc = 0.9850000143051147, time = 0.9514272212982178\n",
      "Training at step=70, batch=180, train loss = 0.011092067696154118, train acc = 0.9950000047683716, time = 0.9520678520202637\n",
      "Training at step=70, batch=210, train loss = 0.032337892800569534, train acc = 0.9800000190734863, time = 0.9522075653076172\n",
      "Training at step=70, batch=240, train loss = 0.10105667263269424, train acc = 0.9800000190734863, time = 0.9518957138061523\n",
      "Training at step=70, batch=270, train loss = 0.014456438831984997, train acc = 0.9950000047683716, time = 0.9484882354736328\n",
      "Testing at step=70, batch=0, test loss = 0.12228740006685257, test acc = 0.9599999785423279, time = 0.35376572608947754\n",
      "Testing at step=70, batch=5, test loss = 0.005726523231714964, test acc = 1.0, time = 0.3526589870452881\n",
      "Testing at step=70, batch=10, test loss = 0.0478312149643898, test acc = 0.9750000238418579, time = 0.35518717765808105\n",
      "Testing at step=70, batch=15, test loss = 0.02413848601281643, test acc = 0.9950000047683716, time = 0.3529071807861328\n",
      "Testing at step=70, batch=20, test loss = 0.16756339371204376, test acc = 0.9599999785423279, time = 0.3525857925415039\n",
      "Testing at step=70, batch=25, test loss = 0.07385644316673279, test acc = 0.9800000190734863, time = 0.35581350326538086\n",
      "Testing at step=70, batch=30, test loss = 0.02688967064023018, test acc = 0.9800000190734863, time = 0.35318589210510254\n",
      "Testing at step=70, batch=35, test loss = 0.1853555291891098, test acc = 0.9599999785423279, time = 0.3519713878631592\n",
      "Testing at step=70, batch=40, test loss = 0.08654069900512695, test acc = 0.9800000190734863, time = 0.35173726081848145\n",
      "Testing at step=70, batch=45, test loss = 0.04688193276524544, test acc = 0.9800000190734863, time = 0.3537750244140625\n",
      "Step 70 finished in 314.85389590263367, Train loss = 0.029988146948162465, Test loss = 0.09848350412212312; Train Acc = 0.9899500091870626, Test Acc = 0.9754000091552735\n",
      "Training at step=71, batch=0, train loss = 0.016244756057858467, train acc = 0.9950000047683716, time = 0.9525904655456543\n",
      "Training at step=71, batch=30, train loss = 0.018762685358524323, train acc = 1.0, time = 0.9487552642822266\n",
      "Training at step=71, batch=60, train loss = 0.04418277367949486, train acc = 0.9850000143051147, time = 0.949573278427124\n",
      "Training at step=71, batch=90, train loss = 0.011676863767206669, train acc = 1.0, time = 0.9498505592346191\n",
      "Training at step=71, batch=120, train loss = 0.015570792369544506, train acc = 0.9950000047683716, time = 0.950728178024292\n",
      "Training at step=71, batch=150, train loss = 0.047320667654275894, train acc = 0.9800000190734863, time = 0.958573579788208\n",
      "Training at step=71, batch=180, train loss = 0.009389009326696396, train acc = 0.9950000047683716, time = 0.951117753982544\n",
      "Training at step=71, batch=210, train loss = 0.03657792508602142, train acc = 0.9850000143051147, time = 0.9482548236846924\n",
      "Training at step=71, batch=240, train loss = 0.04801945760846138, train acc = 0.9900000095367432, time = 0.9487545490264893\n",
      "Training at step=71, batch=270, train loss = 0.023783216252923012, train acc = 0.9900000095367432, time = 0.948317289352417\n",
      "Testing at step=71, batch=0, test loss = 0.08364376425743103, test acc = 0.9850000143051147, time = 0.3527510166168213\n",
      "Testing at step=71, batch=5, test loss = 0.06032286584377289, test acc = 0.9800000190734863, time = 0.35280656814575195\n",
      "Testing at step=71, batch=10, test loss = 0.11063889414072037, test acc = 0.9800000190734863, time = 0.35208892822265625\n",
      "Testing at step=71, batch=15, test loss = 0.0402042381465435, test acc = 0.9900000095367432, time = 0.35403990745544434\n",
      "Testing at step=71, batch=20, test loss = 0.14676281809806824, test acc = 0.9800000190734863, time = 0.3536520004272461\n",
      "Testing at step=71, batch=25, test loss = 0.06861940026283264, test acc = 0.9800000190734863, time = 0.3516685962677002\n",
      "Testing at step=71, batch=30, test loss = 0.08283597975969315, test acc = 0.9850000143051147, time = 0.3515963554382324\n",
      "Testing at step=71, batch=35, test loss = 0.17582279443740845, test acc = 0.9599999785423279, time = 0.3536105155944824\n",
      "Testing at step=71, batch=40, test loss = 0.06566060334444046, test acc = 0.9700000286102295, time = 0.3513360023498535\n",
      "Testing at step=71, batch=45, test loss = 0.06390557438135147, test acc = 0.9850000143051147, time = 0.3544309139251709\n",
      "Step 71 finished in 314.75264620780945, Train loss = 0.029884866756231834, Test loss = 0.10227354653179646; Train Acc = 0.9899500087896983, Test Acc = 0.9758000111579895\n",
      "Training at step=72, batch=0, train loss = 0.012256563641130924, train acc = 1.0, time = 0.9502758979797363\n",
      "Training at step=72, batch=30, train loss = 0.008977357298135757, train acc = 1.0, time = 0.9498844146728516\n",
      "Training at step=72, batch=60, train loss = 0.015829872339963913, train acc = 1.0, time = 0.9492254257202148\n",
      "Training at step=72, batch=90, train loss = 0.026663288474082947, train acc = 0.9900000095367432, time = 0.9541893005371094\n",
      "Training at step=72, batch=120, train loss = 0.059664882719516754, train acc = 0.9850000143051147, time = 0.952369213104248\n",
      "Training at step=72, batch=150, train loss = 0.02516855299472809, train acc = 0.9900000095367432, time = 0.9497261047363281\n",
      "Training at step=72, batch=180, train loss = 0.044569578021764755, train acc = 0.9850000143051147, time = 0.9484386444091797\n",
      "Training at step=72, batch=210, train loss = 0.0053084129467606544, train acc = 1.0, time = 0.9498839378356934\n",
      "Training at step=72, batch=240, train loss = 0.04981313645839691, train acc = 0.9850000143051147, time = 0.9493408203125\n",
      "Training at step=72, batch=270, train loss = 0.026212677359580994, train acc = 0.9900000095367432, time = 0.951493501663208\n",
      "Testing at step=72, batch=0, test loss = 0.21387583017349243, test acc = 0.9750000238418579, time = 0.3534553050994873\n",
      "Testing at step=72, batch=5, test loss = 0.044528983533382416, test acc = 0.9900000095367432, time = 0.35179591178894043\n",
      "Testing at step=72, batch=10, test loss = 0.10516135394573212, test acc = 0.9800000190734863, time = 0.3513317108154297\n",
      "Testing at step=72, batch=15, test loss = 0.029820775613188744, test acc = 0.9850000143051147, time = 0.3530852794647217\n",
      "Testing at step=72, batch=20, test loss = 0.15734189748764038, test acc = 0.9549999833106995, time = 0.351499080657959\n",
      "Testing at step=72, batch=25, test loss = 0.032782990485429764, test acc = 0.9850000143051147, time = 0.35373950004577637\n",
      "Testing at step=72, batch=30, test loss = 0.1893368363380432, test acc = 0.9599999785423279, time = 0.3543882369995117\n",
      "Testing at step=72, batch=35, test loss = 0.10888288170099258, test acc = 0.9750000238418579, time = 0.35157275199890137\n",
      "Testing at step=72, batch=40, test loss = 0.11899113655090332, test acc = 0.9700000286102295, time = 0.35210633277893066\n",
      "Testing at step=72, batch=45, test loss = 0.23859809339046478, test acc = 0.949999988079071, time = 0.3536050319671631\n",
      "Step 72 finished in 314.5814299583435, Train loss = 0.028418385942156118, Test loss = 0.10313522931188344; Train Acc = 0.9908666751782099, Test Acc = 0.9753000092506409\n",
      "Training at step=73, batch=0, train loss = 0.030666038393974304, train acc = 0.9900000095367432, time = 0.9549412727355957\n",
      "Training at step=73, batch=30, train loss = 0.05109070613980293, train acc = 0.9800000190734863, time = 0.9480416774749756\n",
      "Training at step=73, batch=60, train loss = 0.01863022707402706, train acc = 0.9950000047683716, time = 0.9498839378356934\n",
      "Training at step=73, batch=90, train loss = 0.04052400588989258, train acc = 0.9850000143051147, time = 0.9517049789428711\n",
      "Training at step=73, batch=120, train loss = 0.007744020316749811, train acc = 0.9950000047683716, time = 0.952080249786377\n",
      "Training at step=73, batch=150, train loss = 0.011020348407328129, train acc = 0.9950000047683716, time = 0.9602158069610596\n",
      "Training at step=73, batch=180, train loss = 0.021568894386291504, train acc = 0.9900000095367432, time = 0.9513988494873047\n",
      "Training at step=73, batch=210, train loss = 0.012296630069613457, train acc = 0.9950000047683716, time = 0.9501972198486328\n",
      "Training at step=73, batch=240, train loss = 0.03171564266085625, train acc = 0.9950000047683716, time = 0.9518661499023438\n",
      "Training at step=73, batch=270, train loss = 0.025192787870764732, train acc = 0.9850000143051147, time = 0.9490737915039062\n",
      "Testing at step=73, batch=0, test loss = 0.030844885855913162, test acc = 0.9900000095367432, time = 0.3536562919616699\n",
      "Testing at step=73, batch=5, test loss = 0.09284863620996475, test acc = 0.9750000238418579, time = 0.3502082824707031\n",
      "Testing at step=73, batch=10, test loss = 0.11809089034795761, test acc = 0.9700000286102295, time = 0.35135817527770996\n",
      "Testing at step=73, batch=15, test loss = 0.18282480537891388, test acc = 0.9750000238418579, time = 0.36658787727355957\n",
      "Testing at step=73, batch=20, test loss = 0.11769846826791763, test acc = 0.9649999737739563, time = 0.3500843048095703\n",
      "Testing at step=73, batch=25, test loss = 0.07752995938062668, test acc = 0.9850000143051147, time = 0.3592407703399658\n",
      "Testing at step=73, batch=30, test loss = 0.12460743635892868, test acc = 0.9700000286102295, time = 0.35144734382629395\n",
      "Testing at step=73, batch=35, test loss = 0.11028977483510971, test acc = 0.9750000238418579, time = 0.34949254989624023\n",
      "Testing at step=73, batch=40, test loss = 0.15822595357894897, test acc = 0.9649999737739563, time = 0.3497939109802246\n",
      "Testing at step=73, batch=45, test loss = 0.050352707505226135, test acc = 0.9800000190734863, time = 0.3517324924468994\n",
      "Step 73 finished in 314.7209298610687, Train loss = 0.029143331629456953, Test loss = 0.0974259240180254; Train Acc = 0.9904000089565913, Test Acc = 0.9764000153541565\n",
      "Training at step=74, batch=0, train loss = 0.03213713318109512, train acc = 0.9850000143051147, time = 0.948340654373169\n",
      "Training at step=74, batch=30, train loss = 0.007482482586055994, train acc = 1.0, time = 0.9491925239562988\n",
      "Training at step=74, batch=60, train loss = 0.06843341886997223, train acc = 0.9850000143051147, time = 0.9523975849151611\n",
      "Training at step=74, batch=90, train loss = 0.028627004474401474, train acc = 0.9900000095367432, time = 0.9492290019989014\n",
      "Training at step=74, batch=120, train loss = 0.01517284195870161, train acc = 0.9950000047683716, time = 0.9649741649627686\n",
      "Training at step=74, batch=150, train loss = 0.01129972841590643, train acc = 1.0, time = 0.953392744064331\n",
      "Training at step=74, batch=180, train loss = 0.07157766073942184, train acc = 0.9800000190734863, time = 0.9499330520629883\n",
      "Training at step=74, batch=210, train loss = 0.04353611171245575, train acc = 0.9950000047683716, time = 0.9508039951324463\n",
      "Training at step=74, batch=240, train loss = 0.04310044273734093, train acc = 0.9900000095367432, time = 0.950326681137085\n",
      "Training at step=74, batch=270, train loss = 0.024067021906375885, train acc = 0.9900000095367432, time = 0.9492862224578857\n",
      "Testing at step=74, batch=0, test loss = 0.16039930284023285, test acc = 0.9549999833106995, time = 0.35399436950683594\n",
      "Testing at step=74, batch=5, test loss = 0.1908494234085083, test acc = 0.9599999785423279, time = 0.3530130386352539\n",
      "Testing at step=74, batch=10, test loss = 0.16756896674633026, test acc = 0.9800000190734863, time = 0.352130651473999\n",
      "Testing at step=74, batch=15, test loss = 0.03790578991174698, test acc = 0.9850000143051147, time = 0.3540823459625244\n",
      "Testing at step=74, batch=20, test loss = 0.11760994791984558, test acc = 0.9649999737739563, time = 0.35291600227355957\n",
      "Testing at step=74, batch=25, test loss = 0.1426348090171814, test acc = 0.9800000190734863, time = 0.3541526794433594\n",
      "Testing at step=74, batch=30, test loss = 0.14649824798107147, test acc = 0.9700000286102295, time = 0.35340094566345215\n",
      "Testing at step=74, batch=35, test loss = 0.03758440911769867, test acc = 0.9850000143051147, time = 0.35211777687072754\n",
      "Testing at step=74, batch=40, test loss = 0.050620317459106445, test acc = 0.9850000143051147, time = 0.35271620750427246\n",
      "Testing at step=74, batch=45, test loss = 0.14983679354190826, test acc = 0.9649999737739563, time = 0.3528604507446289\n",
      "Step 74 finished in 314.87934947013855, Train loss = 0.028686699400035043, Test loss = 0.10937235446646809; Train Acc = 0.9900333426396052, Test Acc = 0.9752000093460083\n",
      "Training at step=75, batch=0, train loss = 0.017990842461586, train acc = 0.9950000047683716, time = 0.9547669887542725\n",
      "Training at step=75, batch=30, train loss = 0.004635674878954887, train acc = 1.0, time = 0.9481635093688965\n",
      "Training at step=75, batch=60, train loss = 0.028636157512664795, train acc = 0.9900000095367432, time = 0.9506740570068359\n",
      "Training at step=75, batch=90, train loss = 0.013465169817209244, train acc = 0.9950000047683716, time = 0.9499368667602539\n",
      "Training at step=75, batch=120, train loss = 0.036725059151649475, train acc = 0.9850000143051147, time = 0.9486570358276367\n",
      "Training at step=75, batch=150, train loss = 0.03720996528863907, train acc = 0.9800000190734863, time = 0.949211835861206\n",
      "Training at step=75, batch=180, train loss = 0.018496073782444, train acc = 0.9900000095367432, time = 0.9557895660400391\n",
      "Training at step=75, batch=210, train loss = 0.06095794215798378, train acc = 0.9750000238418579, time = 0.9507081508636475\n",
      "Training at step=75, batch=240, train loss = 0.06224702298641205, train acc = 0.9800000190734863, time = 0.9489789009094238\n",
      "Training at step=75, batch=270, train loss = 0.028757160529494286, train acc = 0.9900000095367432, time = 0.9493515491485596\n",
      "Testing at step=75, batch=0, test loss = 0.11720595508813858, test acc = 0.9700000286102295, time = 0.3512580394744873\n",
      "Testing at step=75, batch=5, test loss = 0.12923689186573029, test acc = 0.9599999785423279, time = 0.35414671897888184\n",
      "Testing at step=75, batch=10, test loss = 0.1191011592745781, test acc = 0.9700000286102295, time = 0.35113096237182617\n",
      "Testing at step=75, batch=15, test loss = 0.05381878837943077, test acc = 0.9750000238418579, time = 0.35300278663635254\n",
      "Testing at step=75, batch=20, test loss = 0.16340140998363495, test acc = 0.9649999737739563, time = 0.35253047943115234\n",
      "Testing at step=75, batch=25, test loss = 0.193180650472641, test acc = 0.9750000238418579, time = 0.35128307342529297\n",
      "Testing at step=75, batch=30, test loss = 0.20091880857944489, test acc = 0.9599999785423279, time = 0.35143280029296875\n",
      "Testing at step=75, batch=35, test loss = 0.14376991987228394, test acc = 0.9700000286102295, time = 0.3514387607574463\n",
      "Testing at step=75, batch=40, test loss = 0.12260682135820389, test acc = 0.9750000238418579, time = 0.3542158603668213\n",
      "Testing at step=75, batch=45, test loss = 0.08776402473449707, test acc = 0.9649999737739563, time = 0.35318875312805176\n",
      "Step 75 finished in 314.2156894207001, Train loss = 0.02890240269480273, Test loss = 0.10638039711862803; Train Acc = 0.9899000092347463, Test Acc = 0.9752000105381012\n",
      "Training at step=76, batch=0, train loss = 0.00583144836127758, train acc = 1.0, time = 0.9513230323791504\n",
      "Training at step=76, batch=30, train loss = 0.049614544957876205, train acc = 0.9900000095367432, time = 0.9468080997467041\n",
      "Training at step=76, batch=60, train loss = 0.019038530066609383, train acc = 0.9900000095367432, time = 0.9476761817932129\n",
      "Training at step=76, batch=90, train loss = 0.0056466879323124886, train acc = 1.0, time = 0.9492177963256836\n",
      "Training at step=76, batch=120, train loss = 0.08559592068195343, train acc = 0.9850000143051147, time = 0.9548087120056152\n",
      "Training at step=76, batch=150, train loss = 0.0126745430752635, train acc = 1.0, time = 0.9586577415466309\n",
      "Training at step=76, batch=180, train loss = 0.05849660560488701, train acc = 0.9850000143051147, time = 0.9525594711303711\n",
      "Training at step=76, batch=210, train loss = 0.07182896882295609, train acc = 0.9800000190734863, time = 0.9631180763244629\n",
      "Training at step=76, batch=240, train loss = 0.02258647233247757, train acc = 0.9900000095367432, time = 0.9495100975036621\n",
      "Training at step=76, batch=270, train loss = 0.040271248668432236, train acc = 0.9900000095367432, time = 0.95100998878479\n",
      "Testing at step=76, batch=0, test loss = 0.3989797830581665, test acc = 0.9549999833106995, time = 0.3536849021911621\n",
      "Testing at step=76, batch=5, test loss = 0.05818207561969757, test acc = 0.9800000190734863, time = 0.3526127338409424\n",
      "Testing at step=76, batch=10, test loss = 0.14916758239269257, test acc = 0.9700000286102295, time = 0.3537569046020508\n",
      "Testing at step=76, batch=15, test loss = 0.14157405495643616, test acc = 0.9649999737739563, time = 0.3513944149017334\n",
      "Testing at step=76, batch=20, test loss = 0.08949267119169235, test acc = 0.9750000238418579, time = 0.35303258895874023\n",
      "Testing at step=76, batch=25, test loss = 0.05981160327792168, test acc = 0.9750000238418579, time = 0.353055477142334\n",
      "Testing at step=76, batch=30, test loss = 0.031384315341711044, test acc = 0.9850000143051147, time = 0.35270071029663086\n",
      "Testing at step=76, batch=35, test loss = 0.109867163002491, test acc = 0.9800000190734863, time = 0.35187625885009766\n",
      "Testing at step=76, batch=40, test loss = 0.10951128602027893, test acc = 0.9700000286102295, time = 0.35051894187927246\n",
      "Testing at step=76, batch=45, test loss = 0.06157960370182991, test acc = 0.9800000190734863, time = 0.3514542579650879\n",
      "Step 76 finished in 315.01097798347473, Train loss = 0.026336309924178448, Test loss = 0.11504919558763504; Train Acc = 0.9908833418289821, Test Acc = 0.974800009727478\n",
      "Training at step=77, batch=0, train loss = 0.01151548232883215, train acc = 0.9950000047683716, time = 0.9525034427642822\n",
      "Training at step=77, batch=30, train loss = 0.01244587916880846, train acc = 0.9950000047683716, time = 0.9481227397918701\n",
      "Training at step=77, batch=60, train loss = 0.05674121528863907, train acc = 0.9850000143051147, time = 0.9487931728363037\n",
      "Training at step=77, batch=90, train loss = 0.02147699147462845, train acc = 0.9950000047683716, time = 0.9489798545837402\n",
      "Training at step=77, batch=120, train loss = 0.003265978069975972, train acc = 1.0, time = 0.948613166809082\n",
      "Training at step=77, batch=150, train loss = 0.028959359973669052, train acc = 0.9850000143051147, time = 0.9806826114654541\n",
      "Training at step=77, batch=180, train loss = 0.002979616168886423, train acc = 1.0, time = 0.9499425888061523\n",
      "Training at step=77, batch=210, train loss = 0.01910526119172573, train acc = 0.9900000095367432, time = 0.9498982429504395\n",
      "Training at step=77, batch=240, train loss = 0.08507172763347626, train acc = 0.9750000238418579, time = 0.9467566013336182\n",
      "Training at step=77, batch=270, train loss = 0.06642298400402069, train acc = 0.9800000190734863, time = 0.9468677043914795\n",
      "Testing at step=77, batch=0, test loss = 0.13168230652809143, test acc = 0.9750000238418579, time = 0.35121917724609375\n",
      "Testing at step=77, batch=5, test loss = 0.03989148139953613, test acc = 0.9950000047683716, time = 0.35083436965942383\n",
      "Testing at step=77, batch=10, test loss = 0.11447139084339142, test acc = 0.9599999785423279, time = 0.3510134220123291\n",
      "Testing at step=77, batch=15, test loss = 0.1098746657371521, test acc = 0.9800000190734863, time = 0.351604700088501\n",
      "Testing at step=77, batch=20, test loss = 0.06419837474822998, test acc = 0.9850000143051147, time = 0.35033726692199707\n",
      "Testing at step=77, batch=25, test loss = 0.10943672060966492, test acc = 0.9850000143051147, time = 0.3514852523803711\n",
      "Testing at step=77, batch=30, test loss = 0.13816192746162415, test acc = 0.9750000238418579, time = 0.3542153835296631\n",
      "Testing at step=77, batch=35, test loss = 0.16110652685165405, test acc = 0.9700000286102295, time = 0.3509836196899414\n",
      "Testing at step=77, batch=40, test loss = 0.12470453977584839, test acc = 0.9649999737739563, time = 0.3515462875366211\n",
      "Testing at step=77, batch=45, test loss = 0.08938460052013397, test acc = 0.9850000143051147, time = 0.35154199600219727\n",
      "Step 77 finished in 314.7836947441101, Train loss = 0.027895243344440435, Test loss = 0.12533676378428937; Train Acc = 0.9908333412806193, Test Acc = 0.9716000068187713\n",
      "Training at step=78, batch=0, train loss = 0.0367242656648159, train acc = 0.9900000095367432, time = 0.952735424041748\n",
      "Training at step=78, batch=30, train loss = 0.04114817455410957, train acc = 0.9850000143051147, time = 0.949641227722168\n",
      "Training at step=78, batch=60, train loss = 0.04282825440168381, train acc = 0.9750000238418579, time = 0.9491498470306396\n",
      "Training at step=78, batch=90, train loss = 0.04900922626256943, train acc = 0.9800000190734863, time = 0.9489312171936035\n",
      "Training at step=78, batch=120, train loss = 0.026276301592588425, train acc = 0.9950000047683716, time = 0.9477255344390869\n",
      "Training at step=78, batch=150, train loss = 0.027055773884058, train acc = 0.9900000095367432, time = 0.9494497776031494\n",
      "Training at step=78, batch=180, train loss = 0.026521921157836914, train acc = 0.9950000047683716, time = 0.9508271217346191\n",
      "Training at step=78, batch=210, train loss = 0.024403300136327744, train acc = 0.9950000047683716, time = 0.9475522041320801\n",
      "Training at step=78, batch=240, train loss = 0.03957868739962578, train acc = 0.9850000143051147, time = 0.9499890804290771\n",
      "Training at step=78, batch=270, train loss = 0.008965525776147842, train acc = 0.9950000047683716, time = 0.9493882656097412\n",
      "Testing at step=78, batch=0, test loss = 0.025589562952518463, test acc = 0.9900000095367432, time = 0.35745739936828613\n",
      "Testing at step=78, batch=5, test loss = 0.035622525960206985, test acc = 0.9800000190734863, time = 0.35135674476623535\n",
      "Testing at step=78, batch=10, test loss = 0.03427629545331001, test acc = 0.9900000095367432, time = 0.35603833198547363\n",
      "Testing at step=78, batch=15, test loss = 0.09312576055526733, test acc = 0.9700000286102295, time = 0.3593628406524658\n",
      "Testing at step=78, batch=20, test loss = 0.13587385416030884, test acc = 0.9599999785423279, time = 0.3520207405090332\n",
      "Testing at step=78, batch=25, test loss = 0.1287514716386795, test acc = 0.9649999737739563, time = 0.3500101566314697\n",
      "Testing at step=78, batch=30, test loss = 0.10516443103551865, test acc = 0.9750000238418579, time = 0.3502688407897949\n",
      "Testing at step=78, batch=35, test loss = 0.15214575827121735, test acc = 0.9700000286102295, time = 0.3510880470275879\n",
      "Testing at step=78, batch=40, test loss = 0.14753559231758118, test acc = 0.9449999928474426, time = 0.3513317108154297\n",
      "Testing at step=78, batch=45, test loss = 0.06623437255620956, test acc = 0.9850000143051147, time = 0.35100317001342773\n",
      "Step 78 finished in 314.4792046546936, Train loss = 0.027022170471803594, Test loss = 0.11753819422796369; Train Acc = 0.9904333420594533, Test Acc = 0.9721000075340271\n",
      "Training at step=79, batch=0, train loss = 0.03322993218898773, train acc = 0.9800000190734863, time = 0.9482355117797852\n",
      "Training at step=79, batch=30, train loss = 0.002400394296273589, train acc = 1.0, time = 0.9537107944488525\n",
      "Training at step=79, batch=60, train loss = 0.045408476144075394, train acc = 0.9850000143051147, time = 0.9483158588409424\n",
      "Training at step=79, batch=90, train loss = 0.022411255165934563, train acc = 0.9900000095367432, time = 0.9473757743835449\n",
      "Training at step=79, batch=120, train loss = 0.005930934567004442, train acc = 1.0, time = 0.9470493793487549\n",
      "Training at step=79, batch=150, train loss = 0.04667304456233978, train acc = 0.9750000238418579, time = 0.9486510753631592\n",
      "Training at step=79, batch=180, train loss = 0.01844744011759758, train acc = 0.9900000095367432, time = 0.947760820388794\n",
      "Training at step=79, batch=210, train loss = 0.01779354177415371, train acc = 0.9950000047683716, time = 0.9666945934295654\n",
      "Training at step=79, batch=240, train loss = 0.020821543410420418, train acc = 0.9900000095367432, time = 0.9502623081207275\n",
      "Training at step=79, batch=270, train loss = 0.05006657540798187, train acc = 0.9900000095367432, time = 0.9503693580627441\n",
      "Testing at step=79, batch=0, test loss = 0.14123554527759552, test acc = 0.9599999785423279, time = 0.3519423007965088\n",
      "Testing at step=79, batch=5, test loss = 0.16194826364517212, test acc = 0.9700000286102295, time = 0.3601548671722412\n",
      "Testing at step=79, batch=10, test loss = 0.07998914271593094, test acc = 0.9750000238418579, time = 0.35312628746032715\n",
      "Testing at step=79, batch=15, test loss = 0.13176724314689636, test acc = 0.9599999785423279, time = 0.3518252372741699\n",
      "Testing at step=79, batch=20, test loss = 0.12472070753574371, test acc = 0.9700000286102295, time = 0.3513624668121338\n",
      "Testing at step=79, batch=25, test loss = 0.06763225793838501, test acc = 0.9850000143051147, time = 0.3575618267059326\n",
      "Testing at step=79, batch=30, test loss = 0.10057986527681351, test acc = 0.9800000190734863, time = 0.3545956611633301\n",
      "Testing at step=79, batch=35, test loss = 0.012262395583093166, test acc = 0.9950000047683716, time = 0.3542308807373047\n",
      "Testing at step=79, batch=40, test loss = 0.0627906545996666, test acc = 0.9850000143051147, time = 0.3507204055786133\n",
      "Testing at step=79, batch=45, test loss = 0.14608308672904968, test acc = 0.9700000286102295, time = 0.3521404266357422\n",
      "Step 79 finished in 314.4848897457123, Train loss = 0.025715225026166688, Test loss = 0.11263600008562208; Train Acc = 0.9911000082890192, Test Acc = 0.9749000120162964\n",
      "Training at step=80, batch=0, train loss = 0.010527125559747219, train acc = 1.0, time = 0.9511969089508057\n",
      "Training at step=80, batch=30, train loss = 0.02690025046467781, train acc = 0.9800000190734863, time = 0.951573371887207\n",
      "Training at step=80, batch=60, train loss = 0.023696977645158768, train acc = 0.9900000095367432, time = 0.9627606868743896\n",
      "Training at step=80, batch=90, train loss = 0.029779229313135147, train acc = 0.9850000143051147, time = 0.9484515190124512\n",
      "Training at step=80, batch=120, train loss = 0.03979729861021042, train acc = 0.9850000143051147, time = 0.9495291709899902\n",
      "Training at step=80, batch=150, train loss = 0.02488822303712368, train acc = 0.9900000095367432, time = 0.9486093521118164\n",
      "Training at step=80, batch=180, train loss = 0.03373945876955986, train acc = 0.9850000143051147, time = 0.9599053859710693\n",
      "Training at step=80, batch=210, train loss = 0.02040795050561428, train acc = 0.9950000047683716, time = 0.9499716758728027\n",
      "Training at step=80, batch=240, train loss = 0.033842433243989944, train acc = 0.9850000143051147, time = 0.9478912353515625\n",
      "Training at step=80, batch=270, train loss = 0.01680215634405613, train acc = 0.9900000095367432, time = 0.9502344131469727\n",
      "Testing at step=80, batch=0, test loss = 0.10417678952217102, test acc = 0.9700000286102295, time = 0.35425424575805664\n",
      "Testing at step=80, batch=5, test loss = 0.23228363692760468, test acc = 0.9399999976158142, time = 0.35065603256225586\n",
      "Testing at step=80, batch=10, test loss = 0.10168862342834473, test acc = 0.9750000238418579, time = 0.3520090579986572\n",
      "Testing at step=80, batch=15, test loss = 0.12406932562589645, test acc = 0.9649999737739563, time = 0.3553459644317627\n",
      "Testing at step=80, batch=20, test loss = 0.04709344357252121, test acc = 0.9750000238418579, time = 0.35835838317871094\n",
      "Testing at step=80, batch=25, test loss = 0.0702248066663742, test acc = 0.9750000238418579, time = 0.35231995582580566\n",
      "Testing at step=80, batch=30, test loss = 0.07067155092954636, test acc = 0.9750000238418579, time = 0.35193586349487305\n",
      "Testing at step=80, batch=35, test loss = 0.14606255292892456, test acc = 0.9800000190734863, time = 0.35210204124450684\n",
      "Testing at step=80, batch=40, test loss = 0.04932929947972298, test acc = 0.9850000143051147, time = 0.353762149810791\n",
      "Testing at step=80, batch=45, test loss = 0.1560211032629013, test acc = 0.9700000286102295, time = 0.3516347408294678\n",
      "Step 80 finished in 314.7089719772339, Train loss = 0.025811525316676125, Test loss = 0.11663069568574429; Train Acc = 0.9909333415826161, Test Acc = 0.9736000108718872\n",
      "Training at step=81, batch=0, train loss = 0.059253253042697906, train acc = 0.9800000190734863, time = 0.9500677585601807\n",
      "Training at step=81, batch=30, train loss = 0.005163044203072786, train acc = 1.0, time = 0.9496736526489258\n",
      "Training at step=81, batch=60, train loss = 0.010662531480193138, train acc = 1.0, time = 0.9494204521179199\n",
      "Training at step=81, batch=90, train loss = 0.02347252517938614, train acc = 0.9900000095367432, time = 0.9465682506561279\n",
      "Training at step=81, batch=120, train loss = 0.003606179729104042, train acc = 1.0, time = 0.9516210556030273\n",
      "Training at step=81, batch=150, train loss = 0.04407373070716858, train acc = 0.9900000095367432, time = 0.9501128196716309\n",
      "Training at step=81, batch=180, train loss = 0.08732527494430542, train acc = 0.9800000190734863, time = 0.9500410556793213\n",
      "Training at step=81, batch=210, train loss = 0.027175795286893845, train acc = 0.9950000047683716, time = 0.9509425163269043\n",
      "Training at step=81, batch=240, train loss = 0.018819458782672882, train acc = 0.9950000047683716, time = 0.9531548023223877\n",
      "Training at step=81, batch=270, train loss = 0.035448309034109116, train acc = 0.9950000047683716, time = 0.9503073692321777\n",
      "Testing at step=81, batch=0, test loss = 0.13837164640426636, test acc = 0.9700000286102295, time = 0.3577229976654053\n",
      "Testing at step=81, batch=5, test loss = 0.007936271838843822, test acc = 1.0, time = 0.3535170555114746\n",
      "Testing at step=81, batch=10, test loss = 0.16204504668712616, test acc = 0.9549999833106995, time = 0.3530406951904297\n",
      "Testing at step=81, batch=15, test loss = 0.131217360496521, test acc = 0.9750000238418579, time = 0.3546867370605469\n",
      "Testing at step=81, batch=20, test loss = 0.1154053658246994, test acc = 0.9700000286102295, time = 0.3521237373352051\n",
      "Testing at step=81, batch=25, test loss = 0.1415243148803711, test acc = 0.9649999737739563, time = 0.3530910015106201\n",
      "Testing at step=81, batch=30, test loss = 0.04482993483543396, test acc = 0.9900000095367432, time = 0.3534424304962158\n",
      "Testing at step=81, batch=35, test loss = 0.03165905177593231, test acc = 0.9950000047683716, time = 0.3526420593261719\n",
      "Testing at step=81, batch=40, test loss = 0.058411698788404465, test acc = 0.9900000095367432, time = 0.3529939651489258\n",
      "Testing at step=81, batch=45, test loss = 0.08558165282011032, test acc = 0.9750000238418579, time = 0.3544294834136963\n",
      "Step 81 finished in 314.6800310611725, Train loss = 0.024764220400247724, Test loss = 0.12191590242087841; Train Acc = 0.9915166747570038, Test Acc = 0.9743000078201294\n",
      "Training at step=82, batch=0, train loss = 0.015009820461273193, train acc = 0.9950000047683716, time = 0.950373649597168\n",
      "Training at step=82, batch=30, train loss = 0.004370344337075949, train acc = 1.0, time = 0.9513254165649414\n",
      "Training at step=82, batch=60, train loss = 0.01372985728085041, train acc = 0.9950000047683716, time = 0.9505527019500732\n",
      "Training at step=82, batch=90, train loss = 0.02551860362291336, train acc = 0.9900000095367432, time = 0.9675276279449463\n",
      "Training at step=82, batch=120, train loss = 0.011738848872482777, train acc = 0.9950000047683716, time = 0.9477052688598633\n",
      "Training at step=82, batch=150, train loss = 0.01892266795039177, train acc = 0.9950000047683716, time = 0.9502267837524414\n",
      "Training at step=82, batch=180, train loss = 0.004729670472443104, train acc = 1.0, time = 0.950061559677124\n",
      "Training at step=82, batch=210, train loss = 0.01781177707016468, train acc = 0.9900000095367432, time = 0.9500806331634521\n",
      "Training at step=82, batch=240, train loss = 0.008988945744931698, train acc = 0.9950000047683716, time = 0.9492709636688232\n",
      "Training at step=82, batch=270, train loss = 0.007996635511517525, train acc = 0.9950000047683716, time = 0.9475150108337402\n",
      "Testing at step=82, batch=0, test loss = 0.18291215598583221, test acc = 0.9700000286102295, time = 0.35300159454345703\n",
      "Testing at step=82, batch=5, test loss = 0.08625591546297073, test acc = 0.9850000143051147, time = 0.353804349899292\n",
      "Testing at step=82, batch=10, test loss = 0.2774079442024231, test acc = 0.9549999833106995, time = 0.3536264896392822\n",
      "Testing at step=82, batch=15, test loss = 0.04796434938907623, test acc = 0.9850000143051147, time = 0.35149168968200684\n",
      "Testing at step=82, batch=20, test loss = 0.06663484871387482, test acc = 0.9850000143051147, time = 0.3506321907043457\n",
      "Testing at step=82, batch=25, test loss = 0.07727065682411194, test acc = 0.9850000143051147, time = 0.3524434566497803\n",
      "Testing at step=82, batch=30, test loss = 0.150370791554451, test acc = 0.9649999737739563, time = 0.3542001247406006\n",
      "Testing at step=82, batch=35, test loss = 0.08366917818784714, test acc = 0.9750000238418579, time = 0.3511958122253418\n",
      "Testing at step=82, batch=40, test loss = 0.14062218368053436, test acc = 0.9599999785423279, time = 0.35077500343322754\n",
      "Testing at step=82, batch=45, test loss = 0.06127992644906044, test acc = 0.9850000143051147, time = 0.3512110710144043\n",
      "Step 82 finished in 314.7310652732849, Train loss = 0.023579681238819223, Test loss = 0.11329601518809795; Train Acc = 0.9920500075817108, Test Acc = 0.9768000102043152\n",
      "Training at step=83, batch=0, train loss = 0.013301297090947628, train acc = 0.9950000047683716, time = 0.9511501789093018\n",
      "Training at step=83, batch=30, train loss = 0.04754618555307388, train acc = 0.9850000143051147, time = 0.9496140480041504\n",
      "Training at step=83, batch=60, train loss = 0.03094879537820816, train acc = 0.9950000047683716, time = 0.9482970237731934\n",
      "Training at step=83, batch=90, train loss = 0.0434529073536396, train acc = 0.9850000143051147, time = 0.9607882499694824\n",
      "Training at step=83, batch=120, train loss = 0.021042268723249435, train acc = 0.9900000095367432, time = 0.948167085647583\n",
      "Training at step=83, batch=150, train loss = 0.05727569758892059, train acc = 0.9800000190734863, time = 0.9500696659088135\n",
      "Training at step=83, batch=180, train loss = 0.02017713151872158, train acc = 0.9950000047683716, time = 0.9499354362487793\n",
      "Training at step=83, batch=210, train loss = 0.022485366091132164, train acc = 0.9850000143051147, time = 0.9509584903717041\n",
      "Training at step=83, batch=240, train loss = 0.003864970523864031, train acc = 1.0, time = 0.9491450786590576\n",
      "Training at step=83, batch=270, train loss = 0.017655281350016594, train acc = 0.9950000047683716, time = 0.9490742683410645\n",
      "Testing at step=83, batch=0, test loss = 0.014258388429880142, test acc = 0.9950000047683716, time = 0.3514525890350342\n",
      "Testing at step=83, batch=5, test loss = 0.14747199416160583, test acc = 0.9700000286102295, time = 0.3515281677246094\n",
      "Testing at step=83, batch=10, test loss = 0.15352152287960052, test acc = 0.9700000286102295, time = 0.35318970680236816\n",
      "Testing at step=83, batch=15, test loss = 0.10428009182214737, test acc = 0.9700000286102295, time = 0.3503270149230957\n",
      "Testing at step=83, batch=20, test loss = 0.21100112795829773, test acc = 0.9549999833106995, time = 0.35138630867004395\n",
      "Testing at step=83, batch=25, test loss = 0.13955073058605194, test acc = 0.9599999785423279, time = 0.3524191379547119\n",
      "Testing at step=83, batch=30, test loss = 0.114759162068367, test acc = 0.9700000286102295, time = 0.3511621952056885\n",
      "Testing at step=83, batch=35, test loss = 0.09476100653409958, test acc = 0.9800000190734863, time = 0.3521103858947754\n",
      "Testing at step=83, batch=40, test loss = 0.17322438955307007, test acc = 0.9599999785423279, time = 0.349384069442749\n",
      "Testing at step=83, batch=45, test loss = 0.1349935680627823, test acc = 0.9750000238418579, time = 0.35173916816711426\n",
      "Step 83 finished in 314.7002487182617, Train loss = 0.02430579581608375, Test loss = 0.12487951844930649; Train Acc = 0.9920333409309388, Test Acc = 0.9715000092983246\n",
      "Training at step=84, batch=0, train loss = 0.006357103120535612, train acc = 1.0, time = 0.9546771049499512\n",
      "Training at step=84, batch=30, train loss = 0.027345918118953705, train acc = 0.9950000047683716, time = 0.9526894092559814\n",
      "Training at step=84, batch=60, train loss = 0.005313395988196135, train acc = 1.0, time = 0.9499039649963379\n",
      "Training at step=84, batch=90, train loss = 0.04037388414144516, train acc = 0.9900000095367432, time = 0.9490718841552734\n",
      "Training at step=84, batch=120, train loss = 0.034659963101148605, train acc = 0.9900000095367432, time = 0.9500644207000732\n",
      "Training at step=84, batch=150, train loss = 0.017583737149834633, train acc = 0.9900000095367432, time = 0.9493331909179688\n",
      "Training at step=84, batch=180, train loss = 0.012603122740983963, train acc = 0.9900000095367432, time = 0.9486970901489258\n",
      "Training at step=84, batch=210, train loss = 0.026612773537635803, train acc = 0.9900000095367432, time = 0.950587272644043\n",
      "Training at step=84, batch=240, train loss = 0.005670687649399042, train acc = 1.0, time = 0.9499990940093994\n",
      "Training at step=84, batch=270, train loss = 0.014429070986807346, train acc = 0.9950000047683716, time = 0.949120283126831\n",
      "Testing at step=84, batch=0, test loss = 0.16442888975143433, test acc = 0.9700000286102295, time = 0.35234951972961426\n",
      "Testing at step=84, batch=5, test loss = 0.169693261384964, test acc = 0.9599999785423279, time = 0.35068821907043457\n",
      "Testing at step=84, batch=10, test loss = 0.05564352124929428, test acc = 0.9750000238418579, time = 0.35270023345947266\n",
      "Testing at step=84, batch=15, test loss = 0.15741121768951416, test acc = 0.949999988079071, time = 0.3512732982635498\n",
      "Testing at step=84, batch=20, test loss = 0.08280007541179657, test acc = 0.9800000190734863, time = 0.35230422019958496\n",
      "Testing at step=84, batch=25, test loss = 0.1930379420518875, test acc = 0.9700000286102295, time = 0.35297393798828125\n",
      "Testing at step=84, batch=30, test loss = 0.16215524077415466, test acc = 0.9649999737739563, time = 0.3569812774658203\n",
      "Testing at step=84, batch=35, test loss = 0.0989159569144249, test acc = 0.9599999785423279, time = 0.350203275680542\n",
      "Testing at step=84, batch=40, test loss = 0.02888193540275097, test acc = 0.9950000047683716, time = 0.3509976863861084\n",
      "Testing at step=84, batch=45, test loss = 0.1338348090648651, test acc = 0.9750000238418579, time = 0.35182619094848633\n",
      "Step 84 finished in 314.9065866470337, Train loss = 0.02323442662678038, Test loss = 0.11439318865537644; Train Acc = 0.9923666737476985, Test Acc = 0.9738000082969666\n",
      "Training at step=85, batch=0, train loss = 0.013098900206387043, train acc = 0.9950000047683716, time = 0.9511682987213135\n",
      "Training at step=85, batch=30, train loss = 0.019133562222123146, train acc = 0.9950000047683716, time = 0.947345495223999\n",
      "Training at step=85, batch=60, train loss = 0.05591525882482529, train acc = 0.9800000190734863, time = 0.9484126567840576\n",
      "Training at step=85, batch=90, train loss = 0.03851626068353653, train acc = 0.9800000190734863, time = 0.9530632495880127\n",
      "Training at step=85, batch=120, train loss = 0.0194095391780138, train acc = 0.9900000095367432, time = 0.9505610466003418\n",
      "Training at step=85, batch=150, train loss = 0.06020865589380264, train acc = 0.9850000143051147, time = 0.9507608413696289\n",
      "Training at step=85, batch=180, train loss = 0.06527699530124664, train acc = 0.9750000238418579, time = 0.9655487537384033\n",
      "Training at step=85, batch=210, train loss = 0.023507094010710716, train acc = 0.9850000143051147, time = 0.9490940570831299\n",
      "Training at step=85, batch=240, train loss = 0.02257627435028553, train acc = 0.9900000095367432, time = 0.9555132389068604\n",
      "Training at step=85, batch=270, train loss = 0.025866074487566948, train acc = 0.9900000095367432, time = 0.9504384994506836\n",
      "Testing at step=85, batch=0, test loss = 0.05649945139884949, test acc = 0.9900000095367432, time = 0.3534872531890869\n",
      "Testing at step=85, batch=5, test loss = 0.31718865036964417, test acc = 0.9649999737739563, time = 0.35370731353759766\n",
      "Testing at step=85, batch=10, test loss = 0.010970516130328178, test acc = 0.9900000095367432, time = 0.35266923904418945\n",
      "Testing at step=85, batch=15, test loss = 0.13911505043506622, test acc = 0.9850000143051147, time = 0.3515603542327881\n",
      "Testing at step=85, batch=20, test loss = 0.023686550557613373, test acc = 0.9900000095367432, time = 0.35044312477111816\n",
      "Testing at step=85, batch=25, test loss = 0.050837788730859756, test acc = 0.9750000238418579, time = 0.35407543182373047\n",
      "Testing at step=85, batch=30, test loss = 0.25238239765167236, test acc = 0.9649999737739563, time = 0.3522768020629883\n",
      "Testing at step=85, batch=35, test loss = 0.2022210657596588, test acc = 0.9700000286102295, time = 0.3510012626647949\n",
      "Testing at step=85, batch=40, test loss = 0.07406921684741974, test acc = 0.9750000238418579, time = 0.3527524471282959\n",
      "Testing at step=85, batch=45, test loss = 0.1849604845046997, test acc = 0.9700000286102295, time = 0.35291218757629395\n",
      "Step 85 finished in 314.6741030216217, Train loss = 0.02450398937391583, Test loss = 0.1330459686741233; Train Acc = 0.9916500079631806, Test Acc = 0.9757000076770782\n",
      "Training at step=86, batch=0, train loss = 0.0239313505589962, train acc = 0.9950000047683716, time = 0.9539597034454346\n",
      "Training at step=86, batch=30, train loss = 0.029162494465708733, train acc = 0.9950000047683716, time = 0.9493756294250488\n",
      "Training at step=86, batch=60, train loss = 0.03635435923933983, train acc = 0.9900000095367432, time = 0.9522919654846191\n",
      "Training at step=86, batch=90, train loss = 0.037740081548690796, train acc = 0.9900000095367432, time = 0.9520010948181152\n",
      "Training at step=86, batch=120, train loss = 0.010523165576159954, train acc = 0.9950000047683716, time = 0.9499170780181885\n",
      "Training at step=86, batch=150, train loss = 0.01469865720719099, train acc = 0.9950000047683716, time = 0.9494686126708984\n",
      "Training at step=86, batch=180, train loss = 0.012426954694092274, train acc = 0.9950000047683716, time = 0.9494705200195312\n",
      "Training at step=86, batch=210, train loss = 0.015433570370078087, train acc = 0.9950000047683716, time = 0.9542961120605469\n",
      "Training at step=86, batch=240, train loss = 0.056269530206918716, train acc = 0.9750000238418579, time = 0.9528050422668457\n",
      "Training at step=86, batch=270, train loss = 0.010476252064108849, train acc = 0.9950000047683716, time = 0.9493227005004883\n",
      "Testing at step=86, batch=0, test loss = 0.08544036000967026, test acc = 0.9750000238418579, time = 0.35014867782592773\n",
      "Testing at step=86, batch=5, test loss = 0.17457900941371918, test acc = 0.9700000286102295, time = 0.35202932357788086\n",
      "Testing at step=86, batch=10, test loss = 0.1705380082130432, test acc = 0.9549999833106995, time = 0.3497607707977295\n",
      "Testing at step=86, batch=15, test loss = 0.12424102425575256, test acc = 0.9700000286102295, time = 0.35369157791137695\n",
      "Testing at step=86, batch=20, test loss = 0.07589665055274963, test acc = 0.9800000190734863, time = 0.35347604751586914\n",
      "Testing at step=86, batch=25, test loss = 0.07691051810979843, test acc = 0.9850000143051147, time = 0.3650166988372803\n",
      "Testing at step=86, batch=30, test loss = 0.38563036918640137, test acc = 0.9449999928474426, time = 0.3524918556213379\n",
      "Testing at step=86, batch=35, test loss = 0.05201263353228569, test acc = 0.9750000238418579, time = 0.35444211959838867\n",
      "Testing at step=86, batch=40, test loss = 0.10303503274917603, test acc = 0.9750000238418579, time = 0.3532888889312744\n",
      "Testing at step=86, batch=45, test loss = 0.04063909128308296, test acc = 0.9850000143051147, time = 0.3522045612335205\n",
      "Step 86 finished in 314.89800238609314, Train loss = 0.02521685017738491, Test loss = 0.11430599194020033; Train Acc = 0.9913500082492829, Test Acc = 0.973200010061264\n",
      "Training at step=87, batch=0, train loss = 0.010625472292304039, train acc = 0.9950000047683716, time = 0.9486262798309326\n",
      "Training at step=87, batch=30, train loss = 0.0065937768667936325, train acc = 1.0, time = 0.9478905200958252\n",
      "Training at step=87, batch=60, train loss = 0.04424479603767395, train acc = 0.9900000095367432, time = 0.9545066356658936\n",
      "Training at step=87, batch=90, train loss = 0.04645698890089989, train acc = 0.9850000143051147, time = 0.9512648582458496\n",
      "Training at step=87, batch=120, train loss = 0.004047024063766003, train acc = 1.0, time = 0.9496402740478516\n",
      "Training at step=87, batch=150, train loss = 0.002873347606509924, train acc = 1.0, time = 0.9533560276031494\n",
      "Training at step=87, batch=180, train loss = 0.010801545344293118, train acc = 0.9950000047683716, time = 0.9483542442321777\n",
      "Training at step=87, batch=210, train loss = 0.011653434485197067, train acc = 0.9950000047683716, time = 0.9504380226135254\n",
      "Training at step=87, batch=240, train loss = 0.0030795857310295105, train acc = 1.0, time = 0.9477648735046387\n",
      "Training at step=87, batch=270, train loss = 0.003913989756256342, train acc = 1.0, time = 0.9471943378448486\n",
      "Testing at step=87, batch=0, test loss = 0.048339031636714935, test acc = 0.9900000095367432, time = 0.3541727066040039\n",
      "Testing at step=87, batch=5, test loss = 0.062053728848695755, test acc = 0.9800000190734863, time = 0.3514397144317627\n",
      "Testing at step=87, batch=10, test loss = 0.22648291289806366, test acc = 0.949999988079071, time = 0.34966039657592773\n",
      "Testing at step=87, batch=15, test loss = 0.07847633957862854, test acc = 0.9750000238418579, time = 0.3527524471282959\n",
      "Testing at step=87, batch=20, test loss = 0.07895934581756592, test acc = 0.9700000286102295, time = 0.355562686920166\n",
      "Testing at step=87, batch=25, test loss = 0.13420197367668152, test acc = 0.9700000286102295, time = 0.35216736793518066\n",
      "Testing at step=87, batch=30, test loss = 0.10279135406017303, test acc = 0.9750000238418579, time = 0.35074925422668457\n",
      "Testing at step=87, batch=35, test loss = 0.13696229457855225, test acc = 0.9649999737739563, time = 0.35252809524536133\n",
      "Testing at step=87, batch=40, test loss = 0.08862178027629852, test acc = 0.9700000286102295, time = 0.35019588470458984\n",
      "Testing at step=87, batch=45, test loss = 0.08183689415454865, test acc = 0.9850000143051147, time = 0.350940465927124\n",
      "Step 87 finished in 314.4092586040497, Train loss = 0.02276819368513922, Test loss = 0.12465302646160126; Train Acc = 0.9923000073432923, Test Acc = 0.9718000078201294\n",
      "Training at step=88, batch=0, train loss = 0.02352650836110115, train acc = 0.9900000095367432, time = 0.9490716457366943\n",
      "Training at step=88, batch=30, train loss = 0.036918286234140396, train acc = 0.9900000095367432, time = 0.9526374340057373\n",
      "Training at step=88, batch=60, train loss = 0.017035776749253273, train acc = 0.9950000047683716, time = 0.9526975154876709\n",
      "Training at step=88, batch=90, train loss = 0.04234735667705536, train acc = 0.9850000143051147, time = 0.9520113468170166\n",
      "Training at step=88, batch=120, train loss = 0.006838392000645399, train acc = 0.9950000047683716, time = 0.9512307643890381\n",
      "Training at step=88, batch=150, train loss = 0.017676446586847305, train acc = 0.9900000095367432, time = 0.9491603374481201\n",
      "Training at step=88, batch=180, train loss = 0.018276100978255272, train acc = 0.9900000095367432, time = 0.9529242515563965\n",
      "Training at step=88, batch=210, train loss = 0.01052860263735056, train acc = 0.9950000047683716, time = 0.9518401622772217\n",
      "Training at step=88, batch=240, train loss = 0.02226843684911728, train acc = 0.9900000095367432, time = 0.9566681385040283\n",
      "Training at step=88, batch=270, train loss = 0.057726383209228516, train acc = 0.9800000190734863, time = 0.9490931034088135\n",
      "Testing at step=88, batch=0, test loss = 0.09390553832054138, test acc = 0.9800000190734863, time = 0.351534366607666\n",
      "Testing at step=88, batch=5, test loss = 0.08463376760482788, test acc = 0.9800000190734863, time = 0.35361313819885254\n",
      "Testing at step=88, batch=10, test loss = 0.1516430824995041, test acc = 0.9700000286102295, time = 0.3534543514251709\n",
      "Testing at step=88, batch=15, test loss = 0.08820939064025879, test acc = 0.9750000238418579, time = 0.35076355934143066\n",
      "Testing at step=88, batch=20, test loss = 0.056247778236866, test acc = 0.9700000286102295, time = 0.35015296936035156\n",
      "Testing at step=88, batch=25, test loss = 0.14484648406505585, test acc = 0.9599999785423279, time = 0.35225510597229004\n",
      "Testing at step=88, batch=30, test loss = 0.007070157676935196, test acc = 1.0, time = 0.3512694835662842\n",
      "Testing at step=88, batch=35, test loss = 0.007834305986762047, test acc = 0.9950000047683716, time = 0.35311293601989746\n",
      "Testing at step=88, batch=40, test loss = 0.04332270100712776, test acc = 0.9800000190734863, time = 0.3507051467895508\n",
      "Testing at step=88, batch=45, test loss = 0.1631360650062561, test acc = 0.9700000286102295, time = 0.3502504825592041\n",
      "Step 88 finished in 314.869323015213, Train loss = 0.02403405189203719, Test loss = 0.11834881483577192; Train Acc = 0.9916666746139526, Test Acc = 0.9749000120162964\n",
      "Training at step=89, batch=0, train loss = 0.002289622789248824, train acc = 1.0, time = 0.9487388134002686\n",
      "Training at step=89, batch=30, train loss = 0.001664681825786829, train acc = 1.0, time = 0.9527015686035156\n",
      "Training at step=89, batch=60, train loss = 0.008109796792268753, train acc = 1.0, time = 0.9538853168487549\n",
      "Training at step=89, batch=90, train loss = 0.00905476976186037, train acc = 0.9950000047683716, time = 0.9500644207000732\n",
      "Training at step=89, batch=120, train loss = 0.00874834880232811, train acc = 1.0, time = 0.952472448348999\n",
      "Training at step=89, batch=150, train loss = 0.012848254293203354, train acc = 1.0, time = 0.9561831951141357\n",
      "Training at step=89, batch=180, train loss = 0.02184872142970562, train acc = 0.9900000095367432, time = 0.9573192596435547\n",
      "Training at step=89, batch=210, train loss = 0.0064530279487371445, train acc = 1.0, time = 0.9559173583984375\n",
      "Training at step=89, batch=240, train loss = 0.06234082207083702, train acc = 0.9750000238418579, time = 0.949460506439209\n",
      "Training at step=89, batch=270, train loss = 0.010628491640090942, train acc = 1.0, time = 0.9497439861297607\n",
      "Testing at step=89, batch=0, test loss = 0.05125045403838158, test acc = 0.9800000190734863, time = 0.35332202911376953\n",
      "Testing at step=89, batch=5, test loss = 0.08793573081493378, test acc = 0.9750000238418579, time = 0.35389065742492676\n",
      "Testing at step=89, batch=10, test loss = 0.1342223733663559, test acc = 0.9599999785423279, time = 0.3561360836029053\n",
      "Testing at step=89, batch=15, test loss = 0.06235959380865097, test acc = 0.9649999737739563, time = 0.3504626750946045\n",
      "Testing at step=89, batch=20, test loss = 0.09578026831150055, test acc = 0.9800000190734863, time = 0.3498575687408447\n",
      "Testing at step=89, batch=25, test loss = 0.1667160540819168, test acc = 0.9649999737739563, time = 0.3520934581756592\n",
      "Testing at step=89, batch=30, test loss = 0.08104374259710312, test acc = 0.9750000238418579, time = 0.3504064083099365\n",
      "Testing at step=89, batch=35, test loss = 0.11592274904251099, test acc = 0.9649999737739563, time = 0.3500187397003174\n",
      "Testing at step=89, batch=40, test loss = 0.04402201622724533, test acc = 0.9800000190734863, time = 0.35007429122924805\n",
      "Testing at step=89, batch=45, test loss = 0.07831845432519913, test acc = 0.9700000286102295, time = 0.3507723808288574\n",
      "Step 89 finished in 315.58392357826233, Train loss = 0.0216949423373444, Test loss = 0.11885764045640826; Train Acc = 0.9926666736602783, Test Acc = 0.9737000107765198\n",
      "Training at step=90, batch=0, train loss = 0.024572858586907387, train acc = 0.9900000095367432, time = 0.9491262435913086\n",
      "Training at step=90, batch=30, train loss = 0.014966360293328762, train acc = 0.9900000095367432, time = 0.9492983818054199\n",
      "Training at step=90, batch=60, train loss = 0.017331749200820923, train acc = 0.9950000047683716, time = 0.9477791786193848\n",
      "Training at step=90, batch=90, train loss = 0.03643117845058441, train acc = 0.9850000143051147, time = 0.9490458965301514\n",
      "Training at step=90, batch=120, train loss = 0.007189538329839706, train acc = 1.0, time = 0.9496011734008789\n",
      "Training at step=90, batch=150, train loss = 0.010158667340874672, train acc = 0.9950000047683716, time = 0.9505922794342041\n",
      "Training at step=90, batch=180, train loss = 0.015151397325098515, train acc = 0.9900000095367432, time = 0.9499518871307373\n",
      "Training at step=90, batch=210, train loss = 0.009834527969360352, train acc = 1.0, time = 0.9476797580718994\n",
      "Training at step=90, batch=240, train loss = 0.05020039156079292, train acc = 0.9850000143051147, time = 0.9492702484130859\n",
      "Training at step=90, batch=270, train loss = 0.045941222459077835, train acc = 0.9700000286102295, time = 0.9529154300689697\n",
      "Testing at step=90, batch=0, test loss = 0.040114253759384155, test acc = 0.9900000095367432, time = 0.3519096374511719\n",
      "Testing at step=90, batch=5, test loss = 0.24273547530174255, test acc = 0.9599999785423279, time = 0.351884126663208\n",
      "Testing at step=90, batch=10, test loss = 0.03132220357656479, test acc = 0.9850000143051147, time = 0.35053277015686035\n",
      "Testing at step=90, batch=15, test loss = 0.0635661780834198, test acc = 0.9800000190734863, time = 0.3521246910095215\n",
      "Testing at step=90, batch=20, test loss = 0.2728383243083954, test acc = 0.9700000286102295, time = 0.35504937171936035\n",
      "Testing at step=90, batch=25, test loss = 0.08925730735063553, test acc = 0.9800000190734863, time = 0.3503744602203369\n",
      "Testing at step=90, batch=30, test loss = 0.2077181041240692, test acc = 0.9449999928474426, time = 0.3520817756652832\n",
      "Testing at step=90, batch=35, test loss = 0.19225120544433594, test acc = 0.9599999785423279, time = 0.3507692813873291\n",
      "Testing at step=90, batch=40, test loss = 0.04288317635655403, test acc = 0.9850000143051147, time = 0.35892534255981445\n",
      "Testing at step=90, batch=45, test loss = 0.16536767780780792, test acc = 0.9649999737739563, time = 0.3517012596130371\n",
      "Step 90 finished in 314.2915632724762, Train loss = 0.02407063201768324, Test loss = 0.12121100593358278; Train Acc = 0.9917666743199031, Test Acc = 0.9736000108718872\n",
      "Training at step=91, batch=0, train loss = 0.01635030470788479, train acc = 0.9950000047683716, time = 0.9491679668426514\n",
      "Training at step=91, batch=30, train loss = 0.02007005177438259, train acc = 0.9950000047683716, time = 0.9507184028625488\n",
      "Training at step=91, batch=60, train loss = 0.10285650938749313, train acc = 0.9800000190734863, time = 0.9521579742431641\n",
      "Training at step=91, batch=90, train loss = 0.010027481243014336, train acc = 1.0, time = 0.9532589912414551\n",
      "Training at step=91, batch=120, train loss = 0.008017512038350105, train acc = 0.9950000047683716, time = 0.9539110660552979\n",
      "Training at step=91, batch=150, train loss = 0.04971441254019737, train acc = 0.9900000095367432, time = 0.953012228012085\n",
      "Training at step=91, batch=180, train loss = 0.03709396719932556, train acc = 0.9800000190734863, time = 0.9507091045379639\n",
      "Training at step=91, batch=210, train loss = 0.01679934374988079, train acc = 0.9950000047683716, time = 0.9482245445251465\n",
      "Training at step=91, batch=240, train loss = 0.07131413370370865, train acc = 0.9800000190734863, time = 0.9516358375549316\n",
      "Training at step=91, batch=270, train loss = 0.012862085364758968, train acc = 0.9950000047683716, time = 0.9583797454833984\n",
      "Testing at step=91, batch=0, test loss = 0.058330897241830826, test acc = 0.9800000190734863, time = 0.35328125953674316\n",
      "Testing at step=91, batch=5, test loss = 0.12219278514385223, test acc = 0.9700000286102295, time = 0.35266542434692383\n",
      "Testing at step=91, batch=10, test loss = 0.11671413481235504, test acc = 0.9750000238418579, time = 0.351609468460083\n",
      "Testing at step=91, batch=15, test loss = 0.03619543835520744, test acc = 0.9800000190734863, time = 0.3523378372192383\n",
      "Testing at step=91, batch=20, test loss = 0.12589561939239502, test acc = 0.9599999785423279, time = 0.353696346282959\n",
      "Testing at step=91, batch=25, test loss = 0.040858931839466095, test acc = 0.9800000190734863, time = 0.35309863090515137\n",
      "Testing at step=91, batch=30, test loss = 0.250284880399704, test acc = 0.9599999785423279, time = 0.35205578804016113\n",
      "Testing at step=91, batch=35, test loss = 0.13604287803173065, test acc = 0.9649999737739563, time = 0.35118913650512695\n",
      "Testing at step=91, batch=40, test loss = 0.03631431236863136, test acc = 0.9850000143051147, time = 0.35147833824157715\n",
      "Testing at step=91, batch=45, test loss = 0.13405446708202362, test acc = 0.9649999737739563, time = 0.353090763092041\n",
      "Step 91 finished in 315.3853192329407, Train loss = 0.022870780194255834, Test loss = 0.1154986884444952; Train Acc = 0.9924666738510132, Test Acc = 0.9749000060558319\n",
      "Training at step=92, batch=0, train loss = 0.02987879514694214, train acc = 0.9950000047683716, time = 0.9532761573791504\n",
      "Training at step=92, batch=30, train loss = 0.011505771428346634, train acc = 0.9950000047683716, time = 0.9513978958129883\n",
      "Training at step=92, batch=60, train loss = 0.028257938101887703, train acc = 0.9950000047683716, time = 0.9471986293792725\n",
      "Training at step=92, batch=90, train loss = 0.005069587379693985, train acc = 1.0, time = 0.9493777751922607\n",
      "Training at step=92, batch=120, train loss = 0.004634941928088665, train acc = 1.0, time = 0.9545361995697021\n",
      "Training at step=92, batch=150, train loss = 0.008713476359844208, train acc = 0.9950000047683716, time = 0.9499368667602539\n",
      "Training at step=92, batch=180, train loss = 0.027021732181310654, train acc = 0.9900000095367432, time = 0.9513156414031982\n",
      "Training at step=92, batch=210, train loss = 0.02250460535287857, train acc = 0.9950000047683716, time = 0.9544155597686768\n",
      "Training at step=92, batch=240, train loss = 0.0030200439505279064, train acc = 1.0, time = 0.9499228000640869\n",
      "Training at step=92, batch=270, train loss = 0.006071485113352537, train acc = 1.0, time = 0.961829423904419\n",
      "Testing at step=92, batch=0, test loss = 0.08734738081693649, test acc = 0.9700000286102295, time = 0.3523540496826172\n",
      "Testing at step=92, batch=5, test loss = 0.08670089393854141, test acc = 0.9900000095367432, time = 0.3510453701019287\n",
      "Testing at step=92, batch=10, test loss = 0.2272598147392273, test acc = 0.9649999737739563, time = 0.34995293617248535\n",
      "Testing at step=92, batch=15, test loss = 0.08035054802894592, test acc = 0.9850000143051147, time = 0.3578836917877197\n",
      "Testing at step=92, batch=20, test loss = 0.007698954548686743, test acc = 0.9950000047683716, time = 0.3564786911010742\n",
      "Testing at step=92, batch=25, test loss = 0.06623011082410812, test acc = 0.9750000238418579, time = 0.35796546936035156\n",
      "Testing at step=92, batch=30, test loss = 0.09002883732318878, test acc = 0.9850000143051147, time = 0.350954532623291\n",
      "Testing at step=92, batch=35, test loss = 0.1048140823841095, test acc = 0.9750000238418579, time = 0.3497622013092041\n",
      "Testing at step=92, batch=40, test loss = 0.2539515793323517, test acc = 0.9549999833106995, time = 0.3518388271331787\n",
      "Testing at step=92, batch=45, test loss = 0.18241989612579346, test acc = 0.9700000286102295, time = 0.35322022438049316\n",
      "Step 92 finished in 314.9815628528595, Train loss = 0.019084961448873703, Test loss = 0.11907934608869254; Train Acc = 0.9938000059127807, Test Acc = 0.9763000118732452\n",
      "Training at step=93, batch=0, train loss = 0.006591598968952894, train acc = 0.9950000047683716, time = 0.9495012760162354\n",
      "Training at step=93, batch=30, train loss = 0.009557762183248997, train acc = 1.0, time = 0.9521009922027588\n",
      "Training at step=93, batch=60, train loss = 0.04275982454419136, train acc = 0.9850000143051147, time = 0.9494681358337402\n",
      "Training at step=93, batch=90, train loss = 0.040718525648117065, train acc = 0.9850000143051147, time = 0.9523448944091797\n",
      "Training at step=93, batch=120, train loss = 0.012646742165088654, train acc = 0.9950000047683716, time = 0.9524834156036377\n",
      "Training at step=93, batch=150, train loss = 0.037572428584098816, train acc = 0.9850000143051147, time = 0.9512522220611572\n",
      "Training at step=93, batch=180, train loss = 0.016032744199037552, train acc = 0.9950000047683716, time = 0.949333906173706\n",
      "Training at step=93, batch=210, train loss = 0.03362740948796272, train acc = 0.9950000047683716, time = 0.9588747024536133\n",
      "Training at step=93, batch=240, train loss = 0.017566649243235588, train acc = 0.9900000095367432, time = 0.9497380256652832\n",
      "Training at step=93, batch=270, train loss = 0.023148097097873688, train acc = 0.9850000143051147, time = 0.9559526443481445\n",
      "Testing at step=93, batch=0, test loss = 0.2639545202255249, test acc = 0.9549999833106995, time = 0.3546421527862549\n",
      "Testing at step=93, batch=5, test loss = 0.09910191595554352, test acc = 0.9850000143051147, time = 0.35425710678100586\n",
      "Testing at step=93, batch=10, test loss = 0.06557057797908783, test acc = 0.9850000143051147, time = 0.35096240043640137\n",
      "Testing at step=93, batch=15, test loss = 0.09750720858573914, test acc = 0.9750000238418579, time = 0.35390782356262207\n",
      "Testing at step=93, batch=20, test loss = 0.036301419138908386, test acc = 0.9850000143051147, time = 0.3577694892883301\n",
      "Testing at step=93, batch=25, test loss = 0.12373492866754532, test acc = 0.9750000238418579, time = 0.35538148880004883\n",
      "Testing at step=93, batch=30, test loss = 0.029216330498456955, test acc = 0.9850000143051147, time = 0.35134339332580566\n",
      "Testing at step=93, batch=35, test loss = 0.1155150830745697, test acc = 0.9700000286102295, time = 0.3517134189605713\n",
      "Testing at step=93, batch=40, test loss = 0.2069883942604065, test acc = 0.9750000238418579, time = 0.3582453727722168\n",
      "Testing at step=93, batch=45, test loss = 0.08963970839977264, test acc = 0.9750000238418579, time = 0.3582639694213867\n",
      "Step 93 finished in 315.0294020175934, Train loss = 0.02070497801876627, Test loss = 0.13130344804376365; Train Acc = 0.9929500065247218, Test Acc = 0.9734000098705292\n",
      "Training at step=94, batch=0, train loss = 0.03232495114207268, train acc = 0.9950000047683716, time = 0.9513285160064697\n",
      "Training at step=94, batch=30, train loss = 0.0414794385433197, train acc = 0.9750000238418579, time = 0.9495458602905273\n",
      "Training at step=94, batch=60, train loss = 0.056800879538059235, train acc = 0.9950000047683716, time = 0.9475159645080566\n",
      "Training at step=94, batch=90, train loss = 0.01104548666626215, train acc = 0.9950000047683716, time = 0.9520831108093262\n",
      "Training at step=94, batch=120, train loss = 0.005867804866284132, train acc = 1.0, time = 0.9530811309814453\n",
      "Training at step=94, batch=150, train loss = 0.01977621391415596, train acc = 0.9900000095367432, time = 0.9548177719116211\n",
      "Training at step=94, batch=180, train loss = 0.005144436843693256, train acc = 1.0, time = 0.9498872756958008\n",
      "Training at step=94, batch=210, train loss = 0.01757865957915783, train acc = 0.9950000047683716, time = 0.9495491981506348\n",
      "Training at step=94, batch=240, train loss = 0.014940369874238968, train acc = 0.9950000047683716, time = 0.9491779804229736\n",
      "Training at step=94, batch=270, train loss = 0.02763587050139904, train acc = 0.9950000047683716, time = 0.9480042457580566\n",
      "Testing at step=94, batch=0, test loss = 0.12978249788284302, test acc = 0.9750000238418579, time = 0.3509037494659424\n",
      "Testing at step=94, batch=5, test loss = 0.28205740451812744, test acc = 0.9599999785423279, time = 0.35132861137390137\n",
      "Testing at step=94, batch=10, test loss = 0.15350881218910217, test acc = 0.9700000286102295, time = 0.3511977195739746\n",
      "Testing at step=94, batch=15, test loss = 0.07310114800930023, test acc = 0.9850000143051147, time = 0.3519608974456787\n",
      "Testing at step=94, batch=20, test loss = 0.1084846556186676, test acc = 0.9549999833106995, time = 0.3491811752319336\n",
      "Testing at step=94, batch=25, test loss = 0.32427987456321716, test acc = 0.9549999833106995, time = 0.3520631790161133\n",
      "Testing at step=94, batch=30, test loss = 0.19510458409786224, test acc = 0.949999988079071, time = 0.3535308837890625\n",
      "Testing at step=94, batch=35, test loss = 0.18235191702842712, test acc = 0.9700000286102295, time = 0.35082077980041504\n",
      "Testing at step=94, batch=40, test loss = 0.08500196784734726, test acc = 0.9900000095367432, time = 0.3507976531982422\n",
      "Testing at step=94, batch=45, test loss = 0.06506478041410446, test acc = 0.9750000238418579, time = 0.3497905731201172\n",
      "Step 94 finished in 314.76844453811646, Train loss = 0.021790543359238655, Test loss = 0.12807490207254887; Train Acc = 0.9925000069538752, Test Acc = 0.973500007390976\n",
      "Training at step=95, batch=0, train loss = 0.019375700503587723, train acc = 0.9900000095367432, time = 0.9506568908691406\n",
      "Training at step=95, batch=30, train loss = 0.013071801513433456, train acc = 0.9950000047683716, time = 0.948563814163208\n",
      "Training at step=95, batch=60, train loss = 0.0032016579061746597, train acc = 1.0, time = 0.9504942893981934\n",
      "Training at step=95, batch=90, train loss = 0.0677887499332428, train acc = 0.9750000238418579, time = 0.9513251781463623\n",
      "Training at step=95, batch=120, train loss = 0.020989753305912018, train acc = 0.9900000095367432, time = 0.9501588344573975\n",
      "Training at step=95, batch=150, train loss = 0.05544205754995346, train acc = 0.9850000143051147, time = 0.9496340751647949\n",
      "Training at step=95, batch=180, train loss = 0.04653052240610123, train acc = 0.9950000047683716, time = 0.9518148899078369\n",
      "Training at step=95, batch=210, train loss = 0.008417006582021713, train acc = 0.9950000047683716, time = 0.952251672744751\n",
      "Training at step=95, batch=240, train loss = 0.024963654577732086, train acc = 0.9900000095367432, time = 0.9490525722503662\n",
      "Training at step=95, batch=270, train loss = 0.0066945613361895084, train acc = 1.0, time = 0.9517738819122314\n",
      "Testing at step=95, batch=0, test loss = 0.22023296356201172, test acc = 0.9549999833106995, time = 0.3502991199493408\n",
      "Testing at step=95, batch=5, test loss = 0.09757014364004135, test acc = 0.9800000190734863, time = 0.3490314483642578\n",
      "Testing at step=95, batch=10, test loss = 0.2997075021266937, test acc = 0.9750000238418579, time = 0.35012149810791016\n",
      "Testing at step=95, batch=15, test loss = 0.14626136422157288, test acc = 0.9750000238418579, time = 0.351560115814209\n",
      "Testing at step=95, batch=20, test loss = 0.052061546593904495, test acc = 0.9800000190734863, time = 0.35195374488830566\n",
      "Testing at step=95, batch=25, test loss = 0.031077025458216667, test acc = 0.9950000047683716, time = 0.3496696949005127\n",
      "Testing at step=95, batch=30, test loss = 0.05281654745340347, test acc = 0.9850000143051147, time = 0.35037970542907715\n",
      "Testing at step=95, batch=35, test loss = 0.10045231878757477, test acc = 0.9649999737739563, time = 0.35306882858276367\n",
      "Testing at step=95, batch=40, test loss = 0.22217659652233124, test acc = 0.949999988079071, time = 0.35080790519714355\n",
      "Testing at step=95, batch=45, test loss = 0.11210672557353973, test acc = 0.9750000238418579, time = 0.3506913185119629\n",
      "Step 95 finished in 314.2225413322449, Train loss = 0.01926681599773777, Test loss = 0.12170176678337156; Train Acc = 0.9936833393573761, Test Acc = 0.9740000057220459\n",
      "Training at step=96, batch=0, train loss = 0.03449511528015137, train acc = 0.9900000095367432, time = 0.9461047649383545\n",
      "Training at step=96, batch=30, train loss = 0.012741523794829845, train acc = 0.9900000095367432, time = 0.9482681751251221\n",
      "Training at step=96, batch=60, train loss = 0.022112691774964333, train acc = 0.9850000143051147, time = 0.9503066539764404\n",
      "Training at step=96, batch=90, train loss = 0.007473161444067955, train acc = 1.0, time = 0.9479174613952637\n",
      "Training at step=96, batch=120, train loss = 0.016397124156355858, train acc = 0.9950000047683716, time = 0.9469590187072754\n",
      "Training at step=96, batch=150, train loss = 0.016209514811635017, train acc = 0.9950000047683716, time = 0.9465761184692383\n",
      "Training at step=96, batch=180, train loss = 0.008488954044878483, train acc = 1.0, time = 0.9482152462005615\n",
      "Training at step=96, batch=210, train loss = 0.023017216473817825, train acc = 0.9950000047683716, time = 0.9490792751312256\n",
      "Training at step=96, batch=240, train loss = 0.006547913420945406, train acc = 1.0, time = 0.9500770568847656\n",
      "Training at step=96, batch=270, train loss = 0.020306725054979324, train acc = 0.9950000047683716, time = 0.9471886157989502\n",
      "Testing at step=96, batch=0, test loss = 0.18163424730300903, test acc = 0.9700000286102295, time = 0.352738618850708\n",
      "Testing at step=96, batch=5, test loss = 0.03779122978448868, test acc = 0.9900000095367432, time = 0.3508625030517578\n",
      "Testing at step=96, batch=10, test loss = 0.12974977493286133, test acc = 0.9850000143051147, time = 0.35233092308044434\n",
      "Testing at step=96, batch=15, test loss = 0.1462429314851761, test acc = 0.9599999785423279, time = 0.35205912590026855\n",
      "Testing at step=96, batch=20, test loss = 0.06335249543190002, test acc = 0.9800000190734863, time = 0.35094332695007324\n",
      "Testing at step=96, batch=25, test loss = 0.055789824575185776, test acc = 0.9850000143051147, time = 0.35174107551574707\n",
      "Testing at step=96, batch=30, test loss = 0.1468832641839981, test acc = 0.9700000286102295, time = 0.3510422706604004\n",
      "Testing at step=96, batch=35, test loss = 0.3152831792831421, test acc = 0.949999988079071, time = 0.35141468048095703\n",
      "Testing at step=96, batch=40, test loss = 0.11493193358182907, test acc = 0.9850000143051147, time = 0.35123300552368164\n",
      "Testing at step=96, batch=45, test loss = 0.269376665353775, test acc = 0.9750000238418579, time = 0.3509242534637451\n",
      "Step 96 finished in 314.17197942733765, Train loss = 0.019333862329173522, Test loss = 0.13418505873531103; Train Acc = 0.9938000059127807, Test Acc = 0.9742000102996826\n",
      "Training at step=97, batch=0, train loss = 0.00916271097958088, train acc = 0.9950000047683716, time = 0.953883171081543\n",
      "Training at step=97, batch=30, train loss = 0.008770345710217953, train acc = 0.9950000047683716, time = 0.9500648975372314\n",
      "Training at step=97, batch=60, train loss = 0.013654652051627636, train acc = 0.9900000095367432, time = 0.9500229358673096\n",
      "Training at step=97, batch=90, train loss = 0.015165379270911217, train acc = 0.9950000047683716, time = 0.9545872211456299\n",
      "Training at step=97, batch=120, train loss = 0.030530709773302078, train acc = 0.9850000143051147, time = 0.9488010406494141\n",
      "Training at step=97, batch=150, train loss = 0.033928439021110535, train acc = 0.9900000095367432, time = 0.953099250793457\n",
      "Training at step=97, batch=180, train loss = 0.04804069921374321, train acc = 0.9850000143051147, time = 0.9504709243774414\n",
      "Training at step=97, batch=210, train loss = 0.025394678115844727, train acc = 0.9900000095367432, time = 0.9523839950561523\n",
      "Training at step=97, batch=240, train loss = 0.012102816253900528, train acc = 0.9950000047683716, time = 0.9494647979736328\n",
      "Training at step=97, batch=270, train loss = 0.009653965942561626, train acc = 0.9950000047683716, time = 0.9501240253448486\n",
      "Testing at step=97, batch=0, test loss = 0.09017226845026016, test acc = 0.9800000190734863, time = 0.3516347408294678\n",
      "Testing at step=97, batch=5, test loss = 0.0780150517821312, test acc = 0.9800000190734863, time = 0.35095930099487305\n",
      "Testing at step=97, batch=10, test loss = 0.14919382333755493, test acc = 0.9700000286102295, time = 0.3502168655395508\n",
      "Testing at step=97, batch=15, test loss = 0.24520722031593323, test acc = 0.9700000286102295, time = 0.351574182510376\n",
      "Testing at step=97, batch=20, test loss = 0.13312824070453644, test acc = 0.9850000143051147, time = 0.3524608612060547\n",
      "Testing at step=97, batch=25, test loss = 0.17864041030406952, test acc = 0.9599999785423279, time = 0.3520925045013428\n",
      "Testing at step=97, batch=30, test loss = 0.14102958142757416, test acc = 0.9750000238418579, time = 0.349567174911499\n",
      "Testing at step=97, batch=35, test loss = 0.0870438665151596, test acc = 0.9750000238418579, time = 0.35215020179748535\n",
      "Testing at step=97, batch=40, test loss = 0.03992876410484314, test acc = 0.9900000095367432, time = 0.3519628047943115\n",
      "Testing at step=97, batch=45, test loss = 0.06881309300661087, test acc = 0.9900000095367432, time = 0.35146236419677734\n",
      "Step 97 finished in 314.791419506073, Train loss = 0.019870923778701883, Test loss = 0.12906572457402946; Train Acc = 0.9934833393494288, Test Acc = 0.9740000104904175\n",
      "Training at step=98, batch=0, train loss = 0.02569158375263214, train acc = 0.9950000047683716, time = 0.9502012729644775\n",
      "Training at step=98, batch=30, train loss = 0.007768596988171339, train acc = 1.0, time = 0.946995735168457\n",
      "Training at step=98, batch=60, train loss = 0.02727213315665722, train acc = 0.9900000095367432, time = 0.9494693279266357\n",
      "Training at step=98, batch=90, train loss = 0.0094915721565485, train acc = 0.9950000047683716, time = 0.9507405757904053\n",
      "Training at step=98, batch=120, train loss = 0.019593307748436928, train acc = 0.9900000095367432, time = 0.952373743057251\n",
      "Training at step=98, batch=150, train loss = 0.00845024362206459, train acc = 1.0, time = 0.9524896144866943\n",
      "Training at step=98, batch=180, train loss = 0.031098486855626106, train acc = 0.9950000047683716, time = 0.9498393535614014\n",
      "Training at step=98, batch=210, train loss = 0.08539802581071854, train acc = 0.9850000143051147, time = 0.9539625644683838\n",
      "Training at step=98, batch=240, train loss = 0.04413380101323128, train acc = 0.9900000095367432, time = 0.949591875076294\n",
      "Training at step=98, batch=270, train loss = 0.0271317008882761, train acc = 0.9950000047683716, time = 0.9504518508911133\n",
      "Testing at step=98, batch=0, test loss = 0.18399742245674133, test acc = 0.9700000286102295, time = 0.3536217212677002\n",
      "Testing at step=98, batch=5, test loss = 0.04202655702829361, test acc = 0.9850000143051147, time = 0.3517894744873047\n",
      "Testing at step=98, batch=10, test loss = 0.06837299466133118, test acc = 0.9900000095367432, time = 0.3543539047241211\n",
      "Testing at step=98, batch=15, test loss = 0.22851896286010742, test acc = 0.9599999785423279, time = 0.3548102378845215\n",
      "Testing at step=98, batch=20, test loss = 0.02727743424475193, test acc = 0.9950000047683716, time = 0.3519933223724365\n",
      "Testing at step=98, batch=25, test loss = 0.20957441627979279, test acc = 0.9649999737739563, time = 0.35144925117492676\n",
      "Testing at step=98, batch=30, test loss = 0.34581223130226135, test acc = 0.949999988079071, time = 0.35186076164245605\n",
      "Testing at step=98, batch=35, test loss = 0.11051607131958008, test acc = 0.9800000190734863, time = 0.3526897430419922\n",
      "Testing at step=98, batch=40, test loss = 0.13462910056114197, test acc = 0.9700000286102295, time = 0.3530099391937256\n",
      "Testing at step=98, batch=45, test loss = 0.23328836262226105, test acc = 0.9549999833106995, time = 0.35248875617980957\n",
      "Step 98 finished in 314.76692819595337, Train loss = 0.017559415880047405, Test loss = 0.13665606554597617; Train Acc = 0.9941666722297668, Test Acc = 0.9737000095844269\n",
      "Training at step=99, batch=0, train loss = 0.0022445388603955507, train acc = 1.0, time = 0.9489850997924805\n",
      "Training at step=99, batch=30, train loss = 0.012660877779126167, train acc = 0.9950000047683716, time = 0.9512691497802734\n",
      "Training at step=99, batch=60, train loss = 0.030936121940612793, train acc = 0.9850000143051147, time = 0.948096752166748\n",
      "Training at step=99, batch=90, train loss = 0.006005812436342239, train acc = 0.9950000047683716, time = 0.9507999420166016\n",
      "Training at step=99, batch=120, train loss = 0.02660508081316948, train acc = 0.9850000143051147, time = 0.9483215808868408\n",
      "Training at step=99, batch=150, train loss = 0.0038679770659655333, train acc = 1.0, time = 0.9490935802459717\n",
      "Training at step=99, batch=180, train loss = 0.000862862216308713, train acc = 1.0, time = 0.9503331184387207\n",
      "Training at step=99, batch=210, train loss = 0.011196251958608627, train acc = 1.0, time = 0.9494726657867432\n",
      "Training at step=99, batch=240, train loss = 0.0034275995567440987, train acc = 1.0, time = 0.9470844268798828\n",
      "Training at step=99, batch=270, train loss = 0.0018621523631736636, train acc = 1.0, time = 0.9497649669647217\n",
      "Testing at step=99, batch=0, test loss = 0.1336521953344345, test acc = 0.949999988079071, time = 0.35129785537719727\n",
      "Testing at step=99, batch=5, test loss = 0.07136248052120209, test acc = 0.9750000238418579, time = 0.35045480728149414\n",
      "Testing at step=99, batch=10, test loss = 0.07901916652917862, test acc = 0.9750000238418579, time = 0.34999871253967285\n",
      "Testing at step=99, batch=15, test loss = 0.0782010480761528, test acc = 0.9850000143051147, time = 0.35019683837890625\n",
      "Testing at step=99, batch=20, test loss = 0.11535565555095673, test acc = 0.9700000286102295, time = 0.3514747619628906\n",
      "Testing at step=99, batch=25, test loss = 0.07778879255056381, test acc = 0.9750000238418579, time = 0.35281991958618164\n",
      "Testing at step=99, batch=30, test loss = 0.07846949249505997, test acc = 0.9900000095367432, time = 0.35235047340393066\n",
      "Testing at step=99, batch=35, test loss = 0.24536210298538208, test acc = 0.9750000238418579, time = 0.3510587215423584\n",
      "Testing at step=99, batch=40, test loss = 0.060943830758333206, test acc = 0.9800000190734863, time = 0.3522017002105713\n",
      "Testing at step=99, batch=45, test loss = 0.06513829529285431, test acc = 0.9800000190734863, time = 0.3519747257232666\n",
      "Step 99 finished in 314.1275243759155, Train loss = 0.01958089542943829, Test loss = 0.1252339239604771; Train Acc = 0.9932833397388459, Test Acc = 0.9734000086784362\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.1\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T00:55:24.433537Z",
     "iopub.status.busy": "2024-04-03T00:55:24.433325Z",
     "iopub.status.idle": "2024-04-03T00:55:24.787803Z",
     "shell.execute_reply": "2024-04-03T00:55:24.787177Z",
     "shell.execute_reply.started": "2024-04-03T00:55:24.433517Z"
    },
    "id": "U0Q0vFm7B6cg"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
