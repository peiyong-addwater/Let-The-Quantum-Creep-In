{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "authorship_tag": "ABX9TyOMsOj+oVvgvYS0niJGU1v1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ebnFsErY0EM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464100582,
     "user_tz": -660,
     "elapsed": 70769,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "79853453-c681-4231-ac49-20b3ac3fec57",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:06:48.102963Z",
     "start_time": "2024-04-06T21:06:44.653354Z"
    }
   },
   "source": [
    "# Install packages\n",
    "!pip install torch torchvision torchaudio pennylane cotengra quimb torchmetrics --upgrade"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.17.2)\r\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: pennylane in /usr/local/lib/python3.9/dist-packages (0.35.1)\r\n",
      "Requirement already satisfied: cotengra in /usr/local/lib/python3.9/dist-packages (0.5.6)\r\n",
      "Requirement already satisfied: quimb in /usr/local/lib/python3.9/dist-packages (1.7.3)\r\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (1.3.2)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from torch) (4.11.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.3.1)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch) (11.4.5.107)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch) (8.9.2.26)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.2.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\r\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\r\n",
      "Requirement already satisfied: pennylane-lightning>=0.35 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.35.1)\r\n",
      "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.10.0)\r\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.4.4)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\r\n",
      "Requirement already satisfied: autograd in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.6.2)\r\n",
      "Requirement already satisfied: rustworkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.14.2)\r\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.10.2)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\r\n",
      "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.6.9)\r\n",
      "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\r\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.12.3)\r\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\r\n",
      "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.59.1)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (0.11.2)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\r\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\r\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.39->quimb) (0.42.0)\r\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "\n",
    "\n",
    "#import pennylane as qml\n",
    "#import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "#prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# torch.cfloat won't pass the unitary check, but faster\n",
    "COMPLEX_DTYPE = torch.cfloat \n",
    "REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VN2wD-U7bGse",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464110097,
     "user_tz": -660,
     "elapsed": 6856,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "2cc42fb4-c41b-4a6e-a674-46cee2afdeae",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:06:51.236043Z",
     "start_time": "2024-04-06T21:06:48.104623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data"
   ],
   "metadata": {
    "id": "9WmQPQuLbSFw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUhQUP2dbTja",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464114139,
     "user_tz": -660,
     "elapsed": 1173,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "77d2ed9c-f5fc-4378-9bcd-44c9dcb3c1c7",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:06:52.353651Z",
     "start_time": "2024-04-06T21:06:51.237400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 364590483.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/train-images-idx3-ubyte.gz to MNIST/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 71089022.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 59840026.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 15551452.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n",
      "tensor([7, 7, 5, 7, 4, 7, 7, 7, 5, 4, 7, 2, 7, 8, 1, 1, 1, 6, 4, 1, 5, 1, 2, 7,\n",
      "        8, 0, 8, 6, 1, 8, 8, 7, 4, 4, 7, 9, 5, 4, 3, 9, 6, 5, 6, 0, 3, 9, 5, 3,\n",
      "        5, 1, 0, 4, 2, 3, 2, 0, 1, 5, 6, 4, 8, 6, 3, 9])\n",
      "tensor([-1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j,  0.2863+0.j,  0.7098+0.j, -0.7333+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j], dtype=torch.complex64)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Some Utilities"
   ],
   "metadata": {
    "id": "kQI8WWY6byQq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ket = {\n",
    "    '0':torch.tensor([1.,0.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '1':torch.tensor([0.,1.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '+':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) + torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device)),\n",
    "    '-':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) - torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device))\n",
    "}\n",
    "\n",
    "pauli = {\n",
    "    'I':torch.tensor([[1.,0.],[0.,1.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'X':torch.tensor([[0.,1.],[1.,0.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Y':torch.tensor([[0., -1.j],[1.j, 0]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Z':torch.tensor([[1.,0.],[0.,-1.]], dtype = COMPLEX_DTYPE, device=device)\n",
    "}\n",
    "\n",
    "def tensor_product(*args):\n",
    "  input_list = [a for a in args]\n",
    "  return functools.reduce(torch.kron, input_list)\n",
    "\n",
    "def multi_qubit_identity(n_qubits:int):\n",
    "  assert n_qubits>0\n",
    "  if n_qubits == 1:\n",
    "    return pauli['I']\n",
    "  else:\n",
    "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
    "\n",
    "pauli_words_su4 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
    "      #print(key1, key2)\n",
    "      #print(pauli[key1])\n",
    "      #print(pauli[key2])\n",
    "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
    "      #print(pauli_words_su4[key1+key2])\n",
    "\n",
    "pauli_words_su8 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      if not key1+key2+key3 == 'III':\n",
    "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
    "\n",
    "pauli_words_su16 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        if not key1+key2+key3+key4 == 'IIII':\n",
    "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
    "              pauli[key1],\n",
    "              pauli[key2],\n",
    "              pauli[key3],\n",
    "              pauli[key4]\n",
    "          )\n",
    "\n",
    "pauli_words_su32 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        for key5 in pauli.keys():\n",
    "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
    "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
    "                pauli[key1],\n",
    "                pauli[key2],\n",
    "                pauli[key3],\n",
    "                pauli[key4],\n",
    "                pauli[key5]\n",
    "            )\n",
    "\n",
    "def su32_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = list(pauli_words_su32.values())\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, torch.stack(dict_values))\n",
    "  #print(generator.shape)\n",
    "  return torch.matrix_exp(1j*generator)\n",
    "\n",
    "def su4_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = torch.stack(list(pauli_words_su4.values()))\n",
    "  #print(dict_values.shape)\n",
    "  generator = torch.einsum(\"i, ijk -> jk\", params, dict_values)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7B4DTncWbwQl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464142599,
     "user_tz": -660,
     "elapsed": 2803,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "303c7fee-fa98-4e2a-b0b6-7dcbaed6897e",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:06:54.149047Z",
     "start_time": "2024-04-06T21:06:52.355459Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "def measure_sv(\n",
    "    state,\n",
    "    observable\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Measure a statevector with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not\n",
    "  \"\"\"\n",
    "  expectation_value = torch.conj(state)@observable@state\n",
    "  return torch.real(expectation_value)\n",
    "\n",
    "def measure_dm(\n",
    "    rho,\n",
    "    observable\n",
    "):\n",
    "  \"\"\"\n",
    "  Measure a density matrix with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not.\n",
    "  \"\"\"\n",
    "  product = torch.matmul(rho, observable)\n",
    "\n",
    "  # Calculate the trace, which is the sum of diagonal elements\n",
    "  trace = torch.trace(product)\n",
    "\n",
    "  # The expectation value should be real for physical observables\n",
    "  return torch.real(trace)\n",
    "\n",
    "vmap_measure_sv = torch.vmap(measure_sv, in_dims=(None, 0), out_dims=0)\n",
    "vmap_measure_dm = torch.vmap(measure_dm, in_dims=(None, 0), out_dims=0)\n",
    "\n",
    "# assuming the input patch observables (hermitianized) has shape (batchsize,n_patches, c, h, w)\n",
    "# assuming the input set statevectors has shape (c, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_sv_batched_ob = torch.vmap(measure_sv, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "# assuming the input set desnity matrices has shape (batchsize, n_patches, c, 2**n, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_dm_batched_ob = torch.vmap(measure_dm, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "def bitstring_to_state(bitstring:str):\n",
    "  \"\"\"\n",
    "  Convert a bit string, like '0101001' or '+-+-101'\n",
    "  to a statevector. Each character in the bitstring must be among\n",
    "  0, 1, + and -\n",
    "  \"\"\"\n",
    "  assert len(bitstring)>0\n",
    "  for c in bitstring:\n",
    "    assert c in ['0', '1', '+', '-']\n",
    "  single_qubit_states = [ket[c] for c in bitstring]\n",
    "  return tensor_product(*single_qubit_states)\n",
    "\n",
    "\n",
    "#test_patch = torch.randn(size=[5, 2, 3, 4,4], dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_herm_patch = (torch.einsum(\"...jk->...kj\", test_patch)+test_patch)/2\n",
    "#print(test_herm_patch)\n",
    "#print(\"all patches shape\", test_herm_patch.shape)\n",
    "#test_sv = torch.stack([bitstring_to_state('++')]*3, axis = 0)\n",
    "#print(test_sv)\n",
    "#print(\"all sv shape\", test_sv.shape)\n",
    "#res = vmap_measure_channel_sv_batched_ob(test_sv, test_herm_patch)\n",
    "#print(res)\n",
    "#print(\"res shape\",res.shape)\n",
    "\n",
    "#for batchid in range(test_patch.shape[0]):\n",
    "#  batchitem = test_patch[batchid]\n",
    "#  for patch_idx in range(batchitem.shape[0]):\n",
    "#    patch = batchitem[patch_idx]\n",
    "#    for ch_idx in range(patch.shape[0]):\n",
    "#      print(f\"batchid={batchid}; patchid = {patch_idx}; chid = {ch_idx}\")\n",
    "#      ch = patch[ch_idx]\n",
    "#      #print(ch.shape)\n",
    "#      sv_ch = test_sv[ch_idx]\n",
    "#      #print(sv_ch.shape)\n",
    "#      print(measure_sv(sv_ch, ch))\n"
   ],
   "metadata": {
    "id": "Xs0c2F1eBnGc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464146349,
     "user_tz": -660,
     "elapsed": 713,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:06:54.157052Z",
     "start_time": "2024-04-06T21:06:54.150565Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flipped Quanvolution Kernel\n",
    "\n",
    "First, let's define the function to extract patches for a single kernel."
   ],
   "metadata": {
    "id": "fFZqT6bpElaL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_patches(image, patch_size, stride, padding=None):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image with multiple input channels and optional custom padding.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (in_channels, height, width).\n",
    "        patch_size (int): Size of the square patches to extract.\n",
    "        stride (int): Stride between patches.\n",
    "        padding (tuple): Padding value(s) for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n",
    "    pad_l, pad_r, pad_t, pad_b = padding if padding is not None else (0,0,0,0)\n",
    "\n",
    "    if padding is not None:\n",
    "        image = torch.nn.functional.pad(image, (pad_l, pad_r, pad_t, pad_b), mode='constant')\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    height, width = image.shape[-2],  image.shape[-1]\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = image[..., i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    patches = torch.stack(patches, dim=-4)\n",
    "    return patches # has shape (batchsize, n_patches, channel, h, w)\n",
    "\n",
    "\n",
    "#single_img = torch.stack([torch.arange(32*32*1,dtype = torch.double, device=device).reshape((32,32)).type(torch.cdouble)]*3)\n",
    "\n",
    "#test_img = torch.stack([single_img]*2)\n",
    "#print(test_img[0][...,:4,:4])\n",
    "#patches = extract_patches(test_img, patch_size=4, stride=2,padding=(0,0,0,0))\n",
    "#print(patches.shape)\n",
    "#print(patches[0].shape)\n",
    "#print(patches[0][0])\n"
   ],
   "metadata": {
    "id": "He4HdMRHC7T6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464148806,
     "user_tz": -660,
     "elapsed": 1,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:06:54.164073Z",
     "start_time": "2024-04-06T21:06:54.158328Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Three-by-Three Quanv Kernel Function\n",
    "\n",
    "For 3 by 3 patches, we only need a 2-qubit parameterised state, generated by the SU4 gate applied to the $|00\\rangle$ state. The first thing to do is to pad the 3 by 3 patches to 4 by 4. However, for input with size\n",
    "\n",
    "$$\n",
    "D_1\\times H_1 \\times W_1\n",
    "$$\n",
    "\n",
    "where $D_1$ is the number of input channels, we will need $D_1$ different two-qubit states, generated by $D_1$ different parameterised circuits. These circuits will produce $D_1$ expectation values. Suppose each circuit is parameterised by $\\theta_i, i\\in \\{1,2,\\cdots, D_1\\}$, the parameterised (pure) state can be written as\n",
    "\n",
    "$$\n",
    "|\\psi (\\theta_i)⟩ = SU4(\\theta_i)|00⟩\n",
    "$$\n",
    "\n",
    "For a channel $M_i$ of patch $M$ with shape $4\\times 4$, the expectation value is calculated by\n",
    "\n",
    "$$\n",
    "⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$\n",
    "\n",
    "Then, a single pixel value of the output channel corresponding this single kernel is\n",
    "\n",
    "$$\n",
    "\\frac{1}{D_1}\\sum_{i=1}^{D_1}⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$"
   ],
   "metadata": {
    "id": "9hRqflooEqKN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_2q_param_state(theta):\n",
    "  state = bitstring_to_state('00')\n",
    "  state = torch.matmul(su4_op(theta), state)\n",
    "  return state\n",
    "\n",
    "vmap_generate_2q_param_state = torch.vmap(generate_2q_param_state, in_dims=0, out_dims = 0)\n",
    "\n",
    "def single_kernel_op_over_batched_patches(thetas, patch):\n",
    "  # patch has shape (c_in, h, w)\n",
    "  # thetas has shape (c_in, 4^2-1) for SU4 gates\n",
    "  n_theta = thetas.shape[-2]\n",
    "  n_channel = patch.shape[-3]\n",
    "  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n",
    "  states = vmap_generate_2q_param_state(thetas)\n",
    "  #print(\"States shape\", states.shape)\n",
    "  patch = torch.nn.functional.pad(patch, (0, 1, 0, 1), mode='constant')\n",
    "  patch_t = torch.einsum(\"...jk->...kj\", patch)\n",
    "  herm_patch = (patch_t+patch)/2 # has dim (batchsize, num_patches, c, h, w)\n",
    "  #print(\"Herm patch shape\", herm_patch.shape)\n",
    "  channel_out = vmap_measure_channel_sv_batched_ob(states, herm_patch) # has dim (batchsize,n_patches, c)\n",
    "  return torch.sum(channel_out, axis = -1)/n_theta # has dim (batchszie, n_patches)\n",
    "\n",
    "\n",
    "#test_params = torch.randn((1,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_img = dummy_x.to(device)#.type(torch.cdouble)\n",
    "#print(test_img.shape)\n",
    "#test_patches = extract_patches(test_img, patch_size=3, stride=1,padding=(1,1,1,1))\n",
    "#print(test_patches.shape)\n",
    "#print(test_params.shape)\n",
    "#print(vmap_generate_2q_param_state(test_params))\n",
    "#test_out = single_kernel_op_over_batched_patches(test_params, test_patches)\n",
    "#print(test_out.shape)\n",
    "#h = (32-3+1*2)//1 +1\n",
    "#h**2\n"
   ],
   "metadata": {
    "id": "Yzn4KEt5ErG7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464151262,
     "user_tz": -660,
     "elapsed": 1,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:06:54.170026Z",
     "start_time": "2024-04-06T21:06:54.165198Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multiple Output Channels"
   ],
   "metadata": {
    "id": "A4BXEKd8I67_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# For multiple channel output\n",
    "# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n",
    "vmap_vmap_single_kernel_op_through_extracted_patches = torch.vmap(single_kernel_op_over_batched_patches, in_dims=(0, None), out_dims=-2) # output has dim (batchsize, c_out, n_patches)\n",
    "\n",
    "#c_in = 1\n",
    "#c_out = 2\n",
    "\n",
    "#test_params2 = torch.randn((c_out, c_in,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_out2 = vmap_vmap_single_kernel_op_through_extracted_patches(test_params2, test_patches)\n",
    "#print(test_out2.shape)\n",
    "#test_out_2_features = test_out2.reshape((-1,c_out, 32, 32))\n",
    "#print(test_out_2_features.shape)\n"
   ],
   "metadata": {
    "id": "72vkHV_BI80l",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464153721,
     "user_tz": -660,
     "elapsed": 1,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:06:54.174189Z",
     "start_time": "2024-04-06T21:06:54.171179Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch Module for Three-by-Three Flipped Quanv"
   ],
   "metadata": {
    "id": "NuZ6G5-sKK_J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FlippedQuanv3x3(torch.nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, padding):\n",
    "    super(FlippedQuanv3x3, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.pad_l, self.pad_r, self.pad_t, self.pad_b = padding if padding is not None else (0,0,0,0)\n",
    "    self.pad = (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n",
    "    self.weight= torch.nn.Parameter(torch.randn((out_channels, in_channels, 4**2-1)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn((out_channels, 1)).type(COMPLEX_DTYPE))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batchsize ,c_in, h, w)\n",
    "    # weight has shape (c_out, c_in, 15)\n",
    "    # bias has shape (c_out, 1)\n",
    "    x = x.type(COMPLEX_DTYPE)\n",
    "    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n",
    "    h_out = (h_in-3+(self.pad_t+self.pad_b))//self.stride +1\n",
    "    w_out = (w_in-3+(self.pad_l+self.pad_r))//self.stride +1\n",
    "\n",
    "    #print(self.weight.shape)\n",
    "    #print(patches.shape)\n",
    "\n",
    "    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n",
    "\n",
    "\n",
    "    out = out + self.bias\n",
    "    return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n",
    "\n",
    "#test_module = FlippedQuanv3x3(in_channels=1, out_channels=2, stride=1, padding=(1,1,1,1)).to(device)\n",
    "#print(dummy_x.shape)\n",
    "#test_out = test_module(dummy_x.to(device))\n",
    "#print(test_out.shape)"
   ],
   "metadata": {
    "id": "Gww_XdJ5KPJt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464157287,
     "user_tz": -660,
     "elapsed": 529,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:06:54.181452Z",
     "start_time": "2024-04-06T21:06:54.175275Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flipped Quanvolution Replacing Conv2d\n",
    "\n",
    "First, let's just replace `Conv2d` with `FlippedQuanv3x3`."
   ],
   "metadata": {
    "id": "1yumZTjDVu63"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class HybridNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HybridNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(16*28*28, 10).type(REAL_DTYPE)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "net = HybridNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4RC5ou-VzbE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464159741,
     "user_tz": -660,
     "elapsed": 697,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "2c789a28-bef7-4844-938f-ad5276abbc6d",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:06:55.837282Z",
     "start_time": "2024-04-06T21:06:54.183834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "HybridNet(\n",
      "  (layers): Sequential(\n",
      "    (0): FlippedQuanv3x3()\n",
      "    (1): FlippedQuanv3x3()\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=12544, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_241/2687080126.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n",
      "/tmp/ipykernel_241/2683249851.py:30: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:301.)\n",
      "  return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "import torchmetrics\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.2\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4ZY59q1WS4x",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711489771972,
     "user_tz": -660,
     "elapsed": 19385897,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "068dfdb6-d534-467a-9d34-bd19bb63b442",
    "ExecuteTime": {
     "end_time": "2024-04-07T09:48:50.433894Z",
     "start_time": "2024-04-06T21:06:55.838543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 600, Number of test batches = 100\n",
      "Print every train batch = 120, Print every test batch = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_241/2687080126.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at step=0, batch=0, train loss = 2.5739781856536865, train acc = 0.019999999552965164, time = 0.34362101554870605\n",
      "Training at step=0, batch=120, train loss = 0.6837801337242126, train acc = 0.8399999737739563, time = 0.7058751583099365\n",
      "Training at step=0, batch=240, train loss = 0.5002118349075317, train acc = 0.9200000166893005, time = 0.7050154209136963\n",
      "Training at step=0, batch=360, train loss = 0.43308520317077637, train acc = 0.8700000047683716, time = 0.705674409866333\n",
      "Training at step=0, batch=480, train loss = 0.34859180450439453, train acc = 0.8999999761581421, time = 0.7056212425231934\n",
      "Testing at step=0, batch=0, test loss = 0.324838250875473, test acc = 0.9100000262260437, time = 0.4200572967529297\n",
      "Testing at step=0, batch=20, test loss = 0.39782819151878357, test acc = 0.8899999856948853, time = 0.41686224937438965\n",
      "Testing at step=0, batch=40, test loss = 0.4568188190460205, test acc = 0.8500000238418579, time = 0.41733694076538086\n",
      "Testing at step=0, batch=60, test loss = 0.3259071111679077, test acc = 0.8999999761581421, time = 0.41710519790649414\n",
      "Testing at step=0, batch=80, test loss = 0.4287797510623932, test acc = 0.8600000143051147, time = 0.41667795181274414\n",
      "Step 0 finished in 442.3339788913727, Train loss = 0.6634650827944278, Test loss = 0.3930456566810608; Train Acc = 0.8055499992705881, Test Acc = 0.8841000002622604\n",
      "Training at step=1, batch=0, train loss = 0.5300157070159912, train acc = 0.8399999737739563, time = 0.7096703052520752\n",
      "Training at step=1, batch=120, train loss = 0.3136621415615082, train acc = 0.9300000071525574, time = 0.7040219306945801\n",
      "Training at step=1, batch=240, train loss = 0.3772547245025635, train acc = 0.8500000238418579, time = 0.7045621871948242\n",
      "Training at step=1, batch=360, train loss = 0.41644608974456787, train acc = 0.8899999856948853, time = 0.7054331302642822\n",
      "Training at step=1, batch=480, train loss = 0.24430079758167267, train acc = 0.8999999761581421, time = 0.7048125267028809\n",
      "Testing at step=1, batch=0, test loss = 0.31734687089920044, test acc = 0.8799999952316284, time = 0.39134716987609863\n",
      "Testing at step=1, batch=20, test loss = 0.6252158880233765, test acc = 0.8500000238418579, time = 0.4170377254486084\n",
      "Testing at step=1, batch=40, test loss = 0.3248614966869354, test acc = 0.9200000166893005, time = 0.41702771186828613\n",
      "Testing at step=1, batch=60, test loss = 0.41819894313812256, test acc = 0.8899999856948853, time = 0.4169590473175049\n",
      "Testing at step=1, batch=80, test loss = 0.4916608929634094, test acc = 0.8600000143051147, time = 0.41710638999938965\n",
      "Step 1 finished in 456.150399684906, Train loss = 0.3940382376064857, Test loss = 0.35755584165453913; Train Acc = 0.8830666672190031, Test Acc = 0.8958000016212463\n",
      "Training at step=2, batch=0, train loss = 0.39358311891555786, train acc = 0.8500000238418579, time = 0.723463773727417\n",
      "Training at step=2, batch=120, train loss = 0.3587184548377991, train acc = 0.8999999761581421, time = 0.7058062553405762\n",
      "Training at step=2, batch=240, train loss = 0.4572848379611969, train acc = 0.8700000047683716, time = 0.7061977386474609\n",
      "Training at step=2, batch=360, train loss = 0.4715963304042816, train acc = 0.8799999952316284, time = 0.7043948173522949\n",
      "Training at step=2, batch=480, train loss = 0.3788885474205017, train acc = 0.8600000143051147, time = 0.7047240734100342\n",
      "Testing at step=2, batch=0, test loss = 0.5303632616996765, test acc = 0.8999999761581421, time = 0.40659213066101074\n",
      "Testing at step=2, batch=20, test loss = 0.3008715510368347, test acc = 0.9200000166893005, time = 0.4175539016723633\n",
      "Testing at step=2, batch=40, test loss = 0.2929135859012604, test acc = 0.8799999952316284, time = 0.41739726066589355\n",
      "Testing at step=2, batch=60, test loss = 0.3287830054759979, test acc = 0.8999999761581421, time = 0.4174621105194092\n",
      "Testing at step=2, batch=80, test loss = 0.17260463535785675, test acc = 0.9399999976158142, time = 0.4174365997314453\n",
      "Step 2 finished in 456.6513066291809, Train loss = 0.36431608321766057, Test loss = 0.32620058104395866; Train Acc = 0.8907000014185905, Test Acc = 0.9042000001668931\n",
      "Training at step=3, batch=0, train loss = 0.3854794204235077, train acc = 0.8700000047683716, time = 0.7230260372161865\n",
      "Training at step=3, batch=120, train loss = 0.2879258394241333, train acc = 0.9300000071525574, time = 0.7058968544006348\n",
      "Training at step=3, batch=240, train loss = 0.2965336740016937, train acc = 0.8799999952316284, time = 0.7341384887695312\n",
      "Training at step=3, batch=360, train loss = 0.3352455496788025, train acc = 0.9100000262260437, time = 0.7041583061218262\n",
      "Training at step=3, batch=480, train loss = 0.30610495805740356, train acc = 0.8799999952316284, time = 0.7049555778503418\n",
      "Testing at step=3, batch=0, test loss = 0.1778969019651413, test acc = 0.9300000071525574, time = 0.37743067741394043\n",
      "Testing at step=3, batch=20, test loss = 0.3765435814857483, test acc = 0.8700000047683716, time = 0.4155433177947998\n",
      "Testing at step=3, batch=40, test loss = 0.42110106348991394, test acc = 0.8700000047683716, time = 0.41797828674316406\n",
      "Testing at step=3, batch=60, test loss = 0.2009836584329605, test acc = 0.9100000262260437, time = 0.4170951843261719\n",
      "Testing at step=3, batch=80, test loss = 0.43156570196151733, test acc = 0.8299999833106995, time = 0.41724634170532227\n",
      "Step 3 finished in 458.7578978538513, Train loss = 0.3467799230168263, Test loss = 0.3111542811989784; Train Acc = 0.8977166681488356, Test Acc = 0.9076000028848648\n",
      "Training at step=4, batch=0, train loss = 0.41228336095809937, train acc = 0.8899999856948853, time = 0.7214140892028809\n",
      "Training at step=4, batch=120, train loss = 0.516899049282074, train acc = 0.8500000238418579, time = 0.7096712589263916\n",
      "Training at step=4, batch=240, train loss = 0.15684252977371216, train acc = 0.949999988079071, time = 0.704740047454834\n",
      "Training at step=4, batch=360, train loss = 0.3383699059486389, train acc = 0.8899999856948853, time = 0.7044715881347656\n",
      "Training at step=4, batch=480, train loss = 0.533606767654419, train acc = 0.8799999952316284, time = 0.700554609298706\n",
      "Testing at step=4, batch=0, test loss = 0.46818333864212036, test acc = 0.8600000143051147, time = 0.3905632495880127\n",
      "Testing at step=4, batch=20, test loss = 0.3185145854949951, test acc = 0.9200000166893005, time = 0.41657209396362305\n",
      "Testing at step=4, batch=40, test loss = 0.3785054683685303, test acc = 0.9399999976158142, time = 0.4171922206878662\n",
      "Testing at step=4, batch=60, test loss = 0.526338517665863, test acc = 0.8799999952316284, time = 0.4172346591949463\n",
      "Testing at step=4, batch=80, test loss = 0.41446247696876526, test acc = 0.8799999952316284, time = 0.41623950004577637\n",
      "Step 4 finished in 458.073219537735, Train loss = 0.3397034884120027, Test loss = 0.3391837403178215; Train Acc = 0.8993833363056183, Test Acc = 0.8979000043869019\n",
      "Training at step=5, batch=0, train loss = 0.27857768535614014, train acc = 0.8999999761581421, time = 0.7115373611450195\n",
      "Training at step=5, batch=120, train loss = 0.3494388461112976, train acc = 0.9100000262260437, time = 0.7049071788787842\n",
      "Training at step=5, batch=240, train loss = 0.18808706104755402, train acc = 0.9399999976158142, time = 0.7032313346862793\n",
      "Training at step=5, batch=360, train loss = 0.25573593378067017, train acc = 0.8999999761581421, time = 0.7037177085876465\n",
      "Training at step=5, batch=480, train loss = 0.41162532567977905, train acc = 0.9200000166893005, time = 0.7156147956848145\n",
      "Testing at step=5, batch=0, test loss = 0.38675186038017273, test acc = 0.8999999761581421, time = 0.39034438133239746\n",
      "Testing at step=5, batch=20, test loss = 0.25960928201675415, test acc = 0.9200000166893005, time = 0.4170725345611572\n",
      "Testing at step=5, batch=40, test loss = 0.22879236936569214, test acc = 0.949999988079071, time = 0.41848111152648926\n",
      "Testing at step=5, batch=60, test loss = 0.27972033619880676, test acc = 0.9300000071525574, time = 0.4177365303039551\n",
      "Testing at step=5, batch=80, test loss = 0.434874951839447, test acc = 0.9599999785423279, time = 0.4170258045196533\n",
      "Step 5 finished in 456.7122232913971, Train loss = 0.3351922041798631, Test loss = 0.3005824875831604; Train Acc = 0.9010666671395302, Test Acc = 0.9141999983787537\n",
      "Training at step=6, batch=0, train loss = 0.20971935987472534, train acc = 0.9399999976158142, time = 0.72125244140625\n",
      "Training at step=6, batch=120, train loss = 0.3285652995109558, train acc = 0.9100000262260437, time = 0.705601692199707\n",
      "Training at step=6, batch=240, train loss = 0.399718314409256, train acc = 0.8999999761581421, time = 0.7051336765289307\n",
      "Training at step=6, batch=360, train loss = 0.22231420874595642, train acc = 0.9399999976158142, time = 0.7051799297332764\n",
      "Training at step=6, batch=480, train loss = 0.4018006920814514, train acc = 0.8899999856948853, time = 0.705615758895874\n",
      "Testing at step=6, batch=0, test loss = 0.5883222818374634, test acc = 0.8700000047683716, time = 0.3974418640136719\n",
      "Testing at step=6, batch=20, test loss = 0.2479829043149948, test acc = 0.949999988079071, time = 0.41827392578125\n",
      "Testing at step=6, batch=40, test loss = 0.3229185938835144, test acc = 0.8999999761581421, time = 0.41857433319091797\n",
      "Testing at step=6, batch=60, test loss = 0.2275839000940323, test acc = 0.9599999785423279, time = 0.4179978370666504\n",
      "Testing at step=6, batch=80, test loss = 0.3178635239601135, test acc = 0.8899999856948853, time = 0.41748547554016113\n",
      "Step 6 finished in 456.8259787559509, Train loss = 0.3292738200351596, Test loss = 0.31435136169195177; Train Acc = 0.9035666676362356, Test Acc = 0.9133000004291535\n",
      "Training at step=7, batch=0, train loss = 0.20138105750083923, train acc = 0.9200000166893005, time = 0.7175648212432861\n",
      "Training at step=7, batch=120, train loss = 0.15725839138031006, train acc = 0.9599999785423279, time = 0.7107832431793213\n",
      "Training at step=7, batch=240, train loss = 0.46763721108436584, train acc = 0.8700000047683716, time = 0.7064883708953857\n",
      "Training at step=7, batch=360, train loss = 0.22965818643569946, train acc = 0.9599999785423279, time = 0.7037851810455322\n",
      "Training at step=7, batch=480, train loss = 0.3572818338871002, train acc = 0.8899999856948853, time = 0.7052886486053467\n",
      "Testing at step=7, batch=0, test loss = 0.29377204179763794, test acc = 0.9100000262260437, time = 0.3940694332122803\n",
      "Testing at step=7, batch=20, test loss = 0.29089146852493286, test acc = 0.8899999856948853, time = 0.41741204261779785\n",
      "Testing at step=7, batch=40, test loss = 0.44292503595352173, test acc = 0.8500000238418579, time = 0.41762781143188477\n",
      "Testing at step=7, batch=60, test loss = 0.4398801922798157, test acc = 0.8799999952316284, time = 0.41744518280029297\n",
      "Testing at step=7, batch=80, test loss = 0.575852632522583, test acc = 0.8299999833106995, time = 0.417712926864624\n",
      "Step 7 finished in 458.05166006088257, Train loss = 0.32554835177958014, Test loss = 0.3491062617301941; Train Acc = 0.9049666673938433, Test Acc = 0.9001000040769577\n",
      "Training at step=8, batch=0, train loss = 0.21291698515415192, train acc = 0.9399999976158142, time = 0.7182259559631348\n",
      "Training at step=8, batch=120, train loss = 0.42141902446746826, train acc = 0.8899999856948853, time = 0.7063348293304443\n",
      "Training at step=8, batch=240, train loss = 0.19770212471485138, train acc = 0.9399999976158142, time = 0.7044813632965088\n",
      "Training at step=8, batch=360, train loss = 0.3179113268852234, train acc = 0.8999999761581421, time = 0.7061424255371094\n",
      "Training at step=8, batch=480, train loss = 0.27870938181877136, train acc = 0.8999999761581421, time = 0.7064681053161621\n",
      "Testing at step=8, batch=0, test loss = 0.2649197578430176, test acc = 0.9300000071525574, time = 0.40305066108703613\n",
      "Testing at step=8, batch=20, test loss = 0.18605466187000275, test acc = 0.9599999785423279, time = 0.4177863597869873\n",
      "Testing at step=8, batch=40, test loss = 0.3561785817146301, test acc = 0.9300000071525574, time = 0.4168391227722168\n",
      "Testing at step=8, batch=60, test loss = 0.2573542594909668, test acc = 0.9300000071525574, time = 0.4178140163421631\n",
      "Testing at step=8, batch=80, test loss = 0.27290964126586914, test acc = 0.8999999761581421, time = 0.41649675369262695\n",
      "Step 8 finished in 456.0339934825897, Train loss = 0.32164659767101206, Test loss = 0.29694701328873635; Train Acc = 0.9071666679779689, Test Acc = 0.9147000032663345\n",
      "Training at step=9, batch=0, train loss = 0.3133675456047058, train acc = 0.9300000071525574, time = 0.7218561172485352\n",
      "Training at step=9, batch=120, train loss = 0.36550647020339966, train acc = 0.9200000166893005, time = 0.7044475078582764\n",
      "Training at step=9, batch=240, train loss = 0.33997416496276855, train acc = 0.9100000262260437, time = 0.7054095268249512\n",
      "Training at step=9, batch=360, train loss = 0.3570683002471924, train acc = 0.9200000166893005, time = 0.7049329280853271\n",
      "Training at step=9, batch=480, train loss = 0.2077770233154297, train acc = 0.9599999785423279, time = 0.704016923904419\n",
      "Testing at step=9, batch=0, test loss = 0.19193024933338165, test acc = 0.9300000071525574, time = 0.20804405212402344\n",
      "Testing at step=9, batch=20, test loss = 0.13758671283721924, test acc = 0.9599999785423279, time = 0.1380772590637207\n",
      "Testing at step=9, batch=40, test loss = 0.23045742511749268, test acc = 0.9100000262260437, time = 0.14110445976257324\n",
      "Testing at step=9, batch=60, test loss = 0.5042902231216431, test acc = 0.8899999856948853, time = 0.14912652969360352\n",
      "Testing at step=9, batch=80, test loss = 0.36683952808380127, test acc = 0.8399999737739563, time = 0.1410517692565918\n",
      "Step 9 finished in 457.0184602737427, Train loss = 0.3179030952478449, Test loss = 0.30726966336369516; Train Acc = 0.9083166675766309, Test Acc = 0.9099000000953674\n",
      "Training at step=10, batch=0, train loss = 0.3156071603298187, train acc = 0.9200000166893005, time = 0.7242069244384766\n",
      "Training at step=10, batch=120, train loss = 0.22726072371006012, train acc = 0.9300000071525574, time = 0.7343707084655762\n",
      "Training at step=10, batch=240, train loss = 0.4569813907146454, train acc = 0.8500000238418579, time = 0.7336366176605225\n",
      "Training at step=10, batch=360, train loss = 0.3176354467868805, train acc = 0.8899999856948853, time = 0.7055680751800537\n",
      "Training at step=10, batch=480, train loss = 0.2538701593875885, train acc = 0.9300000071525574, time = 0.7049016952514648\n",
      "Testing at step=10, batch=0, test loss = 0.2260836958885193, test acc = 0.9200000166893005, time = 0.4025905132293701\n",
      "Testing at step=10, batch=20, test loss = 0.15738888084888458, test acc = 0.949999988079071, time = 0.4169647693634033\n",
      "Testing at step=10, batch=40, test loss = 0.22415925562381744, test acc = 0.9399999976158142, time = 0.4179117679595947\n",
      "Testing at step=10, batch=60, test loss = 0.2588661313056946, test acc = 0.9300000071525574, time = 0.4181368350982666\n",
      "Testing at step=10, batch=80, test loss = 0.15864475071430206, test acc = 0.949999988079071, time = 0.41681957244873047\n",
      "Step 10 finished in 461.30804657936096, Train loss = 0.3130075608318051, Test loss = 0.29683389380574227; Train Acc = 0.9102833352486293, Test Acc = 0.9158000004291534\n",
      "Training at step=11, batch=0, train loss = 0.2486051768064499, train acc = 0.9300000071525574, time = 0.7254610061645508\n",
      "Training at step=11, batch=120, train loss = 0.31730177998542786, train acc = 0.9100000262260437, time = 0.7049751281738281\n",
      "Training at step=11, batch=240, train loss = 0.38511431217193604, train acc = 0.9100000262260437, time = 0.7040121555328369\n",
      "Training at step=11, batch=360, train loss = 0.3190593421459198, train acc = 0.8999999761581421, time = 0.7054388523101807\n",
      "Training at step=11, batch=480, train loss = 0.24074894189834595, train acc = 0.8999999761581421, time = 0.7046592235565186\n",
      "Testing at step=11, batch=0, test loss = 0.3281663954257965, test acc = 0.8700000047683716, time = 0.3959221839904785\n",
      "Testing at step=11, batch=20, test loss = 0.2813149392604828, test acc = 0.9100000262260437, time = 0.4183492660522461\n",
      "Testing at step=11, batch=40, test loss = 0.31874462962150574, test acc = 0.9200000166893005, time = 0.417311429977417\n",
      "Testing at step=11, batch=60, test loss = 0.4160652160644531, test acc = 0.8700000047683716, time = 0.41799283027648926\n",
      "Testing at step=11, batch=80, test loss = 0.3659057915210724, test acc = 0.8999999761581421, time = 0.41805553436279297\n",
      "Step 11 finished in 456.07953453063965, Train loss = 0.31354326696445545, Test loss = 0.3039502439647913; Train Acc = 0.9089000022411347, Test Acc = 0.9083000022172928\n",
      "Training at step=12, batch=0, train loss = 0.2115192860364914, train acc = 0.9200000166893005, time = 0.721656322479248\n",
      "Training at step=12, batch=120, train loss = 0.48521536588668823, train acc = 0.8600000143051147, time = 0.705716609954834\n",
      "Training at step=12, batch=240, train loss = 0.5167601108551025, train acc = 0.8399999737739563, time = 0.7010958194732666\n",
      "Training at step=12, batch=360, train loss = 0.27923378348350525, train acc = 0.9399999976158142, time = 0.7061612606048584\n",
      "Training at step=12, batch=480, train loss = 0.27447497844696045, train acc = 0.8999999761581421, time = 0.7044196128845215\n",
      "Testing at step=12, batch=0, test loss = 0.1935359537601471, test acc = 0.949999988079071, time = 0.3918428421020508\n",
      "Testing at step=12, batch=20, test loss = 0.43899083137512207, test acc = 0.8600000143051147, time = 0.41672706604003906\n",
      "Testing at step=12, batch=40, test loss = 0.18437281250953674, test acc = 0.9399999976158142, time = 0.4168877601623535\n",
      "Testing at step=12, batch=60, test loss = 0.376886248588562, test acc = 0.8700000047683716, time = 0.41778993606567383\n",
      "Testing at step=12, batch=80, test loss = 0.19431716203689575, test acc = 0.949999988079071, time = 0.41799235343933105\n",
      "Step 12 finished in 456.4218821525574, Train loss = 0.3114426917086045, Test loss = 0.2950641718506813; Train Acc = 0.9095166687170665, Test Acc = 0.9147000020742416\n",
      "Training at step=13, batch=0, train loss = 0.19451108574867249, train acc = 0.9599999785423279, time = 0.7195920944213867\n",
      "Training at step=13, batch=120, train loss = 0.3400035798549652, train acc = 0.8999999761581421, time = 0.7043361663818359\n",
      "Training at step=13, batch=240, train loss = 0.36348360776901245, train acc = 0.8999999761581421, time = 0.704857587814331\n",
      "Training at step=13, batch=360, train loss = 0.28192761540412903, train acc = 0.9100000262260437, time = 0.7038331031799316\n",
      "Training at step=13, batch=480, train loss = 0.3007866442203522, train acc = 0.949999988079071, time = 0.7053124904632568\n",
      "Testing at step=13, batch=0, test loss = 0.2752862572669983, test acc = 0.9399999976158142, time = 0.38898706436157227\n",
      "Testing at step=13, batch=20, test loss = 0.36092543601989746, test acc = 0.9200000166893005, time = 0.4169318675994873\n",
      "Testing at step=13, batch=40, test loss = 0.1478438526391983, test acc = 0.9599999785423279, time = 0.41723179817199707\n",
      "Testing at step=13, batch=60, test loss = 0.4373095631599426, test acc = 0.8700000047683716, time = 0.4170563220977783\n",
      "Testing at step=13, batch=80, test loss = 0.32006704807281494, test acc = 0.9100000262260437, time = 0.41693782806396484\n",
      "Step 13 finished in 456.3226342201233, Train loss = 0.31040113570789496, Test loss = 0.320362263917923; Train Acc = 0.9095500013232232, Test Acc = 0.9077999997138977\n",
      "Training at step=14, batch=0, train loss = 0.4506690204143524, train acc = 0.8600000143051147, time = 0.6863927841186523\n",
      "Training at step=14, batch=120, train loss = 0.3244296610355377, train acc = 0.8899999856948853, time = 0.7052717208862305\n",
      "Training at step=14, batch=240, train loss = 0.17644359171390533, train acc = 0.9599999785423279, time = 0.7046787738800049\n",
      "Training at step=14, batch=360, train loss = 0.17024566233158112, train acc = 0.9700000286102295, time = 0.7052276134490967\n",
      "Training at step=14, batch=480, train loss = 0.3120820224285126, train acc = 0.9100000262260437, time = 0.7055144309997559\n",
      "Testing at step=14, batch=0, test loss = 0.29341548681259155, test acc = 0.9300000071525574, time = 0.25060510635375977\n",
      "Testing at step=14, batch=20, test loss = 0.18438126146793365, test acc = 0.9200000166893005, time = 0.14214730262756348\n",
      "Testing at step=14, batch=40, test loss = 0.3891795873641968, test acc = 0.8799999952316284, time = 0.13713788986206055\n",
      "Testing at step=14, batch=60, test loss = 0.18659378588199615, test acc = 0.949999988079071, time = 0.1372237205505371\n",
      "Testing at step=14, batch=80, test loss = 0.5167925357818604, test acc = 0.8799999952316284, time = 0.1350264549255371\n",
      "Step 14 finished in 457.6079659461975, Train loss = 0.3141320527593295, Test loss = 0.286506113037467; Train Acc = 0.9083500016729037, Test Acc = 0.9159000021219253\n",
      "Training at step=15, batch=0, train loss = 0.35422709584236145, train acc = 0.8899999856948853, time = 0.7307696342468262\n",
      "Training at step=15, batch=120, train loss = 0.19995367527008057, train acc = 0.9399999976158142, time = 0.7045414447784424\n",
      "Training at step=15, batch=240, train loss = 0.31436243653297424, train acc = 0.8899999856948853, time = 0.7024776935577393\n",
      "Training at step=15, batch=360, train loss = 0.2488812804222107, train acc = 0.9100000262260437, time = 0.7035701274871826\n",
      "Training at step=15, batch=480, train loss = 0.2945319414138794, train acc = 0.9300000071525574, time = 0.7057251930236816\n",
      "Testing at step=15, batch=0, test loss = 0.3143533170223236, test acc = 0.9200000166893005, time = 0.10666513442993164\n",
      "Testing at step=15, batch=20, test loss = 0.3236825168132782, test acc = 0.9300000071525574, time = 0.13526082038879395\n",
      "Testing at step=15, batch=40, test loss = 0.2516079843044281, test acc = 0.9300000071525574, time = 0.13532233238220215\n",
      "Testing at step=15, batch=60, test loss = 0.2603752613067627, test acc = 0.9300000071525574, time = 0.13909196853637695\n",
      "Testing at step=15, batch=80, test loss = 0.2845805585384369, test acc = 0.8999999761581421, time = 0.13747429847717285\n",
      "Step 15 finished in 456.5843036174774, Train loss = 0.30523345979551475, Test loss = 0.29143827229738234; Train Acc = 0.9117833346128463, Test Acc = 0.9128000009059906\n",
      "Training at step=16, batch=0, train loss = 0.24454598128795624, train acc = 0.9200000166893005, time = 0.7016568183898926\n",
      "Training at step=16, batch=120, train loss = 0.3796285390853882, train acc = 0.9200000166893005, time = 0.7036263942718506\n",
      "Training at step=16, batch=240, train loss = 0.24319033324718475, train acc = 0.9200000166893005, time = 0.7056419849395752\n",
      "Training at step=16, batch=360, train loss = 0.21136288344860077, train acc = 0.9399999976158142, time = 0.704338550567627\n",
      "Training at step=16, batch=480, train loss = 0.20426130294799805, train acc = 0.9300000071525574, time = 0.7042281627655029\n",
      "Testing at step=16, batch=0, test loss = 0.306721568107605, test acc = 0.9100000262260437, time = 0.20790719985961914\n",
      "Testing at step=16, batch=20, test loss = 0.5532830357551575, test acc = 0.8299999833106995, time = 0.1355905532836914\n",
      "Testing at step=16, batch=40, test loss = 0.20489953458309174, test acc = 0.9399999976158142, time = 0.13757085800170898\n",
      "Testing at step=16, batch=60, test loss = 0.193007692694664, test acc = 0.9300000071525574, time = 0.1381065845489502\n",
      "Testing at step=16, batch=80, test loss = 0.18581968545913696, test acc = 0.9300000071525574, time = 0.1368110179901123\n",
      "Step 16 finished in 456.55768394470215, Train loss = 0.307104751120011, Test loss = 0.3091784549504519; Train Acc = 0.9112833342949549, Test Acc = 0.9084000027179718\n",
      "Training at step=17, batch=0, train loss = 0.25155773758888245, train acc = 0.9399999976158142, time = 0.7270596027374268\n",
      "Training at step=17, batch=120, train loss = 0.550271213054657, train acc = 0.8899999856948853, time = 0.731806755065918\n",
      "Training at step=17, batch=240, train loss = 0.24080462753772736, train acc = 0.9200000166893005, time = 0.694547176361084\n",
      "Training at step=17, batch=360, train loss = 0.3962947130203247, train acc = 0.8999999761581421, time = 0.7055184841156006\n",
      "Training at step=17, batch=480, train loss = 0.4318433403968811, train acc = 0.8899999856948853, time = 0.7052552700042725\n",
      "Testing at step=17, batch=0, test loss = 0.2208552360534668, test acc = 0.9599999785423279, time = 0.40401387214660645\n",
      "Testing at step=17, batch=20, test loss = 0.17801351845264435, test acc = 0.9300000071525574, time = 0.4180116653442383\n",
      "Testing at step=17, batch=40, test loss = 0.3420254588127136, test acc = 0.9200000166893005, time = 0.41695737838745117\n",
      "Testing at step=17, batch=60, test loss = 0.36066535115242004, test acc = 0.9100000262260437, time = 0.4171876907348633\n",
      "Testing at step=17, batch=80, test loss = 0.3225383758544922, test acc = 0.9300000071525574, time = 0.4164125919342041\n",
      "Step 17 finished in 460.7536053657532, Train loss = 0.30188737632085877, Test loss = 0.30118512265384195; Train Acc = 0.912716668744882, Test Acc = 0.9131000006198883\n",
      "Training at step=18, batch=0, train loss = 0.3958745300769806, train acc = 0.8999999761581421, time = 0.7169554233551025\n",
      "Training at step=18, batch=120, train loss = 0.2548365294933319, train acc = 0.9200000166893005, time = 0.7073407173156738\n",
      "Training at step=18, batch=240, train loss = 0.6471558213233948, train acc = 0.8799999952316284, time = 0.7049412727355957\n",
      "Training at step=18, batch=360, train loss = 0.20252519845962524, train acc = 0.949999988079071, time = 0.7053437232971191\n",
      "Training at step=18, batch=480, train loss = 0.3040880560874939, train acc = 0.9200000166893005, time = 0.7043898105621338\n",
      "Testing at step=18, batch=0, test loss = 0.43534836173057556, test acc = 0.8999999761581421, time = 0.39764881134033203\n",
      "Testing at step=18, batch=20, test loss = 0.29227471351623535, test acc = 0.9200000166893005, time = 0.4171168804168701\n",
      "Testing at step=18, batch=40, test loss = 0.4113731384277344, test acc = 0.8600000143051147, time = 0.41603565216064453\n",
      "Testing at step=18, batch=60, test loss = 0.29680290818214417, test acc = 0.9100000262260437, time = 0.4169003963470459\n",
      "Testing at step=18, batch=80, test loss = 0.18005457520484924, test acc = 0.9399999976158142, time = 0.41556477546691895\n",
      "Step 18 finished in 457.4690442085266, Train loss = 0.301111330849429, Test loss = 0.2961940681934357; Train Acc = 0.9127833358446757, Test Acc = 0.9153000009059906\n",
      "Training at step=19, batch=0, train loss = 0.281988263130188, train acc = 0.9399999976158142, time = 0.711799144744873\n",
      "Training at step=19, batch=120, train loss = 0.33832260966300964, train acc = 0.9200000166893005, time = 0.7327492237091064\n",
      "Training at step=19, batch=240, train loss = 0.28624436259269714, train acc = 0.949999988079071, time = 0.7050206661224365\n",
      "Training at step=19, batch=360, train loss = 0.22172050178050995, train acc = 0.9300000071525574, time = 0.7059786319732666\n",
      "Training at step=19, batch=480, train loss = 0.14761021733283997, train acc = 0.949999988079071, time = 0.7056717872619629\n",
      "Testing at step=19, batch=0, test loss = 0.36023056507110596, test acc = 0.8799999952316284, time = 0.3925759792327881\n",
      "Testing at step=19, batch=20, test loss = 0.3925025463104248, test acc = 0.9100000262260437, time = 0.4165477752685547\n",
      "Testing at step=19, batch=40, test loss = 0.15892626345157623, test acc = 0.9700000286102295, time = 0.41762280464172363\n",
      "Testing at step=19, batch=60, test loss = 0.297181099653244, test acc = 0.9100000262260437, time = 0.4176790714263916\n",
      "Testing at step=19, batch=80, test loss = 0.2919958829879761, test acc = 0.9300000071525574, time = 0.41672563552856445\n",
      "Step 19 finished in 459.1053116321564, Train loss = 0.3047912218545874, Test loss = 0.28157361954450605; Train Acc = 0.9107500006755193, Test Acc = 0.9190000021457672\n",
      "Training at step=20, batch=0, train loss = 0.36472228169441223, train acc = 0.8999999761581421, time = 0.7200148105621338\n",
      "Training at step=20, batch=120, train loss = 0.27536025643348694, train acc = 0.9200000166893005, time = 0.7033259868621826\n",
      "Training at step=20, batch=240, train loss = 0.4169420897960663, train acc = 0.8299999833106995, time = 0.705237627029419\n",
      "Training at step=20, batch=360, train loss = 0.3848680555820465, train acc = 0.8899999856948853, time = 0.7056243419647217\n",
      "Training at step=20, batch=480, train loss = 0.2590414881706238, train acc = 0.9300000071525574, time = 0.7040746212005615\n",
      "Testing at step=20, batch=0, test loss = 0.46574902534484863, test acc = 0.9100000262260437, time = 0.39119434356689453\n",
      "Testing at step=20, batch=20, test loss = 0.4542798697948456, test acc = 0.8899999856948853, time = 0.4171485900878906\n",
      "Testing at step=20, batch=40, test loss = 0.24299955368041992, test acc = 0.9300000071525574, time = 0.41636109352111816\n",
      "Testing at step=20, batch=60, test loss = 0.17032434046268463, test acc = 0.949999988079071, time = 0.41739368438720703\n",
      "Testing at step=20, batch=80, test loss = 0.5247600674629211, test acc = 0.8999999761581421, time = 0.4167814254760742\n",
      "Step 20 finished in 456.13591980934143, Train loss = 0.29905805182953676, Test loss = 0.28784595005214214; Train Acc = 0.914000001847744, Test Acc = 0.9178000009059906\n",
      "Training at step=21, batch=0, train loss = 0.3177446126937866, train acc = 0.8899999856948853, time = 0.7115237712860107\n",
      "Training at step=21, batch=120, train loss = 0.17752455174922943, train acc = 0.9300000071525574, time = 0.7048518657684326\n",
      "Training at step=21, batch=240, train loss = 0.3828091025352478, train acc = 0.8600000143051147, time = 0.704261064529419\n",
      "Training at step=21, batch=360, train loss = 0.18944303691387177, train acc = 0.9700000286102295, time = 0.7058258056640625\n",
      "Training at step=21, batch=480, train loss = 0.39700213074684143, train acc = 0.9300000071525574, time = 0.7053289413452148\n",
      "Testing at step=21, batch=0, test loss = 0.16768032312393188, test acc = 0.949999988079071, time = 0.21495771408081055\n",
      "Testing at step=21, batch=20, test loss = 0.2985604703426361, test acc = 0.9200000166893005, time = 0.1419532299041748\n",
      "Testing at step=21, batch=40, test loss = 0.2788107991218567, test acc = 0.9100000262260437, time = 0.1420588493347168\n",
      "Testing at step=21, batch=60, test loss = 0.5046565532684326, test acc = 0.8700000047683716, time = 0.41727161407470703\n",
      "Testing at step=21, batch=80, test loss = 0.4630471467971802, test acc = 0.8799999952316284, time = 0.41619133949279785\n",
      "Step 21 finished in 456.13226675987244, Train loss = 0.3000829157605767, Test loss = 0.2786427916586399; Train Acc = 0.913416668176651, Test Acc = 0.9220000040531159\n",
      "Training at step=22, batch=0, train loss = 0.3634091317653656, train acc = 0.8899999856948853, time = 0.721498966217041\n",
      "Training at step=22, batch=120, train loss = 0.41005635261535645, train acc = 0.9100000262260437, time = 0.7211182117462158\n",
      "Training at step=22, batch=240, train loss = 0.35412120819091797, train acc = 0.9200000166893005, time = 0.7061882019042969\n",
      "Training at step=22, batch=360, train loss = 0.2447309046983719, train acc = 0.9200000166893005, time = 0.7315561771392822\n",
      "Training at step=22, batch=480, train loss = 0.16455137729644775, train acc = 0.949999988079071, time = 0.7323217391967773\n",
      "Testing at step=22, batch=0, test loss = 0.3549445867538452, test acc = 0.9100000262260437, time = 0.3573906421661377\n",
      "Testing at step=22, batch=20, test loss = 0.29912254214286804, test acc = 0.9399999976158142, time = 0.41818952560424805\n",
      "Testing at step=22, batch=40, test loss = 0.17584241926670074, test acc = 0.9399999976158142, time = 0.41766929626464844\n",
      "Testing at step=22, batch=60, test loss = 0.15445975959300995, test acc = 0.9399999976158142, time = 0.41826772689819336\n",
      "Testing at step=22, batch=80, test loss = 0.2922728657722473, test acc = 0.9100000262260437, time = 0.41708993911743164\n",
      "Step 22 finished in 463.92500925064087, Train loss = 0.29793779240300255, Test loss = 0.2992649154365063; Train Acc = 0.9144666688640912, Test Acc = 0.9096000027656556\n",
      "Training at step=23, batch=0, train loss = 0.23977790772914886, train acc = 0.8899999856948853, time = 0.7222423553466797\n",
      "Training at step=23, batch=120, train loss = 0.33076027035713196, train acc = 0.8999999761581421, time = 0.7050342559814453\n",
      "Training at step=23, batch=240, train loss = 0.3727603852748871, train acc = 0.8999999761581421, time = 0.7046525478363037\n",
      "Training at step=23, batch=360, train loss = 0.38853809237480164, train acc = 0.9100000262260437, time = 0.7057983875274658\n",
      "Training at step=23, batch=480, train loss = 0.1433493047952652, train acc = 0.949999988079071, time = 0.7054052352905273\n",
      "Testing at step=23, batch=0, test loss = 0.27965307235717773, test acc = 0.9399999976158142, time = 0.391171932220459\n",
      "Testing at step=23, batch=20, test loss = 0.24311108887195587, test acc = 0.9200000166893005, time = 0.4171011447906494\n",
      "Testing at step=23, batch=40, test loss = 0.33739566802978516, test acc = 0.9200000166893005, time = 0.4167931079864502\n",
      "Testing at step=23, batch=60, test loss = 0.20667210221290588, test acc = 0.9200000166893005, time = 0.41689586639404297\n",
      "Testing at step=23, batch=80, test loss = 0.373419851064682, test acc = 0.8700000047683716, time = 0.41843581199645996\n",
      "Step 23 finished in 456.8810923099518, Train loss = 0.29707142391552527, Test loss = 0.28600602053105834; Train Acc = 0.9150333364804586, Test Acc = 0.9177000051736832\n",
      "Training at step=24, batch=0, train loss = 0.306482195854187, train acc = 0.9100000262260437, time = 0.721898078918457\n",
      "Training at step=24, batch=120, train loss = 0.3512825071811676, train acc = 0.8999999761581421, time = 0.7068965435028076\n",
      "Training at step=24, batch=240, train loss = 0.21118701994419098, train acc = 0.9200000166893005, time = 0.7046768665313721\n",
      "Training at step=24, batch=360, train loss = 0.3121815621852875, train acc = 0.9399999976158142, time = 0.7063150405883789\n",
      "Training at step=24, batch=480, train loss = 0.2980419993400574, train acc = 0.8999999761581421, time = 0.705176591873169\n",
      "Testing at step=24, batch=0, test loss = 0.39855849742889404, test acc = 0.8999999761581421, time = 0.4193594455718994\n",
      "Testing at step=24, batch=20, test loss = 0.1877722144126892, test acc = 0.9200000166893005, time = 0.4177556037902832\n",
      "Testing at step=24, batch=40, test loss = 0.3236722946166992, test acc = 0.9100000262260437, time = 0.41539478302001953\n",
      "Testing at step=24, batch=60, test loss = 0.23708575963974, test acc = 0.9599999785423279, time = 0.4167466163635254\n",
      "Testing at step=24, batch=80, test loss = 0.3199087381362915, test acc = 0.9300000071525574, time = 0.41741371154785156\n",
      "Step 24 finished in 456.27800393104553, Train loss = 0.2975477141017715, Test loss = 0.28629290603101254; Train Acc = 0.9143000021576881, Test Acc = 0.9154000014066697\n",
      "Training at step=25, batch=0, train loss = 0.11969698965549469, train acc = 0.9599999785423279, time = 0.7216465473175049\n",
      "Training at step=25, batch=120, train loss = 0.24972866475582123, train acc = 0.8999999761581421, time = 0.7046825885772705\n",
      "Training at step=25, batch=240, train loss = 0.34012627601623535, train acc = 0.9100000262260437, time = 0.7054557800292969\n",
      "Training at step=25, batch=360, train loss = 0.28342965245246887, train acc = 0.949999988079071, time = 0.7077467441558838\n",
      "Training at step=25, batch=480, train loss = 0.17202936112880707, train acc = 0.9599999785423279, time = 0.7039260864257812\n",
      "Testing at step=25, batch=0, test loss = 0.27805793285369873, test acc = 0.8999999761581421, time = 0.2849109172821045\n",
      "Testing at step=25, batch=20, test loss = 0.2098546177148819, test acc = 0.8999999761581421, time = 0.41777515411376953\n",
      "Testing at step=25, batch=40, test loss = 0.15058958530426025, test acc = 0.9599999785423279, time = 0.4176771640777588\n",
      "Testing at step=25, batch=60, test loss = 0.2862600088119507, test acc = 0.9200000166893005, time = 0.4159731864929199\n",
      "Testing at step=25, batch=80, test loss = 0.28355106711387634, test acc = 0.8799999952316284, time = 0.4174995422363281\n",
      "Step 25 finished in 456.0782108306885, Train loss = 0.29628139711916446, Test loss = 0.2828371132910252; Train Acc = 0.9146666680773099, Test Acc = 0.9197000026702881\n",
      "Training at step=26, batch=0, train loss = 0.3242126703262329, train acc = 0.9100000262260437, time = 0.7158386707305908\n",
      "Training at step=26, batch=120, train loss = 0.3603076934814453, train acc = 0.8999999761581421, time = 0.7071459293365479\n",
      "Training at step=26, batch=240, train loss = 0.2798357307910919, train acc = 0.9100000262260437, time = 0.7053849697113037\n",
      "Training at step=26, batch=360, train loss = 0.24932309985160828, train acc = 0.949999988079071, time = 0.705329418182373\n",
      "Training at step=26, batch=480, train loss = 0.2874123156070709, train acc = 0.9300000071525574, time = 0.7070026397705078\n",
      "Testing at step=26, batch=0, test loss = 0.29613035917282104, test acc = 0.949999988079071, time = 0.20970559120178223\n",
      "Testing at step=26, batch=20, test loss = 0.27557438611984253, test acc = 0.8999999761581421, time = 0.1422712802886963\n",
      "Testing at step=26, batch=40, test loss = 0.29648852348327637, test acc = 0.8700000047683716, time = 0.14759111404418945\n",
      "Testing at step=26, batch=60, test loss = 0.4163139760494232, test acc = 0.9100000262260437, time = 0.24516892433166504\n",
      "Testing at step=26, batch=80, test loss = 0.29043853282928467, test acc = 0.8999999761581421, time = 0.14718174934387207\n",
      "Step 26 finished in 456.84011793136597, Train loss = 0.2922219917178154, Test loss = 0.2781476862728596; Train Acc = 0.9163500016927719, Test Acc = 0.9226000040769577\n",
      "Training at step=27, batch=0, train loss = 0.4038655459880829, train acc = 0.8799999952316284, time = 0.7257580757141113\n",
      "Training at step=27, batch=120, train loss = 0.1376204937696457, train acc = 0.9700000286102295, time = 0.7048146724700928\n",
      "Training at step=27, batch=240, train loss = 0.27899616956710815, train acc = 0.9300000071525574, time = 0.7053694725036621\n",
      "Training at step=27, batch=360, train loss = 0.36094310879707336, train acc = 0.8700000047683716, time = 0.6909723281860352\n",
      "Training at step=27, batch=480, train loss = 0.2338264435529709, train acc = 0.9399999976158142, time = 0.7034118175506592\n",
      "Testing at step=27, batch=0, test loss = 0.1942325234413147, test acc = 0.9200000166893005, time = 0.20326828956604004\n",
      "Testing at step=27, batch=20, test loss = 0.334823340177536, test acc = 0.9100000262260437, time = 0.1420886516571045\n",
      "Testing at step=27, batch=40, test loss = 0.17436477541923523, test acc = 0.9399999976158142, time = 0.13993191719055176\n",
      "Testing at step=27, batch=60, test loss = 0.32659149169921875, test acc = 0.8700000047683716, time = 0.14168596267700195\n",
      "Testing at step=27, batch=80, test loss = 0.18309654295444489, test acc = 0.9200000166893005, time = 0.14214015007019043\n",
      "Step 27 finished in 455.4326057434082, Train loss = 0.29299437926461297, Test loss = 0.2740336712449789; Train Acc = 0.9158500018715858, Test Acc = 0.922200003862381\n",
      "Training at step=28, batch=0, train loss = 0.18941634893417358, train acc = 0.9599999785423279, time = 0.712202787399292\n",
      "Training at step=28, batch=120, train loss = 0.4028725028038025, train acc = 0.8899999856948853, time = 0.7050971984863281\n",
      "Training at step=28, batch=240, train loss = 0.3024965524673462, train acc = 0.8999999761581421, time = 0.7066981792449951\n",
      "Training at step=28, batch=360, train loss = 0.232091024518013, train acc = 0.949999988079071, time = 0.694878101348877\n",
      "Training at step=28, batch=480, train loss = 0.18174929916858673, train acc = 0.9399999976158142, time = 0.7055954933166504\n",
      "Testing at step=28, batch=0, test loss = 0.28395724296569824, test acc = 0.9100000262260437, time = 0.21747994422912598\n",
      "Testing at step=28, batch=20, test loss = 0.23606233298778534, test acc = 0.9200000166893005, time = 0.14188885688781738\n",
      "Testing at step=28, batch=40, test loss = 0.19322039186954498, test acc = 0.949999988079071, time = 0.14425086975097656\n",
      "Testing at step=28, batch=60, test loss = 0.29254814982414246, test acc = 0.9100000262260437, time = 0.4172379970550537\n",
      "Testing at step=28, batch=80, test loss = 0.3250911831855774, test acc = 0.8899999856948853, time = 0.41636228561401367\n",
      "Step 28 finished in 456.2554337978363, Train loss = 0.2909452121456464, Test loss = 0.28451993986964225; Train Acc = 0.917283333837986, Test Acc = 0.9162000036239624\n",
      "Training at step=29, batch=0, train loss = 0.4984719455242157, train acc = 0.8600000143051147, time = 0.7225847244262695\n",
      "Training at step=29, batch=120, train loss = 0.49271729588508606, train acc = 0.8500000238418579, time = 0.7288858890533447\n",
      "Training at step=29, batch=240, train loss = 0.48155248165130615, train acc = 0.8500000238418579, time = 0.7108545303344727\n",
      "Training at step=29, batch=360, train loss = 0.26019972562789917, train acc = 0.9200000166893005, time = 0.7333252429962158\n",
      "Training at step=29, batch=480, train loss = 0.44296029210090637, train acc = 0.9100000262260437, time = 0.7140340805053711\n",
      "Testing at step=29, batch=0, test loss = 0.25327569246292114, test acc = 0.9200000166893005, time = 0.20953893661499023\n",
      "Testing at step=29, batch=20, test loss = 0.27628397941589355, test acc = 0.9300000071525574, time = 0.1418299674987793\n",
      "Testing at step=29, batch=40, test loss = 0.31237122416496277, test acc = 0.9100000262260437, time = 0.1432197093963623\n",
      "Testing at step=29, batch=60, test loss = 0.25715935230255127, test acc = 0.9399999976158142, time = 0.14525818824768066\n",
      "Testing at step=29, batch=80, test loss = 0.3906749486923218, test acc = 0.9200000166893005, time = 0.14266180992126465\n",
      "Step 29 finished in 467.32447838783264, Train loss = 0.2940701772272587, Test loss = 0.27706236004829404; Train Acc = 0.9146000015735626, Test Acc = 0.9185000026226043\n",
      "Training at step=30, batch=0, train loss = 0.28285273909568787, train acc = 0.8999999761581421, time = 0.7072052955627441\n",
      "Training at step=30, batch=120, train loss = 0.18314534425735474, train acc = 0.949999988079071, time = 0.705087423324585\n",
      "Training at step=30, batch=240, train loss = 0.5033131241798401, train acc = 0.8999999761581421, time = 0.705397367477417\n",
      "Training at step=30, batch=360, train loss = 0.4406898021697998, train acc = 0.8799999952316284, time = 0.7034890651702881\n",
      "Training at step=30, batch=480, train loss = 0.4179152548313141, train acc = 0.8600000143051147, time = 0.7044651508331299\n",
      "Testing at step=30, batch=0, test loss = 0.35476619005203247, test acc = 0.8700000047683716, time = 0.2152092456817627\n",
      "Testing at step=30, batch=20, test loss = 0.1885611116886139, test acc = 0.9200000166893005, time = 0.14097952842712402\n",
      "Testing at step=30, batch=40, test loss = 0.16908997297286987, test acc = 0.9599999785423279, time = 0.13996171951293945\n",
      "Testing at step=30, batch=60, test loss = 0.41058066487312317, test acc = 0.8500000238418579, time = 0.14144349098205566\n",
      "Testing at step=30, batch=80, test loss = 0.21320785582065582, test acc = 0.9399999976158142, time = 0.14168739318847656\n",
      "Step 30 finished in 455.7064232826233, Train loss = 0.2913064700489243, Test loss = 0.28610331006348133; Train Acc = 0.9170000018676122, Test Acc = 0.9180000025033951\n",
      "Training at step=31, batch=0, train loss = 0.20035097002983093, train acc = 0.9399999976158142, time = 0.7029922008514404\n",
      "Training at step=31, batch=120, train loss = 0.22941385209560394, train acc = 0.9399999976158142, time = 0.703066349029541\n",
      "Training at step=31, batch=240, train loss = 0.15849357843399048, train acc = 0.9599999785423279, time = 0.7054755687713623\n",
      "Training at step=31, batch=360, train loss = 0.1696702390909195, train acc = 0.9700000286102295, time = 0.6993570327758789\n",
      "Training at step=31, batch=480, train loss = 0.30432265996932983, train acc = 0.9200000166893005, time = 0.7044274806976318\n",
      "Testing at step=31, batch=0, test loss = 0.3783038854598999, test acc = 0.9200000166893005, time = 0.39897966384887695\n",
      "Testing at step=31, batch=20, test loss = 0.19613981246948242, test acc = 0.9300000071525574, time = 0.4171469211578369\n",
      "Testing at step=31, batch=40, test loss = 0.25221195816993713, test acc = 0.9200000166893005, time = 0.41715431213378906\n",
      "Testing at step=31, batch=60, test loss = 0.5357556343078613, test acc = 0.8799999952316284, time = 0.41588902473449707\n",
      "Testing at step=31, batch=80, test loss = 0.21763135492801666, test acc = 0.9399999976158142, time = 0.41767311096191406\n",
      "Step 31 finished in 456.207937002182, Train loss = 0.2897288421789805, Test loss = 0.2809694840013981; Train Acc = 0.9175166674455006, Test Acc = 0.9200000029802322\n",
      "Training at step=32, batch=0, train loss = 0.352355033159256, train acc = 0.9300000071525574, time = 0.7193930149078369\n",
      "Training at step=32, batch=120, train loss = 0.2257077842950821, train acc = 0.949999988079071, time = 0.7052969932556152\n",
      "Training at step=32, batch=240, train loss = 0.17909163236618042, train acc = 0.949999988079071, time = 0.7031314373016357\n",
      "Training at step=32, batch=360, train loss = 0.3027305006980896, train acc = 0.9300000071525574, time = 0.7036683559417725\n",
      "Training at step=32, batch=480, train loss = 0.6048910617828369, train acc = 0.9200000166893005, time = 0.7036805152893066\n",
      "Testing at step=32, batch=0, test loss = 0.35670554637908936, test acc = 0.8700000047683716, time = 0.21052885055541992\n",
      "Testing at step=32, batch=20, test loss = 0.28366073966026306, test acc = 0.9200000166893005, time = 0.37666893005371094\n",
      "Testing at step=32, batch=40, test loss = 0.19646373391151428, test acc = 0.949999988079071, time = 0.4176003932952881\n",
      "Testing at step=32, batch=60, test loss = 0.2582413852214813, test acc = 0.9399999976158142, time = 0.41800379753112793\n",
      "Testing at step=32, batch=80, test loss = 0.2598658800125122, test acc = 0.9200000166893005, time = 0.41716909408569336\n",
      "Step 32 finished in 456.4241569042206, Train loss = 0.2920303006966909, Test loss = 0.281145730689168; Train Acc = 0.9166833358009656, Test Acc = 0.9217000013589859\n",
      "Training at step=33, batch=0, train loss = 0.22990837693214417, train acc = 0.9100000262260437, time = 0.7183670997619629\n",
      "Training at step=33, batch=120, train loss = 0.21072961390018463, train acc = 0.9300000071525574, time = 0.7331812381744385\n",
      "Training at step=33, batch=240, train loss = 0.31065550446510315, train acc = 0.9100000262260437, time = 0.7040033340454102\n",
      "Training at step=33, batch=360, train loss = 0.23935040831565857, train acc = 0.949999988079071, time = 0.7031717300415039\n",
      "Training at step=33, batch=480, train loss = 0.30639392137527466, train acc = 0.9300000071525574, time = 0.7017922401428223\n",
      "Testing at step=33, batch=0, test loss = 0.27336883544921875, test acc = 0.9200000166893005, time = 0.3857731819152832\n",
      "Testing at step=33, batch=20, test loss = 0.17529548704624176, test acc = 0.949999988079071, time = 0.41684818267822266\n",
      "Testing at step=33, batch=40, test loss = 0.2813801169395447, test acc = 0.8999999761581421, time = 0.41741490364074707\n",
      "Testing at step=33, batch=60, test loss = 0.32239001989364624, test acc = 0.8799999952316284, time = 0.41734862327575684\n",
      "Testing at step=33, batch=80, test loss = 0.4685783386230469, test acc = 0.8600000143051147, time = 0.4164724349975586\n",
      "Step 33 finished in 457.6016249656677, Train loss = 0.2901714871575435, Test loss = 0.28380899228155615; Train Acc = 0.9165166693925858, Test Acc = 0.9206000047922135\n",
      "Training at step=34, batch=0, train loss = 0.45497509837150574, train acc = 0.8799999952316284, time = 0.7155055999755859\n",
      "Training at step=34, batch=120, train loss = 0.19836091995239258, train acc = 0.949999988079071, time = 0.703474760055542\n",
      "Training at step=34, batch=240, train loss = 0.39408713579177856, train acc = 0.8799999952316284, time = 0.7041923999786377\n",
      "Training at step=34, batch=360, train loss = 0.2673299312591553, train acc = 0.9300000071525574, time = 0.7062468528747559\n",
      "Training at step=34, batch=480, train loss = 0.3197988271713257, train acc = 0.8899999856948853, time = 0.6046442985534668\n",
      "Testing at step=34, batch=0, test loss = 0.28299012780189514, test acc = 0.8999999761581421, time = 0.217041015625\n",
      "Testing at step=34, batch=20, test loss = 0.5698409676551819, test acc = 0.8899999856948853, time = 0.13584113121032715\n",
      "Testing at step=34, batch=40, test loss = 0.2249288558959961, test acc = 0.9399999976158142, time = 0.13639187812805176\n",
      "Testing at step=34, batch=60, test loss = 0.21086455881595612, test acc = 0.9399999976158142, time = 0.1359262466430664\n",
      "Testing at step=34, batch=80, test loss = 0.16312271356582642, test acc = 0.9399999976158142, time = 0.1366136074066162\n",
      "Step 34 finished in 456.29217553138733, Train loss = 0.2879521521553397, Test loss = 0.27538207910954954; Train Acc = 0.9184166686733564, Test Acc = 0.9193000000715256\n",
      "Training at step=35, batch=0, train loss = 0.1956363320350647, train acc = 0.9399999976158142, time = 0.7072100639343262\n",
      "Training at step=35, batch=120, train loss = 0.3024442791938782, train acc = 0.8999999761581421, time = 0.7043898105621338\n",
      "Training at step=35, batch=240, train loss = 0.3050225079059601, train acc = 0.9100000262260437, time = 0.7044711112976074\n",
      "Training at step=35, batch=360, train loss = 0.46058663725852966, train acc = 0.8799999952316284, time = 0.7063064575195312\n",
      "Training at step=35, batch=480, train loss = 0.27347588539123535, train acc = 0.9399999976158142, time = 0.7074141502380371\n",
      "Testing at step=35, batch=0, test loss = 0.17404912412166595, test acc = 0.9300000071525574, time = 0.39593935012817383\n",
      "Testing at step=35, batch=20, test loss = 0.4499690532684326, test acc = 0.9100000262260437, time = 0.4175679683685303\n",
      "Testing at step=35, batch=40, test loss = 0.15145868062973022, test acc = 0.949999988079071, time = 0.4185752868652344\n",
      "Testing at step=35, batch=60, test loss = 0.21404850482940674, test acc = 0.9100000262260437, time = 0.41828179359436035\n",
      "Testing at step=35, batch=80, test loss = 0.13175930082798004, test acc = 0.9599999785423279, time = 0.4176161289215088\n",
      "Step 35 finished in 456.4702389240265, Train loss = 0.28999040063470605, Test loss = 0.2789183945208788; Train Acc = 0.9172000019749006, Test Acc = 0.9202000021934509\n",
      "Training at step=36, batch=0, train loss = 0.3632431924343109, train acc = 0.8899999856948853, time = 0.7236685752868652\n",
      "Training at step=36, batch=120, train loss = 0.21646234393119812, train acc = 0.9300000071525574, time = 0.7028264999389648\n",
      "Training at step=36, batch=240, train loss = 0.2761783301830292, train acc = 0.9200000166893005, time = 0.7038652896881104\n",
      "Training at step=36, batch=360, train loss = 0.28555431962013245, train acc = 0.9100000262260437, time = 0.70577073097229\n",
      "Training at step=36, batch=480, train loss = 0.3394604027271271, train acc = 0.8999999761581421, time = 0.7040472030639648\n",
      "Testing at step=36, batch=0, test loss = 0.25631678104400635, test acc = 0.9399999976158142, time = 0.3727881908416748\n",
      "Testing at step=36, batch=20, test loss = 0.22561849653720856, test acc = 0.9399999976158142, time = 0.4176907539367676\n",
      "Testing at step=36, batch=40, test loss = 0.26786747574806213, test acc = 0.8999999761581421, time = 0.41725993156433105\n",
      "Testing at step=36, batch=60, test loss = 0.2587076425552368, test acc = 0.9100000262260437, time = 0.41774892807006836\n",
      "Testing at step=36, batch=80, test loss = 0.35432368516921997, test acc = 0.8999999761581421, time = 0.417844295501709\n",
      "Step 36 finished in 457.6254417896271, Train loss = 0.28892702794323366, Test loss = 0.2831181388348341; Train Acc = 0.9186500008900961, Test Acc = 0.9195000034570694\n",
      "Training at step=37, batch=0, train loss = 0.16420471668243408, train acc = 0.949999988079071, time = 0.7181689739227295\n",
      "Training at step=37, batch=120, train loss = 0.41768309473991394, train acc = 0.8600000143051147, time = 0.70135498046875\n",
      "Training at step=37, batch=240, train loss = 0.27515238523483276, train acc = 0.8899999856948853, time = 0.7041075229644775\n",
      "Training at step=37, batch=360, train loss = 0.3047719895839691, train acc = 0.8999999761581421, time = 0.7036807537078857\n",
      "Training at step=37, batch=480, train loss = 0.2748645544052124, train acc = 0.9200000166893005, time = 0.7052977085113525\n",
      "Testing at step=37, batch=0, test loss = 0.43234771490097046, test acc = 0.8700000047683716, time = 0.20489740371704102\n",
      "Testing at step=37, batch=20, test loss = 0.41254502534866333, test acc = 0.8700000047683716, time = 0.14223098754882812\n",
      "Testing at step=37, batch=40, test loss = 0.4596973657608032, test acc = 0.8799999952316284, time = 0.1424121856689453\n",
      "Testing at step=37, batch=60, test loss = 0.1958690881729126, test acc = 0.9399999976158142, time = 0.14673161506652832\n",
      "Testing at step=37, batch=80, test loss = 0.24281622469425201, test acc = 0.9399999976158142, time = 0.14245867729187012\n",
      "Step 37 finished in 456.29790234565735, Train loss = 0.2865945845469832, Test loss = 0.3143009102344513; Train Acc = 0.9183333350221315, Test Acc = 0.9089000010490418\n",
      "Training at step=38, batch=0, train loss = 0.4790830910205841, train acc = 0.8799999952316284, time = 0.7345371246337891\n",
      "Training at step=38, batch=120, train loss = 0.23520490527153015, train acc = 0.9300000071525574, time = 0.6814906597137451\n",
      "Training at step=38, batch=240, train loss = 0.31591978669166565, train acc = 0.8700000047683716, time = 0.7067370414733887\n",
      "Training at step=38, batch=360, train loss = 0.2995688319206238, train acc = 0.9300000071525574, time = 0.7094969749450684\n",
      "Training at step=38, batch=480, train loss = 0.3156222105026245, train acc = 0.8899999856948853, time = 0.7078871726989746\n",
      "Testing at step=38, batch=0, test loss = 0.304255872964859, test acc = 0.8799999952316284, time = 0.20435714721679688\n",
      "Testing at step=38, batch=20, test loss = 0.18787771463394165, test acc = 0.8999999761581421, time = 0.1477363109588623\n",
      "Testing at step=38, batch=40, test loss = 0.2714138925075531, test acc = 0.9100000262260437, time = 0.13987278938293457\n",
      "Testing at step=38, batch=60, test loss = 0.1797589510679245, test acc = 0.949999988079071, time = 0.14161896705627441\n",
      "Testing at step=38, batch=80, test loss = 0.15161791443824768, test acc = 0.9399999976158142, time = 0.1459040641784668\n",
      "Step 38 finished in 456.9116015434265, Train loss = 0.28523125166694324, Test loss = 0.28026918448507787; Train Acc = 0.9179833352565765, Test Acc = 0.9192000031471252\n",
      "Training at step=39, batch=0, train loss = 0.2506975531578064, train acc = 0.9599999785423279, time = 0.7175087928771973\n",
      "Training at step=39, batch=120, train loss = 0.1940256506204605, train acc = 0.949999988079071, time = 0.7049381732940674\n",
      "Training at step=39, batch=240, train loss = 0.2754882574081421, train acc = 0.9200000166893005, time = 0.7050507068634033\n",
      "Training at step=39, batch=360, train loss = 0.28706416487693787, train acc = 0.8999999761581421, time = 0.7044723033905029\n",
      "Training at step=39, batch=480, train loss = 0.26963478326797485, train acc = 0.9200000166893005, time = 0.7051208019256592\n",
      "Testing at step=39, batch=0, test loss = 0.4008115530014038, test acc = 0.8899999856948853, time = 0.3857889175415039\n",
      "Testing at step=39, batch=20, test loss = 0.40511006116867065, test acc = 0.8899999856948853, time = 0.4173851013183594\n",
      "Testing at step=39, batch=40, test loss = 0.18147911131381989, test acc = 0.9599999785423279, time = 0.41592860221862793\n",
      "Testing at step=39, batch=60, test loss = 0.14677685499191284, test acc = 0.9300000071525574, time = 0.417294979095459\n",
      "Testing at step=39, batch=80, test loss = 0.19895553588867188, test acc = 0.9599999785423279, time = 0.4168417453765869\n",
      "Step 39 finished in 456.27601528167725, Train loss = 0.28559380893905956, Test loss = 0.2793913947045803; Train Acc = 0.9193500004212062, Test Acc = 0.919600002169609\n",
      "Training at step=40, batch=0, train loss = 0.47130927443504333, train acc = 0.8600000143051147, time = 0.720050573348999\n",
      "Training at step=40, batch=120, train loss = 0.2054871767759323, train acc = 0.949999988079071, time = 0.7066447734832764\n",
      "Training at step=40, batch=240, train loss = 0.24727016687393188, train acc = 0.9399999976158142, time = 0.7053511142730713\n",
      "Training at step=40, batch=360, train loss = 0.4547884464263916, train acc = 0.8899999856948853, time = 0.7051329612731934\n",
      "Training at step=40, batch=480, train loss = 0.1933986097574234, train acc = 0.9200000166893005, time = 0.7051970958709717\n",
      "Testing at step=40, batch=0, test loss = 0.32090404629707336, test acc = 0.9100000262260437, time = 0.22078824043273926\n",
      "Testing at step=40, batch=20, test loss = 0.611472487449646, test acc = 0.8799999952316284, time = 0.13619756698608398\n",
      "Testing at step=40, batch=40, test loss = 0.18715600669384003, test acc = 0.9200000166893005, time = 0.13757920265197754\n",
      "Testing at step=40, batch=60, test loss = 0.24200917780399323, test acc = 0.9200000166893005, time = 0.13988900184631348\n",
      "Testing at step=40, batch=80, test loss = 0.3157121241092682, test acc = 0.8899999856948853, time = 0.14019417762756348\n",
      "Step 40 finished in 456.1335391998291, Train loss = 0.28508760321885346, Test loss = 0.2861403754353523; Train Acc = 0.9189000009497007, Test Acc = 0.9164000022411346\n",
      "Training at step=41, batch=0, train loss = 0.35232773423194885, train acc = 0.8799999952316284, time = 0.711113452911377\n",
      "Training at step=41, batch=120, train loss = 0.3511483073234558, train acc = 0.9200000166893005, time = 0.7048439979553223\n",
      "Training at step=41, batch=240, train loss = 0.3709695041179657, train acc = 0.8500000238418579, time = 0.7042124271392822\n",
      "Training at step=41, batch=360, train loss = 0.26510530710220337, train acc = 0.8999999761581421, time = 0.7053506374359131\n",
      "Training at step=41, batch=480, train loss = 0.19490589201450348, train acc = 0.9399999976158142, time = 0.6944417953491211\n",
      "Testing at step=41, batch=0, test loss = 0.1934247612953186, test acc = 0.949999988079071, time = 0.3887782096862793\n",
      "Testing at step=41, batch=20, test loss = 0.4230813682079315, test acc = 0.8799999952316284, time = 0.4175717830657959\n",
      "Testing at step=41, batch=40, test loss = 0.3608958423137665, test acc = 0.8999999761581421, time = 0.4172666072845459\n",
      "Testing at step=41, batch=60, test loss = 0.29777562618255615, test acc = 0.9200000166893005, time = 0.4174644947052002\n",
      "Testing at step=41, batch=80, test loss = 0.27807438373565674, test acc = 0.8899999856948853, time = 0.4156794548034668\n",
      "Step 41 finished in 456.2017674446106, Train loss = 0.2868983662997683, Test loss = 0.28175420220941305; Train Acc = 0.9184666686256726, Test Acc = 0.9206999969482422\n",
      "Training at step=42, batch=0, train loss = 0.12285912781953812, train acc = 0.9700000286102295, time = 0.7197470664978027\n",
      "Training at step=42, batch=120, train loss = 0.29779770970344543, train acc = 0.9100000262260437, time = 0.7040486335754395\n",
      "Training at step=42, batch=240, train loss = 0.5391916632652283, train acc = 0.8999999761581421, time = 0.7052793502807617\n",
      "Training at step=42, batch=360, train loss = 0.2889024615287781, train acc = 0.9200000166893005, time = 0.7050080299377441\n",
      "Training at step=42, batch=480, train loss = 0.26848429441452026, train acc = 0.8899999856948853, time = 0.7055208683013916\n",
      "Testing at step=42, batch=0, test loss = 0.4734450578689575, test acc = 0.8700000047683716, time = 0.24949026107788086\n",
      "Testing at step=42, batch=20, test loss = 0.2725856900215149, test acc = 0.8999999761581421, time = 0.41662096977233887\n",
      "Testing at step=42, batch=40, test loss = 0.2782400846481323, test acc = 0.9300000071525574, time = 0.417834997177124\n",
      "Testing at step=42, batch=60, test loss = 0.2745126783847809, test acc = 0.9100000262260437, time = 0.4186723232269287\n",
      "Testing at step=42, batch=80, test loss = 0.4794836938381195, test acc = 0.8500000238418579, time = 0.4169120788574219\n",
      "Step 42 finished in 454.90671396255493, Train loss = 0.28550008847067754, Test loss = 0.28568807028234006; Train Acc = 0.9181833342711131, Test Acc = 0.9176999992132187\n",
      "Training at step=43, batch=0, train loss = 0.26720309257507324, train acc = 0.9100000262260437, time = 0.7167327404022217\n",
      "Training at step=43, batch=120, train loss = 0.24025197327136993, train acc = 0.949999988079071, time = 0.7326364517211914\n",
      "Training at step=43, batch=240, train loss = 0.305767685174942, train acc = 0.9200000166893005, time = 0.7333495616912842\n",
      "Training at step=43, batch=360, train loss = 0.2438686490058899, train acc = 0.9200000166893005, time = 0.7036118507385254\n",
      "Training at step=43, batch=480, train loss = 0.2194785326719284, train acc = 0.9599999785423279, time = 0.7053847312927246\n",
      "Testing at step=43, batch=0, test loss = 0.2105785310268402, test acc = 0.949999988079071, time = 0.3875565528869629\n",
      "Testing at step=43, batch=20, test loss = 0.21604856848716736, test acc = 0.949999988079071, time = 0.4172079563140869\n",
      "Testing at step=43, batch=40, test loss = 0.23828059434890747, test acc = 0.9399999976158142, time = 0.41622471809387207\n",
      "Testing at step=43, batch=60, test loss = 0.22799421846866608, test acc = 0.9599999785423279, time = 0.41661572456359863\n",
      "Testing at step=43, batch=80, test loss = 0.4059475362300873, test acc = 0.9200000166893005, time = 0.4173619747161865\n",
      "Step 43 finished in 461.80736231803894, Train loss = 0.28417909242212774, Test loss = 0.27366255715489385; Train Acc = 0.9192833357055982, Test Acc = 0.9212000018358231\n",
      "Training at step=44, batch=0, train loss = 0.20938871800899506, train acc = 0.9300000071525574, time = 0.727409839630127\n",
      "Training at step=44, batch=120, train loss = 0.14283859729766846, train acc = 0.9700000286102295, time = 0.7045860290527344\n",
      "Training at step=44, batch=240, train loss = 0.32385390996932983, train acc = 0.9300000071525574, time = 0.7037107944488525\n",
      "Training at step=44, batch=360, train loss = 0.16233313083648682, train acc = 0.949999988079071, time = 0.7013013362884521\n",
      "Training at step=44, batch=480, train loss = 0.305113822221756, train acc = 0.9300000071525574, time = 0.7075965404510498\n",
      "Testing at step=44, batch=0, test loss = 0.31749486923217773, test acc = 0.8999999761581421, time = 0.24966669082641602\n",
      "Testing at step=44, batch=20, test loss = 0.41963687539100647, test acc = 0.9200000166893005, time = 0.14098215103149414\n",
      "Testing at step=44, batch=40, test loss = 0.4195353388786316, test acc = 0.8999999761581421, time = 0.14180493354797363\n",
      "Testing at step=44, batch=60, test loss = 0.23685003817081451, test acc = 0.9100000262260437, time = 0.1373598575592041\n",
      "Testing at step=44, batch=80, test loss = 0.2146340310573578, test acc = 0.9200000166893005, time = 0.1408076286315918\n",
      "Step 44 finished in 456.50476598739624, Train loss = 0.28385098148137333, Test loss = 0.27224491991102695; Train Acc = 0.9192666682600975, Test Acc = 0.9215000027418137\n",
      "Training at step=45, batch=0, train loss = 0.20557676255702972, train acc = 0.9399999976158142, time = 0.7122635841369629\n",
      "Training at step=45, batch=120, train loss = 0.23251937329769135, train acc = 0.9200000166893005, time = 0.7074527740478516\n",
      "Training at step=45, batch=240, train loss = 0.3032571077346802, train acc = 0.8799999952316284, time = 0.7045760154724121\n",
      "Training at step=45, batch=360, train loss = 0.43422946333885193, train acc = 0.9200000166893005, time = 0.7027289867401123\n",
      "Training at step=45, batch=480, train loss = 0.37710171937942505, train acc = 0.9300000071525574, time = 0.704150915145874\n",
      "Testing at step=45, batch=0, test loss = 0.19845886528491974, test acc = 0.9300000071525574, time = 0.2121715545654297\n",
      "Testing at step=45, batch=20, test loss = 0.26308485865592957, test acc = 0.9200000166893005, time = 0.13689899444580078\n",
      "Testing at step=45, batch=40, test loss = 0.27367833256721497, test acc = 0.9300000071525574, time = 0.1365222930908203\n",
      "Testing at step=45, batch=60, test loss = 0.3667697608470917, test acc = 0.8999999761581421, time = 0.14010381698608398\n",
      "Testing at step=45, batch=80, test loss = 0.27210190892219543, test acc = 0.9399999976158142, time = 0.13818359375\n",
      "Step 45 finished in 456.3872022628784, Train loss = 0.2811836381132404, Test loss = 0.274469039067626; Train Acc = 0.9204166693488757, Test Acc = 0.9193000048398972\n",
      "Training at step=46, batch=0, train loss = 0.3040419816970825, train acc = 0.8600000143051147, time = 0.7153286933898926\n",
      "Training at step=46, batch=120, train loss = 0.20290933549404144, train acc = 0.949999988079071, time = 0.7058050632476807\n",
      "Training at step=46, batch=240, train loss = 0.24749433994293213, train acc = 0.8899999856948853, time = 0.7041144371032715\n",
      "Training at step=46, batch=360, train loss = 0.4213814437389374, train acc = 0.8899999856948853, time = 0.7060220241546631\n",
      "Training at step=46, batch=480, train loss = 0.2781265079975128, train acc = 0.8899999856948853, time = 0.7045917510986328\n",
      "Testing at step=46, batch=0, test loss = 0.13399913907051086, test acc = 0.949999988079071, time = 0.391923189163208\n",
      "Testing at step=46, batch=20, test loss = 0.18756696581840515, test acc = 0.949999988079071, time = 0.41718006134033203\n",
      "Testing at step=46, batch=40, test loss = 0.27621930837631226, test acc = 0.8799999952316284, time = 0.4170200824737549\n",
      "Testing at step=46, batch=60, test loss = 0.22784020006656647, test acc = 0.9100000262260437, time = 0.4171905517578125\n",
      "Testing at step=46, batch=80, test loss = 0.2561013102531433, test acc = 0.9200000166893005, time = 0.4165074825286865\n",
      "Step 46 finished in 456.31087279319763, Train loss = 0.2818941028912862, Test loss = 0.2818476866930723; Train Acc = 0.9196166678269704, Test Acc = 0.918400000333786\n",
      "Training at step=47, batch=0, train loss = 0.21739894151687622, train acc = 0.9599999785423279, time = 0.7204842567443848\n",
      "Training at step=47, batch=120, train loss = 0.24009636044502258, train acc = 0.9399999976158142, time = 0.7329204082489014\n",
      "Training at step=47, batch=240, train loss = 0.21436338126659393, train acc = 0.9300000071525574, time = 0.733222246170044\n",
      "Training at step=47, batch=360, train loss = 0.3113833963871002, train acc = 0.9100000262260437, time = 0.7316169738769531\n",
      "Training at step=47, batch=480, train loss = 0.33596572279930115, train acc = 0.9100000262260437, time = 0.7059445381164551\n",
      "Testing at step=47, batch=0, test loss = 0.21357940137386322, test acc = 0.9300000071525574, time = 0.38910818099975586\n",
      "Testing at step=47, batch=20, test loss = 0.14468950033187866, test acc = 0.9399999976158142, time = 0.41649699211120605\n",
      "Testing at step=47, batch=40, test loss = 0.26558053493499756, test acc = 0.8899999856948853, time = 0.4183480739593506\n",
      "Testing at step=47, batch=60, test loss = 0.2888626158237457, test acc = 0.9200000166893005, time = 0.4163360595703125\n",
      "Testing at step=47, batch=80, test loss = 0.39322933554649353, test acc = 0.8899999856948853, time = 0.4161233901977539\n",
      "Step 47 finished in 467.0310654640198, Train loss = 0.2811347289880117, Test loss = 0.270462511330843; Train Acc = 0.9197000023722649, Test Acc = 0.9209999984502792\n",
      "Training at step=48, batch=0, train loss = 0.3230914771556854, train acc = 0.9100000262260437, time = 0.7075920104980469\n",
      "Training at step=48, batch=120, train loss = 0.35975638031959534, train acc = 0.9300000071525574, time = 0.7045843601226807\n",
      "Training at step=48, batch=240, train loss = 0.22501766681671143, train acc = 0.9399999976158142, time = 0.704939603805542\n",
      "Training at step=48, batch=360, train loss = 0.34778884053230286, train acc = 0.8799999952316284, time = 0.7068202495574951\n",
      "Training at step=48, batch=480, train loss = 0.334615558385849, train acc = 0.9200000166893005, time = 0.7058887481689453\n",
      "Testing at step=48, batch=0, test loss = 0.3155374526977539, test acc = 0.9300000071525574, time = 0.3926537036895752\n",
      "Testing at step=48, batch=20, test loss = 0.4875013828277588, test acc = 0.8899999856948853, time = 0.4174354076385498\n",
      "Testing at step=48, batch=40, test loss = 0.16371312737464905, test acc = 0.9200000166893005, time = 0.416903018951416\n",
      "Testing at step=48, batch=60, test loss = 0.15187548100948334, test acc = 0.949999988079071, time = 0.41693663597106934\n",
      "Testing at step=48, batch=80, test loss = 0.2753024101257324, test acc = 0.8600000143051147, time = 0.41667771339416504\n",
      "Step 48 finished in 456.5301287174225, Train loss = 0.28169337795426447, Test loss = 0.2780983304977417; Train Acc = 0.9202833359440168, Test Acc = 0.9193000018596649\n",
      "Training at step=49, batch=0, train loss = 0.2521475553512573, train acc = 0.9399999976158142, time = 0.7178821563720703\n",
      "Training at step=49, batch=120, train loss = 0.28989291191101074, train acc = 0.9100000262260437, time = 0.7078852653503418\n",
      "Training at step=49, batch=240, train loss = 0.2194305658340454, train acc = 0.9100000262260437, time = 0.702418327331543\n",
      "Training at step=49, batch=360, train loss = 0.3290667235851288, train acc = 0.9100000262260437, time = 0.7042033672332764\n",
      "Training at step=49, batch=480, train loss = 0.23117417097091675, train acc = 0.9399999976158142, time = 0.7055137157440186\n",
      "Testing at step=49, batch=0, test loss = 0.37182074785232544, test acc = 0.8899999856948853, time = 0.21106982231140137\n",
      "Testing at step=49, batch=20, test loss = 0.29124248027801514, test acc = 0.9300000071525574, time = 0.14109206199645996\n",
      "Testing at step=49, batch=40, test loss = 0.22672076523303986, test acc = 0.9200000166893005, time = 0.14107680320739746\n",
      "Testing at step=49, batch=60, test loss = 0.25583386421203613, test acc = 0.8999999761581421, time = 0.14081573486328125\n",
      "Testing at step=49, batch=80, test loss = 0.26114270091056824, test acc = 0.8999999761581421, time = 0.14326024055480957\n",
      "Step 49 finished in 456.35717248916626, Train loss = 0.281162308777372, Test loss = 0.285794712677598; Train Acc = 0.9206666687130928, Test Acc = 0.9183000040054321\n",
      "Training at step=50, batch=0, train loss = 0.29048576951026917, train acc = 0.9100000262260437, time = 0.7216920852661133\n",
      "Training at step=50, batch=120, train loss = 0.1629362255334854, train acc = 0.9599999785423279, time = 0.7027304172515869\n",
      "Training at step=50, batch=240, train loss = 0.2691890597343445, train acc = 0.9200000166893005, time = 0.7043731212615967\n",
      "Training at step=50, batch=360, train loss = 0.17120447754859924, train acc = 0.9300000071525574, time = 0.7072544097900391\n",
      "Training at step=50, batch=480, train loss = 0.37863409519195557, train acc = 0.8999999761581421, time = 0.7036564350128174\n",
      "Testing at step=50, batch=0, test loss = 0.26386019587516785, test acc = 0.949999988079071, time = 0.20732927322387695\n",
      "Testing at step=50, batch=20, test loss = 0.31724825501441956, test acc = 0.9100000262260437, time = 0.14276862144470215\n",
      "Testing at step=50, batch=40, test loss = 0.18692463636398315, test acc = 0.9399999976158142, time = 0.14966320991516113\n",
      "Testing at step=50, batch=60, test loss = 0.3908799886703491, test acc = 0.9100000262260437, time = 0.14628386497497559\n",
      "Testing at step=50, batch=80, test loss = 0.2294405996799469, test acc = 0.9200000166893005, time = 0.14626479148864746\n",
      "Step 50 finished in 456.5734450817108, Train loss = 0.2807921315729618, Test loss = 0.28891847245395186; Train Acc = 0.9205666684110959, Test Acc = 0.9177000057697297\n",
      "Training at step=51, batch=0, train loss = 0.39318251609802246, train acc = 0.8600000143051147, time = 0.729015588760376\n",
      "Training at step=51, batch=120, train loss = 0.37940511107444763, train acc = 0.8700000047683716, time = 0.7053840160369873\n",
      "Training at step=51, batch=240, train loss = 0.2396312654018402, train acc = 0.9399999976158142, time = 0.7034001350402832\n",
      "Training at step=51, batch=360, train loss = 0.24479138851165771, train acc = 0.9100000262260437, time = 0.7037310600280762\n",
      "Training at step=51, batch=480, train loss = 0.24689771234989166, train acc = 0.9200000166893005, time = 0.7045621871948242\n",
      "Testing at step=51, batch=0, test loss = 0.2805712819099426, test acc = 0.8700000047683716, time = 0.39127016067504883\n",
      "Testing at step=51, batch=20, test loss = 0.2903251349925995, test acc = 0.9100000262260437, time = 0.4172861576080322\n",
      "Testing at step=51, batch=40, test loss = 0.3250361680984497, test acc = 0.8999999761581421, time = 0.41574645042419434\n",
      "Testing at step=51, batch=60, test loss = 0.3325432240962982, test acc = 0.8799999952316284, time = 0.41686391830444336\n",
      "Testing at step=51, batch=80, test loss = 0.15490780770778656, test acc = 0.949999988079071, time = 0.41745996475219727\n",
      "Step 51 finished in 456.66220259666443, Train loss = 0.28166890719284615, Test loss = 0.2725686226040125; Train Acc = 0.9193333354592323, Test Acc = 0.9218000042438507\n",
      "Training at step=52, batch=0, train loss = 0.19617804884910583, train acc = 0.949999988079071, time = 0.7209577560424805\n",
      "Training at step=52, batch=120, train loss = 0.3338858485221863, train acc = 0.8999999761581421, time = 0.7044553756713867\n",
      "Training at step=52, batch=240, train loss = 0.202140212059021, train acc = 0.9399999976158142, time = 0.7038202285766602\n",
      "Training at step=52, batch=360, train loss = 0.17107343673706055, train acc = 0.949999988079071, time = 0.7039203643798828\n",
      "Training at step=52, batch=480, train loss = 0.2542722225189209, train acc = 0.9200000166893005, time = 0.7041614055633545\n",
      "Testing at step=52, batch=0, test loss = 0.17236870527267456, test acc = 0.9599999785423279, time = 0.38685131072998047\n",
      "Testing at step=52, batch=20, test loss = 0.19404825568199158, test acc = 0.9300000071525574, time = 0.4162106513977051\n",
      "Testing at step=52, batch=40, test loss = 0.20279905200004578, test acc = 0.949999988079071, time = 0.41577982902526855\n",
      "Testing at step=52, batch=60, test loss = 0.24715957045555115, test acc = 0.9300000071525574, time = 0.4189794063568115\n",
      "Testing at step=52, batch=80, test loss = 0.1310572624206543, test acc = 0.9599999785423279, time = 0.4175291061401367\n",
      "Step 52 finished in 456.2801010608673, Train loss = 0.27880408414949975, Test loss = 0.2767058955132961; Train Acc = 0.9210166683793068, Test Acc = 0.9193999999761582\n",
      "Training at step=53, batch=0, train loss = 0.2825812101364136, train acc = 0.9100000262260437, time = 0.7193858623504639\n",
      "Training at step=53, batch=120, train loss = 0.36875277757644653, train acc = 0.8999999761581421, time = 0.7054002285003662\n",
      "Training at step=53, batch=240, train loss = 0.21363092958927155, train acc = 0.9700000286102295, time = 0.7052063941955566\n",
      "Training at step=53, batch=360, train loss = 0.3493315875530243, train acc = 0.8899999856948853, time = 0.7057287693023682\n",
      "Training at step=53, batch=480, train loss = 0.2143748253583908, train acc = 0.9100000262260437, time = 0.6894495487213135\n",
      "Testing at step=53, batch=0, test loss = 0.1705203503370285, test acc = 0.9200000166893005, time = 0.39020371437072754\n",
      "Testing at step=53, batch=20, test loss = 0.274191290140152, test acc = 0.9300000071525574, time = 0.4171595573425293\n",
      "Testing at step=53, batch=40, test loss = 0.22573721408843994, test acc = 0.9399999976158142, time = 0.4163210391998291\n",
      "Testing at step=53, batch=60, test loss = 0.32948043942451477, test acc = 0.8999999761581421, time = 0.4175550937652588\n",
      "Testing at step=53, batch=80, test loss = 0.2811411917209625, test acc = 0.9100000262260437, time = 0.41678643226623535\n",
      "Step 53 finished in 455.77745628356934, Train loss = 0.2783882982780536, Test loss = 0.27769918739795685; Train Acc = 0.92113333572944, Test Acc = 0.9189000004529952\n",
      "Training at step=54, batch=0, train loss = 0.19041121006011963, train acc = 0.9399999976158142, time = 0.7221493721008301\n",
      "Training at step=54, batch=120, train loss = 0.282528281211853, train acc = 0.9300000071525574, time = 0.7321896553039551\n",
      "Training at step=54, batch=240, train loss = 0.29495561122894287, train acc = 0.9200000166893005, time = 0.7339098453521729\n",
      "Training at step=54, batch=360, train loss = 0.1578860878944397, train acc = 0.9599999785423279, time = 0.7315683364868164\n",
      "Training at step=54, batch=480, train loss = 0.14212964475154877, train acc = 0.949999988079071, time = 0.7326371669769287\n",
      "Testing at step=54, batch=0, test loss = 0.3141270875930786, test acc = 0.8799999952316284, time = 0.3906247615814209\n",
      "Testing at step=54, batch=20, test loss = 0.17311838269233704, test acc = 0.9599999785423279, time = 0.14150238037109375\n",
      "Testing at step=54, batch=40, test loss = 0.21455520391464233, test acc = 0.949999988079071, time = 0.1409895420074463\n",
      "Testing at step=54, batch=60, test loss = 0.22940115630626678, test acc = 0.9399999976158142, time = 0.138167142868042\n",
      "Testing at step=54, batch=80, test loss = 0.29787150025367737, test acc = 0.9300000071525574, time = 0.14211726188659668\n",
      "Step 54 finished in 469.3385672569275, Train loss = 0.27849845406909784, Test loss = 0.2881647506356239; Train Acc = 0.9216000021497408, Test Acc = 0.9166999995708466\n",
      "Training at step=55, batch=0, train loss = 0.22716236114501953, train acc = 0.9300000071525574, time = 0.47711968421936035\n",
      "Training at step=55, batch=120, train loss = 0.2225983440876007, train acc = 0.9399999976158142, time = 0.7340214252471924\n",
      "Training at step=55, batch=240, train loss = 0.26706463098526, train acc = 0.8899999856948853, time = 0.7344856262207031\n",
      "Training at step=55, batch=360, train loss = 0.22803068161010742, train acc = 0.9599999785423279, time = 0.7064096927642822\n",
      "Training at step=55, batch=480, train loss = 0.2242100089788437, train acc = 0.9399999976158142, time = 0.704721212387085\n",
      "Testing at step=55, batch=0, test loss = 0.3483597934246063, test acc = 0.8999999761581421, time = 0.3866288661956787\n",
      "Testing at step=55, batch=20, test loss = 0.2613709568977356, test acc = 0.8999999761581421, time = 0.4168205261230469\n",
      "Testing at step=55, batch=40, test loss = 0.350773423910141, test acc = 0.9100000262260437, time = 0.41605305671691895\n",
      "Testing at step=55, batch=60, test loss = 0.29439565539360046, test acc = 0.9200000166893005, time = 0.4170420169830322\n",
      "Testing at step=55, batch=80, test loss = 0.4639420211315155, test acc = 0.8899999856948853, time = 0.41738080978393555\n",
      "Step 55 finished in 462.0654456615448, Train loss = 0.2805071980382005, Test loss = 0.2803540971875191; Train Acc = 0.9208000018199285, Test Acc = 0.9214000010490417\n",
      "Training at step=56, batch=0, train loss = 0.18537290394306183, train acc = 0.9300000071525574, time = 0.7261841297149658\n",
      "Training at step=56, batch=120, train loss = 0.3573797643184662, train acc = 0.9300000071525574, time = 0.7047584056854248\n",
      "Training at step=56, batch=240, train loss = 0.3062177002429962, train acc = 0.9100000262260437, time = 0.7034313678741455\n",
      "Training at step=56, batch=360, train loss = 0.47050830721855164, train acc = 0.9200000166893005, time = 0.7038354873657227\n",
      "Training at step=56, batch=480, train loss = 0.4023590385913849, train acc = 0.8999999761581421, time = 0.7041549682617188\n",
      "Testing at step=56, batch=0, test loss = 0.2132016122341156, test acc = 0.9300000071525574, time = 0.2076103687286377\n",
      "Testing at step=56, batch=20, test loss = 0.31197869777679443, test acc = 0.9100000262260437, time = 0.14013242721557617\n",
      "Testing at step=56, batch=40, test loss = 0.25282180309295654, test acc = 0.9300000071525574, time = 0.1394813060760498\n",
      "Testing at step=56, batch=60, test loss = 0.2500458061695099, test acc = 0.9300000071525574, time = 0.1383378505706787\n",
      "Testing at step=56, batch=80, test loss = 0.2512954771518707, test acc = 0.8999999761581421, time = 0.1373579502105713\n",
      "Step 56 finished in 455.8009412288666, Train loss = 0.2765036466593544, Test loss = 0.2765588823705912; Train Acc = 0.9220166688164075, Test Acc = 0.9210000026226044\n",
      "Training at step=57, batch=0, train loss = 0.3580344021320343, train acc = 0.9100000262260437, time = 0.6975791454315186\n",
      "Training at step=57, batch=120, train loss = 0.34594276547431946, train acc = 0.8899999856948853, time = 0.7055068016052246\n",
      "Training at step=57, batch=240, train loss = 0.4967484772205353, train acc = 0.8600000143051147, time = 0.7036683559417725\n",
      "Training at step=57, batch=360, train loss = 0.20438823103904724, train acc = 0.9399999976158142, time = 0.7021665573120117\n",
      "Training at step=57, batch=480, train loss = 0.26188233494758606, train acc = 0.9300000071525574, time = 0.7065029144287109\n",
      "Testing at step=57, batch=0, test loss = 0.20419827103614807, test acc = 0.9399999976158142, time = 0.21585679054260254\n",
      "Testing at step=57, batch=20, test loss = 0.3017345070838928, test acc = 0.9599999785423279, time = 0.10621762275695801\n",
      "Testing at step=57, batch=40, test loss = 0.3095315098762512, test acc = 0.9200000166893005, time = 0.41634297370910645\n",
      "Testing at step=57, batch=60, test loss = 0.21042750775814056, test acc = 0.9300000071525574, time = 0.41713595390319824\n",
      "Testing at step=57, batch=80, test loss = 0.45107877254486084, test acc = 0.8799999952316284, time = 0.4186224937438965\n",
      "Step 57 finished in 456.34405422210693, Train loss = 0.2782328531021873, Test loss = 0.27344092935323716; Train Acc = 0.9213833363850912, Test Acc = 0.9244000041484832\n",
      "Training at step=58, batch=0, train loss = 0.3345921039581299, train acc = 0.949999988079071, time = 0.7229452133178711\n",
      "Training at step=58, batch=120, train loss = 0.3609369695186615, train acc = 0.9200000166893005, time = 0.7032132148742676\n",
      "Training at step=58, batch=240, train loss = 0.5501807928085327, train acc = 0.8399999737739563, time = 0.7053265571594238\n",
      "Training at step=58, batch=360, train loss = 0.10857411473989487, train acc = 0.9599999785423279, time = 0.7042067050933838\n",
      "Training at step=58, batch=480, train loss = 0.3750614821910858, train acc = 0.8899999856948853, time = 0.7063548564910889\n",
      "Testing at step=58, batch=0, test loss = 0.39371857047080994, test acc = 0.9200000166893005, time = 0.40283751487731934\n",
      "Testing at step=58, batch=20, test loss = 0.14219291508197784, test acc = 0.9599999785423279, time = 0.41611337661743164\n",
      "Testing at step=58, batch=40, test loss = 0.2716256380081177, test acc = 0.9300000071525574, time = 0.41785550117492676\n",
      "Testing at step=58, batch=60, test loss = 0.19264090061187744, test acc = 0.949999988079071, time = 0.41741228103637695\n",
      "Testing at step=58, batch=80, test loss = 0.35863614082336426, test acc = 0.8899999856948853, time = 0.4170958995819092\n",
      "Step 58 finished in 456.0733869075775, Train loss = 0.27680316091825563, Test loss = 0.27845096357166765; Train Acc = 0.9210833358764648, Test Acc = 0.9206000012159348\n",
      "Training at step=59, batch=0, train loss = 0.319682240486145, train acc = 0.9300000071525574, time = 0.5635247230529785\n",
      "Training at step=59, batch=120, train loss = 0.3086882531642914, train acc = 0.9200000166893005, time = 0.70587158203125\n",
      "Training at step=59, batch=240, train loss = 0.25957775115966797, train acc = 0.8899999856948853, time = 0.6917414665222168\n",
      "Training at step=59, batch=360, train loss = 0.5582900643348694, train acc = 0.8899999856948853, time = 0.7040805816650391\n",
      "Training at step=59, batch=480, train loss = 0.3904334306716919, train acc = 0.8899999856948853, time = 0.7055618762969971\n",
      "Testing at step=59, batch=0, test loss = 0.28154614567756653, test acc = 0.8899999856948853, time = 0.4039187431335449\n",
      "Testing at step=59, batch=20, test loss = 0.11580415815114975, test acc = 0.9599999785423279, time = 0.4166128635406494\n",
      "Testing at step=59, batch=40, test loss = 0.3848728537559509, test acc = 0.9100000262260437, time = 0.41754913330078125\n",
      "Testing at step=59, batch=60, test loss = 0.20912069082260132, test acc = 0.9200000166893005, time = 0.41674351692199707\n",
      "Testing at step=59, batch=80, test loss = 0.22148951888084412, test acc = 0.9100000262260437, time = 0.4162611961364746\n",
      "Step 59 finished in 457.1961102485657, Train loss = 0.2776032377158602, Test loss = 0.2716025876253843; Train Acc = 0.9218000010649363, Test Acc = 0.9227000039815902\n",
      "Training at step=60, batch=0, train loss = 0.1778421401977539, train acc = 0.949999988079071, time = 0.5435013771057129\n",
      "Training at step=60, batch=120, train loss = 0.24677987396717072, train acc = 0.9399999976158142, time = 0.703941822052002\n",
      "Training at step=60, batch=240, train loss = 0.2580585181713104, train acc = 0.9100000262260437, time = 0.7039833068847656\n",
      "Training at step=60, batch=360, train loss = 0.3640483617782593, train acc = 0.8799999952316284, time = 0.704962968826294\n",
      "Training at step=60, batch=480, train loss = 0.23575536906719208, train acc = 0.9399999976158142, time = 0.705521821975708\n",
      "Testing at step=60, batch=0, test loss = 0.20024292171001434, test acc = 0.9200000166893005, time = 0.21461772918701172\n",
      "Testing at step=60, batch=20, test loss = 0.2897019684314728, test acc = 0.9100000262260437, time = 0.41707611083984375\n",
      "Testing at step=60, batch=40, test loss = 0.14913660287857056, test acc = 0.949999988079071, time = 0.41644787788391113\n",
      "Testing at step=60, batch=60, test loss = 0.2437276393175125, test acc = 0.9399999976158142, time = 0.41687560081481934\n",
      "Testing at step=60, batch=80, test loss = 0.1698135882616043, test acc = 0.9399999976158142, time = 0.4169654846191406\n",
      "Step 60 finished in 456.0829713344574, Train loss = 0.2755954080695907, Test loss = 0.27523105941712855; Train Acc = 0.9221333345770836, Test Acc = 0.9200999999046325\n",
      "Training at step=61, batch=0, train loss = 0.2666034996509552, train acc = 0.9300000071525574, time = 0.5410623550415039\n",
      "Training at step=61, batch=120, train loss = 0.11647443473339081, train acc = 0.9700000286102295, time = 0.6909382343292236\n",
      "Training at step=61, batch=240, train loss = 0.547339677810669, train acc = 0.8500000238418579, time = 0.7063336372375488\n",
      "Training at step=61, batch=360, train loss = 0.3363308608531952, train acc = 0.8899999856948853, time = 0.7047650814056396\n",
      "Training at step=61, batch=480, train loss = 0.26411259174346924, train acc = 0.949999988079071, time = 0.7042245864868164\n",
      "Testing at step=61, batch=0, test loss = 0.3156510591506958, test acc = 0.9399999976158142, time = 0.3942861557006836\n",
      "Testing at step=61, batch=20, test loss = 0.305199533700943, test acc = 0.8799999952316284, time = 0.4183323383331299\n",
      "Testing at step=61, batch=40, test loss = 0.18961374461650848, test acc = 0.9399999976158142, time = 0.41692662239074707\n",
      "Testing at step=61, batch=60, test loss = 0.4453381597995758, test acc = 0.8399999737739563, time = 0.41790246963500977\n",
      "Testing at step=61, batch=80, test loss = 0.21526385843753815, test acc = 0.9300000071525574, time = 0.41673922538757324\n",
      "Step 61 finished in 457.715208530426, Train loss = 0.27505148041993377, Test loss = 0.2786842790991068; Train Acc = 0.9234333350261053, Test Acc = 0.9204000020027161\n",
      "Training at step=62, batch=0, train loss = 0.13283836841583252, train acc = 0.9599999785423279, time = 0.5306780338287354\n",
      "Training at step=62, batch=120, train loss = 0.42374029755592346, train acc = 0.8299999833106995, time = 0.7042615413665771\n",
      "Training at step=62, batch=240, train loss = 0.3679736256599426, train acc = 0.9100000262260437, time = 0.7029366493225098\n",
      "Training at step=62, batch=360, train loss = 0.2773508131504059, train acc = 0.9100000262260437, time = 0.7045173645019531\n",
      "Training at step=62, batch=480, train loss = 0.2766231596469879, train acc = 0.9399999976158142, time = 0.7065696716308594\n",
      "Testing at step=62, batch=0, test loss = 0.3112228810787201, test acc = 0.9100000262260437, time = 0.21513080596923828\n",
      "Testing at step=62, batch=20, test loss = 0.4465990364551544, test acc = 0.9100000262260437, time = 0.14399409294128418\n",
      "Testing at step=62, batch=40, test loss = 0.13131865859031677, test acc = 0.9599999785423279, time = 0.14050722122192383\n",
      "Testing at step=62, batch=60, test loss = 0.39764198660850525, test acc = 0.8999999761581421, time = 0.13439464569091797\n",
      "Testing at step=62, batch=80, test loss = 0.20736253261566162, test acc = 0.9100000262260437, time = 0.1435234546661377\n",
      "Step 62 finished in 456.3217132091522, Train loss = 0.27584556685139733, Test loss = 0.2684874926507473; Train Acc = 0.9217833353082339, Test Acc = 0.9243000000715256\n",
      "Training at step=63, batch=0, train loss = 0.14416958391666412, train acc = 0.9700000286102295, time = 0.48475146293640137\n",
      "Training at step=63, batch=120, train loss = 0.2666672468185425, train acc = 0.9200000166893005, time = 0.7053465843200684\n",
      "Training at step=63, batch=240, train loss = 0.36692342162132263, train acc = 0.8700000047683716, time = 0.7068483829498291\n",
      "Training at step=63, batch=360, train loss = 0.17735260725021362, train acc = 0.9599999785423279, time = 0.7049684524536133\n",
      "Training at step=63, batch=480, train loss = 0.40428635478019714, train acc = 0.9200000166893005, time = 0.7047257423400879\n",
      "Testing at step=63, batch=0, test loss = 0.17623065412044525, test acc = 0.949999988079071, time = 0.2070760726928711\n",
      "Testing at step=63, batch=20, test loss = 0.17457054555416107, test acc = 0.9200000166893005, time = 0.1514127254486084\n",
      "Testing at step=63, batch=40, test loss = 0.3173034191131592, test acc = 0.8700000047683716, time = 0.15037250518798828\n",
      "Testing at step=63, batch=60, test loss = 0.30014315247535706, test acc = 0.9100000262260437, time = 0.1494591236114502\n",
      "Testing at step=63, batch=80, test loss = 0.36501869559288025, test acc = 0.9399999976158142, time = 0.14409685134887695\n",
      "Step 63 finished in 456.4589331150055, Train loss = 0.2760255130380392, Test loss = 0.26669531427323817; Train Acc = 0.9222833348313967, Test Acc = 0.924600003361702\n",
      "Training at step=64, batch=0, train loss = 0.19505639374256134, train acc = 0.9100000262260437, time = 0.5360090732574463\n",
      "Training at step=64, batch=120, train loss = 0.2950611710548401, train acc = 0.9200000166893005, time = 0.7052125930786133\n",
      "Training at step=64, batch=240, train loss = 0.2934761345386505, train acc = 0.8899999856948853, time = 0.7066400051116943\n",
      "Training at step=64, batch=360, train loss = 0.3319650888442993, train acc = 0.8999999761581421, time = 0.7051758766174316\n",
      "Training at step=64, batch=480, train loss = 0.2612065076828003, train acc = 0.8899999856948853, time = 0.6515254974365234\n",
      "Testing at step=64, batch=0, test loss = 0.3384087085723877, test acc = 0.8999999761581421, time = 0.20963096618652344\n",
      "Testing at step=64, batch=20, test loss = 0.15905804932117462, test acc = 0.9700000286102295, time = 0.1416318416595459\n",
      "Testing at step=64, batch=40, test loss = 0.3160117268562317, test acc = 0.9100000262260437, time = 0.14834356307983398\n",
      "Testing at step=64, batch=60, test loss = 0.17327873408794403, test acc = 0.9399999976158142, time = 0.13941502571105957\n",
      "Testing at step=64, batch=80, test loss = 0.43315207958221436, test acc = 0.8999999761581421, time = 0.14078593254089355\n",
      "Step 64 finished in 456.71741008758545, Train loss = 0.27465357547005015, Test loss = 0.2723838917911053; Train Acc = 0.9231000031034152, Test Acc = 0.9223000037670136\n",
      "Training at step=65, batch=0, train loss = 0.24126456677913666, train acc = 0.9100000262260437, time = 0.5282535552978516\n",
      "Training at step=65, batch=120, train loss = 0.22781431674957275, train acc = 0.9200000166893005, time = 0.7026033401489258\n",
      "Training at step=65, batch=240, train loss = 0.2779932916164398, train acc = 0.9300000071525574, time = 0.7053117752075195\n",
      "Training at step=65, batch=360, train loss = 0.23624397814273834, train acc = 0.9599999785423279, time = 0.7062745094299316\n",
      "Training at step=65, batch=480, train loss = 0.19625164568424225, train acc = 0.9399999976158142, time = 0.7309055328369141\n",
      "Testing at step=65, batch=0, test loss = 0.36159077286720276, test acc = 0.9399999976158142, time = 0.25020360946655273\n",
      "Testing at step=65, batch=20, test loss = 0.4863046705722809, test acc = 0.8999999761581421, time = 0.14320898056030273\n",
      "Testing at step=65, batch=40, test loss = 0.20511150360107422, test acc = 0.9399999976158142, time = 0.14145231246948242\n",
      "Testing at step=65, batch=60, test loss = 0.23459360003471375, test acc = 0.9399999976158142, time = 0.14792227745056152\n",
      "Testing at step=65, batch=80, test loss = 0.2742573618888855, test acc = 0.9399999976158142, time = 0.1421964168548584\n",
      "Step 65 finished in 459.48214387893677, Train loss = 0.2743216583381097, Test loss = 0.26963565677404405; Train Acc = 0.9223333356777826, Test Acc = 0.9238999998569488\n",
      "Training at step=66, batch=0, train loss = 0.31592777371406555, train acc = 0.9100000262260437, time = 0.604353666305542\n",
      "Training at step=66, batch=120, train loss = 0.2197057604789734, train acc = 0.9300000071525574, time = 0.7322230339050293\n",
      "Training at step=66, batch=240, train loss = 0.2322956919670105, train acc = 0.9200000166893005, time = 0.732445478439331\n",
      "Training at step=66, batch=360, train loss = 0.26052671670913696, train acc = 0.9300000071525574, time = 0.7041547298431396\n",
      "Training at step=66, batch=480, train loss = 0.2122146189212799, train acc = 0.9399999976158142, time = 0.7047195434570312\n",
      "Testing at step=66, batch=0, test loss = 0.30151230096817017, test acc = 0.9300000071525574, time = 0.38723158836364746\n",
      "Testing at step=66, batch=20, test loss = 0.4132053256034851, test acc = 0.8999999761581421, time = 0.41725993156433105\n",
      "Testing at step=66, batch=40, test loss = 0.26997172832489014, test acc = 0.8700000047683716, time = 0.41677379608154297\n",
      "Testing at step=66, batch=60, test loss = 0.33997049927711487, test acc = 0.8799999952316284, time = 0.4161529541015625\n",
      "Testing at step=66, batch=80, test loss = 0.5570018291473389, test acc = 0.8399999737739563, time = 0.1366879940032959\n",
      "Step 66 finished in 461.8935315608978, Train loss = 0.27442753344774246, Test loss = 0.2822640732675791; Train Acc = 0.9230333358049393, Test Acc = 0.918700001835823\n",
      "Training at step=67, batch=0, train loss = 0.1083492785692215, train acc = 0.9800000190734863, time = 0.4758141040802002\n",
      "Training at step=67, batch=120, train loss = 0.2662554681301117, train acc = 0.9100000262260437, time = 0.7036912441253662\n",
      "Training at step=67, batch=240, train loss = 0.22735178470611572, train acc = 0.9200000166893005, time = 0.7053945064544678\n",
      "Training at step=67, batch=360, train loss = 0.2860640287399292, train acc = 0.9300000071525574, time = 0.7053372859954834\n",
      "Training at step=67, batch=480, train loss = 0.17032121121883392, train acc = 0.949999988079071, time = 0.7028236389160156\n",
      "Testing at step=67, batch=0, test loss = 0.4374679625034332, test acc = 0.8600000143051147, time = 0.19399237632751465\n",
      "Testing at step=67, batch=20, test loss = 0.3252767324447632, test acc = 0.8700000047683716, time = 0.14664316177368164\n",
      "Testing at step=67, batch=40, test loss = 0.22430549561977386, test acc = 0.9100000262260437, time = 0.1464240550994873\n",
      "Testing at step=67, batch=60, test loss = 0.23242241144180298, test acc = 0.9300000071525574, time = 0.14243602752685547\n",
      "Testing at step=67, batch=80, test loss = 0.2568819522857666, test acc = 0.949999988079071, time = 0.14139628410339355\n",
      "Step 67 finished in 456.6630322933197, Train loss = 0.2745393188049396, Test loss = 0.265378707870841; Train Acc = 0.9226166694362958, Test Acc = 0.9249000030755997\n",
      "Training at step=68, batch=0, train loss = 0.3553617000579834, train acc = 0.8899999856948853, time = 0.47332334518432617\n",
      "Training at step=68, batch=120, train loss = 0.39914217591285706, train acc = 0.8999999761581421, time = 0.703406572341919\n",
      "Training at step=68, batch=240, train loss = 0.5583244562149048, train acc = 0.8899999856948853, time = 0.707350492477417\n",
      "Training at step=68, batch=360, train loss = 0.28640085458755493, train acc = 0.9399999976158142, time = 0.705115556716919\n",
      "Training at step=68, batch=480, train loss = 0.2592708468437195, train acc = 0.9300000071525574, time = 0.7039346694946289\n",
      "Testing at step=68, batch=0, test loss = 0.28044411540031433, test acc = 0.9200000166893005, time = 0.40334200859069824\n",
      "Testing at step=68, batch=20, test loss = 0.12848594784736633, test acc = 0.949999988079071, time = 0.41765356063842773\n",
      "Testing at step=68, batch=40, test loss = 0.1900690644979477, test acc = 0.949999988079071, time = 0.4173130989074707\n",
      "Testing at step=68, batch=60, test loss = 0.28043967485427856, test acc = 0.9200000166893005, time = 0.41751551628112793\n",
      "Testing at step=68, batch=80, test loss = 0.21129179000854492, test acc = 0.9599999785423279, time = 0.4170229434967041\n",
      "Step 68 finished in 455.9357216358185, Train loss = 0.2743035966778795, Test loss = 0.2720128711313009; Train Acc = 0.9228000020980835, Test Acc = 0.9231000018119812\n",
      "Training at step=69, batch=0, train loss = 0.3387054204940796, train acc = 0.8799999952316284, time = 0.7220721244812012\n",
      "Training at step=69, batch=120, train loss = 0.22257015109062195, train acc = 0.949999988079071, time = 0.7055575847625732\n",
      "Training at step=69, batch=240, train loss = 0.2134285420179367, train acc = 0.9399999976158142, time = 0.7067594528198242\n",
      "Training at step=69, batch=360, train loss = 0.28413450717926025, train acc = 0.8999999761581421, time = 0.7037293910980225\n",
      "Training at step=69, batch=480, train loss = 0.439461886882782, train acc = 0.8999999761581421, time = 0.7046480178833008\n",
      "Testing at step=69, batch=0, test loss = 0.23193952441215515, test acc = 0.9300000071525574, time = 0.20641827583312988\n",
      "Testing at step=69, batch=20, test loss = 0.24070744216442108, test acc = 0.9300000071525574, time = 0.1403179168701172\n",
      "Testing at step=69, batch=40, test loss = 0.30186355113983154, test acc = 0.8999999761581421, time = 0.144791841506958\n",
      "Testing at step=69, batch=60, test loss = 0.1836225390434265, test acc = 0.9300000071525574, time = 0.14747881889343262\n",
      "Testing at step=69, batch=80, test loss = 0.27416500449180603, test acc = 0.9599999785423279, time = 0.1387767791748047\n",
      "Step 69 finished in 456.5926456451416, Train loss = 0.27333679924408594, Test loss = 0.279369140714407; Train Acc = 0.9236500019828479, Test Acc = 0.922199998497963\n",
      "Training at step=70, batch=0, train loss = 0.2504344880580902, train acc = 0.949999988079071, time = 0.49018073081970215\n",
      "Training at step=70, batch=120, train loss = 0.28751668334007263, train acc = 0.9300000071525574, time = 0.7045512199401855\n",
      "Training at step=70, batch=240, train loss = 0.3398839235305786, train acc = 0.9100000262260437, time = 0.807490348815918\n",
      "Training at step=70, batch=360, train loss = 0.2830263078212738, train acc = 0.9300000071525574, time = 0.7038207054138184\n",
      "Training at step=70, batch=480, train loss = 0.16711965203285217, train acc = 0.9300000071525574, time = 0.7037067413330078\n",
      "Testing at step=70, batch=0, test loss = 0.2202085256576538, test acc = 0.9200000166893005, time = 0.3930375576019287\n",
      "Testing at step=70, batch=20, test loss = 0.31300270557403564, test acc = 0.8999999761581421, time = 0.41709232330322266\n",
      "Testing at step=70, batch=40, test loss = 0.31842878460884094, test acc = 0.9399999976158142, time = 0.41686153411865234\n",
      "Testing at step=70, batch=60, test loss = 0.1019379124045372, test acc = 0.9599999785423279, time = 0.41648173332214355\n",
      "Testing at step=70, batch=80, test loss = 0.23351670801639557, test acc = 0.9599999785423279, time = 0.4168081283569336\n",
      "Step 70 finished in 456.03538060188293, Train loss = 0.27430188956360024, Test loss = 0.2679588929563761; Train Acc = 0.9224500010410944, Test Acc = 0.9256000000238419\n",
      "Training at step=71, batch=0, train loss = 0.3104186952114105, train acc = 0.9100000262260437, time = 0.7332115173339844\n",
      "Training at step=71, batch=120, train loss = 0.17816562950611115, train acc = 0.949999988079071, time = 0.7061135768890381\n",
      "Training at step=71, batch=240, train loss = 0.24175608158111572, train acc = 0.9300000071525574, time = 0.7035400867462158\n",
      "Training at step=71, batch=360, train loss = 0.3227766752243042, train acc = 0.9200000166893005, time = 0.7065227031707764\n",
      "Training at step=71, batch=480, train loss = 0.3153381049633026, train acc = 0.9200000166893005, time = 0.7060878276824951\n",
      "Testing at step=71, batch=0, test loss = 0.14394117891788483, test acc = 0.949999988079071, time = 0.3778212070465088\n",
      "Testing at step=71, batch=20, test loss = 0.10864727199077606, test acc = 0.9700000286102295, time = 0.41733431816101074\n",
      "Testing at step=71, batch=40, test loss = 0.27725648880004883, test acc = 0.9399999976158142, time = 0.417280912399292\n",
      "Testing at step=71, batch=60, test loss = 0.20050619542598724, test acc = 0.9300000071525574, time = 0.4170565605163574\n",
      "Testing at step=71, batch=80, test loss = 0.19820506870746613, test acc = 0.9399999976158142, time = 0.4159891605377197\n",
      "Step 71 finished in 456.4251205921173, Train loss = 0.2735713363438845, Test loss = 0.27013742856681344; Train Acc = 0.9230833343664805, Test Acc = 0.9211000007390976\n",
      "Training at step=72, batch=0, train loss = 0.2664032280445099, train acc = 0.9300000071525574, time = 0.5430433750152588\n",
      "Training at step=72, batch=120, train loss = 0.3491702675819397, train acc = 0.9100000262260437, time = 0.703291654586792\n",
      "Training at step=72, batch=240, train loss = 0.38906440138816833, train acc = 0.9100000262260437, time = 0.7061262130737305\n",
      "Training at step=72, batch=360, train loss = 0.20726066827774048, train acc = 0.949999988079071, time = 0.7024450302124023\n",
      "Training at step=72, batch=480, train loss = 0.3560679256916046, train acc = 0.9300000071525574, time = 0.7061052322387695\n",
      "Testing at step=72, batch=0, test loss = 0.11953125149011612, test acc = 0.949999988079071, time = 0.21613192558288574\n",
      "Testing at step=72, batch=20, test loss = 0.17682218551635742, test acc = 0.9599999785423279, time = 0.4194648265838623\n",
      "Testing at step=72, batch=40, test loss = 0.23161064088344574, test acc = 0.9200000166893005, time = 0.41602587699890137\n",
      "Testing at step=72, batch=60, test loss = 0.36732596158981323, test acc = 0.9200000166893005, time = 0.41627001762390137\n",
      "Testing at step=72, batch=80, test loss = 0.4482920467853546, test acc = 0.8700000047683716, time = 0.4170796871185303\n",
      "Step 72 finished in 457.3302962779999, Train loss = 0.2731594719737768, Test loss = 0.2697984907031059; Train Acc = 0.9238000010450681, Test Acc = 0.9235000020265579\n",
      "Training at step=73, batch=0, train loss = 0.2939661145210266, train acc = 0.9599999785423279, time = 0.5363843441009521\n",
      "Training at step=73, batch=120, train loss = 0.3411508798599243, train acc = 0.8999999761581421, time = 0.7056653499603271\n",
      "Training at step=73, batch=240, train loss = 0.21944956481456757, train acc = 0.9399999976158142, time = 0.7038986682891846\n",
      "Training at step=73, batch=360, train loss = 0.262039452791214, train acc = 0.9300000071525574, time = 0.7048254013061523\n",
      "Training at step=73, batch=480, train loss = 0.2688271403312683, train acc = 0.9300000071525574, time = 0.7030227184295654\n",
      "Testing at step=73, batch=0, test loss = 0.28576400876045227, test acc = 0.8899999856948853, time = 0.21659469604492188\n",
      "Testing at step=73, batch=20, test loss = 0.2544690668582916, test acc = 0.9200000166893005, time = 0.1494441032409668\n",
      "Testing at step=73, batch=40, test loss = 0.20260658860206604, test acc = 0.9200000166893005, time = 0.15037107467651367\n",
      "Testing at step=73, batch=60, test loss = 0.1304529309272766, test acc = 0.9700000286102295, time = 0.1493823528289795\n",
      "Testing at step=73, batch=80, test loss = 0.2574627697467804, test acc = 0.9599999785423279, time = 0.14937925338745117\n",
      "Step 73 finished in 456.74133348464966, Train loss = 0.2737656237433354, Test loss = 0.27076979756355285; Train Acc = 0.922733335296313, Test Acc = 0.9224000018835068\n",
      "Training at step=74, batch=0, train loss = 0.18666896224021912, train acc = 0.9399999976158142, time = 0.4753682613372803\n",
      "Training at step=74, batch=120, train loss = 0.17809472978115082, train acc = 0.9399999976158142, time = 0.7026941776275635\n",
      "Training at step=74, batch=240, train loss = 0.31680211424827576, train acc = 0.9300000071525574, time = 0.7028086185455322\n",
      "Training at step=74, batch=360, train loss = 0.4024461805820465, train acc = 0.8799999952316284, time = 0.7052605152130127\n",
      "Training at step=74, batch=480, train loss = 0.35162779688835144, train acc = 0.9100000262260437, time = 0.7056052684783936\n",
      "Testing at step=74, batch=0, test loss = 0.27349719405174255, test acc = 0.8999999761581421, time = 0.3904154300689697\n",
      "Testing at step=74, batch=20, test loss = 0.31812575459480286, test acc = 0.9300000071525574, time = 0.4174966812133789\n",
      "Testing at step=74, batch=40, test loss = 0.23039129376411438, test acc = 0.9200000166893005, time = 0.4169020652770996\n",
      "Testing at step=74, batch=60, test loss = 0.28148919343948364, test acc = 0.8799999952316284, time = 0.41739654541015625\n",
      "Testing at step=74, batch=80, test loss = 0.1624598503112793, test acc = 0.949999988079071, time = 0.4170875549316406\n",
      "Step 74 finished in 456.3419153690338, Train loss = 0.2727165474618475, Test loss = 0.2656313481926918; Train Acc = 0.9234000021219253, Test Acc = 0.9222999984025955\n",
      "Training at step=75, batch=0, train loss = 0.1677839308977127, train acc = 0.9599999785423279, time = 0.5378539562225342\n",
      "Training at step=75, batch=120, train loss = 0.1924826055765152, train acc = 0.949999988079071, time = 0.707303524017334\n",
      "Training at step=75, batch=240, train loss = 0.2968295216560364, train acc = 0.9100000262260437, time = 0.6909632682800293\n",
      "Training at step=75, batch=360, train loss = 0.342804491519928, train acc = 0.8999999761581421, time = 0.7056596279144287\n",
      "Training at step=75, batch=480, train loss = 0.16414423286914825, train acc = 0.949999988079071, time = 0.7051668167114258\n",
      "Testing at step=75, batch=0, test loss = 0.28889140486717224, test acc = 0.9100000262260437, time = 0.2109849452972412\n",
      "Testing at step=75, batch=20, test loss = 0.41862449049949646, test acc = 0.8999999761581421, time = 0.13759684562683105\n",
      "Testing at step=75, batch=40, test loss = 0.22710078954696655, test acc = 0.9399999976158142, time = 0.14328718185424805\n",
      "Testing at step=75, batch=60, test loss = 0.2941403388977051, test acc = 0.9200000166893005, time = 0.13569903373718262\n",
      "Testing at step=75, batch=80, test loss = 0.19728578627109528, test acc = 0.9399999976158142, time = 0.1360006332397461\n",
      "Step 75 finished in 455.90707063674927, Train loss = 0.2718048365041614, Test loss = 0.27337805718183517; Train Acc = 0.9227666688958803, Test Acc = 0.9222000026702881\n",
      "Training at step=76, batch=0, train loss = 0.26720690727233887, train acc = 0.8899999856948853, time = 0.5430729389190674\n",
      "Training at step=76, batch=120, train loss = 0.3405004143714905, train acc = 0.8899999856948853, time = 0.7036070823669434\n",
      "Training at step=76, batch=240, train loss = 0.22332462668418884, train acc = 0.9200000166893005, time = 0.7073047161102295\n",
      "Training at step=76, batch=360, train loss = 0.23489202558994293, train acc = 0.9399999976158142, time = 0.7055377960205078\n",
      "Training at step=76, batch=480, train loss = 0.27563247084617615, train acc = 0.8999999761581421, time = 0.7046632766723633\n",
      "Testing at step=76, batch=0, test loss = 0.2785443961620331, test acc = 0.9399999976158142, time = 0.20461535453796387\n",
      "Testing at step=76, batch=20, test loss = 0.16003938019275665, test acc = 0.949999988079071, time = 0.1427476406097412\n",
      "Testing at step=76, batch=40, test loss = 0.2880570590496063, test acc = 0.9300000071525574, time = 0.14213943481445312\n",
      "Testing at step=76, batch=60, test loss = 0.4201229214668274, test acc = 0.8899999856948853, time = 0.14769649505615234\n",
      "Testing at step=76, batch=80, test loss = 0.2888936698436737, test acc = 0.9399999976158142, time = 0.1419377326965332\n",
      "Step 76 finished in 456.156697511673, Train loss = 0.2707125423600276, Test loss = 0.2764303471893072; Train Acc = 0.9235166692733765, Test Acc = 0.9210000038146973\n",
      "Training at step=77, batch=0, train loss = 0.3238081634044647, train acc = 0.9399999976158142, time = 0.5362234115600586\n",
      "Training at step=77, batch=120, train loss = 0.1762772798538208, train acc = 0.949999988079071, time = 0.7057912349700928\n",
      "Training at step=77, batch=240, train loss = 0.23038938641548157, train acc = 0.9200000166893005, time = 0.6967027187347412\n",
      "Training at step=77, batch=360, train loss = 0.21696794033050537, train acc = 0.9200000166893005, time = 0.7057759761810303\n",
      "Training at step=77, batch=480, train loss = 0.17250560224056244, train acc = 0.9399999976158142, time = 0.7025468349456787\n",
      "Testing at step=77, batch=0, test loss = 0.48489490151405334, test acc = 0.8999999761581421, time = 0.3825798034667969\n",
      "Testing at step=77, batch=20, test loss = 0.3375316262245178, test acc = 0.9300000071525574, time = 0.4180469512939453\n",
      "Testing at step=77, batch=40, test loss = 0.49483931064605713, test acc = 0.9200000166893005, time = 0.41838645935058594\n",
      "Testing at step=77, batch=60, test loss = 0.14590714871883392, test acc = 0.9300000071525574, time = 0.4160332679748535\n",
      "Testing at step=77, batch=80, test loss = 0.19164521992206573, test acc = 0.9399999976158142, time = 0.4172518253326416\n",
      "Step 77 finished in 456.349511384964, Train loss = 0.27283442535748087, Test loss = 0.266535000577569; Train Acc = 0.9235833348830541, Test Acc = 0.9239000028371811\n",
      "Training at step=78, batch=0, train loss = 0.31928569078445435, train acc = 0.9200000166893005, time = 0.5366189479827881\n",
      "Training at step=78, batch=120, train loss = 0.2701249420642853, train acc = 0.9300000071525574, time = 0.7036311626434326\n",
      "Training at step=78, batch=240, train loss = 0.31353920698165894, train acc = 0.9100000262260437, time = 0.7047920227050781\n",
      "Training at step=78, batch=360, train loss = 0.2127939611673355, train acc = 0.9200000166893005, time = 0.7017490863800049\n",
      "Training at step=78, batch=480, train loss = 0.5463343262672424, train acc = 0.8999999761581421, time = 0.704660177230835\n",
      "Testing at step=78, batch=0, test loss = 0.27061033248901367, test acc = 0.8799999952316284, time = 0.20940136909484863\n",
      "Testing at step=78, batch=20, test loss = 0.3455284833908081, test acc = 0.9300000071525574, time = 0.14653801918029785\n",
      "Testing at step=78, batch=40, test loss = 0.3791206479072571, test acc = 0.8899999856948853, time = 0.14063024520874023\n",
      "Testing at step=78, batch=60, test loss = 0.403687447309494, test acc = 0.9100000262260437, time = 0.14019060134887695\n",
      "Testing at step=78, batch=80, test loss = 0.21268042922019958, test acc = 0.9399999976158142, time = 0.14505577087402344\n",
      "Step 78 finished in 457.9097349643707, Train loss = 0.27255499047537646, Test loss = 0.2770274710655212; Train Acc = 0.9239833365877469, Test Acc = 0.9198000019788742\n",
      "Training at step=79, batch=0, train loss = 0.27590087056159973, train acc = 0.8999999761581421, time = 0.5562083721160889\n",
      "Training at step=79, batch=120, train loss = 0.1978989541530609, train acc = 0.9100000262260437, time = 0.7061712741851807\n",
      "Training at step=79, batch=240, train loss = 0.09244612604379654, train acc = 0.9700000286102295, time = 0.7049670219421387\n",
      "Training at step=79, batch=360, train loss = 0.2826804220676422, train acc = 0.9399999976158142, time = 0.7044799327850342\n",
      "Training at step=79, batch=480, train loss = 0.32551681995391846, train acc = 0.9200000166893005, time = 0.7019968032836914\n",
      "Testing at step=79, batch=0, test loss = 0.3180643916130066, test acc = 0.9300000071525574, time = 0.2532672882080078\n",
      "Testing at step=79, batch=20, test loss = 0.2528369128704071, test acc = 0.9100000262260437, time = 0.4176361560821533\n",
      "Testing at step=79, batch=40, test loss = 0.40007561445236206, test acc = 0.8899999856948853, time = 0.4171762466430664\n",
      "Testing at step=79, batch=60, test loss = 0.29221856594085693, test acc = 0.9200000166893005, time = 0.41669249534606934\n",
      "Testing at step=79, batch=80, test loss = 0.22356392443180084, test acc = 0.9200000166893005, time = 0.4175848960876465\n",
      "Step 79 finished in 456.0236735343933, Train loss = 0.27004965286701915, Test loss = 0.2677208881080151; Train Acc = 0.9246833361188571, Test Acc = 0.9233000010251999\n",
      "Training at step=80, batch=0, train loss = 0.21780504286289215, train acc = 0.9300000071525574, time = 0.5505461692810059\n",
      "Training at step=80, batch=120, train loss = 0.20942220091819763, train acc = 0.9300000071525574, time = 0.7056868076324463\n",
      "Training at step=80, batch=240, train loss = 0.2963272035121918, train acc = 0.9200000166893005, time = 0.7055966854095459\n",
      "Training at step=80, batch=360, train loss = 0.3724997341632843, train acc = 0.9100000262260437, time = 0.7029895782470703\n",
      "Training at step=80, batch=480, train loss = 0.15782707929611206, train acc = 0.9700000286102295, time = 0.7042458057403564\n",
      "Testing at step=80, batch=0, test loss = 0.16841797530651093, test acc = 0.9700000286102295, time = 0.2216806411743164\n",
      "Testing at step=80, batch=20, test loss = 0.41181278228759766, test acc = 0.8899999856948853, time = 0.1388099193572998\n",
      "Testing at step=80, batch=40, test loss = 0.29776453971862793, test acc = 0.8999999761581421, time = 0.13988161087036133\n",
      "Testing at step=80, batch=60, test loss = 0.3724465072154999, test acc = 0.8999999761581421, time = 0.1376330852508545\n",
      "Testing at step=80, batch=80, test loss = 0.26625028252601624, test acc = 0.9599999785423279, time = 0.1420760154724121\n",
      "Step 80 finished in 455.9841408729553, Train loss = 0.27118036499867837, Test loss = 0.27021457966417073; Train Acc = 0.922983335951964, Test Acc = 0.9228000038862229\n",
      "Training at step=81, batch=0, train loss = 0.2728738486766815, train acc = 0.9200000166893005, time = 0.545151948928833\n",
      "Training at step=81, batch=120, train loss = 0.20192831754684448, train acc = 0.9200000166893005, time = 0.7094943523406982\n",
      "Training at step=81, batch=240, train loss = 0.2491322159767151, train acc = 0.8999999761581421, time = 0.7058944702148438\n",
      "Training at step=81, batch=360, train loss = 0.22974050045013428, train acc = 0.9300000071525574, time = 0.7055940628051758\n",
      "Training at step=81, batch=480, train loss = 0.17117951810359955, train acc = 0.9200000166893005, time = 0.7050821781158447\n",
      "Testing at step=81, batch=0, test loss = 0.16201511025428772, test acc = 0.9399999976158142, time = 0.39046645164489746\n",
      "Testing at step=81, batch=20, test loss = 0.2948513329029083, test acc = 0.8999999761581421, time = 0.41725826263427734\n",
      "Testing at step=81, batch=40, test loss = 0.1782313883304596, test acc = 0.9300000071525574, time = 0.41726040840148926\n",
      "Testing at step=81, batch=60, test loss = 0.30678799748420715, test acc = 0.8799999952316284, time = 0.4158918857574463\n",
      "Testing at step=81, batch=80, test loss = 0.37374892830848694, test acc = 0.9300000071525574, time = 0.4165010452270508\n",
      "Step 81 finished in 456.522029876709, Train loss = 0.2697534131879608, Test loss = 0.2674937291443348; Train Acc = 0.9245500028133392, Test Acc = 0.9237000012397766\n",
      "Training at step=82, batch=0, train loss = 0.2116754651069641, train acc = 0.9300000071525574, time = 0.5430266857147217\n",
      "Training at step=82, batch=120, train loss = 0.3225083649158478, train acc = 0.8799999952316284, time = 0.7046387195587158\n",
      "Training at step=82, batch=240, train loss = 0.26591920852661133, train acc = 0.9300000071525574, time = 0.7036936283111572\n",
      "Training at step=82, batch=360, train loss = 0.19741705060005188, train acc = 0.9399999976158142, time = 0.6454253196716309\n",
      "Training at step=82, batch=480, train loss = 0.40976470708847046, train acc = 0.8700000047683716, time = 0.7028226852416992\n",
      "Testing at step=82, batch=0, test loss = 0.2539924085140228, test acc = 0.9300000071525574, time = 0.21533441543579102\n",
      "Testing at step=82, batch=20, test loss = 0.3239702582359314, test acc = 0.9200000166893005, time = 0.14175891876220703\n",
      "Testing at step=82, batch=40, test loss = 0.34384962916374207, test acc = 0.9100000262260437, time = 0.14658308029174805\n",
      "Testing at step=82, batch=60, test loss = 0.2382136881351471, test acc = 0.8999999761581421, time = 0.14209699630737305\n",
      "Testing at step=82, batch=80, test loss = 0.25159719586372375, test acc = 0.9200000166893005, time = 0.14408254623413086\n",
      "Step 82 finished in 457.3618311882019, Train loss = 0.26852603293955324, Test loss = 0.2693358989804983; Train Acc = 0.9249500021338463, Test Acc = 0.9215000009536743\n",
      "Training at step=83, batch=0, train loss = 0.21835321187973022, train acc = 0.9300000071525574, time = 0.544668436050415\n",
      "Training at step=83, batch=120, train loss = 0.23910711705684662, train acc = 0.9399999976158142, time = 0.7039117813110352\n",
      "Training at step=83, batch=240, train loss = 0.33820420503616333, train acc = 0.9200000166893005, time = 0.7056660652160645\n",
      "Training at step=83, batch=360, train loss = 0.1861298680305481, train acc = 0.9599999785423279, time = 0.705650806427002\n",
      "Training at step=83, batch=480, train loss = 0.20465579628944397, train acc = 0.9200000166893005, time = 0.7041354179382324\n",
      "Testing at step=83, batch=0, test loss = 0.36619794368743896, test acc = 0.9200000166893005, time = 0.15173602104187012\n",
      "Testing at step=83, batch=20, test loss = 0.3318158686161041, test acc = 0.9100000262260437, time = 0.14736628532409668\n",
      "Testing at step=83, batch=40, test loss = 0.1778431534767151, test acc = 0.9399999976158142, time = 0.14696764945983887\n",
      "Testing at step=83, batch=60, test loss = 0.11679672449827194, test acc = 0.9700000286102295, time = 0.14288115501403809\n",
      "Testing at step=83, batch=80, test loss = 0.28323590755462646, test acc = 0.9300000071525574, time = 0.14068961143493652\n",
      "Step 83 finished in 456.56691098213196, Train loss = 0.27047937986751397, Test loss = 0.271995767429471; Train Acc = 0.9243333357572555, Test Acc = 0.9222000044584274\n",
      "Training at step=84, batch=0, train loss = 0.17302890121936798, train acc = 0.9300000071525574, time = 0.545767068862915\n",
      "Training at step=84, batch=120, train loss = 0.2088213562965393, train acc = 0.9300000071525574, time = 0.7036492824554443\n",
      "Training at step=84, batch=240, train loss = 0.2561008632183075, train acc = 0.9100000262260437, time = 0.7053844928741455\n",
      "Training at step=84, batch=360, train loss = 0.3547358810901642, train acc = 0.9399999976158142, time = 0.70346999168396\n",
      "Training at step=84, batch=480, train loss = 0.223246768116951, train acc = 0.949999988079071, time = 0.7056455612182617\n",
      "Testing at step=84, batch=0, test loss = 0.19995757937431335, test acc = 0.9200000166893005, time = 0.2136082649230957\n",
      "Testing at step=84, batch=20, test loss = 0.2268749624490738, test acc = 0.9200000166893005, time = 0.22166943550109863\n",
      "Testing at step=84, batch=40, test loss = 0.24828407168388367, test acc = 0.8999999761581421, time = 0.14672446250915527\n",
      "Testing at step=84, batch=60, test loss = 0.38127246499061584, test acc = 0.8999999761581421, time = 0.14723730087280273\n",
      "Testing at step=84, batch=80, test loss = 0.19945450127124786, test acc = 0.9200000166893005, time = 0.14954304695129395\n",
      "Step 84 finished in 456.43984937667847, Train loss = 0.2704834393784404, Test loss = 0.2827033144235611; Train Acc = 0.9245666689674059, Test Acc = 0.9179000008106232\n",
      "Training at step=85, batch=0, train loss = 0.2665449380874634, train acc = 0.9100000262260437, time = 0.5564186573028564\n",
      "Training at step=85, batch=120, train loss = 0.14405690133571625, train acc = 0.949999988079071, time = 0.7060546875\n",
      "Training at step=85, batch=240, train loss = 0.1699567437171936, train acc = 0.9800000190734863, time = 0.7041385173797607\n",
      "Training at step=85, batch=360, train loss = 0.30285826325416565, train acc = 0.9399999976158142, time = 0.7103478908538818\n",
      "Training at step=85, batch=480, train loss = 0.1345996856689453, train acc = 0.949999988079071, time = 0.707496166229248\n",
      "Testing at step=85, batch=0, test loss = 0.20100851356983185, test acc = 0.9200000166893005, time = 0.387479305267334\n",
      "Testing at step=85, batch=20, test loss = 0.24140207469463348, test acc = 0.9200000166893005, time = 0.14046239852905273\n",
      "Testing at step=85, batch=40, test loss = 0.32055938243865967, test acc = 0.8999999761581421, time = 0.1388096809387207\n",
      "Testing at step=85, batch=60, test loss = 0.34783709049224854, test acc = 0.9100000262260437, time = 0.13777947425842285\n",
      "Testing at step=85, batch=80, test loss = 0.23994842171669006, test acc = 0.9100000262260437, time = 0.13809680938720703\n",
      "Step 85 finished in 457.8136131763458, Train loss = 0.2695807923004031, Test loss = 0.279036079198122; Train Acc = 0.9240666694442431, Test Acc = 0.9161000025272369\n",
      "Training at step=86, batch=0, train loss = 0.2677299976348877, train acc = 0.9100000262260437, time = 0.5356631278991699\n",
      "Training at step=86, batch=120, train loss = 0.32060593366622925, train acc = 0.8899999856948853, time = 0.705996036529541\n",
      "Training at step=86, batch=240, train loss = 0.17330564558506012, train acc = 0.949999988079071, time = 0.7075610160827637\n",
      "Training at step=86, batch=360, train loss = 0.11119729280471802, train acc = 0.9700000286102295, time = 0.7074699401855469\n",
      "Training at step=86, batch=480, train loss = 0.23402629792690277, train acc = 0.9300000071525574, time = 0.7035422325134277\n",
      "Testing at step=86, batch=0, test loss = 0.2662179172039032, test acc = 0.9399999976158142, time = 0.1786332130432129\n",
      "Testing at step=86, batch=20, test loss = 0.12854254245758057, test acc = 0.9599999785423279, time = 0.14029669761657715\n",
      "Testing at step=86, batch=40, test loss = 0.2696963846683502, test acc = 0.9200000166893005, time = 0.14553546905517578\n",
      "Testing at step=86, batch=60, test loss = 0.30505990982055664, test acc = 0.8899999856948853, time = 0.13579177856445312\n",
      "Testing at step=86, batch=80, test loss = 0.24338951706886292, test acc = 0.9300000071525574, time = 0.13609004020690918\n",
      "Step 86 finished in 456.23320484161377, Train loss = 0.2685389926657081, Test loss = 0.2828582772612572; Train Acc = 0.9245000023643176, Test Acc = 0.9162000018358231\n",
      "Training at step=87, batch=0, train loss = 0.21817581355571747, train acc = 0.949999988079071, time = 0.5601980686187744\n",
      "Training at step=87, batch=120, train loss = 0.35057002305984497, train acc = 0.8600000143051147, time = 0.6995711326599121\n",
      "Training at step=87, batch=240, train loss = 0.20024722814559937, train acc = 0.9700000286102295, time = 0.7046153545379639\n",
      "Training at step=87, batch=360, train loss = 0.10080606490373611, train acc = 0.9700000286102295, time = 0.7053124904632568\n",
      "Training at step=87, batch=480, train loss = 0.18125733733177185, train acc = 0.9399999976158142, time = 0.7034385204315186\n",
      "Testing at step=87, batch=0, test loss = 0.2017616629600525, test acc = 0.9300000071525574, time = 0.3880171775817871\n",
      "Testing at step=87, batch=20, test loss = 0.3343362510204315, test acc = 0.8999999761581421, time = 0.41663527488708496\n",
      "Testing at step=87, batch=40, test loss = 0.3404204845428467, test acc = 0.8899999856948853, time = 0.41750025749206543\n",
      "Testing at step=87, batch=60, test loss = 0.4765967130661011, test acc = 0.8799999952316284, time = 0.41759514808654785\n",
      "Testing at step=87, batch=80, test loss = 0.2524951100349426, test acc = 0.9300000071525574, time = 0.41710996627807617\n",
      "Step 87 finished in 456.2808082103729, Train loss = 0.26884300651649634, Test loss = 0.27268982619047166; Train Acc = 0.9251166679461797, Test Acc = 0.9239000022411347\n",
      "Training at step=88, batch=0, train loss = 0.4244397282600403, train acc = 0.9200000166893005, time = 0.5594878196716309\n",
      "Training at step=88, batch=120, train loss = 0.24724338948726654, train acc = 0.9300000071525574, time = 0.7066640853881836\n",
      "Training at step=88, batch=240, train loss = 0.5256855487823486, train acc = 0.8799999952316284, time = 0.6900103092193604\n",
      "Training at step=88, batch=360, train loss = 0.6901560425758362, train acc = 0.8700000047683716, time = 0.7060270309448242\n",
      "Training at step=88, batch=480, train loss = 0.14364612102508545, train acc = 0.9599999785423279, time = 0.7042548656463623\n",
      "Testing at step=88, batch=0, test loss = 0.3669069707393646, test acc = 0.9300000071525574, time = 0.3974003791809082\n",
      "Testing at step=88, batch=20, test loss = 0.14706575870513916, test acc = 0.9399999976158142, time = 0.41734886169433594\n",
      "Testing at step=88, batch=40, test loss = 0.13800685107707977, test acc = 0.949999988079071, time = 0.4166698455810547\n",
      "Testing at step=88, batch=60, test loss = 0.3404933512210846, test acc = 0.8799999952316284, time = 0.4161534309387207\n",
      "Testing at step=88, batch=80, test loss = 0.5287372469902039, test acc = 0.8299999833106995, time = 0.41608119010925293\n",
      "Step 88 finished in 455.53536677360535, Train loss = 0.2702725566551089, Test loss = 0.2725274561345577; Train Acc = 0.9243166693051657, Test Acc = 0.9217000013589859\n",
      "Training at step=89, batch=0, train loss = 0.30869153141975403, train acc = 0.949999988079071, time = 0.5424206256866455\n",
      "Training at step=89, batch=120, train loss = 0.14388054609298706, train acc = 0.9399999976158142, time = 0.7033710479736328\n",
      "Training at step=89, batch=240, train loss = 0.36196327209472656, train acc = 0.9200000166893005, time = 0.7062413692474365\n",
      "Training at step=89, batch=360, train loss = 0.1595727950334549, train acc = 0.9599999785423279, time = 0.7093555927276611\n",
      "Training at step=89, batch=480, train loss = 0.34799015522003174, train acc = 0.8899999856948853, time = 0.7086036205291748\n",
      "Testing at step=89, batch=0, test loss = 0.3095508813858032, test acc = 0.8999999761581421, time = 0.2109839916229248\n",
      "Testing at step=89, batch=20, test loss = 0.40190818905830383, test acc = 0.9300000071525574, time = 0.1329200267791748\n",
      "Testing at step=89, batch=40, test loss = 0.2336929738521576, test acc = 0.949999988079071, time = 0.14090657234191895\n",
      "Testing at step=89, batch=60, test loss = 0.21225491166114807, test acc = 0.9599999785423279, time = 0.13695979118347168\n",
      "Testing at step=89, batch=80, test loss = 0.1518443524837494, test acc = 0.9599999785423279, time = 0.14170074462890625\n",
      "Step 89 finished in 455.76824259757996, Train loss = 0.268667025603354, Test loss = 0.26985002726316454; Train Acc = 0.9247166685263316, Test Acc = 0.9226999980211258\n",
      "Training at step=90, batch=0, train loss = 0.33783191442489624, train acc = 0.9300000071525574, time = 0.5499176979064941\n",
      "Training at step=90, batch=120, train loss = 0.21257933974266052, train acc = 0.9200000166893005, time = 0.7049553394317627\n",
      "Training at step=90, batch=240, train loss = 0.2495948076248169, train acc = 0.9100000262260437, time = 0.7052445411682129\n",
      "Training at step=90, batch=360, train loss = 0.10474870353937149, train acc = 0.9800000190734863, time = 0.70456862449646\n",
      "Training at step=90, batch=480, train loss = 0.34083911776542664, train acc = 0.9200000166893005, time = 0.7044031620025635\n",
      "Testing at step=90, batch=0, test loss = 0.19283349812030792, test acc = 0.9300000071525574, time = 0.2157895565032959\n",
      "Testing at step=90, batch=20, test loss = 0.2739061713218689, test acc = 0.9100000262260437, time = 0.4158358573913574\n",
      "Testing at step=90, batch=40, test loss = 0.24510210752487183, test acc = 0.9100000262260437, time = 0.41676950454711914\n",
      "Testing at step=90, batch=60, test loss = 0.1831834465265274, test acc = 0.9599999785423279, time = 0.41708874702453613\n",
      "Testing at step=90, batch=80, test loss = 0.2109937071800232, test acc = 0.9200000166893005, time = 0.41779375076293945\n",
      "Step 90 finished in 456.5010771751404, Train loss = 0.2684275575727224, Test loss = 0.2647527851164341; Train Acc = 0.9252500021457672, Test Acc = 0.9249000036716462\n",
      "Training at step=91, batch=0, train loss = 0.2808384597301483, train acc = 0.8899999856948853, time = 0.5477099418640137\n",
      "Training at step=91, batch=120, train loss = 0.16784177720546722, train acc = 0.9599999785423279, time = 0.705467700958252\n",
      "Training at step=91, batch=240, train loss = 0.4040659964084625, train acc = 0.9200000166893005, time = 0.705554723739624\n",
      "Training at step=91, batch=360, train loss = 0.265227347612381, train acc = 0.9200000166893005, time = 0.7050855159759521\n",
      "Training at step=91, batch=480, train loss = 0.16409024596214294, train acc = 0.949999988079071, time = 0.7062838077545166\n",
      "Testing at step=91, batch=0, test loss = 0.15063099563121796, test acc = 0.9700000286102295, time = 0.3902463912963867\n",
      "Testing at step=91, batch=20, test loss = 0.17665685713291168, test acc = 0.949999988079071, time = 0.4167945384979248\n",
      "Testing at step=91, batch=40, test loss = 0.2491941899061203, test acc = 0.8999999761581421, time = 0.4182600975036621\n",
      "Testing at step=91, batch=60, test loss = 0.21974533796310425, test acc = 0.9100000262260437, time = 0.41797423362731934\n",
      "Testing at step=91, batch=80, test loss = 0.32530105113983154, test acc = 0.8899999856948853, time = 0.41744446754455566\n",
      "Step 91 finished in 455.5800266265869, Train loss = 0.2691457082827886, Test loss = 0.26322997324168684; Train Acc = 0.9241000018517176, Test Acc = 0.9272000014781951\n",
      "Training at step=92, batch=0, train loss = 0.2896856963634491, train acc = 0.9300000071525574, time = 0.5384330749511719\n",
      "Training at step=92, batch=120, train loss = 0.28600019216537476, train acc = 0.9200000166893005, time = 0.7041339874267578\n",
      "Training at step=92, batch=240, train loss = 0.3197251558303833, train acc = 0.8700000047683716, time = 0.7029762268066406\n",
      "Training at step=92, batch=360, train loss = 0.19377481937408447, train acc = 0.9300000071525574, time = 0.7058205604553223\n",
      "Training at step=92, batch=480, train loss = 0.312432199716568, train acc = 0.9300000071525574, time = 0.703148365020752\n",
      "Testing at step=92, batch=0, test loss = 0.2600530683994293, test acc = 0.9200000166893005, time = 0.3887460231781006\n",
      "Testing at step=92, batch=20, test loss = 0.24603666365146637, test acc = 0.9399999976158142, time = 0.41692376136779785\n",
      "Testing at step=92, batch=40, test loss = 0.12874741852283478, test acc = 0.9599999785423279, time = 0.4175076484680176\n",
      "Testing at step=92, batch=60, test loss = 0.27386701107025146, test acc = 0.9200000166893005, time = 0.41767120361328125\n",
      "Testing at step=92, batch=80, test loss = 0.38996410369873047, test acc = 0.9100000262260437, time = 0.4181373119354248\n",
      "Step 92 finished in 456.21327114105225, Train loss = 0.26741101186722516, Test loss = 0.2703176851570606; Train Acc = 0.9246666673819224, Test Acc = 0.9241000002622605\n",
      "Training at step=93, batch=0, train loss = 0.31894388794898987, train acc = 0.8999999761581421, time = 0.5417892932891846\n",
      "Training at step=93, batch=120, train loss = 0.3351530432701111, train acc = 0.8600000143051147, time = 0.704838752746582\n",
      "Training at step=93, batch=240, train loss = 0.4054231643676758, train acc = 0.8500000238418579, time = 0.7050085067749023\n",
      "Training at step=93, batch=360, train loss = 0.4039868116378784, train acc = 0.8600000143051147, time = 0.7049345970153809\n",
      "Training at step=93, batch=480, train loss = 0.20398764312267303, train acc = 0.9399999976158142, time = 0.7032883167266846\n",
      "Testing at step=93, batch=0, test loss = 0.23681873083114624, test acc = 0.9100000262260437, time = 0.39913320541381836\n",
      "Testing at step=93, batch=20, test loss = 0.3384799063205719, test acc = 0.9200000166893005, time = 0.41667985916137695\n",
      "Testing at step=93, batch=40, test loss = 0.2759627401828766, test acc = 0.9100000262260437, time = 0.4176361560821533\n",
      "Testing at step=93, batch=60, test loss = 0.17636768519878387, test acc = 0.949999988079071, time = 0.4175388813018799\n",
      "Testing at step=93, batch=80, test loss = 0.3068605363368988, test acc = 0.949999988079071, time = 0.417161226272583\n",
      "Step 93 finished in 456.3993353843689, Train loss = 0.26755135035763183, Test loss = 0.26956669077277184; Train Acc = 0.9245333355665207, Test Acc = 0.9247000026702881\n",
      "Training at step=94, batch=0, train loss = 0.25971928238868713, train acc = 0.9300000071525574, time = 0.5574719905853271\n",
      "Training at step=94, batch=120, train loss = 0.2749718427658081, train acc = 0.9300000071525574, time = 0.702843427658081\n",
      "Training at step=94, batch=240, train loss = 0.2685171961784363, train acc = 0.8999999761581421, time = 0.7040774822235107\n",
      "Training at step=94, batch=360, train loss = 0.2027783989906311, train acc = 0.9300000071525574, time = 0.7047688961029053\n",
      "Training at step=94, batch=480, train loss = 0.32707273960113525, train acc = 0.8700000047683716, time = 0.7054345607757568\n",
      "Testing at step=94, batch=0, test loss = 0.36222898960113525, test acc = 0.8999999761581421, time = 0.3893303871154785\n",
      "Testing at step=94, batch=20, test loss = 0.44534265995025635, test acc = 0.8899999856948853, time = 0.417449951171875\n",
      "Testing at step=94, batch=40, test loss = 0.327555388212204, test acc = 0.8799999952316284, time = 0.41814303398132324\n",
      "Testing at step=94, batch=60, test loss = 0.3033013343811035, test acc = 0.9300000071525574, time = 0.41961073875427246\n",
      "Testing at step=94, batch=80, test loss = 0.22624889016151428, test acc = 0.9399999976158142, time = 0.41760706901550293\n",
      "Step 94 finished in 455.9689998626709, Train loss = 0.2674807724853357, Test loss = 0.26410604499280455; Train Acc = 0.9253000018994013, Test Acc = 0.9247000002861023\n",
      "Training at step=95, batch=0, train loss = 0.2645108103752136, train acc = 0.8899999856948853, time = 0.5404829978942871\n",
      "Training at step=95, batch=120, train loss = 0.3347487151622772, train acc = 0.9200000166893005, time = 0.7082345485687256\n",
      "Training at step=95, batch=240, train loss = 0.18460942804813385, train acc = 0.9200000166893005, time = 0.7061755657196045\n",
      "Training at step=95, batch=360, train loss = 0.13842414319515228, train acc = 0.9599999785423279, time = 0.7047555446624756\n",
      "Training at step=95, batch=480, train loss = 0.381158709526062, train acc = 0.8999999761581421, time = 0.7060515880584717\n",
      "Testing at step=95, batch=0, test loss = 0.22792549431324005, test acc = 0.9200000166893005, time = 0.3907489776611328\n",
      "Testing at step=95, batch=20, test loss = 0.4010452330112457, test acc = 0.9100000262260437, time = 0.41837358474731445\n",
      "Testing at step=95, batch=40, test loss = 0.4493221342563629, test acc = 0.8999999761581421, time = 0.41776084899902344\n",
      "Testing at step=95, batch=60, test loss = 0.3296070098876953, test acc = 0.8700000047683716, time = 0.4170346260070801\n",
      "Testing at step=95, batch=80, test loss = 0.20546722412109375, test acc = 0.9399999976158142, time = 0.41719985008239746\n",
      "Step 95 finished in 456.0031645298004, Train loss = 0.2669765554989378, Test loss = 0.2686246866732836; Train Acc = 0.9253166677554449, Test Acc = 0.9252999997138978\n",
      "Training at step=96, batch=0, train loss = 0.14842724800109863, train acc = 0.9800000190734863, time = 0.5576531887054443\n",
      "Training at step=96, batch=120, train loss = 0.21645361185073853, train acc = 0.9300000071525574, time = 0.7332689762115479\n",
      "Training at step=96, batch=240, train loss = 0.3873547613620758, train acc = 0.9200000166893005, time = 0.7328987121582031\n",
      "Training at step=96, batch=360, train loss = 0.3074894845485687, train acc = 0.9200000166893005, time = 0.7042827606201172\n",
      "Training at step=96, batch=480, train loss = 0.309573769569397, train acc = 0.9200000166893005, time = 0.7067351341247559\n",
      "Testing at step=96, batch=0, test loss = 0.1415156126022339, test acc = 0.9599999785423279, time = 0.3849325180053711\n",
      "Testing at step=96, batch=20, test loss = 0.17597731947898865, test acc = 0.949999988079071, time = 0.41724491119384766\n",
      "Testing at step=96, batch=40, test loss = 0.2440904676914215, test acc = 0.9200000166893005, time = 0.417041540145874\n",
      "Testing at step=96, batch=60, test loss = 0.2760566771030426, test acc = 0.9300000071525574, time = 0.41763806343078613\n",
      "Testing at step=96, batch=80, test loss = 0.36898282170295715, test acc = 0.9399999976158142, time = 0.4165818691253662\n",
      "Step 96 finished in 463.2440149784088, Train loss = 0.26673077168564, Test loss = 0.2650214533507824; Train Acc = 0.9258500018715858, Test Acc = 0.9246000015735626\n",
      "Training at step=97, batch=0, train loss = 0.19407986104488373, train acc = 0.9200000166893005, time = 0.5358197689056396\n",
      "Training at step=97, batch=120, train loss = 0.15709583461284637, train acc = 0.9599999785423279, time = 0.7038118839263916\n",
      "Training at step=97, batch=240, train loss = 0.3305385708808899, train acc = 0.9399999976158142, time = 0.7074434757232666\n",
      "Training at step=97, batch=360, train loss = 0.2882259488105774, train acc = 0.8999999761581421, time = 0.703136682510376\n",
      "Training at step=97, batch=480, train loss = 0.23986399173736572, train acc = 0.9399999976158142, time = 0.7045183181762695\n",
      "Testing at step=97, batch=0, test loss = 0.2914665639400482, test acc = 0.8999999761581421, time = 0.21544480323791504\n",
      "Testing at step=97, batch=20, test loss = 0.21418924629688263, test acc = 0.9200000166893005, time = 0.14729881286621094\n",
      "Testing at step=97, batch=40, test loss = 0.23671996593475342, test acc = 0.9399999976158142, time = 0.13648271560668945\n",
      "Testing at step=97, batch=60, test loss = 0.2678951025009155, test acc = 0.8799999952316284, time = 0.14179301261901855\n",
      "Testing at step=97, batch=80, test loss = 0.3578251600265503, test acc = 0.9100000262260437, time = 0.14529752731323242\n",
      "Step 97 finished in 456.06053280830383, Train loss = 0.2664522950723767, Test loss = 0.27024924077093604; Train Acc = 0.9251000012954076, Test Acc = 0.9266000008583068\n",
      "Training at step=98, batch=0, train loss = 0.2943735420703888, train acc = 0.8899999856948853, time = 0.5491237640380859\n",
      "Training at step=98, batch=120, train loss = 0.27113956212997437, train acc = 0.9200000166893005, time = 0.704787015914917\n",
      "Training at step=98, batch=240, train loss = 0.2737233340740204, train acc = 0.9100000262260437, time = 0.7023859024047852\n",
      "Training at step=98, batch=360, train loss = 0.11893375217914581, train acc = 0.9599999785423279, time = 0.7054948806762695\n",
      "Training at step=98, batch=480, train loss = 0.18423469364643097, train acc = 0.9399999976158142, time = 0.70503830909729\n",
      "Testing at step=98, batch=0, test loss = 0.22600345313549042, test acc = 0.9200000166893005, time = 0.389523983001709\n",
      "Testing at step=98, batch=20, test loss = 0.6794336438179016, test acc = 0.8399999737739563, time = 0.41745567321777344\n",
      "Testing at step=98, batch=40, test loss = 0.22540636360645294, test acc = 0.8999999761581421, time = 0.41825366020202637\n",
      "Testing at step=98, batch=60, test loss = 0.26097360253334045, test acc = 0.949999988079071, time = 0.4196345806121826\n",
      "Testing at step=98, batch=80, test loss = 0.3236449062824249, test acc = 0.8799999952316284, time = 0.41748619079589844\n",
      "Step 98 finished in 456.2471306324005, Train loss = 0.26716653883457187, Test loss = 0.2641792481392622; Train Acc = 0.9248833347360293, Test Acc = 0.9237000000476837\n",
      "Training at step=99, batch=0, train loss = 0.3281667232513428, train acc = 0.9200000166893005, time = 0.537304162979126\n",
      "Training at step=99, batch=120, train loss = 0.2962225079536438, train acc = 0.9599999785423279, time = 0.7048201560974121\n",
      "Training at step=99, batch=240, train loss = 0.27961453795433044, train acc = 0.9100000262260437, time = 0.7036693096160889\n",
      "Training at step=99, batch=360, train loss = 0.11051368713378906, train acc = 0.9700000286102295, time = 0.7056496143341064\n",
      "Training at step=99, batch=480, train loss = 0.24416297674179077, train acc = 0.9300000071525574, time = 0.703730583190918\n",
      "Testing at step=99, batch=0, test loss = 0.24548335373401642, test acc = 0.8799999952316284, time = 0.21154475212097168\n",
      "Testing at step=99, batch=20, test loss = 0.19061563909053802, test acc = 0.9399999976158142, time = 0.14210867881774902\n",
      "Testing at step=99, batch=40, test loss = 0.2654576599597931, test acc = 0.8999999761581421, time = 0.14171552658081055\n",
      "Testing at step=99, batch=60, test loss = 0.17991676926612854, test acc = 0.9399999976158142, time = 0.14145565032958984\n",
      "Testing at step=99, batch=80, test loss = 0.5210859179496765, test acc = 0.8799999952316284, time = 0.14197492599487305\n",
      "Step 99 finished in 455.53223848342896, Train loss = 0.26816999015708765, Test loss = 0.2662945983558893; Train Acc = 0.9246333357691765, Test Acc = 0.9232000005245209\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"1-mnist_quanv_classical_linear.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "U0Q0vFm7B6cg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711489773116,
     "user_tz": -660,
     "elapsed": 1144,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "42ddda3e-3765-4cd9-f7af-6eac61c51da8",
    "ExecuteTime": {
     "end_time": "2024-04-07T09:48:51.056793Z",
     "start_time": "2024-04-07T09:48:50.435204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T09:48:51.061881Z",
     "start_time": "2024-04-07T09:48:51.057846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"1-mnist_quanv_classical_linear.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ],
   "outputs": [],
   "execution_count": 13
  }
 ]
}
