{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "authorship_tag": "ABX9TyOMsOj+oVvgvYS0niJGU1v1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ebnFsErY0EM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464100582,
     "user_tz": -660,
     "elapsed": 70769,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "79853453-c681-4231-ac49-20b3ac3fec57",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Install packages\n",
    "!pip install torch torchvision torchaudio pennylane cotengra quimb torchmetrics --upgrade"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "\n",
    "\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VN2wD-U7bGse",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464110097,
     "user_tz": -660,
     "elapsed": 6856,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "2cc42fb4-c41b-4a6e-a674-46cee2afdeae"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data"
   ],
   "metadata": {
    "id": "9WmQPQuLbSFw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUhQUP2dbTja",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464114139,
     "user_tz": -660,
     "elapsed": 1173,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "77d2ed9c-f5fc-4378-9bcd-44c9dcb3c1c7"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 193116714.56it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting MNIST/MNIST/raw/train-images-idx3-ubyte.gz to MNIST/MNIST/raw\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 28388960.35it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 176241466.75it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4321807.80it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n",
      "tensor([7, 7, 5, 7, 4, 7, 7, 7, 5, 4, 7, 2, 7, 8, 1, 1, 1, 6, 4, 1, 5, 1, 2, 7,\n",
      "        8, 0, 8, 6, 1, 8, 8, 7, 4, 4, 7, 9, 5, 4, 3, 9, 6, 5, 6, 0, 3, 9, 5, 3,\n",
      "        5, 1, 0, 4, 2, 3, 2, 0, 1, 5, 6, 4, 8, 6, 3, 9])\n",
      "tensor([-1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j,  0.2863+0.j,  0.7098+0.j, -0.7333+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j], dtype=torch.complex64)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Some Utilities"
   ],
   "metadata": {
    "id": "kQI8WWY6byQq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ket = {\n",
    "    '0':torch.tensor([1.,0.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '1':torch.tensor([0.,1.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '+':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) + torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device)),\n",
    "    '-':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) - torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device))\n",
    "}\n",
    "\n",
    "pauli = {\n",
    "    'I':torch.tensor([[1.,0.],[0.,1.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'X':torch.tensor([[0.,1.],[1.,0.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Y':torch.tensor([[0., -1.j],[1.j, 0]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Z':torch.tensor([[1.,0.],[0.,-1.]], dtype = COMPLEX_DTYPE, device=device)\n",
    "}\n",
    "\n",
    "def tensor_product(*args):\n",
    "  input_list = [a for a in args]\n",
    "  return functools.reduce(torch.kron, input_list)\n",
    "\n",
    "def multi_qubit_identity(n_qubits:int):\n",
    "  assert n_qubits>0\n",
    "  if n_qubits == 1:\n",
    "    return pauli['I']\n",
    "  else:\n",
    "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
    "\n",
    "pauli_words_su4 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
    "      #print(key1, key2)\n",
    "      #print(pauli[key1])\n",
    "      #print(pauli[key2])\n",
    "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
    "      #print(pauli_words_su4[key1+key2])\n",
    "\n",
    "pauli_words_su8 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      if not key1+key2+key3 == 'III':\n",
    "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
    "\n",
    "pauli_words_su16 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        if not key1+key2+key3+key4 == 'IIII':\n",
    "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
    "              pauli[key1],\n",
    "              pauli[key2],\n",
    "              pauli[key3],\n",
    "              pauli[key4]\n",
    "          )\n",
    "\n",
    "pauli_words_su32 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        for key5 in pauli.keys():\n",
    "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
    "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
    "                pauli[key1],\n",
    "                pauli[key2],\n",
    "                pauli[key3],\n",
    "                pauli[key4],\n",
    "                pauli[key5]\n",
    "            )\n",
    "\n",
    "def su32_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = list(pauli_words_su32.values())\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, torch.stack(dict_values))\n",
    "  #print(generator.shape)\n",
    "  return torch.matrix_exp(1j*generator)\n",
    "\n",
    "def su4_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = torch.stack(list(pauli_words_su4.values()))\n",
    "  #print(dict_values.shape)\n",
    "  generator = torch.einsum(\"i, ijk -> jk\", params, dict_values)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "test_params = torch.randn(4**2-1,  device=device).type(COMPLEX_DTYPE)\n",
    "print(test_params.shape)\n",
    "test_op = su4_op(test_params)\n",
    "print(test_op)\n",
    "print(qml.is_unitary(qml.QubitUnitary(test_op.cpu().numpy(), wires=[0,1]))) # will be false if not torch.cdouble\n",
    "\n",
    "rx = torch.linalg.matrix_exp(1j*torch.pi*pauli[\"X\"]/2)\n",
    "print(qml.is_unitary(qml.QubitUnitary(rx.cpu().numpy(), wires=[0])))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7B4DTncWbwQl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464142599,
     "user_tz": -660,
     "elapsed": 2803,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "303c7fee-fa98-4e2a-b0b6-7dcbaed6897e"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([15])\n",
      "tensor([[ 0.5814+0.5563j, -0.2624+0.0584j, -0.2256-0.0349j, -0.3977-0.2645j],\n",
      "        [-0.1473+0.0662j,  0.5740+0.3966j, -0.6925-0.0374j,  0.0056-0.0791j],\n",
      "        [ 0.2395-0.4834j,  0.1500-0.2045j,  0.0092-0.1729j,  0.0740-0.7805j],\n",
      "        [ 0.0519-0.1811j, -0.4827-0.3790j, -0.6611+0.0019j,  0.3540+0.1599j]],\n",
      "       device='cuda:0', dtype=torch.complex64)\n",
      "False\n",
      "True\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def measure_sv(\n",
    "    state,\n",
    "    observable\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Measure a statevector with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not\n",
    "  \"\"\"\n",
    "  expectation_value = torch.conj(state)@observable@state\n",
    "  return torch.real(expectation_value)\n",
    "\n",
    "def measure_dm(\n",
    "    rho,\n",
    "    observable\n",
    "):\n",
    "  \"\"\"\n",
    "  Measure a density matrix with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not.\n",
    "  \"\"\"\n",
    "  product = torch.matmul(rho, observable)\n",
    "\n",
    "  # Calculate the trace, which is the sum of diagonal elements\n",
    "  trace = torch.trace(product)\n",
    "\n",
    "  # The expectation value should be real for physical observables\n",
    "  return torch.real(trace)\n",
    "\n",
    "vmap_measure_sv = torch.vmap(measure_sv, in_dims=(None, 0), out_dims=0)\n",
    "vmap_measure_dm = torch.vmap(measure_dm, in_dims=(None, 0), out_dims=0)\n",
    "\n",
    "# assuming the input patch observables (hermitianized) has shape (batchsize,n_patches, c, h, w)\n",
    "# assuming the input set statevectors has shape (c, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_sv_batched_ob = torch.vmap(measure_sv, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "# assuming the input set desnity matrices has shape (batchsize, n_patches, c, 2**n, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_dm_batched_ob = torch.vmap(measure_dm, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "def bitstring_to_state(bitstring:str):\n",
    "  \"\"\"\n",
    "  Convert a bit string, like '0101001' or '+-+-101'\n",
    "  to a statevector. Each character in the bitstring must be among\n",
    "  0, 1, + and -\n",
    "  \"\"\"\n",
    "  assert len(bitstring)>0\n",
    "  for c in bitstring:\n",
    "    assert c in ['0', '1', '+', '-']\n",
    "  single_qubit_states = [ket[c] for c in bitstring]\n",
    "  return tensor_product(*single_qubit_states)\n",
    "\n",
    "\n",
    "#test_patch = torch.randn(size=[5, 2, 3, 4,4], dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_herm_patch = (torch.einsum(\"...jk->...kj\", test_patch)+test_patch)/2\n",
    "#print(test_herm_patch)\n",
    "#print(\"all patches shape\", test_herm_patch.shape)\n",
    "#test_sv = torch.stack([bitstring_to_state('++')]*3, axis = 0)\n",
    "#print(test_sv)\n",
    "#print(\"all sv shape\", test_sv.shape)\n",
    "#res = vmap_measure_channel_sv_batched_ob(test_sv, test_herm_patch)\n",
    "#print(res)\n",
    "#print(\"res shape\",res.shape)\n",
    "\n",
    "#for batchid in range(test_patch.shape[0]):\n",
    "#  batchitem = test_patch[batchid]\n",
    "#  for patch_idx in range(batchitem.shape[0]):\n",
    "#    patch = batchitem[patch_idx]\n",
    "#    for ch_idx in range(patch.shape[0]):\n",
    "#      print(f\"batchid={batchid}; patchid = {patch_idx}; chid = {ch_idx}\")\n",
    "#      ch = patch[ch_idx]\n",
    "#      #print(ch.shape)\n",
    "#      sv_ch = test_sv[ch_idx]\n",
    "#      #print(sv_ch.shape)\n",
    "#      print(measure_sv(sv_ch, ch))\n"
   ],
   "metadata": {
    "id": "Xs0c2F1eBnGc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464146349,
     "user_tz": -660,
     "elapsed": 713,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flipped Quanvolution Kernel\n",
    "\n",
    "First, let's define the function to extract patches for a single kernel."
   ],
   "metadata": {
    "id": "fFZqT6bpElaL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_patches(image, patch_size, stride, padding=None):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image with multiple input channels and optional custom padding.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (in_channels, height, width).\n",
    "        patch_size (int): Size of the square patches to extract.\n",
    "        stride (int): Stride between patches.\n",
    "        padding (tuple): Padding value(s) for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n",
    "    pad_l, pad_r, pad_t, pad_b = padding if padding is not None else (0,0,0,0)\n",
    "\n",
    "    if padding is not None:\n",
    "        image = torch.nn.functional.pad(image, (pad_l, pad_r, pad_t, pad_b), mode='constant')\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    height, width = image.shape[-2],  image.shape[-1]\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = image[..., i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    patches = torch.stack(patches, dim=-4)\n",
    "    return patches # has shape (batchsize, n_patches, channel, h, w)\n",
    "\n",
    "\n",
    "#single_img = torch.stack([torch.arange(32*32*1,dtype = torch.double, device=device).reshape((32,32)).type(torch.cdouble)]*3)\n",
    "\n",
    "#test_img = torch.stack([single_img]*2)\n",
    "#print(test_img[0][...,:4,:4])\n",
    "#patches = extract_patches(test_img, patch_size=4, stride=2,padding=(0,0,0,0))\n",
    "#print(patches.shape)\n",
    "#print(patches[0].shape)\n",
    "#print(patches[0][0])\n"
   ],
   "metadata": {
    "id": "He4HdMRHC7T6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464148806,
     "user_tz": -660,
     "elapsed": 1,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Three-by-Three Quanv Kernel Function\n",
    "\n",
    "For 3 by 3 patches, we only need a 2-qubit parameterised state, generated by the SU4 gate applied to the $|00\\rangle$ state. The first thing to do is to pad the 3 by 3 patches to 4 by 4. However, for input with size\n",
    "\n",
    "$$\n",
    "D_1\\times H_1 \\times W_1\n",
    "$$\n",
    "\n",
    "where $D_1$ is the number of input channels, we will need $D_1$ different two-qubit states, generated by $D_1$ different parameterised circuits. These circuits will produce $D_1$ expectation values. Suppose each circuit is parameterised by $\\theta_i, i\\in \\{1,2,\\cdots, D_1\\}$, the parameterised (pure) state can be written as\n",
    "\n",
    "$$\n",
    "|\\psi (\\theta_i)⟩ = SU4(\\theta_i)|00⟩\n",
    "$$\n",
    "\n",
    "For a channel $M_i$ of patch $M$ with shape $4\\times 4$, the expectation value is calculated by\n",
    "\n",
    "$$\n",
    "⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$\n",
    "\n",
    "Then, a single pixel value of the output channel corresponding this single kernel is\n",
    "\n",
    "$$\n",
    "\\frac{1}{D_1}\\sum_{i=1}^{D_1}⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$"
   ],
   "metadata": {
    "id": "9hRqflooEqKN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_2q_param_state(theta):\n",
    "  state = bitstring_to_state('00')\n",
    "  state = torch.matmul(su4_op(theta), state)\n",
    "  return state\n",
    "\n",
    "vmap_generate_2q_param_state = torch.vmap(generate_2q_param_state, in_dims=0, out_dims = 0)\n",
    "\n",
    "def single_kernel_op_over_batched_patches(thetas, patch):\n",
    "  # patch has shape (c_in, h, w)\n",
    "  # thetas has shape (c_in, 4^2-1) for SU4 gates\n",
    "  n_theta = thetas.shape[-2]\n",
    "  n_channel = patch.shape[-3]\n",
    "  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n",
    "  states = vmap_generate_2q_param_state(thetas)\n",
    "  #print(\"States shape\", states.shape)\n",
    "  patch = torch.nn.functional.pad(patch, (0, 1, 0, 1), mode='constant')\n",
    "  patch_t = torch.einsum(\"...jk->...kj\", patch)\n",
    "  herm_patch = (patch_t+patch)/2 # has dim (batchsize, num_patches, c, h, w)\n",
    "  #print(\"Herm patch shape\", herm_patch.shape)\n",
    "  channel_out = vmap_measure_channel_sv_batched_ob(states, herm_patch) # has dim (batchsize,n_patches, c)\n",
    "  return torch.sum(channel_out, axis = -1)/n_theta # has dim (batchszie, n_patches)\n",
    "\n",
    "\n",
    "#test_params = torch.randn((1,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_img = dummy_x.to(device)#.type(torch.cdouble)\n",
    "#print(test_img.shape)\n",
    "#test_patches = extract_patches(test_img, patch_size=3, stride=1,padding=(1,1,1,1))\n",
    "#print(test_patches.shape)\n",
    "#print(test_params.shape)\n",
    "#print(vmap_generate_2q_param_state(test_params))\n",
    "#test_out = single_kernel_op_over_batched_patches(test_params, test_patches)\n",
    "#print(test_out.shape)\n",
    "#h = (32-3+1*2)//1 +1\n",
    "#h**2\n"
   ],
   "metadata": {
    "id": "Yzn4KEt5ErG7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464151262,
     "user_tz": -660,
     "elapsed": 1,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multiple Output Channels"
   ],
   "metadata": {
    "id": "A4BXEKd8I67_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# For multiple channel output\n",
    "# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n",
    "vmap_vmap_single_kernel_op_through_extracted_patches = torch.vmap(single_kernel_op_over_batched_patches, in_dims=(0, None), out_dims=-2) # output has dim (batchsize, c_out, n_patches)\n",
    "\n",
    "#c_in = 1\n",
    "#c_out = 2\n",
    "\n",
    "#test_params2 = torch.randn((c_out, c_in,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_out2 = vmap_vmap_single_kernel_op_through_extracted_patches(test_params2, test_patches)\n",
    "#print(test_out2.shape)\n",
    "#test_out_2_features = test_out2.reshape((-1,c_out, 32, 32))\n",
    "#print(test_out_2_features.shape)\n"
   ],
   "metadata": {
    "id": "72vkHV_BI80l",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464153721,
     "user_tz": -660,
     "elapsed": 1,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch Module for Three-by-Three Flipped Quanv"
   ],
   "metadata": {
    "id": "NuZ6G5-sKK_J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FlippedQuanv3x3(torch.nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, padding):\n",
    "    super(FlippedQuanv3x3, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.pad_l, self.pad_r, self.pad_t, self.pad_b = padding if padding is not None else (0,0,0,0)\n",
    "    self.pad = (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n",
    "    self.weight= torch.nn.Parameter(torch.randn((out_channels, in_channels, 4**2-1)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn((out_channels, 1)).type(COMPLEX_DTYPE))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batchsize ,c_in, h, w)\n",
    "    # weight has shape (c_out, c_in, 15)\n",
    "    # bias has shape (c_out, 1)\n",
    "    x = x.type(COMPLEX_DTYPE)\n",
    "    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n",
    "    h_out = (h_in-3+(self.pad_t+self.pad_b))//self.stride +1\n",
    "    w_out = (w_in-3+(self.pad_l+self.pad_r))//self.stride +1\n",
    "\n",
    "    #print(self.weight.shape)\n",
    "    #print(patches.shape)\n",
    "\n",
    "    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n",
    "\n",
    "\n",
    "    out = out + self.bias\n",
    "    return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n",
    "\n",
    "#test_module = FlippedQuanv3x3(in_channels=1, out_channels=2, stride=1, padding=(1,1,1,1)).to(device)\n",
    "#print(dummy_x.shape)\n",
    "#test_out = test_module(dummy_x.to(device))\n",
    "#print(test_out.shape)"
   ],
   "metadata": {
    "id": "Gww_XdJ5KPJt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464157287,
     "user_tz": -660,
     "elapsed": 529,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flipped Quanvolution Replacing Conv2d\n",
    "\n",
    "First, let's just replace `Conv2d` with `FlippedQuanv3x3`."
   ],
   "metadata": {
    "id": "1yumZTjDVu63"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class HybridNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HybridNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(16*28*28, 10).type(REAL_DTYPE)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "net = HybridNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4RC5ou-VzbE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711464159741,
     "user_tz": -660,
     "elapsed": 697,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "2c789a28-bef7-4844-938f-ad5276abbc6d"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "HybridNet(\n",
      "  (layers): Sequential(\n",
      "    (0): FlippedQuanv3x3()\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): FlippedQuanv3x3()\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=28800, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-4-e7258b7aa638>:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n",
      "<ipython-input-9-5de9597dfc31>:30: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:301.)\n",
      "  return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torchmetrics\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.2\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4ZY59q1WS4x",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711489771972,
     "user_tz": -660,
     "elapsed": 19385897,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "068dfdb6-d534-467a-9d34-bd19bb63b442"
   },
   "execution_count": 11,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 600, Number of test batches = 100\n",
      "Print every train batch = 120, Print every test batch = 20\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-e7258b7aa638>:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training at step=0, batch=0, train loss = 2.4362151622772217, train acc = 0.10999999940395355, time = 0.524777889251709\n",
      "Training at step=0, batch=120, train loss = 0.7639809250831604, train acc = 0.7699999809265137, time = 0.38411569595336914\n",
      "Training at step=0, batch=240, train loss = 0.4522012770175934, train acc = 0.8999999761581421, time = 0.37320852279663086\n",
      "Training at step=0, batch=360, train loss = 0.5071191787719727, train acc = 0.8399999737739563, time = 0.3756887912750244\n",
      "Training at step=0, batch=480, train loss = 0.36773478984832764, train acc = 0.8999999761581421, time = 0.3734762668609619\n",
      "Testing at step=0, batch=0, test loss = 0.18719378113746643, test acc = 0.9599999785423279, time = 0.13209056854248047\n",
      "Testing at step=0, batch=20, test loss = 0.3547637462615967, test acc = 0.9200000166893005, time = 0.13251280784606934\n",
      "Testing at step=0, batch=40, test loss = 0.49304068088531494, test acc = 0.8500000238418579, time = 0.13129758834838867\n",
      "Testing at step=0, batch=60, test loss = 0.2100629359483719, test acc = 0.9300000071525574, time = 0.1357707977294922\n",
      "Testing at step=0, batch=80, test loss = 0.37623462080955505, test acc = 0.8899999856948853, time = 0.1318216323852539\n",
      "Step 0 finished in 255.73063325881958, Train loss = 0.6458162353932857, Test loss = 0.3871793216466904; Train Acc = 0.8089166653032104, Test Acc = 0.8848999989032745\n",
      "Training at step=1, batch=0, train loss = 0.44001829624176025, train acc = 0.9100000262260437, time = 0.3730952739715576\n",
      "Training at step=1, batch=120, train loss = 0.34838172793388367, train acc = 0.8899999856948853, time = 0.37223196029663086\n",
      "Training at step=1, batch=240, train loss = 0.2930440604686737, train acc = 0.9100000262260437, time = 0.3748462200164795\n",
      "Training at step=1, batch=360, train loss = 0.416244238615036, train acc = 0.8799999952316284, time = 0.37515807151794434\n",
      "Training at step=1, batch=480, train loss = 0.38239380717277527, train acc = 0.8399999737739563, time = 0.37345361709594727\n",
      "Testing at step=1, batch=0, test loss = 0.3138531446456909, test acc = 0.9200000166893005, time = 0.13208842277526855\n",
      "Testing at step=1, batch=20, test loss = 0.3146560490131378, test acc = 0.8899999856948853, time = 0.131638765335083\n",
      "Testing at step=1, batch=40, test loss = 0.33456963300704956, test acc = 0.9100000262260437, time = 0.13201165199279785\n",
      "Testing at step=1, batch=60, test loss = 0.29531100392341614, test acc = 0.9200000166893005, time = 0.13126659393310547\n",
      "Testing at step=1, batch=80, test loss = 0.21278300881385803, test acc = 0.9599999785423279, time = 0.1337876319885254\n",
      "Step 1 finished in 255.27692294120789, Train loss = 0.3660050810376803, Test loss = 0.31427057325839997; Train Acc = 0.8895666667819023, Test Acc = 0.9065000009536743\n",
      "Training at step=2, batch=0, train loss = 0.327474981546402, train acc = 0.8700000047683716, time = 0.38784122467041016\n",
      "Training at step=2, batch=120, train loss = 0.3626636266708374, train acc = 0.8899999856948853, time = 0.3729441165924072\n",
      "Training at step=2, batch=240, train loss = 0.2790089547634125, train acc = 0.9200000166893005, time = 0.37253832817077637\n",
      "Training at step=2, batch=360, train loss = 0.363167941570282, train acc = 0.8999999761581421, time = 0.37691164016723633\n",
      "Training at step=2, batch=480, train loss = 0.21189534664154053, train acc = 0.9399999976158142, time = 0.37400054931640625\n",
      "Testing at step=2, batch=0, test loss = 0.26695331931114197, test acc = 0.9200000166893005, time = 0.1334850788116455\n",
      "Testing at step=2, batch=20, test loss = 0.36109447479248047, test acc = 0.8999999761581421, time = 0.13346529006958008\n",
      "Testing at step=2, batch=40, test loss = 0.23994384706020355, test acc = 0.9399999976158142, time = 0.13230657577514648\n",
      "Testing at step=2, batch=60, test loss = 0.2560117244720459, test acc = 0.9399999976158142, time = 0.13199639320373535\n",
      "Testing at step=2, batch=80, test loss = 0.34259024262428284, test acc = 0.9100000262260437, time = 0.1335597038269043\n",
      "Step 2 finished in 255.4641978740692, Train loss = 0.32888689247270425, Test loss = 0.3186455529928207; Train Acc = 0.9020666682720184, Test Acc = 0.9031000006198883\n",
      "Training at step=3, batch=0, train loss = 0.33784714341163635, train acc = 0.8500000238418579, time = 0.3747272491455078\n",
      "Training at step=3, batch=120, train loss = 0.1669948697090149, train acc = 0.9700000286102295, time = 0.37209367752075195\n",
      "Training at step=3, batch=240, train loss = 0.42006179690361023, train acc = 0.8999999761581421, time = 0.3744528293609619\n",
      "Training at step=3, batch=360, train loss = 0.18676599860191345, train acc = 0.9399999976158142, time = 0.3747386932373047\n",
      "Training at step=3, batch=480, train loss = 0.24895116686820984, train acc = 0.9100000262260437, time = 0.38269710540771484\n",
      "Testing at step=3, batch=0, test loss = 0.29022642970085144, test acc = 0.8799999952316284, time = 0.13275671005249023\n",
      "Testing at step=3, batch=20, test loss = 0.20213182270526886, test acc = 0.9399999976158142, time = 0.1325056552886963\n",
      "Testing at step=3, batch=40, test loss = 0.3268544375896454, test acc = 0.9100000262260437, time = 0.13467621803283691\n",
      "Testing at step=3, batch=60, test loss = 0.4479813277721405, test acc = 0.8600000143051147, time = 0.13266873359680176\n",
      "Testing at step=3, batch=80, test loss = 0.25790607929229736, test acc = 0.9100000262260437, time = 0.13439369201660156\n",
      "Step 3 finished in 256.1767237186432, Train loss = 0.3063506480306387, Test loss = 0.2848937967419624; Train Acc = 0.9097666683793068, Test Acc = 0.9133000051975251\n",
      "Training at step=4, batch=0, train loss = 0.2044648677110672, train acc = 0.9300000071525574, time = 0.37446069717407227\n",
      "Training at step=4, batch=120, train loss = 0.2561153173446655, train acc = 0.9200000166893005, time = 0.37281250953674316\n",
      "Training at step=4, batch=240, train loss = 0.19330990314483643, train acc = 0.9200000166893005, time = 0.3746509552001953\n",
      "Training at step=4, batch=360, train loss = 0.1751277893781662, train acc = 0.9700000286102295, time = 0.3717682361602783\n",
      "Training at step=4, batch=480, train loss = 0.3438246250152588, train acc = 0.8899999856948853, time = 0.37567138671875\n",
      "Testing at step=4, batch=0, test loss = 0.14862212538719177, test acc = 0.949999988079071, time = 0.13279414176940918\n",
      "Testing at step=4, batch=20, test loss = 0.32111960649490356, test acc = 0.8600000143051147, time = 0.1327824592590332\n",
      "Testing at step=4, batch=40, test loss = 0.3305319845676422, test acc = 0.8899999856948853, time = 0.13268661499023438\n",
      "Testing at step=4, batch=60, test loss = 0.27583205699920654, test acc = 0.9200000166893005, time = 0.13539338111877441\n",
      "Testing at step=4, batch=80, test loss = 0.2824409008026123, test acc = 0.9100000262260437, time = 0.13754820823669434\n",
      "Step 4 finished in 255.75234723091125, Train loss = 0.2909378979479273, Test loss = 0.26892635017633437; Train Acc = 0.9131000022093455, Test Acc = 0.9220000040531159\n",
      "Training at step=5, batch=0, train loss = 0.3889198303222656, train acc = 0.9200000166893005, time = 0.37431836128234863\n",
      "Training at step=5, batch=120, train loss = 0.2786131799221039, train acc = 0.9200000166893005, time = 0.3784160614013672\n",
      "Training at step=5, batch=240, train loss = 0.20988254249095917, train acc = 0.9599999785423279, time = 0.3715376853942871\n",
      "Training at step=5, batch=360, train loss = 0.22452136874198914, train acc = 0.9399999976158142, time = 0.3723921775817871\n",
      "Training at step=5, batch=480, train loss = 0.26755836606025696, train acc = 0.9200000166893005, time = 0.3705103397369385\n",
      "Testing at step=5, batch=0, test loss = 0.2796550989151001, test acc = 0.9200000166893005, time = 0.13294744491577148\n",
      "Testing at step=5, batch=20, test loss = 0.24904033541679382, test acc = 0.9200000166893005, time = 0.13209271430969238\n",
      "Testing at step=5, batch=40, test loss = 0.34317266941070557, test acc = 0.9300000071525574, time = 0.13350248336791992\n",
      "Testing at step=5, batch=60, test loss = 0.21332202851772308, test acc = 0.9399999976158142, time = 0.13175749778747559\n",
      "Testing at step=5, batch=80, test loss = 0.31533971428871155, test acc = 0.8799999952316284, time = 0.13130879402160645\n",
      "Step 5 finished in 254.7264289855957, Train loss = 0.2811645748466253, Test loss = 0.2694374316930771; Train Acc = 0.9169166692097982, Test Acc = 0.9210000026226044\n",
      "Training at step=6, batch=0, train loss = 0.24970215559005737, train acc = 0.9200000166893005, time = 0.37196922302246094\n",
      "Training at step=6, batch=120, train loss = 0.33670857548713684, train acc = 0.9100000262260437, time = 0.372727632522583\n",
      "Training at step=6, batch=240, train loss = 0.35230159759521484, train acc = 0.9100000262260437, time = 0.37229251861572266\n",
      "Training at step=6, batch=360, train loss = 0.3726835548877716, train acc = 0.8999999761581421, time = 0.37326550483703613\n",
      "Training at step=6, batch=480, train loss = 0.12455883622169495, train acc = 0.9700000286102295, time = 0.37081122398376465\n",
      "Testing at step=6, batch=0, test loss = 0.24404960870742798, test acc = 0.9200000166893005, time = 0.13243460655212402\n",
      "Testing at step=6, batch=20, test loss = 0.18730045855045319, test acc = 0.9399999976158142, time = 0.1337440013885498\n",
      "Testing at step=6, batch=40, test loss = 0.2705134451389313, test acc = 0.9300000071525574, time = 0.1351180076599121\n",
      "Testing at step=6, batch=60, test loss = 0.19825123250484467, test acc = 0.9300000071525574, time = 0.13228368759155273\n",
      "Testing at step=6, batch=80, test loss = 0.23437915742397308, test acc = 0.949999988079071, time = 0.1320209503173828\n",
      "Step 6 finished in 255.44199466705322, Train loss = 0.2667263799284895, Test loss = 0.2501652207225561; Train Acc = 0.9212500030795733, Test Acc = 0.927700002193451\n",
      "Training at step=7, batch=0, train loss = 0.19384798407554626, train acc = 0.9300000071525574, time = 0.3716006278991699\n",
      "Training at step=7, batch=120, train loss = 0.18089008331298828, train acc = 0.9399999976158142, time = 0.3765723705291748\n",
      "Training at step=7, batch=240, train loss = 0.21261578798294067, train acc = 0.949999988079071, time = 0.37648797035217285\n",
      "Training at step=7, batch=360, train loss = 0.2566315829753876, train acc = 0.9100000262260437, time = 0.3755760192871094\n",
      "Training at step=7, batch=480, train loss = 0.23313409090042114, train acc = 0.9100000262260437, time = 0.37290430068969727\n",
      "Testing at step=7, batch=0, test loss = 0.19974742829799652, test acc = 0.9399999976158142, time = 0.13268661499023438\n",
      "Testing at step=7, batch=20, test loss = 0.27002355456352234, test acc = 0.8999999761581421, time = 0.13281536102294922\n",
      "Testing at step=7, batch=40, test loss = 0.2071087658405304, test acc = 0.9399999976158142, time = 0.13129401206970215\n",
      "Testing at step=7, batch=60, test loss = 0.15363572537899017, test acc = 0.9399999976158142, time = 0.13203120231628418\n",
      "Testing at step=7, batch=80, test loss = 0.20738552510738373, test acc = 0.949999988079071, time = 0.13262391090393066\n",
      "Step 7 finished in 255.3157753944397, Train loss = 0.25973136819899084, Test loss = 0.2517762743681669; Train Acc = 0.9232666684190433, Test Acc = 0.9261000025272369\n",
      "Training at step=8, batch=0, train loss = 0.19867029786109924, train acc = 0.9300000071525574, time = 0.37494921684265137\n",
      "Training at step=8, batch=120, train loss = 0.3727446496486664, train acc = 0.9100000262260437, time = 0.3722250461578369\n",
      "Training at step=8, batch=240, train loss = 0.37704184651374817, train acc = 0.9100000262260437, time = 0.37509822845458984\n",
      "Training at step=8, batch=360, train loss = 0.1712852418422699, train acc = 0.9399999976158142, time = 0.3729894161224365\n",
      "Training at step=8, batch=480, train loss = 0.13888385891914368, train acc = 0.9599999785423279, time = 0.3799278736114502\n",
      "Testing at step=8, batch=0, test loss = 0.17774446308612823, test acc = 0.9200000166893005, time = 0.13461685180664062\n",
      "Testing at step=8, batch=20, test loss = 0.29177573323249817, test acc = 0.8999999761581421, time = 0.13779044151306152\n",
      "Testing at step=8, batch=40, test loss = 0.3629799783229828, test acc = 0.9100000262260437, time = 0.13193082809448242\n",
      "Testing at step=8, batch=60, test loss = 0.16160878539085388, test acc = 0.9399999976158142, time = 0.13267111778259277\n",
      "Testing at step=8, batch=80, test loss = 0.2316136360168457, test acc = 0.9300000071525574, time = 0.13157987594604492\n",
      "Step 8 finished in 255.73291087150574, Train loss = 0.24891262179861465, Test loss = 0.23201700270175935; Train Acc = 0.9272000006834666, Test Acc = 0.9339000034332275\n",
      "Training at step=9, batch=0, train loss = 0.40136802196502686, train acc = 0.8999999761581421, time = 0.372600793838501\n",
      "Training at step=9, batch=120, train loss = 0.4081023335456848, train acc = 0.8999999761581421, time = 0.3748197555541992\n",
      "Training at step=9, batch=240, train loss = 0.40471404790878296, train acc = 0.8799999952316284, time = 0.37396740913391113\n",
      "Training at step=9, batch=360, train loss = 0.3595564663410187, train acc = 0.8899999856948853, time = 0.3728790283203125\n",
      "Training at step=9, batch=480, train loss = 0.13399820029735565, train acc = 0.949999988079071, time = 0.3725416660308838\n",
      "Testing at step=9, batch=0, test loss = 0.2347865253686905, test acc = 0.9100000262260437, time = 0.1323685646057129\n",
      "Testing at step=9, batch=20, test loss = 0.21990668773651123, test acc = 0.9300000071525574, time = 0.13248825073242188\n",
      "Testing at step=9, batch=40, test loss = 0.2431573122739792, test acc = 0.949999988079071, time = 0.13199305534362793\n",
      "Testing at step=9, batch=60, test loss = 0.27549341320991516, test acc = 0.9399999976158142, time = 0.13237690925598145\n",
      "Testing at step=9, batch=80, test loss = 0.1336444914340973, test acc = 0.9599999785423279, time = 0.13338565826416016\n",
      "Step 9 finished in 255.70591759681702, Train loss = 0.24051329979052147, Test loss = 0.22121296308934688; Train Acc = 0.9288833345969518, Test Acc = 0.9371000021696091\n",
      "Training at step=10, batch=0, train loss = 0.13797716796398163, train acc = 0.9599999785423279, time = 0.3733479976654053\n",
      "Training at step=10, batch=120, train loss = 0.32057124376296997, train acc = 0.8799999952316284, time = 0.37288546562194824\n",
      "Training at step=10, batch=240, train loss = 0.4665640592575073, train acc = 0.8999999761581421, time = 0.3770148754119873\n",
      "Training at step=10, batch=360, train loss = 0.2667893171310425, train acc = 0.9399999976158142, time = 0.37445783615112305\n",
      "Training at step=10, batch=480, train loss = 0.13151749968528748, train acc = 0.949999988079071, time = 0.3778674602508545\n",
      "Testing at step=10, batch=0, test loss = 0.1988123506307602, test acc = 0.9399999976158142, time = 0.13328313827514648\n",
      "Testing at step=10, batch=20, test loss = 0.21939514577388763, test acc = 0.949999988079071, time = 0.13347315788269043\n",
      "Testing at step=10, batch=40, test loss = 0.44013816118240356, test acc = 0.8899999856948853, time = 0.13170599937438965\n",
      "Testing at step=10, batch=60, test loss = 0.13965436816215515, test acc = 0.9599999785423279, time = 0.13176178932189941\n",
      "Testing at step=10, batch=80, test loss = 0.08604881912469864, test acc = 0.9800000190734863, time = 0.13386797904968262\n",
      "Step 10 finished in 255.02237701416016, Train loss = 0.23202041190117598, Test loss = 0.2351972185075283; Train Acc = 0.9310333356261253, Test Acc = 0.9336000007390975\n",
      "Training at step=11, batch=0, train loss = 0.2709130346775055, train acc = 0.9300000071525574, time = 0.3757953643798828\n",
      "Training at step=11, batch=120, train loss = 0.22341686487197876, train acc = 0.9200000166893005, time = 0.37192463874816895\n",
      "Training at step=11, batch=240, train loss = 0.1702364981174469, train acc = 0.9399999976158142, time = 0.3785974979400635\n",
      "Training at step=11, batch=360, train loss = 0.2782798707485199, train acc = 0.9200000166893005, time = 0.37241220474243164\n",
      "Training at step=11, batch=480, train loss = 0.2720438241958618, train acc = 0.9399999976158142, time = 0.3709554672241211\n",
      "Testing at step=11, batch=0, test loss = 0.25838562846183777, test acc = 0.9300000071525574, time = 0.13235950469970703\n",
      "Testing at step=11, batch=20, test loss = 0.1027897521853447, test acc = 0.9700000286102295, time = 0.1326761245727539\n",
      "Testing at step=11, batch=40, test loss = 0.19676078855991364, test acc = 0.9300000071525574, time = 0.13170385360717773\n",
      "Testing at step=11, batch=60, test loss = 0.15633121132850647, test acc = 0.9599999785423279, time = 0.13193511962890625\n",
      "Testing at step=11, batch=80, test loss = 0.2811649739742279, test acc = 0.949999988079071, time = 0.13162946701049805\n",
      "Step 11 finished in 255.46651458740234, Train loss = 0.22544485754022994, Test loss = 0.20802279219031333; Train Acc = 0.9343833345174789, Test Acc = 0.9413999998569489\n",
      "Training at step=12, batch=0, train loss = 0.1233549565076828, train acc = 0.9700000286102295, time = 0.3790304660797119\n",
      "Training at step=12, batch=120, train loss = 0.1802259087562561, train acc = 0.9399999976158142, time = 0.3726017475128174\n",
      "Training at step=12, batch=240, train loss = 0.20049387216567993, train acc = 0.949999988079071, time = 0.37381911277770996\n",
      "Training at step=12, batch=360, train loss = 0.17514580488204956, train acc = 0.949999988079071, time = 0.372333288192749\n",
      "Training at step=12, batch=480, train loss = 0.09323611110448837, train acc = 0.9800000190734863, time = 0.3734016418457031\n",
      "Testing at step=12, batch=0, test loss = 0.07735060900449753, test acc = 0.9800000190734863, time = 0.13318514823913574\n",
      "Testing at step=12, batch=20, test loss = 0.15745800733566284, test acc = 0.949999988079071, time = 0.13193392753601074\n",
      "Testing at step=12, batch=40, test loss = 0.16031906008720398, test acc = 0.9399999976158142, time = 0.1330549716949463\n",
      "Testing at step=12, batch=60, test loss = 0.19951263070106506, test acc = 0.9200000166893005, time = 0.13260555267333984\n",
      "Testing at step=12, batch=80, test loss = 0.28970402479171753, test acc = 0.9200000166893005, time = 0.1330583095550537\n",
      "Step 12 finished in 254.85697078704834, Train loss = 0.21828582325329382, Test loss = 0.2112636025249958; Train Acc = 0.9360166677832603, Test Acc = 0.9405000007152557\n",
      "Training at step=13, batch=0, train loss = 0.18675613403320312, train acc = 0.9300000071525574, time = 0.37482142448425293\n",
      "Training at step=13, batch=120, train loss = 0.18678031861782074, train acc = 0.9599999785423279, time = 0.3717644214630127\n",
      "Training at step=13, batch=240, train loss = 0.3021586537361145, train acc = 0.8999999761581421, time = 0.3802297115325928\n",
      "Training at step=13, batch=360, train loss = 0.20724697411060333, train acc = 0.9300000071525574, time = 0.3737053871154785\n",
      "Training at step=13, batch=480, train loss = 0.26593706011772156, train acc = 0.9200000166893005, time = 0.3718116283416748\n",
      "Testing at step=13, batch=0, test loss = 0.2349119931459427, test acc = 0.9300000071525574, time = 0.1318962574005127\n",
      "Testing at step=13, batch=20, test loss = 0.18911096453666687, test acc = 0.9300000071525574, time = 0.13188552856445312\n",
      "Testing at step=13, batch=40, test loss = 0.2776486277580261, test acc = 0.9399999976158142, time = 0.13123154640197754\n",
      "Testing at step=13, batch=60, test loss = 0.29403361678123474, test acc = 0.9200000166893005, time = 0.13245940208435059\n",
      "Testing at step=13, batch=80, test loss = 0.3824845552444458, test acc = 0.8999999761581421, time = 0.13248062133789062\n",
      "Step 13 finished in 255.50076174736023, Train loss = 0.21256275163342556, Test loss = 0.2444438872486353; Train Acc = 0.9375500012437502, Test Acc = 0.9263000029325485\n",
      "Training at step=14, batch=0, train loss = 0.32511818408966064, train acc = 0.8999999761581421, time = 0.37566208839416504\n",
      "Training at step=14, batch=120, train loss = 0.2091432511806488, train acc = 0.9399999976158142, time = 0.37334179878234863\n",
      "Training at step=14, batch=240, train loss = 0.11420466750860214, train acc = 0.9700000286102295, time = 0.37517476081848145\n",
      "Training at step=14, batch=360, train loss = 0.16798566281795502, train acc = 0.9399999976158142, time = 0.3750138282775879\n",
      "Training at step=14, batch=480, train loss = 0.38372766971588135, train acc = 0.8500000238418579, time = 0.3713071346282959\n",
      "Testing at step=14, batch=0, test loss = 0.09471481293439865, test acc = 0.9800000190734863, time = 0.13307547569274902\n",
      "Testing at step=14, batch=20, test loss = 0.22379370033740997, test acc = 0.9399999976158142, time = 0.1320202350616455\n",
      "Testing at step=14, batch=40, test loss = 0.20912210643291473, test acc = 0.9300000071525574, time = 0.1324167251586914\n",
      "Testing at step=14, batch=60, test loss = 0.12648512423038483, test acc = 0.949999988079071, time = 0.13280153274536133\n",
      "Testing at step=14, batch=80, test loss = 0.19494915008544922, test acc = 0.9800000190734863, time = 0.13297367095947266\n",
      "Step 14 finished in 255.2259316444397, Train loss = 0.20567155697072545, Test loss = 0.19999444238841535; Train Acc = 0.9394500011205673, Test Acc = 0.939600002169609\n",
      "Training at step=15, batch=0, train loss = 0.10628760606050491, train acc = 0.9700000286102295, time = 0.3742635250091553\n",
      "Training at step=15, batch=120, train loss = 0.18756669759750366, train acc = 0.9300000071525574, time = 0.372744083404541\n",
      "Training at step=15, batch=240, train loss = 0.1613663285970688, train acc = 0.9800000190734863, time = 0.3718445301055908\n",
      "Training at step=15, batch=360, train loss = 0.08165845274925232, train acc = 0.9800000190734863, time = 0.3740365505218506\n",
      "Training at step=15, batch=480, train loss = 0.22771549224853516, train acc = 0.9399999976158142, time = 0.37342023849487305\n",
      "Testing at step=15, batch=0, test loss = 0.2719930410385132, test acc = 0.9200000166893005, time = 0.13231945037841797\n",
      "Testing at step=15, batch=20, test loss = 0.17344099283218384, test acc = 0.9399999976158142, time = 0.132157564163208\n",
      "Testing at step=15, batch=40, test loss = 0.17071402072906494, test acc = 0.949999988079071, time = 0.1328284740447998\n",
      "Testing at step=15, batch=60, test loss = 0.24841894209384918, test acc = 0.9399999976158142, time = 0.13306570053100586\n",
      "Testing at step=15, batch=80, test loss = 0.17083898186683655, test acc = 0.949999988079071, time = 0.13192391395568848\n",
      "Step 15 finished in 255.26954674720764, Train loss = 0.20040380638092756, Test loss = 0.21278859734535216; Train Acc = 0.9416000006596247, Test Acc = 0.9333000010251999\n",
      "Training at step=16, batch=0, train loss = 0.09152697771787643, train acc = 0.9800000190734863, time = 0.3774559497833252\n",
      "Training at step=16, batch=120, train loss = 0.1289781630039215, train acc = 0.9700000286102295, time = 0.37227916717529297\n",
      "Training at step=16, batch=240, train loss = 0.13905814290046692, train acc = 0.9800000190734863, time = 0.37278246879577637\n",
      "Training at step=16, batch=360, train loss = 0.18902559578418732, train acc = 0.9599999785423279, time = 0.3726937770843506\n",
      "Training at step=16, batch=480, train loss = 0.13104817271232605, train acc = 0.9599999785423279, time = 0.3877530097961426\n",
      "Testing at step=16, batch=0, test loss = 0.33969002962112427, test acc = 0.9300000071525574, time = 0.13178324699401855\n",
      "Testing at step=16, batch=20, test loss = 0.17984944581985474, test acc = 0.9200000166893005, time = 0.13275671005249023\n",
      "Testing at step=16, batch=40, test loss = 0.32054775953292847, test acc = 0.8999999761581421, time = 0.1342635154724121\n",
      "Testing at step=16, batch=60, test loss = 0.17328353226184845, test acc = 0.9800000190734863, time = 0.13279318809509277\n",
      "Testing at step=16, batch=80, test loss = 0.268290251493454, test acc = 0.8999999761581421, time = 0.13882184028625488\n",
      "Step 16 finished in 255.54380702972412, Train loss = 0.19710526805991926, Test loss = 0.2019772296771407; Train Acc = 0.9421666688720385, Test Acc = 0.9419000005722046\n",
      "Training at step=17, batch=0, train loss = 0.24004465341567993, train acc = 0.9100000262260437, time = 0.37378978729248047\n",
      "Training at step=17, batch=120, train loss = 0.09033918380737305, train acc = 0.9800000190734863, time = 0.3733847141265869\n",
      "Training at step=17, batch=240, train loss = 0.21227143704891205, train acc = 0.9200000166893005, time = 0.3720223903656006\n",
      "Training at step=17, batch=360, train loss = 0.3370850682258606, train acc = 0.9200000166893005, time = 0.37328195571899414\n",
      "Training at step=17, batch=480, train loss = 0.2693865895271301, train acc = 0.8999999761581421, time = 0.37554168701171875\n",
      "Testing at step=17, batch=0, test loss = 0.2901553809642792, test acc = 0.9300000071525574, time = 0.13343501091003418\n",
      "Testing at step=17, batch=20, test loss = 0.1664237380027771, test acc = 0.9300000071525574, time = 0.13243961334228516\n",
      "Testing at step=17, batch=40, test loss = 0.14671316742897034, test acc = 0.9599999785423279, time = 0.13282489776611328\n",
      "Testing at step=17, batch=60, test loss = 0.22314223647117615, test acc = 0.9300000071525574, time = 0.13341164588928223\n",
      "Testing at step=17, batch=80, test loss = 0.1491936296224594, test acc = 0.949999988079071, time = 0.13193821907043457\n",
      "Step 17 finished in 255.50997924804688, Train loss = 0.19334479307755828, Test loss = 0.20050229143351317; Train Acc = 0.9435000011324882, Test Acc = 0.9408000010251999\n",
      "Training at step=18, batch=0, train loss = 0.08170346915721893, train acc = 0.9700000286102295, time = 0.3750574588775635\n",
      "Training at step=18, batch=120, train loss = 0.10730692744255066, train acc = 0.9700000286102295, time = 0.3766045570373535\n",
      "Training at step=18, batch=240, train loss = 0.22779804468154907, train acc = 0.9200000166893005, time = 0.3734300136566162\n",
      "Training at step=18, batch=360, train loss = 0.13568679988384247, train acc = 0.9599999785423279, time = 0.38265109062194824\n",
      "Training at step=18, batch=480, train loss = 0.23112167418003082, train acc = 0.9399999976158142, time = 0.3797764778137207\n",
      "Testing at step=18, batch=0, test loss = 0.18841122090816498, test acc = 0.949999988079071, time = 0.137192964553833\n",
      "Testing at step=18, batch=20, test loss = 0.3309116065502167, test acc = 0.9300000071525574, time = 0.13357925415039062\n",
      "Testing at step=18, batch=40, test loss = 0.2832978367805481, test acc = 0.8999999761581421, time = 0.1346602439880371\n",
      "Testing at step=18, batch=60, test loss = 0.1298508644104004, test acc = 0.949999988079071, time = 0.132171630859375\n",
      "Testing at step=18, batch=80, test loss = 0.328491747379303, test acc = 0.9100000262260437, time = 0.1322765350341797\n",
      "Step 18 finished in 256.66995191574097, Train loss = 0.18854618025943637, Test loss = 0.2047929649800062; Train Acc = 0.9448833346366883, Test Acc = 0.9382000005245209\n",
      "Training at step=19, batch=0, train loss = 0.2414265275001526, train acc = 0.9399999976158142, time = 0.3755826950073242\n",
      "Training at step=19, batch=120, train loss = 0.22351667284965515, train acc = 0.9100000262260437, time = 0.37410640716552734\n",
      "Training at step=19, batch=240, train loss = 0.15339376032352448, train acc = 0.9599999785423279, time = 0.37372875213623047\n",
      "Training at step=19, batch=360, train loss = 0.10780883580446243, train acc = 0.9800000190734863, time = 0.37610530853271484\n",
      "Training at step=19, batch=480, train loss = 0.10388579964637756, train acc = 0.949999988079071, time = 0.3748352527618408\n",
      "Testing at step=19, batch=0, test loss = 0.07577662914991379, test acc = 0.9900000095367432, time = 0.13416790962219238\n",
      "Testing at step=19, batch=20, test loss = 0.14293208718299866, test acc = 0.9399999976158142, time = 0.1334233283996582\n",
      "Testing at step=19, batch=40, test loss = 0.18962860107421875, test acc = 0.9800000190734863, time = 0.13259577751159668\n",
      "Testing at step=19, batch=60, test loss = 0.12553748488426208, test acc = 0.9700000286102295, time = 0.13261985778808594\n",
      "Testing at step=19, batch=80, test loss = 0.11435392498970032, test acc = 0.9599999785423279, time = 0.13280534744262695\n",
      "Step 19 finished in 255.8590157032013, Train loss = 0.1841750267520547, Test loss = 0.18671643685549497; Train Acc = 0.9462833350896835, Test Acc = 0.9456000059843064\n",
      "Training at step=20, batch=0, train loss = 0.14483565092086792, train acc = 0.949999988079071, time = 0.3736100196838379\n",
      "Training at step=20, batch=120, train loss = 0.22849251329898834, train acc = 0.9300000071525574, time = 0.37276387214660645\n",
      "Training at step=20, batch=240, train loss = 0.14888796210289001, train acc = 0.9599999785423279, time = 0.37246203422546387\n",
      "Training at step=20, batch=360, train loss = 0.11744164675474167, train acc = 0.9599999785423279, time = 0.3743627071380615\n",
      "Training at step=20, batch=480, train loss = 0.21130064129829407, train acc = 0.949999988079071, time = 0.3709733486175537\n",
      "Testing at step=20, batch=0, test loss = 0.06824534386396408, test acc = 0.9900000095367432, time = 0.13352465629577637\n",
      "Testing at step=20, batch=20, test loss = 0.0990452989935875, test acc = 0.9800000190734863, time = 0.13352346420288086\n",
      "Testing at step=20, batch=40, test loss = 0.1793435662984848, test acc = 0.9100000262260437, time = 0.13208270072937012\n",
      "Testing at step=20, batch=60, test loss = 0.25329968333244324, test acc = 0.949999988079071, time = 0.1319584846496582\n",
      "Testing at step=20, batch=80, test loss = 0.23641593754291534, test acc = 0.9399999976158142, time = 0.13165521621704102\n",
      "Step 20 finished in 255.38987922668457, Train loss = 0.1787581631106635, Test loss = 0.1978213033080101; Train Acc = 0.9483000011245409, Test Acc = 0.94200000166893\n",
      "Training at step=21, batch=0, train loss = 0.07778707146644592, train acc = 0.9700000286102295, time = 0.3756551742553711\n",
      "Training at step=21, batch=120, train loss = 0.17709989845752716, train acc = 0.9200000166893005, time = 0.3777952194213867\n",
      "Training at step=21, batch=240, train loss = 0.3112521767616272, train acc = 0.9100000262260437, time = 0.37480688095092773\n",
      "Training at step=21, batch=360, train loss = 0.07307888567447662, train acc = 0.9800000190734863, time = 0.3756561279296875\n",
      "Training at step=21, batch=480, train loss = 0.25015950202941895, train acc = 0.9300000071525574, time = 0.372525691986084\n",
      "Testing at step=21, batch=0, test loss = 0.19314976036548615, test acc = 0.949999988079071, time = 0.1344151496887207\n",
      "Testing at step=21, batch=20, test loss = 0.2572612166404724, test acc = 0.949999988079071, time = 0.1330881118774414\n",
      "Testing at step=21, batch=40, test loss = 0.27746930718421936, test acc = 0.9399999976158142, time = 0.13274264335632324\n",
      "Testing at step=21, batch=60, test loss = 0.1349617838859558, test acc = 0.9700000286102295, time = 0.13272666931152344\n",
      "Testing at step=21, batch=80, test loss = 0.1023048385977745, test acc = 0.9599999785423279, time = 0.13161873817443848\n",
      "Step 21 finished in 256.0478048324585, Train loss = 0.17709803245961667, Test loss = 0.17562962856143713; Train Acc = 0.9475166675448418, Test Acc = 0.9500000047683715\n",
      "Training at step=22, batch=0, train loss = 0.19457648694515228, train acc = 0.9200000166893005, time = 0.37447571754455566\n",
      "Training at step=22, batch=120, train loss = 0.092963807284832, train acc = 0.9599999785423279, time = 0.3728210926055908\n",
      "Training at step=22, batch=240, train loss = 0.2344178557395935, train acc = 0.9300000071525574, time = 0.37883734703063965\n",
      "Training at step=22, batch=360, train loss = 0.18232044577598572, train acc = 0.9399999976158142, time = 0.3740723133087158\n",
      "Training at step=22, batch=480, train loss = 0.1454901546239853, train acc = 0.949999988079071, time = 0.37629175186157227\n",
      "Testing at step=22, batch=0, test loss = 0.08620700985193253, test acc = 0.9700000286102295, time = 0.13352656364440918\n",
      "Testing at step=22, batch=20, test loss = 0.14348819851875305, test acc = 0.9700000286102295, time = 0.1334388256072998\n",
      "Testing at step=22, batch=40, test loss = 0.18114227056503296, test acc = 0.949999988079071, time = 0.13290786743164062\n",
      "Testing at step=22, batch=60, test loss = 0.10062488913536072, test acc = 0.9800000190734863, time = 0.13317131996154785\n",
      "Testing at step=22, batch=80, test loss = 0.22214792668819427, test acc = 0.9100000262260437, time = 0.13210082054138184\n",
      "Step 22 finished in 256.02190494537354, Train loss = 0.17330881373335918, Test loss = 0.17785490736365317; Train Acc = 0.9480333339174588, Test Acc = 0.947100003361702\n",
      "Training at step=23, batch=0, train loss = 0.17530664801597595, train acc = 0.9599999785423279, time = 0.37597107887268066\n",
      "Training at step=23, batch=120, train loss = 0.1528133749961853, train acc = 0.949999988079071, time = 0.37578892707824707\n",
      "Training at step=23, batch=240, train loss = 0.11073650419712067, train acc = 0.9599999785423279, time = 0.3732492923736572\n",
      "Training at step=23, batch=360, train loss = 0.13626186549663544, train acc = 0.949999988079071, time = 0.37263941764831543\n",
      "Training at step=23, batch=480, train loss = 0.21723659336566925, train acc = 0.9399999976158142, time = 0.37241339683532715\n",
      "Testing at step=23, batch=0, test loss = 0.17883659899234772, test acc = 0.9599999785423279, time = 0.133284330368042\n",
      "Testing at step=23, batch=20, test loss = 0.24761268496513367, test acc = 0.9399999976158142, time = 0.13151121139526367\n",
      "Testing at step=23, batch=40, test loss = 0.18792198598384857, test acc = 0.9599999785423279, time = 0.13390874862670898\n",
      "Testing at step=23, batch=60, test loss = 0.2501731216907501, test acc = 0.9300000071525574, time = 0.13238048553466797\n",
      "Testing at step=23, batch=80, test loss = 0.17465446889400482, test acc = 0.9399999976158142, time = 0.13174057006835938\n",
      "Step 23 finished in 254.98572278022766, Train loss = 0.16585951337901256, Test loss = 0.1799929591268301; Train Acc = 0.9515833343068759, Test Acc = 0.9468000000715255\n",
      "Training at step=24, batch=0, train loss = 0.1530166119337082, train acc = 0.9399999976158142, time = 0.3741333484649658\n",
      "Training at step=24, batch=120, train loss = 0.11105427891016006, train acc = 0.9599999785423279, time = 0.37653374671936035\n",
      "Training at step=24, batch=240, train loss = 0.23055514693260193, train acc = 0.949999988079071, time = 0.37214088439941406\n",
      "Training at step=24, batch=360, train loss = 0.25708234310150146, train acc = 0.9399999976158142, time = 0.3710639476776123\n",
      "Training at step=24, batch=480, train loss = 0.09885870665311813, train acc = 0.9599999785423279, time = 0.37174224853515625\n",
      "Testing at step=24, batch=0, test loss = 0.35282444953918457, test acc = 0.8999999761581421, time = 0.13234877586364746\n",
      "Testing at step=24, batch=20, test loss = 0.09247072041034698, test acc = 0.949999988079071, time = 0.1319255828857422\n",
      "Testing at step=24, batch=40, test loss = 0.11425133049488068, test acc = 0.949999988079071, time = 0.13341045379638672\n",
      "Testing at step=24, batch=60, test loss = 0.1287524253129959, test acc = 0.9599999785423279, time = 0.13176560401916504\n",
      "Testing at step=24, batch=80, test loss = 0.3351517915725708, test acc = 0.9300000071525574, time = 0.13129830360412598\n",
      "Step 24 finished in 255.3262984752655, Train loss = 0.16602323713401954, Test loss = 0.17395414117723704; Train Acc = 0.9513333351413409, Test Acc = 0.9480999994277954\n",
      "Training at step=25, batch=0, train loss = 0.1168326586484909, train acc = 0.9800000190734863, time = 0.37518835067749023\n",
      "Training at step=25, batch=120, train loss = 0.25278475880622864, train acc = 0.949999988079071, time = 0.37369585037231445\n",
      "Training at step=25, batch=240, train loss = 0.2047613561153412, train acc = 0.9200000166893005, time = 0.37383604049682617\n",
      "Training at step=25, batch=360, train loss = 0.1369687169790268, train acc = 0.9900000095367432, time = 0.3755195140838623\n",
      "Training at step=25, batch=480, train loss = 0.17704220116138458, train acc = 0.949999988079071, time = 0.37222719192504883\n",
      "Testing at step=25, batch=0, test loss = 0.1366526335477829, test acc = 0.9599999785423279, time = 0.32953906059265137\n",
      "Testing at step=25, batch=20, test loss = 0.20024771988391876, test acc = 0.9200000166893005, time = 0.1340346336364746\n",
      "Testing at step=25, batch=40, test loss = 0.2148040533065796, test acc = 0.9599999785423279, time = 0.1336042881011963\n",
      "Testing at step=25, batch=60, test loss = 0.1979767382144928, test acc = 0.9599999785423279, time = 0.13500142097473145\n",
      "Testing at step=25, batch=80, test loss = 0.17606836557388306, test acc = 0.9399999976158142, time = 0.13317275047302246\n",
      "Step 25 finished in 255.59847044944763, Train loss = 0.1632471091300249, Test loss = 0.16190646130591632; Train Acc = 0.9520833361148834, Test Acc = 0.9518000024557114\n",
      "Training at step=26, batch=0, train loss = 0.19037780165672302, train acc = 0.9399999976158142, time = 0.37535786628723145\n",
      "Training at step=26, batch=120, train loss = 0.21701295673847198, train acc = 0.9300000071525574, time = 0.37331366539001465\n",
      "Training at step=26, batch=240, train loss = 0.19458900392055511, train acc = 0.9300000071525574, time = 0.3769075870513916\n",
      "Training at step=26, batch=360, train loss = 0.22749793529510498, train acc = 0.9399999976158142, time = 0.37155795097351074\n",
      "Training at step=26, batch=480, train loss = 0.2583653926849365, train acc = 0.9300000071525574, time = 0.3710484504699707\n",
      "Testing at step=26, batch=0, test loss = 0.1386498361825943, test acc = 0.9599999785423279, time = 0.1322946548461914\n",
      "Testing at step=26, batch=20, test loss = 0.15023161470890045, test acc = 0.9599999785423279, time = 0.13294100761413574\n",
      "Testing at step=26, batch=40, test loss = 0.10492489486932755, test acc = 0.949999988079071, time = 0.13243985176086426\n",
      "Testing at step=26, batch=60, test loss = 0.2483350932598114, test acc = 0.9100000262260437, time = 0.13238239288330078\n",
      "Testing at step=26, batch=80, test loss = 0.1822236031293869, test acc = 0.9300000071525574, time = 0.1332540512084961\n",
      "Step 26 finished in 254.70737671852112, Train loss = 0.16233979305562873, Test loss = 0.15793462943285705; Train Acc = 0.9513000012437502, Test Acc = 0.9522000020742416\n",
      "Training at step=27, batch=0, train loss = 0.23744134604930878, train acc = 0.9300000071525574, time = 0.3726210594177246\n",
      "Training at step=27, batch=120, train loss = 0.12476496398448944, train acc = 0.9399999976158142, time = 0.3761758804321289\n",
      "Training at step=27, batch=240, train loss = 0.13387024402618408, train acc = 0.949999988079071, time = 0.3789408206939697\n",
      "Training at step=27, batch=360, train loss = 0.1989598423242569, train acc = 0.949999988079071, time = 0.3794684410095215\n",
      "Training at step=27, batch=480, train loss = 0.19752176105976105, train acc = 0.9399999976158142, time = 0.3728983402252197\n",
      "Testing at step=27, batch=0, test loss = 0.16890615224838257, test acc = 0.9599999785423279, time = 0.13298916816711426\n",
      "Testing at step=27, batch=20, test loss = 0.13625626266002655, test acc = 0.949999988079071, time = 0.13227176666259766\n",
      "Testing at step=27, batch=40, test loss = 0.199473574757576, test acc = 0.9599999785423279, time = 0.13225984573364258\n",
      "Testing at step=27, batch=60, test loss = 0.13293184340000153, test acc = 0.9599999785423279, time = 0.13293838500976562\n",
      "Testing at step=27, batch=80, test loss = 0.16218215227127075, test acc = 0.9599999785423279, time = 0.1321849822998047\n",
      "Step 27 finished in 255.5889585018158, Train loss = 0.15352291651070119, Test loss = 0.15769051648676397; Train Acc = 0.9545333366592725, Test Acc = 0.953100003004074\n",
      "Training at step=28, batch=0, train loss = 0.2371625006198883, train acc = 0.9399999976158142, time = 0.3726637363433838\n",
      "Training at step=28, batch=120, train loss = 0.19958190619945526, train acc = 0.9300000071525574, time = 0.37697768211364746\n",
      "Training at step=28, batch=240, train loss = 0.06775125116109848, train acc = 0.9800000190734863, time = 0.37727904319763184\n",
      "Training at step=28, batch=360, train loss = 0.21749991178512573, train acc = 0.9300000071525574, time = 0.38104248046875\n",
      "Training at step=28, batch=480, train loss = 0.19073055684566498, train acc = 0.9599999785423279, time = 0.3762531280517578\n",
      "Testing at step=28, batch=0, test loss = 0.1327577531337738, test acc = 0.9399999976158142, time = 0.13522124290466309\n",
      "Testing at step=28, batch=20, test loss = 0.2021586298942566, test acc = 0.9200000166893005, time = 0.13452649116516113\n",
      "Testing at step=28, batch=40, test loss = 0.20632076263427734, test acc = 0.9200000166893005, time = 0.13410377502441406\n",
      "Testing at step=28, batch=60, test loss = 0.1276649832725525, test acc = 0.949999988079071, time = 0.13417530059814453\n",
      "Testing at step=28, batch=80, test loss = 0.2200850248336792, test acc = 0.9599999785423279, time = 0.14659953117370605\n",
      "Step 28 finished in 258.3084383010864, Train loss = 0.15062866843305528, Test loss = 0.16400748981162905; Train Acc = 0.9550833359360695, Test Acc = 0.9506000012159348\n",
      "Training at step=29, batch=0, train loss = 0.10365943610668182, train acc = 0.9700000286102295, time = 0.3798666000366211\n",
      "Training at step=29, batch=120, train loss = 0.07860495150089264, train acc = 0.9800000190734863, time = 0.3775808811187744\n",
      "Training at step=29, batch=240, train loss = 0.11924133449792862, train acc = 0.9700000286102295, time = 0.3765273094177246\n",
      "Training at step=29, batch=360, train loss = 0.1529991328716278, train acc = 0.9399999976158142, time = 0.37784695625305176\n",
      "Training at step=29, batch=480, train loss = 0.04613136127591133, train acc = 0.9900000095367432, time = 0.3755216598510742\n",
      "Testing at step=29, batch=0, test loss = 0.11312054842710495, test acc = 0.9800000190734863, time = 0.1332411766052246\n",
      "Testing at step=29, batch=20, test loss = 0.07862517237663269, test acc = 0.9800000190734863, time = 0.13214492797851562\n",
      "Testing at step=29, batch=40, test loss = 0.05952833220362663, test acc = 0.9800000190734863, time = 0.1317603588104248\n",
      "Testing at step=29, batch=60, test loss = 0.3838593363761902, test acc = 0.9200000166893005, time = 0.13350629806518555\n",
      "Testing at step=29, batch=80, test loss = 0.19756004214286804, test acc = 0.949999988079071, time = 0.1320021152496338\n",
      "Step 29 finished in 256.85641074180603, Train loss = 0.14970944312711557, Test loss = 0.15125576984137296; Train Acc = 0.9557666694124539, Test Acc = 0.9552000015974045\n",
      "Training at step=30, batch=0, train loss = 0.04916497319936752, train acc = 0.9900000095367432, time = 0.37879085540771484\n",
      "Training at step=30, batch=120, train loss = 0.20288099348545074, train acc = 0.9300000071525574, time = 0.37390637397766113\n",
      "Training at step=30, batch=240, train loss = 0.16455097496509552, train acc = 0.949999988079071, time = 0.3719301223754883\n",
      "Training at step=30, batch=360, train loss = 0.25074097514152527, train acc = 0.9200000166893005, time = 0.3730335235595703\n",
      "Training at step=30, batch=480, train loss = 0.07844569534063339, train acc = 0.9700000286102295, time = 0.37287211418151855\n",
      "Testing at step=30, batch=0, test loss = 0.156266987323761, test acc = 0.9200000166893005, time = 0.13297343254089355\n",
      "Testing at step=30, batch=20, test loss = 0.12365445494651794, test acc = 0.949999988079071, time = 0.1326918601989746\n",
      "Testing at step=30, batch=40, test loss = 0.12648308277130127, test acc = 0.949999988079071, time = 0.13189172744750977\n",
      "Testing at step=30, batch=60, test loss = 0.15766535699367523, test acc = 0.9399999976158142, time = 0.13216638565063477\n",
      "Testing at step=30, batch=80, test loss = 0.15098227560520172, test acc = 0.9599999785423279, time = 0.1318988800048828\n",
      "Step 30 finished in 255.35255885124207, Train loss = 0.14690259606267014, Test loss = 0.14836001701653004; Train Acc = 0.9566333377361298, Test Acc = 0.9545000064373016\n",
      "Training at step=31, batch=0, train loss = 0.2185107171535492, train acc = 0.9399999976158142, time = 0.37272143363952637\n",
      "Training at step=31, batch=120, train loss = 0.05584521219134331, train acc = 0.9800000190734863, time = 0.37227654457092285\n",
      "Training at step=31, batch=240, train loss = 0.10641017556190491, train acc = 0.949999988079071, time = 0.3793501853942871\n",
      "Training at step=31, batch=360, train loss = 0.10810161381959915, train acc = 0.9599999785423279, time = 0.3723111152648926\n",
      "Training at step=31, batch=480, train loss = 0.047653455287218094, train acc = 0.9900000095367432, time = 0.37342286109924316\n",
      "Testing at step=31, batch=0, test loss = 0.1603589951992035, test acc = 0.949999988079071, time = 0.13469195365905762\n",
      "Testing at step=31, batch=20, test loss = 0.19371004402637482, test acc = 0.9599999785423279, time = 0.1318187713623047\n",
      "Testing at step=31, batch=40, test loss = 0.19364520907402039, test acc = 0.949999988079071, time = 0.13220930099487305\n",
      "Testing at step=31, batch=60, test loss = 0.11007227003574371, test acc = 0.949999988079071, time = 0.13228297233581543\n",
      "Testing at step=31, batch=80, test loss = 0.09305167198181152, test acc = 0.9700000286102295, time = 0.13213443756103516\n",
      "Step 31 finished in 255.47473788261414, Train loss = 0.14290579978066187, Test loss = 0.14331958219408988; Train Acc = 0.9576500031352043, Test Acc = 0.9569000041484833\n",
      "Training at step=32, batch=0, train loss = 0.06614615768194199, train acc = 0.9800000190734863, time = 0.37410402297973633\n",
      "Training at step=32, batch=120, train loss = 0.15330259501934052, train acc = 0.9399999976158142, time = 0.3733048439025879\n",
      "Training at step=32, batch=240, train loss = 0.18817967176437378, train acc = 0.9599999785423279, time = 0.37216854095458984\n",
      "Training at step=32, batch=360, train loss = 0.2886650562286377, train acc = 0.9399999976158142, time = 0.37331724166870117\n",
      "Training at step=32, batch=480, train loss = 0.24401618540287018, train acc = 0.9300000071525574, time = 0.3746621608734131\n",
      "Testing at step=32, batch=0, test loss = 0.12610381841659546, test acc = 0.9599999785423279, time = 0.13597798347473145\n",
      "Testing at step=32, batch=20, test loss = 0.09662235528230667, test acc = 0.9700000286102295, time = 0.13435983657836914\n",
      "Testing at step=32, batch=40, test loss = 0.16631704568862915, test acc = 0.9399999976158142, time = 0.1325218677520752\n",
      "Testing at step=32, batch=60, test loss = 0.12179738283157349, test acc = 0.9399999976158142, time = 0.13324427604675293\n",
      "Testing at step=32, batch=80, test loss = 0.08319276571273804, test acc = 0.9700000286102295, time = 0.13476943969726562\n",
      "Step 32 finished in 255.36048316955566, Train loss = 0.13939027401308218, Test loss = 0.1525416835770011; Train Acc = 0.9586500031749408, Test Acc = 0.9533000028133393\n",
      "Training at step=33, batch=0, train loss = 0.14528268575668335, train acc = 0.9700000286102295, time = 0.37592315673828125\n",
      "Training at step=33, batch=120, train loss = 0.20998075604438782, train acc = 0.9399999976158142, time = 0.3721427917480469\n",
      "Training at step=33, batch=240, train loss = 0.10368771851062775, train acc = 0.9800000190734863, time = 0.3727395534515381\n",
      "Training at step=33, batch=360, train loss = 0.08299282938241959, train acc = 0.9599999785423279, time = 0.37416934967041016\n",
      "Training at step=33, batch=480, train loss = 0.15320482850074768, train acc = 0.9700000286102295, time = 0.37475156784057617\n",
      "Testing at step=33, batch=0, test loss = 0.12484948337078094, test acc = 0.9700000286102295, time = 0.1320328712463379\n",
      "Testing at step=33, batch=20, test loss = 0.15093842148780823, test acc = 0.9599999785423279, time = 0.13266301155090332\n",
      "Testing at step=33, batch=40, test loss = 0.28258538246154785, test acc = 0.9100000262260437, time = 0.1325526237487793\n",
      "Testing at step=33, batch=60, test loss = 0.15932609140872955, test acc = 0.9599999785423279, time = 0.13217949867248535\n",
      "Testing at step=33, batch=80, test loss = 0.13427740335464478, test acc = 0.9399999976158142, time = 0.13195371627807617\n",
      "Step 33 finished in 254.86329293251038, Train loss = 0.13637296578846872, Test loss = 0.14400214646011592; Train Acc = 0.9592500031987826, Test Acc = 0.9558000051975251\n",
      "Training at step=34, batch=0, train loss = 0.118113212287426, train acc = 0.9399999976158142, time = 0.37336015701293945\n",
      "Training at step=34, batch=120, train loss = 0.18333500623703003, train acc = 0.9399999976158142, time = 0.3768889904022217\n",
      "Training at step=34, batch=240, train loss = 0.16326658427715302, train acc = 0.9599999785423279, time = 0.37189579010009766\n",
      "Training at step=34, batch=360, train loss = 0.1051270067691803, train acc = 0.9700000286102295, time = 0.3720111846923828\n",
      "Training at step=34, batch=480, train loss = 0.1135731115937233, train acc = 0.9599999785423279, time = 0.3753354549407959\n",
      "Testing at step=34, batch=0, test loss = 0.12597538530826569, test acc = 0.9700000286102295, time = 0.13221096992492676\n",
      "Testing at step=34, batch=20, test loss = 0.08690255135297775, test acc = 0.9700000286102295, time = 0.13119196891784668\n",
      "Testing at step=34, batch=40, test loss = 0.34045958518981934, test acc = 0.8899999856948853, time = 0.13305401802062988\n",
      "Testing at step=34, batch=60, test loss = 0.11915150284767151, test acc = 0.949999988079071, time = 0.13273286819458008\n",
      "Testing at step=34, batch=80, test loss = 0.08710074424743652, test acc = 0.9700000286102295, time = 0.13278651237487793\n",
      "Step 34 finished in 254.9712896347046, Train loss = 0.13234777092312774, Test loss = 0.13902271687984466; Train Acc = 0.9607833382487297, Test Acc = 0.9574000024795533\n",
      "Training at step=35, batch=0, train loss = 0.11615358293056488, train acc = 0.9800000190734863, time = 0.37425732612609863\n",
      "Training at step=35, batch=120, train loss = 0.0875500738620758, train acc = 0.949999988079071, time = 0.37752866744995117\n",
      "Training at step=35, batch=240, train loss = 0.09024442732334137, train acc = 0.949999988079071, time = 0.3718557357788086\n",
      "Training at step=35, batch=360, train loss = 0.12198617309331894, train acc = 0.9700000286102295, time = 0.37724900245666504\n",
      "Training at step=35, batch=480, train loss = 0.10413122922182083, train acc = 0.9599999785423279, time = 0.37532925605773926\n",
      "Testing at step=35, batch=0, test loss = 0.308114230632782, test acc = 0.9200000166893005, time = 0.1326596736907959\n",
      "Testing at step=35, batch=20, test loss = 0.15582972764968872, test acc = 0.9700000286102295, time = 0.13230085372924805\n",
      "Testing at step=35, batch=40, test loss = 0.20743341743946075, test acc = 0.949999988079071, time = 0.1327204704284668\n",
      "Testing at step=35, batch=60, test loss = 0.11224702000617981, test acc = 0.9700000286102295, time = 0.13219141960144043\n",
      "Testing at step=35, batch=80, test loss = 0.1982155442237854, test acc = 0.949999988079071, time = 0.1334536075592041\n",
      "Step 35 finished in 255.41701340675354, Train loss = 0.13129497981319824, Test loss = 0.15340489439666272; Train Acc = 0.9607833380500476, Test Acc = 0.9532000011205674\n",
      "Training at step=36, batch=0, train loss = 0.07784124463796616, train acc = 0.9700000286102295, time = 0.37506723403930664\n",
      "Training at step=36, batch=120, train loss = 0.09161178767681122, train acc = 0.9800000190734863, time = 0.3746504783630371\n",
      "Training at step=36, batch=240, train loss = 0.06220962479710579, train acc = 0.9700000286102295, time = 0.3724355697631836\n",
      "Training at step=36, batch=360, train loss = 0.09812330454587936, train acc = 0.9700000286102295, time = 0.3720681667327881\n",
      "Training at step=36, batch=480, train loss = 0.16186706721782684, train acc = 0.9700000286102295, time = 0.37134218215942383\n",
      "Testing at step=36, batch=0, test loss = 0.15613259375095367, test acc = 0.9700000286102295, time = 0.1319417953491211\n",
      "Testing at step=36, batch=20, test loss = 0.2427130937576294, test acc = 0.9100000262260437, time = 0.13102030754089355\n",
      "Testing at step=36, batch=40, test loss = 0.06010717526078224, test acc = 0.9900000095367432, time = 0.1315629482269287\n",
      "Testing at step=36, batch=60, test loss = 0.2693502902984619, test acc = 0.9200000166893005, time = 0.13297557830810547\n",
      "Testing at step=36, batch=80, test loss = 0.11274450272321701, test acc = 0.9700000286102295, time = 0.131697416305542\n",
      "Step 36 finished in 254.93378114700317, Train loss = 0.12675058966502548, Test loss = 0.15308578621596097; Train Acc = 0.962766671081384, Test Acc = 0.9544000029563904\n",
      "Training at step=37, batch=0, train loss = 0.04378741234540939, train acc = 1.0, time = 0.37209510803222656\n",
      "Training at step=37, batch=120, train loss = 0.17334595322608948, train acc = 0.9399999976158142, time = 0.37386298179626465\n",
      "Training at step=37, batch=240, train loss = 0.10551164299249649, train acc = 0.9700000286102295, time = 0.371074914932251\n",
      "Training at step=37, batch=360, train loss = 0.1314975619316101, train acc = 0.9599999785423279, time = 0.3720431327819824\n",
      "Training at step=37, batch=480, train loss = 0.0637454092502594, train acc = 0.9900000095367432, time = 0.37366175651550293\n",
      "Testing at step=37, batch=0, test loss = 0.14988191425800323, test acc = 0.9399999976158142, time = 0.13229870796203613\n",
      "Testing at step=37, batch=20, test loss = 0.12555640935897827, test acc = 0.9599999785423279, time = 0.1322619915008545\n",
      "Testing at step=37, batch=40, test loss = 0.1241125762462616, test acc = 0.949999988079071, time = 0.13178420066833496\n",
      "Testing at step=37, batch=60, test loss = 0.1233367845416069, test acc = 0.9700000286102295, time = 0.13306069374084473\n",
      "Testing at step=37, batch=80, test loss = 0.1026599109172821, test acc = 0.949999988079071, time = 0.13179826736450195\n",
      "Step 37 finished in 254.94254565238953, Train loss = 0.12688713145442307, Test loss = 0.13420764649286865; Train Acc = 0.9623833386103312, Test Acc = 0.9595000046491623\n",
      "Training at step=38, batch=0, train loss = 0.06757266074419022, train acc = 0.9700000286102295, time = 0.37465357780456543\n",
      "Training at step=38, batch=120, train loss = 0.10431228578090668, train acc = 0.9700000286102295, time = 0.37221670150756836\n",
      "Training at step=38, batch=240, train loss = 0.11476451903581619, train acc = 0.9599999785423279, time = 0.37291765213012695\n",
      "Training at step=38, batch=360, train loss = 0.11464691162109375, train acc = 0.9599999785423279, time = 0.37009716033935547\n",
      "Training at step=38, batch=480, train loss = 0.14304938912391663, train acc = 0.9599999785423279, time = 0.3711278438568115\n",
      "Testing at step=38, batch=0, test loss = 0.15308085083961487, test acc = 0.9300000071525574, time = 0.132673978805542\n",
      "Testing at step=38, batch=20, test loss = 0.1583271622657776, test acc = 0.9599999785423279, time = 0.1324758529663086\n",
      "Testing at step=38, batch=40, test loss = 0.15010851621627808, test acc = 0.9599999785423279, time = 0.13398241996765137\n",
      "Testing at step=38, batch=60, test loss = 0.09958577156066895, test acc = 0.9599999785423279, time = 0.13232183456420898\n",
      "Testing at step=38, batch=80, test loss = 0.08716508001089096, test acc = 0.9700000286102295, time = 0.13268637657165527\n",
      "Step 38 finished in 254.89857387542725, Train loss = 0.12427650076026718, Test loss = 0.13999023206532002; Train Acc = 0.9632000048955282, Test Acc = 0.9562000000476837\n",
      "Training at step=39, batch=0, train loss = 0.07786650955677032, train acc = 0.9800000190734863, time = 0.3732883930206299\n",
      "Training at step=39, batch=120, train loss = 0.08732132613658905, train acc = 0.9800000190734863, time = 0.37164759635925293\n",
      "Training at step=39, batch=240, train loss = 0.32909825444221497, train acc = 0.9200000166893005, time = 0.37259578704833984\n",
      "Training at step=39, batch=360, train loss = 0.13329295814037323, train acc = 0.9399999976158142, time = 0.37806010246276855\n",
      "Training at step=39, batch=480, train loss = 0.1524152159690857, train acc = 0.9300000071525574, time = 0.3744049072265625\n",
      "Testing at step=39, batch=0, test loss = 0.09607420116662979, test acc = 0.949999988079071, time = 0.13518571853637695\n",
      "Testing at step=39, batch=20, test loss = 0.09196572005748749, test acc = 0.9599999785423279, time = 0.1344149112701416\n",
      "Testing at step=39, batch=40, test loss = 0.11635971814393997, test acc = 0.9599999785423279, time = 0.1349625587463379\n",
      "Testing at step=39, batch=60, test loss = 0.16647595167160034, test acc = 0.9700000286102295, time = 0.13480257987976074\n",
      "Testing at step=39, batch=80, test loss = 0.08161970227956772, test acc = 0.9800000190734863, time = 0.13531279563903809\n",
      "Step 39 finished in 255.7467176914215, Train loss = 0.12451851013116538, Test loss = 0.13594620723277331; Train Acc = 0.9623166713118553, Test Acc = 0.9588000029325485\n",
      "Training at step=40, batch=0, train loss = 0.1709444671869278, train acc = 0.9599999785423279, time = 0.37794995307922363\n",
      "Training at step=40, batch=120, train loss = 0.12004198879003525, train acc = 0.9700000286102295, time = 0.3732779026031494\n",
      "Training at step=40, batch=240, train loss = 0.1264658421278, train acc = 0.949999988079071, time = 0.373945951461792\n",
      "Training at step=40, batch=360, train loss = 0.10911828279495239, train acc = 0.9700000286102295, time = 0.3736889362335205\n",
      "Training at step=40, batch=480, train loss = 0.12617704272270203, train acc = 0.9700000286102295, time = 0.3725106716156006\n",
      "Testing at step=40, batch=0, test loss = 0.10410193353891373, test acc = 0.9700000286102295, time = 0.13262295722961426\n",
      "Testing at step=40, batch=20, test loss = 0.1824684739112854, test acc = 0.9300000071525574, time = 0.13217926025390625\n",
      "Testing at step=40, batch=40, test loss = 0.056977011263370514, test acc = 0.9800000190734863, time = 0.13158273696899414\n",
      "Testing at step=40, batch=60, test loss = 0.16266432404518127, test acc = 0.9700000286102295, time = 0.13221049308776855\n",
      "Testing at step=40, batch=80, test loss = 0.164342001080513, test acc = 0.9300000071525574, time = 0.13185906410217285\n",
      "Step 40 finished in 255.27061200141907, Train loss = 0.12038957741421957, Test loss = 0.12788450572639704; Train Acc = 0.9646000044544538, Test Acc = 0.96180000603199\n",
      "Training at step=41, batch=0, train loss = 0.06787184625864029, train acc = 0.9800000190734863, time = 0.37482380867004395\n",
      "Training at step=41, batch=120, train loss = 0.21812045574188232, train acc = 0.9399999976158142, time = 0.3737773895263672\n",
      "Training at step=41, batch=240, train loss = 0.06900867074728012, train acc = 0.9800000190734863, time = 0.3762226104736328\n",
      "Training at step=41, batch=360, train loss = 0.16177049279212952, train acc = 0.9700000286102295, time = 0.37499141693115234\n",
      "Training at step=41, batch=480, train loss = 0.15407004952430725, train acc = 0.9300000071525574, time = 0.3728446960449219\n",
      "Testing at step=41, batch=0, test loss = 0.09146265685558319, test acc = 0.9700000286102295, time = 0.13276171684265137\n",
      "Testing at step=41, batch=20, test loss = 0.044722724705934525, test acc = 0.9900000095367432, time = 0.13307929039001465\n",
      "Testing at step=41, batch=40, test loss = 0.16888371109962463, test acc = 0.949999988079071, time = 0.13254332542419434\n",
      "Testing at step=41, batch=60, test loss = 0.17370542883872986, test acc = 0.9599999785423279, time = 0.13254642486572266\n",
      "Testing at step=41, batch=80, test loss = 0.13275028765201569, test acc = 0.9399999976158142, time = 0.13290882110595703\n",
      "Step 41 finished in 255.40887832641602, Train loss = 0.11794494770467281, Test loss = 0.13307756688445807; Train Acc = 0.9645166722933451, Test Acc = 0.9601000040769577\n",
      "Training at step=42, batch=0, train loss = 0.08786783367395401, train acc = 0.9800000190734863, time = 0.3748502731323242\n",
      "Training at step=42, batch=120, train loss = 0.09211984276771545, train acc = 0.9700000286102295, time = 0.37515687942504883\n",
      "Training at step=42, batch=240, train loss = 0.08244950324296951, train acc = 0.9800000190734863, time = 0.3751089572906494\n",
      "Training at step=42, batch=360, train loss = 0.07987694442272186, train acc = 0.9700000286102295, time = 0.37081265449523926\n",
      "Training at step=42, batch=480, train loss = 0.14916881918907166, train acc = 0.9599999785423279, time = 0.37307095527648926\n",
      "Testing at step=42, batch=0, test loss = 0.23040622472763062, test acc = 0.9100000262260437, time = 0.13202476501464844\n",
      "Testing at step=42, batch=20, test loss = 0.2052960842847824, test acc = 0.9200000166893005, time = 0.1319897174835205\n",
      "Testing at step=42, batch=40, test loss = 0.031052833423018456, test acc = 1.0, time = 0.13173723220825195\n",
      "Testing at step=42, batch=60, test loss = 0.1621330827474594, test acc = 0.949999988079071, time = 0.13130474090576172\n",
      "Testing at step=42, batch=80, test loss = 0.07971247285604477, test acc = 0.9599999785423279, time = 0.13216543197631836\n",
      "Step 42 finished in 255.0430805683136, Train loss = 0.11780053505053123, Test loss = 0.14300912072882055; Train Acc = 0.9641333391269048, Test Acc = 0.9534000021219253\n",
      "Training at step=43, batch=0, train loss = 0.137708842754364, train acc = 0.9700000286102295, time = 0.3722808361053467\n",
      "Training at step=43, batch=120, train loss = 0.09615705162286758, train acc = 0.9599999785423279, time = 0.3714613914489746\n",
      "Training at step=43, batch=240, train loss = 0.10201267898082733, train acc = 0.9800000190734863, time = 0.37113285064697266\n",
      "Training at step=43, batch=360, train loss = 0.14049473404884338, train acc = 0.9599999785423279, time = 0.37266039848327637\n",
      "Training at step=43, batch=480, train loss = 0.06941001117229462, train acc = 0.9700000286102295, time = 0.37534403800964355\n",
      "Testing at step=43, batch=0, test loss = 0.07140085846185684, test acc = 0.9599999785423279, time = 0.13272333145141602\n",
      "Testing at step=43, batch=20, test loss = 0.14075297117233276, test acc = 0.9700000286102295, time = 0.13489937782287598\n",
      "Testing at step=43, batch=40, test loss = 0.13003034889698029, test acc = 0.9700000286102295, time = 0.13306570053100586\n",
      "Testing at step=43, batch=60, test loss = 0.12474093586206436, test acc = 0.9599999785423279, time = 0.1322927474975586\n",
      "Testing at step=43, batch=80, test loss = 0.21068048477172852, test acc = 0.949999988079071, time = 0.1329176425933838\n",
      "Step 43 finished in 255.35403418540955, Train loss = 0.11656184712424875, Test loss = 0.1307908084616065; Train Acc = 0.9657833385467529, Test Acc = 0.9615000075101853\n",
      "Training at step=44, batch=0, train loss = 0.07012676447629929, train acc = 0.9700000286102295, time = 0.3730647563934326\n",
      "Training at step=44, batch=120, train loss = 0.1332162320613861, train acc = 0.9599999785423279, time = 0.37302494049072266\n",
      "Training at step=44, batch=240, train loss = 0.12713541090488434, train acc = 0.9700000286102295, time = 0.37061119079589844\n",
      "Training at step=44, batch=360, train loss = 0.2228718400001526, train acc = 0.9100000262260437, time = 0.3720991611480713\n",
      "Training at step=44, batch=480, train loss = 0.25700053572654724, train acc = 0.9300000071525574, time = 0.37811779975891113\n",
      "Testing at step=44, batch=0, test loss = 0.19068211317062378, test acc = 0.9200000166893005, time = 0.13242053985595703\n",
      "Testing at step=44, batch=20, test loss = 0.044226374477148056, test acc = 0.9900000095367432, time = 0.13283014297485352\n",
      "Testing at step=44, batch=40, test loss = 0.07788115739822388, test acc = 0.9800000190734863, time = 0.13126587867736816\n",
      "Testing at step=44, batch=60, test loss = 0.08810047060251236, test acc = 0.9599999785423279, time = 0.13173580169677734\n",
      "Testing at step=44, batch=80, test loss = 0.06025656312704086, test acc = 0.9900000095367432, time = 0.13173389434814453\n",
      "Step 44 finished in 254.58300805091858, Train loss = 0.11374219722735385, Test loss = 0.13179057145491244; Train Acc = 0.9655833386381467, Test Acc = 0.9601000034809113\n",
      "Training at step=45, batch=0, train loss = 0.14414897561073303, train acc = 0.9700000286102295, time = 0.37402772903442383\n",
      "Training at step=45, batch=120, train loss = 0.22885988652706146, train acc = 0.9399999976158142, time = 0.37076592445373535\n",
      "Training at step=45, batch=240, train loss = 0.09511344134807587, train acc = 0.9700000286102295, time = 0.37358617782592773\n",
      "Training at step=45, batch=360, train loss = 0.08480217307806015, train acc = 0.9700000286102295, time = 0.3725569248199463\n",
      "Training at step=45, batch=480, train loss = 0.20566511154174805, train acc = 0.9300000071525574, time = 0.37408900260925293\n",
      "Testing at step=45, batch=0, test loss = 0.30676159262657166, test acc = 0.9300000071525574, time = 0.1334824562072754\n",
      "Testing at step=45, batch=20, test loss = 0.16568632423877716, test acc = 0.9399999976158142, time = 0.13153505325317383\n",
      "Testing at step=45, batch=40, test loss = 0.11290457844734192, test acc = 0.9700000286102295, time = 0.13226866722106934\n",
      "Testing at step=45, batch=60, test loss = 0.16940069198608398, test acc = 0.9599999785423279, time = 0.1318356990814209\n",
      "Testing at step=45, batch=80, test loss = 0.17846278846263885, test acc = 0.9700000286102295, time = 0.1309499740600586\n",
      "Step 45 finished in 254.79569482803345, Train loss = 0.11179707675861815, Test loss = 0.13536556869745253; Train Acc = 0.9660666725039482, Test Acc = 0.9576000034809112\n",
      "Training at step=46, batch=0, train loss = 0.20320814847946167, train acc = 0.9300000071525574, time = 0.3804507255554199\n",
      "Training at step=46, batch=120, train loss = 0.22261008620262146, train acc = 0.949999988079071, time = 0.377687931060791\n",
      "Training at step=46, batch=240, train loss = 0.25356924533843994, train acc = 0.9599999785423279, time = 0.3745877742767334\n",
      "Training at step=46, batch=360, train loss = 0.18241740763187408, train acc = 0.949999988079071, time = 0.3741331100463867\n",
      "Training at step=46, batch=480, train loss = 0.05112560838460922, train acc = 0.9800000190734863, time = 0.3724191188812256\n",
      "Testing at step=46, batch=0, test loss = 0.09052740037441254, test acc = 0.9800000190734863, time = 0.1327505111694336\n",
      "Testing at step=46, batch=20, test loss = 0.050389789044857025, test acc = 0.9900000095367432, time = 0.13192391395568848\n",
      "Testing at step=46, batch=40, test loss = 0.10210008919239044, test acc = 0.949999988079071, time = 0.13220858573913574\n",
      "Testing at step=46, batch=60, test loss = 0.19755682349205017, test acc = 0.949999988079071, time = 0.1397998332977295\n",
      "Testing at step=46, batch=80, test loss = 0.11088770627975464, test acc = 0.9700000286102295, time = 0.13190770149230957\n",
      "Step 46 finished in 255.39255666732788, Train loss = 0.11005967469265064, Test loss = 0.12098622802644968; Train Acc = 0.9665666720271111, Test Acc = 0.9628000056743622\n",
      "Training at step=47, batch=0, train loss = 0.22652187943458557, train acc = 0.9300000071525574, time = 0.3763580322265625\n",
      "Training at step=47, batch=120, train loss = 0.029335206374526024, train acc = 1.0, time = 0.3729593753814697\n",
      "Training at step=47, batch=240, train loss = 0.07367672771215439, train acc = 0.9700000286102295, time = 0.37830042839050293\n",
      "Training at step=47, batch=360, train loss = 0.11790256202220917, train acc = 0.9700000286102295, time = 0.375150203704834\n",
      "Training at step=47, batch=480, train loss = 0.08832062035799026, train acc = 0.9800000190734863, time = 0.3702855110168457\n",
      "Testing at step=47, batch=0, test loss = 0.1474049687385559, test acc = 0.9599999785423279, time = 0.13331270217895508\n",
      "Testing at step=47, batch=20, test loss = 0.08437629789113998, test acc = 0.9599999785423279, time = 0.13307523727416992\n",
      "Testing at step=47, batch=40, test loss = 0.045920610427856445, test acc = 0.9800000190734863, time = 0.13257980346679688\n",
      "Testing at step=47, batch=60, test loss = 0.14839661121368408, test acc = 0.9599999785423279, time = 0.13335371017456055\n",
      "Testing at step=47, batch=80, test loss = 0.2871214747428894, test acc = 0.9300000071525574, time = 0.1327967643737793\n",
      "Step 47 finished in 255.36719369888306, Train loss = 0.10885394517642756, Test loss = 0.12790140219032764; Train Acc = 0.9670000064373017, Test Acc = 0.9603000050783157\n",
      "Training at step=48, batch=0, train loss = 0.08971387147903442, train acc = 0.9599999785423279, time = 0.37222838401794434\n",
      "Training at step=48, batch=120, train loss = 0.07235970348119736, train acc = 0.9800000190734863, time = 0.37093091011047363\n",
      "Training at step=48, batch=240, train loss = 0.07802486419677734, train acc = 0.9700000286102295, time = 0.3721458911895752\n",
      "Training at step=48, batch=360, train loss = 0.12078670412302017, train acc = 0.949999988079071, time = 0.37178707122802734\n",
      "Training at step=48, batch=480, train loss = 0.09389030188322067, train acc = 0.9900000095367432, time = 0.37471795082092285\n",
      "Testing at step=48, batch=0, test loss = 0.18329960107803345, test acc = 0.9599999785423279, time = 0.13196396827697754\n",
      "Testing at step=48, batch=20, test loss = 0.19128213822841644, test acc = 0.9599999785423279, time = 0.13314294815063477\n",
      "Testing at step=48, batch=40, test loss = 0.11198561638593674, test acc = 0.9700000286102295, time = 0.13146567344665527\n",
      "Testing at step=48, batch=60, test loss = 0.18087807297706604, test acc = 0.9300000071525574, time = 0.13153886795043945\n",
      "Testing at step=48, batch=80, test loss = 0.07845087349414825, test acc = 0.9700000286102295, time = 0.13296937942504883\n",
      "Step 48 finished in 255.00353813171387, Train loss = 0.10784542713003854, Test loss = 0.12880750913172961; Train Acc = 0.9675666734576225, Test Acc = 0.9594000035524368\n",
      "Training at step=49, batch=0, train loss = 0.08231569081544876, train acc = 0.9800000190734863, time = 0.3728151321411133\n",
      "Training at step=49, batch=120, train loss = 0.11167678236961365, train acc = 0.9399999976158142, time = 0.37087345123291016\n",
      "Training at step=49, batch=240, train loss = 0.06588197499513626, train acc = 0.9800000190734863, time = 0.3766052722930908\n",
      "Training at step=49, batch=360, train loss = 0.06818053126335144, train acc = 0.9599999785423279, time = 0.37165164947509766\n",
      "Training at step=49, batch=480, train loss = 0.09766540676355362, train acc = 0.9599999785423279, time = 0.3721463680267334\n",
      "Testing at step=49, batch=0, test loss = 0.08173226565122604, test acc = 0.9700000286102295, time = 0.13559770584106445\n",
      "Testing at step=49, batch=20, test loss = 0.10809274762868881, test acc = 0.9599999785423279, time = 0.13568878173828125\n",
      "Testing at step=49, batch=40, test loss = 0.07415826618671417, test acc = 0.9800000190734863, time = 0.13319635391235352\n",
      "Testing at step=49, batch=60, test loss = 0.17025171220302582, test acc = 0.949999988079071, time = 0.13492941856384277\n",
      "Testing at step=49, batch=80, test loss = 0.05450630187988281, test acc = 0.9800000190734863, time = 0.13291215896606445\n",
      "Step 49 finished in 256.8473529815674, Train loss = 0.10653957061314334, Test loss = 0.12216854069381952; Train Acc = 0.9681666726867358, Test Acc = 0.9640000063180924\n",
      "Training at step=50, batch=0, train loss = 0.09824815392494202, train acc = 0.9700000286102295, time = 0.3795645236968994\n",
      "Training at step=50, batch=120, train loss = 0.03791041299700737, train acc = 1.0, time = 0.37437939643859863\n",
      "Training at step=50, batch=240, train loss = 0.14209064841270447, train acc = 0.9599999785423279, time = 0.37510251998901367\n",
      "Training at step=50, batch=360, train loss = 0.06852523982524872, train acc = 0.9700000286102295, time = 0.37401413917541504\n",
      "Training at step=50, batch=480, train loss = 0.19800293445587158, train acc = 0.9399999976158142, time = 0.37428855895996094\n",
      "Testing at step=50, batch=0, test loss = 0.15951047837734222, test acc = 0.9599999785423279, time = 0.13316702842712402\n",
      "Testing at step=50, batch=20, test loss = 0.15144461393356323, test acc = 0.949999988079071, time = 0.13319015502929688\n",
      "Testing at step=50, batch=40, test loss = 0.3269112706184387, test acc = 0.8999999761581421, time = 0.1322770118713379\n",
      "Testing at step=50, batch=60, test loss = 0.22996635735034943, test acc = 0.9399999976158142, time = 0.1329360008239746\n",
      "Testing at step=50, batch=80, test loss = 0.10919805616140366, test acc = 0.9700000286102295, time = 0.13232779502868652\n",
      "Step 50 finished in 256.4806926250458, Train loss = 0.10563169562878708, Test loss = 0.12861306127160788; Train Acc = 0.9678333391745885, Test Acc = 0.9603000020980835\n",
      "Training at step=51, batch=0, train loss = 0.13734903931617737, train acc = 0.9599999785423279, time = 0.3760204315185547\n",
      "Training at step=51, batch=120, train loss = 0.14466296136379242, train acc = 0.9700000286102295, time = 0.37679195404052734\n",
      "Training at step=51, batch=240, train loss = 0.2014726996421814, train acc = 0.9599999785423279, time = 0.37732768058776855\n",
      "Training at step=51, batch=360, train loss = 0.0490342378616333, train acc = 0.9800000190734863, time = 0.3739280700683594\n",
      "Training at step=51, batch=480, train loss = 0.03956656903028488, train acc = 1.0, time = 0.3733100891113281\n",
      "Testing at step=51, batch=0, test loss = 0.18582460284233093, test acc = 0.9599999785423279, time = 0.1320033073425293\n",
      "Testing at step=51, batch=20, test loss = 0.13655194640159607, test acc = 0.9599999785423279, time = 0.13438868522644043\n",
      "Testing at step=51, batch=40, test loss = 0.15060806274414062, test acc = 0.9800000190734863, time = 0.1334531307220459\n",
      "Testing at step=51, batch=60, test loss = 0.20225979387760162, test acc = 0.9300000071525574, time = 0.13399147987365723\n",
      "Testing at step=51, batch=80, test loss = 0.11920423805713654, test acc = 0.9800000190734863, time = 0.13321661949157715\n",
      "Step 51 finished in 256.0596709251404, Train loss = 0.10458852947068711, Test loss = 0.11690119275823235; Train Acc = 0.9678666722774506, Test Acc = 0.965000006556511\n",
      "Training at step=52, batch=0, train loss = 0.1629551500082016, train acc = 0.9599999785423279, time = 0.37312769889831543\n",
      "Training at step=52, batch=120, train loss = 0.1314552277326584, train acc = 0.9599999785423279, time = 0.37342071533203125\n",
      "Training at step=52, batch=240, train loss = 0.11582077294588089, train acc = 0.9700000286102295, time = 0.3805873394012451\n",
      "Training at step=52, batch=360, train loss = 0.0963478535413742, train acc = 0.9599999785423279, time = 0.3723278045654297\n",
      "Training at step=52, batch=480, train loss = 0.07205252349376678, train acc = 0.9800000190734863, time = 0.37545275688171387\n",
      "Testing at step=52, batch=0, test loss = 0.047251395881175995, test acc = 0.9900000095367432, time = 0.13239264488220215\n",
      "Testing at step=52, batch=20, test loss = 0.27399590611457825, test acc = 0.949999988079071, time = 0.13343358039855957\n",
      "Testing at step=52, batch=40, test loss = 0.050265148282051086, test acc = 0.9900000095367432, time = 0.13297533988952637\n",
      "Testing at step=52, batch=60, test loss = 0.06993874907493591, test acc = 0.9800000190734863, time = 0.13485383987426758\n",
      "Testing at step=52, batch=80, test loss = 0.18892136216163635, test acc = 0.949999988079071, time = 0.13197875022888184\n",
      "Step 52 finished in 255.9715871810913, Train loss = 0.10236228728356461, Test loss = 0.11295489272102714; Train Acc = 0.9690500074625015, Test Acc = 0.9653000044822693\n",
      "Training at step=53, batch=0, train loss = 0.04744098708033562, train acc = 0.9800000190734863, time = 0.37601184844970703\n",
      "Training at step=53, batch=120, train loss = 0.10587315261363983, train acc = 0.9599999785423279, time = 0.37337732315063477\n",
      "Training at step=53, batch=240, train loss = 0.13833104074001312, train acc = 0.9599999785423279, time = 0.3746981620788574\n",
      "Training at step=53, batch=360, train loss = 0.04845920950174332, train acc = 0.9900000095367432, time = 0.37346315383911133\n",
      "Training at step=53, batch=480, train loss = 0.11807918548583984, train acc = 0.9599999785423279, time = 0.3728156089782715\n",
      "Testing at step=53, batch=0, test loss = 0.18750393390655518, test acc = 0.9599999785423279, time = 0.13262438774108887\n",
      "Testing at step=53, batch=20, test loss = 0.09259608387947083, test acc = 0.9700000286102295, time = 0.132232666015625\n",
      "Testing at step=53, batch=40, test loss = 0.12444540113210678, test acc = 0.9800000190734863, time = 0.1447901725769043\n",
      "Testing at step=53, batch=60, test loss = 0.09648057818412781, test acc = 0.9700000286102295, time = 0.13373398780822754\n",
      "Testing at step=53, batch=80, test loss = 0.13720925152301788, test acc = 0.9599999785423279, time = 0.13328790664672852\n",
      "Step 53 finished in 255.42182731628418, Train loss = 0.10143014057228962, Test loss = 0.11883109321817756; Train Acc = 0.9693333398302396, Test Acc = 0.9641000068187714\n",
      "Training at step=54, batch=0, train loss = 0.0975382924079895, train acc = 0.9800000190734863, time = 0.3778660297393799\n",
      "Training at step=54, batch=120, train loss = 0.04950951412320137, train acc = 1.0, time = 0.37290263175964355\n",
      "Training at step=54, batch=240, train loss = 0.12283390760421753, train acc = 0.9599999785423279, time = 0.37427473068237305\n",
      "Training at step=54, batch=360, train loss = 0.2189236432313919, train acc = 0.9399999976158142, time = 0.37308454513549805\n",
      "Training at step=54, batch=480, train loss = 0.14816197752952576, train acc = 0.9599999785423279, time = 0.3774595260620117\n",
      "Testing at step=54, batch=0, test loss = 0.13324473798274994, test acc = 0.9599999785423279, time = 0.13228368759155273\n",
      "Testing at step=54, batch=20, test loss = 0.16018036007881165, test acc = 0.9100000262260437, time = 0.13235020637512207\n",
      "Testing at step=54, batch=40, test loss = 0.15629100799560547, test acc = 0.949999988079071, time = 0.13211297988891602\n",
      "Testing at step=54, batch=60, test loss = 0.1592235267162323, test acc = 0.9300000071525574, time = 0.13545465469360352\n",
      "Testing at step=54, batch=80, test loss = 0.2124461680650711, test acc = 0.9599999785423279, time = 0.13321161270141602\n",
      "Step 54 finished in 255.8700532913208, Train loss = 0.10058895451948047, Test loss = 0.138283972106874; Train Acc = 0.9691500076651574, Test Acc = 0.9571000039577484\n",
      "Training at step=55, batch=0, train loss = 0.2560507655143738, train acc = 0.9700000286102295, time = 0.37581324577331543\n",
      "Training at step=55, batch=120, train loss = 0.0778098851442337, train acc = 0.9800000190734863, time = 0.3746042251586914\n",
      "Training at step=55, batch=240, train loss = 0.0915890708565712, train acc = 0.949999988079071, time = 0.37538886070251465\n",
      "Training at step=55, batch=360, train loss = 0.10521844029426575, train acc = 0.9599999785423279, time = 0.3763918876647949\n",
      "Training at step=55, batch=480, train loss = 0.06583601236343384, train acc = 0.9800000190734863, time = 0.37713146209716797\n",
      "Testing at step=55, batch=0, test loss = 0.12115225940942764, test acc = 0.9599999785423279, time = 0.13383793830871582\n",
      "Testing at step=55, batch=20, test loss = 0.048998624086380005, test acc = 0.9900000095367432, time = 0.13364338874816895\n",
      "Testing at step=55, batch=40, test loss = 0.12805405259132385, test acc = 0.949999988079071, time = 0.1329188346862793\n",
      "Testing at step=55, batch=60, test loss = 0.14359532296657562, test acc = 0.9700000286102295, time = 0.13475966453552246\n",
      "Testing at step=55, batch=80, test loss = 0.1037946492433548, test acc = 0.9900000095367432, time = 0.13405585289001465\n",
      "Step 55 finished in 256.7352578639984, Train loss = 0.09953422040678561, Test loss = 0.11323653522878885; Train Acc = 0.9700333413481712, Test Acc = 0.9672000068426132\n",
      "Training at step=56, batch=0, train loss = 0.042687829583883286, train acc = 0.9900000095367432, time = 0.37796497344970703\n",
      "Training at step=56, batch=120, train loss = 0.12334510684013367, train acc = 0.949999988079071, time = 0.38135719299316406\n",
      "Training at step=56, batch=240, train loss = 0.09114590287208557, train acc = 0.9700000286102295, time = 0.3751809597015381\n",
      "Training at step=56, batch=360, train loss = 0.10031996667385101, train acc = 0.9800000190734863, time = 0.3752155303955078\n",
      "Training at step=56, batch=480, train loss = 0.0490783229470253, train acc = 0.9800000190734863, time = 0.3743281364440918\n",
      "Testing at step=56, batch=0, test loss = 0.023035338148474693, test acc = 0.9900000095367432, time = 0.13285231590270996\n",
      "Testing at step=56, batch=20, test loss = 0.07944244146347046, test acc = 0.9700000286102295, time = 0.13376450538635254\n",
      "Testing at step=56, batch=40, test loss = 0.16235747933387756, test acc = 0.9399999976158142, time = 0.13730239868164062\n",
      "Testing at step=56, batch=60, test loss = 0.05744470655918121, test acc = 0.9800000190734863, time = 0.13327312469482422\n",
      "Testing at step=56, batch=80, test loss = 0.17612837255001068, test acc = 0.949999988079071, time = 0.13336896896362305\n",
      "Step 56 finished in 257.0385925769806, Train loss = 0.09753601871896535, Test loss = 0.11102085154503584; Train Acc = 0.9701000077525774, Test Acc = 0.9660000073909759\n",
      "Training at step=57, batch=0, train loss = 0.20921654999256134, train acc = 0.9399999976158142, time = 0.37744760513305664\n",
      "Training at step=57, batch=120, train loss = 0.1538100689649582, train acc = 0.9700000286102295, time = 0.37554478645324707\n",
      "Training at step=57, batch=240, train loss = 0.09430108219385147, train acc = 0.9599999785423279, time = 0.37605810165405273\n",
      "Training at step=57, batch=360, train loss = 0.15064620971679688, train acc = 0.9200000166893005, time = 0.375272274017334\n",
      "Training at step=57, batch=480, train loss = 0.046506501734256744, train acc = 0.9900000095367432, time = 0.3738515377044678\n",
      "Testing at step=57, batch=0, test loss = 0.08696522563695908, test acc = 0.949999988079071, time = 0.13278675079345703\n",
      "Testing at step=57, batch=20, test loss = 0.13571053743362427, test acc = 0.9599999785423279, time = 0.13331842422485352\n",
      "Testing at step=57, batch=40, test loss = 0.13595113158226013, test acc = 0.9700000286102295, time = 0.1343541145324707\n",
      "Testing at step=57, batch=60, test loss = 0.03987887501716614, test acc = 0.9800000190734863, time = 0.13455510139465332\n",
      "Testing at step=57, batch=80, test loss = 0.3608587682247162, test acc = 0.9100000262260437, time = 0.13349270820617676\n",
      "Step 57 finished in 256.8766345977783, Train loss = 0.0975318817111353, Test loss = 0.12303019005805255; Train Acc = 0.970000006655852, Test Acc = 0.9632000064849854\n",
      "Training at step=58, batch=0, train loss = 0.10018467903137207, train acc = 0.9700000286102295, time = 0.3798048496246338\n",
      "Training at step=58, batch=120, train loss = 0.045716144144535065, train acc = 0.9900000095367432, time = 0.3751034736633301\n",
      "Training at step=58, batch=240, train loss = 0.11451166868209839, train acc = 0.9599999785423279, time = 0.3755068778991699\n",
      "Training at step=58, batch=360, train loss = 0.08547646552324295, train acc = 0.9599999785423279, time = 0.3771977424621582\n",
      "Training at step=58, batch=480, train loss = 0.20770996809005737, train acc = 0.9700000286102295, time = 0.37547779083251953\n",
      "Testing at step=58, batch=0, test loss = 0.07368261367082596, test acc = 0.9599999785423279, time = 0.1360018253326416\n",
      "Testing at step=58, batch=20, test loss = 0.09849029779434204, test acc = 0.9800000190734863, time = 0.13553094863891602\n",
      "Testing at step=58, batch=40, test loss = 0.07017423212528229, test acc = 0.9599999785423279, time = 0.1337299346923828\n",
      "Testing at step=58, batch=60, test loss = 0.11822856962680817, test acc = 0.9399999976158142, time = 0.1343245506286621\n",
      "Testing at step=58, batch=80, test loss = 0.07872211933135986, test acc = 0.9800000190734863, time = 0.1336679458618164\n",
      "Step 58 finished in 257.00768637657166, Train loss = 0.09621559232783815, Test loss = 0.11751788806170226; Train Acc = 0.9710333408912023, Test Acc = 0.962900003194809\n",
      "Training at step=59, batch=0, train loss = 0.07843951880931854, train acc = 0.9700000286102295, time = 0.38193798065185547\n",
      "Training at step=59, batch=120, train loss = 0.06851407885551453, train acc = 0.9700000286102295, time = 0.3771507740020752\n",
      "Training at step=59, batch=240, train loss = 0.06318927556276321, train acc = 0.9800000190734863, time = 0.37436652183532715\n",
      "Training at step=59, batch=360, train loss = 0.14852005243301392, train acc = 0.9700000286102295, time = 0.37676095962524414\n",
      "Training at step=59, batch=480, train loss = 0.07241728901863098, train acc = 0.9599999785423279, time = 0.37608957290649414\n",
      "Testing at step=59, batch=0, test loss = 0.06318845599889755, test acc = 0.9900000095367432, time = 0.1334395408630371\n",
      "Testing at step=59, batch=20, test loss = 0.10048031806945801, test acc = 0.9599999785423279, time = 0.1336209774017334\n",
      "Testing at step=59, batch=40, test loss = 0.1484839916229248, test acc = 0.949999988079071, time = 0.13390660285949707\n",
      "Testing at step=59, batch=60, test loss = 0.03639274463057518, test acc = 0.9900000095367432, time = 0.13323736190795898\n",
      "Testing at step=59, batch=80, test loss = 0.13020466268062592, test acc = 0.9700000286102295, time = 0.133162260055542\n",
      "Step 59 finished in 256.6565821170807, Train loss = 0.0958050342835486, Test loss = 0.11181809919886292; Train Acc = 0.9708500083287557, Test Acc = 0.9672000086307526\n",
      "Training at step=60, batch=0, train loss = 0.16236545145511627, train acc = 0.949999988079071, time = 0.37610650062561035\n",
      "Training at step=60, batch=120, train loss = 0.03855773061513901, train acc = 1.0, time = 0.3751819133758545\n",
      "Training at step=60, batch=240, train loss = 0.0995400995016098, train acc = 0.949999988079071, time = 0.3744659423828125\n",
      "Training at step=60, batch=360, train loss = 0.10424573719501495, train acc = 0.9800000190734863, time = 0.3767974376678467\n",
      "Training at step=60, batch=480, train loss = 0.07425271719694138, train acc = 0.9800000190734863, time = 0.38010501861572266\n",
      "Testing at step=60, batch=0, test loss = 0.08678063750267029, test acc = 0.9700000286102295, time = 0.13642430305480957\n",
      "Testing at step=60, batch=20, test loss = 0.09530923515558243, test acc = 0.9800000190734863, time = 0.13539695739746094\n",
      "Testing at step=60, batch=40, test loss = 0.11182907223701477, test acc = 0.9800000190734863, time = 0.13380193710327148\n",
      "Testing at step=60, batch=60, test loss = 0.09593530744314194, test acc = 0.9700000286102295, time = 0.13440346717834473\n",
      "Testing at step=60, batch=80, test loss = 0.22149644792079926, test acc = 0.949999988079071, time = 0.1345820426940918\n",
      "Step 60 finished in 257.27062606811523, Train loss = 0.09407884736390164, Test loss = 0.11124091807752848; Train Acc = 0.9708833418289821, Test Acc = 0.9666000080108642\n",
      "Training at step=61, batch=0, train loss = 0.08577582985162735, train acc = 0.9800000190734863, time = 0.3798232078552246\n",
      "Training at step=61, batch=120, train loss = 0.09805631637573242, train acc = 0.9599999785423279, time = 0.3785128593444824\n",
      "Training at step=61, batch=240, train loss = 0.04644034057855606, train acc = 0.9900000095367432, time = 0.37854719161987305\n",
      "Training at step=61, batch=360, train loss = 0.044920627027750015, train acc = 1.0, time = 0.3782541751861572\n",
      "Training at step=61, batch=480, train loss = 0.03778327628970146, train acc = 0.9900000095367432, time = 0.38048434257507324\n",
      "Testing at step=61, batch=0, test loss = 0.15094530582427979, test acc = 0.9300000071525574, time = 0.13547205924987793\n",
      "Testing at step=61, batch=20, test loss = 0.2621515691280365, test acc = 0.9300000071525574, time = 0.13344192504882812\n",
      "Testing at step=61, batch=40, test loss = 0.07716843485832214, test acc = 0.9900000095367432, time = 0.13469552993774414\n",
      "Testing at step=61, batch=60, test loss = 0.0766998827457428, test acc = 0.9700000286102295, time = 0.13458561897277832\n",
      "Testing at step=61, batch=80, test loss = 0.18733881413936615, test acc = 0.949999988079071, time = 0.13401269912719727\n",
      "Step 61 finished in 258.35073137283325, Train loss = 0.09217759838017324, Test loss = 0.11926077151671052; Train Acc = 0.971650008559227, Test Acc = 0.9625000059604645\n",
      "Training at step=62, batch=0, train loss = 0.15474046766757965, train acc = 0.9399999976158142, time = 0.3784952163696289\n",
      "Training at step=62, batch=120, train loss = 0.11302856355905533, train acc = 0.949999988079071, time = 0.37841272354125977\n",
      "Training at step=62, batch=240, train loss = 0.20763926208019257, train acc = 0.9399999976158142, time = 0.376131534576416\n",
      "Training at step=62, batch=360, train loss = 0.10148332267999649, train acc = 0.9599999785423279, time = 0.3755321502685547\n",
      "Training at step=62, batch=480, train loss = 0.14347079396247864, train acc = 0.949999988079071, time = 0.37804365158081055\n",
      "Testing at step=62, batch=0, test loss = 0.02682691253721714, test acc = 1.0, time = 0.1349964141845703\n",
      "Testing at step=62, batch=20, test loss = 0.02123689092695713, test acc = 1.0, time = 0.13383007049560547\n",
      "Testing at step=62, batch=40, test loss = 0.04341960325837135, test acc = 0.9900000095367432, time = 0.13329744338989258\n",
      "Testing at step=62, batch=60, test loss = 0.0866432785987854, test acc = 0.9800000190734863, time = 0.13415169715881348\n",
      "Testing at step=62, batch=80, test loss = 0.08650367707014084, test acc = 0.9700000286102295, time = 0.13506674766540527\n",
      "Step 62 finished in 258.13220143318176, Train loss = 0.0919187564138944, Test loss = 0.10890779735520482; Train Acc = 0.9723500080903371, Test Acc = 0.9657000070810318\n",
      "Training at step=63, batch=0, train loss = 0.12382963299751282, train acc = 0.9599999785423279, time = 0.3788759708404541\n",
      "Training at step=63, batch=120, train loss = 0.02807863987982273, train acc = 0.9900000095367432, time = 0.3773345947265625\n",
      "Training at step=63, batch=240, train loss = 0.016456319019198418, train acc = 1.0, time = 0.3835580348968506\n",
      "Training at step=63, batch=360, train loss = 0.09939923137426376, train acc = 0.9800000190734863, time = 0.3770151138305664\n",
      "Training at step=63, batch=480, train loss = 0.0929485484957695, train acc = 0.9800000190734863, time = 0.3780708312988281\n",
      "Testing at step=63, batch=0, test loss = 0.08155031502246857, test acc = 0.9900000095367432, time = 0.1335744857788086\n",
      "Testing at step=63, batch=20, test loss = 0.06373422592878342, test acc = 0.9800000190734863, time = 0.13503074645996094\n",
      "Testing at step=63, batch=40, test loss = 0.1707027554512024, test acc = 0.949999988079071, time = 0.13376522064208984\n",
      "Testing at step=63, batch=60, test loss = 0.13802987337112427, test acc = 0.9599999785423279, time = 0.13546395301818848\n",
      "Testing at step=63, batch=80, test loss = 0.26491889357566833, test acc = 0.9300000071525574, time = 0.13526225090026855\n",
      "Step 63 finished in 258.25222992897034, Train loss = 0.08974625383658956, Test loss = 0.11258829981088639; Train Acc = 0.9727666754523913, Test Acc = 0.9645000034570694\n",
      "Training at step=64, batch=0, train loss = 0.040483683347702026, train acc = 0.9700000286102295, time = 0.3787360191345215\n",
      "Training at step=64, batch=120, train loss = 0.14903755486011505, train acc = 0.9800000190734863, time = 0.37607240676879883\n",
      "Training at step=64, batch=240, train loss = 0.06416846811771393, train acc = 0.9700000286102295, time = 0.3758735656738281\n",
      "Training at step=64, batch=360, train loss = 0.07376682758331299, train acc = 0.9800000190734863, time = 0.3777158260345459\n",
      "Training at step=64, batch=480, train loss = 0.03559671714901924, train acc = 0.9900000095367432, time = 0.3782382011413574\n",
      "Testing at step=64, batch=0, test loss = 0.10120970010757446, test acc = 0.9800000190734863, time = 0.13425445556640625\n",
      "Testing at step=64, batch=20, test loss = 0.07920567691326141, test acc = 0.9700000286102295, time = 0.13420438766479492\n",
      "Testing at step=64, batch=40, test loss = 0.12305714935064316, test acc = 0.949999988079071, time = 0.13493037223815918\n",
      "Testing at step=64, batch=60, test loss = 0.1319342702627182, test acc = 0.9700000286102295, time = 0.1330857276916504\n",
      "Testing at step=64, batch=80, test loss = 0.13597136735916138, test acc = 0.949999988079071, time = 0.13469910621643066\n",
      "Step 64 finished in 258.3120219707489, Train loss = 0.09016170958522707, Test loss = 0.11441378708928823; Train Acc = 0.9728833426038425, Test Acc = 0.9641000026464462\n",
      "Training at step=65, batch=0, train loss = 0.06873910874128342, train acc = 0.9700000286102295, time = 0.3774442672729492\n",
      "Training at step=65, batch=120, train loss = 0.08212434500455856, train acc = 0.9700000286102295, time = 0.37564921379089355\n",
      "Training at step=65, batch=240, train loss = 0.038709405809640884, train acc = 0.9800000190734863, time = 0.37761735916137695\n",
      "Training at step=65, batch=360, train loss = 0.05400567129254341, train acc = 0.9900000095367432, time = 0.3764071464538574\n",
      "Training at step=65, batch=480, train loss = 0.08733581751585007, train acc = 0.9599999785423279, time = 0.3757054805755615\n",
      "Testing at step=65, batch=0, test loss = 0.1554587036371231, test acc = 0.9300000071525574, time = 0.1350078582763672\n",
      "Testing at step=65, batch=20, test loss = 0.09172365069389343, test acc = 0.9800000190734863, time = 0.13463973999023438\n",
      "Testing at step=65, batch=40, test loss = 0.11670513451099396, test acc = 0.9700000286102295, time = 0.13380169868469238\n",
      "Testing at step=65, batch=60, test loss = 0.08202429115772247, test acc = 0.9800000190734863, time = 0.13634514808654785\n",
      "Testing at step=65, batch=80, test loss = 0.16529659926891327, test acc = 0.949999988079071, time = 0.13459467887878418\n",
      "Step 65 finished in 258.0440034866333, Train loss = 0.088074294407852, Test loss = 0.10756656043231487; Train Acc = 0.9734833413362503, Test Acc = 0.9670000046491622\n",
      "Training at step=66, batch=0, train loss = 0.10161981731653214, train acc = 0.9700000286102295, time = 0.3762397766113281\n",
      "Training at step=66, batch=120, train loss = 0.13706213235855103, train acc = 0.9599999785423279, time = 0.378887414932251\n",
      "Training at step=66, batch=240, train loss = 0.04920739307999611, train acc = 0.9800000190734863, time = 0.3788187503814697\n",
      "Training at step=66, batch=360, train loss = 0.06487007439136505, train acc = 0.9800000190734863, time = 0.37737441062927246\n",
      "Training at step=66, batch=480, train loss = 0.0751313790678978, train acc = 0.9800000190734863, time = 0.3760817050933838\n",
      "Testing at step=66, batch=0, test loss = 0.028874672949314117, test acc = 0.9900000095367432, time = 0.1356830596923828\n",
      "Testing at step=66, batch=20, test loss = 0.05509393289685249, test acc = 0.9800000190734863, time = 0.134904146194458\n",
      "Testing at step=66, batch=40, test loss = 0.20948339998722076, test acc = 0.949999988079071, time = 0.1340334415435791\n",
      "Testing at step=66, batch=60, test loss = 0.057004623115062714, test acc = 0.9800000190734863, time = 0.13489222526550293\n",
      "Testing at step=66, batch=80, test loss = 0.25522834062576294, test acc = 0.9399999976158142, time = 0.1330564022064209\n",
      "Step 66 finished in 257.84763169288635, Train loss = 0.0877603353653103, Test loss = 0.11095039607957005; Train Acc = 0.9730500104029973, Test Acc = 0.9659000062942504\n",
      "Training at step=67, batch=0, train loss = 0.04868786036968231, train acc = 0.9700000286102295, time = 0.38018369674682617\n",
      "Training at step=67, batch=120, train loss = 0.10882958769798279, train acc = 0.9599999785423279, time = 0.3775060176849365\n",
      "Training at step=67, batch=240, train loss = 0.12566371262073517, train acc = 0.9700000286102295, time = 0.37816596031188965\n",
      "Training at step=67, batch=360, train loss = 0.15741851925849915, train acc = 0.949999988079071, time = 0.3790619373321533\n",
      "Training at step=67, batch=480, train loss = 0.040476080030202866, train acc = 0.9900000095367432, time = 0.37455129623413086\n",
      "Testing at step=67, batch=0, test loss = 0.10474438965320587, test acc = 0.9700000286102295, time = 0.1343066692352295\n",
      "Testing at step=67, batch=20, test loss = 0.16514739394187927, test acc = 0.9399999976158142, time = 0.1343390941619873\n",
      "Testing at step=67, batch=40, test loss = 0.04154301807284355, test acc = 0.9900000095367432, time = 0.134185791015625\n",
      "Testing at step=67, batch=60, test loss = 0.036275580525398254, test acc = 0.9900000095367432, time = 0.13542413711547852\n",
      "Testing at step=67, batch=80, test loss = 0.07846616208553314, test acc = 0.9599999785423279, time = 0.1337723731994629\n",
      "Step 67 finished in 257.65044045448303, Train loss = 0.08711296561794976, Test loss = 0.11598976630717515; Train Acc = 0.9737833425402641, Test Acc = 0.9654000049829483\n",
      "Training at step=68, batch=0, train loss = 0.05846884101629257, train acc = 0.9900000095367432, time = 0.3775155544281006\n",
      "Training at step=68, batch=120, train loss = 0.1587619036436081, train acc = 0.949999988079071, time = 0.3825986385345459\n",
      "Training at step=68, batch=240, train loss = 0.08344563841819763, train acc = 0.9599999785423279, time = 0.3783257007598877\n",
      "Training at step=68, batch=360, train loss = 0.04212600737810135, train acc = 0.9900000095367432, time = 0.3764026165008545\n",
      "Training at step=68, batch=480, train loss = 0.2545734643936157, train acc = 0.9200000166893005, time = 0.378650426864624\n",
      "Testing at step=68, batch=0, test loss = 0.04053841158747673, test acc = 1.0, time = 0.1366724967956543\n",
      "Testing at step=68, batch=20, test loss = 0.14588306844234467, test acc = 0.949999988079071, time = 0.13373565673828125\n",
      "Testing at step=68, batch=40, test loss = 0.14056459069252014, test acc = 0.949999988079071, time = 0.13421249389648438\n",
      "Testing at step=68, batch=60, test loss = 0.11646230518817902, test acc = 0.9599999785423279, time = 0.13408851623535156\n",
      "Testing at step=68, batch=80, test loss = 0.0691450908780098, test acc = 0.9700000286102295, time = 0.133162260055542\n",
      "Step 68 finished in 257.75161933898926, Train loss = 0.08660996051660429, Test loss = 0.11608540849760175; Train Acc = 0.9737666748960813, Test Acc = 0.964200005531311\n",
      "Training at step=69, batch=0, train loss = 0.07515478134155273, train acc = 0.9900000095367432, time = 0.37687015533447266\n",
      "Training at step=69, batch=120, train loss = 0.05744769051671028, train acc = 0.9800000190734863, time = 0.3764915466308594\n",
      "Training at step=69, batch=240, train loss = 0.13154307007789612, train acc = 0.9300000071525574, time = 0.37615299224853516\n",
      "Training at step=69, batch=360, train loss = 0.03864823654294014, train acc = 0.9900000095367432, time = 0.3768432140350342\n",
      "Training at step=69, batch=480, train loss = 0.12021190673112869, train acc = 0.9700000286102295, time = 0.3780992031097412\n",
      "Testing at step=69, batch=0, test loss = 0.08519762009382248, test acc = 0.9599999785423279, time = 0.1342942714691162\n",
      "Testing at step=69, batch=20, test loss = 0.10673797875642776, test acc = 0.9800000190734863, time = 0.14386868476867676\n",
      "Testing at step=69, batch=40, test loss = 0.08953704684972763, test acc = 0.9599999785423279, time = 0.13630986213684082\n",
      "Testing at step=69, batch=60, test loss = 0.13703037798404694, test acc = 0.9700000286102295, time = 0.13368010520935059\n",
      "Testing at step=69, batch=80, test loss = 0.10634291917085648, test acc = 0.9599999785423279, time = 0.13539409637451172\n",
      "Step 69 finished in 257.95254588127136, Train loss = 0.08628936065671344, Test loss = 0.10945125408470631; Train Acc = 0.9737833418448766, Test Acc = 0.9674000072479249\n",
      "Training at step=70, batch=0, train loss = 0.03848046436905861, train acc = 0.9900000095367432, time = 0.38133978843688965\n",
      "Training at step=70, batch=120, train loss = 0.17879781126976013, train acc = 0.9700000286102295, time = 0.375232458114624\n",
      "Training at step=70, batch=240, train loss = 0.0552293099462986, train acc = 0.9700000286102295, time = 0.37653350830078125\n",
      "Training at step=70, batch=360, train loss = 0.05834294483065605, train acc = 0.9800000190734863, time = 0.37891674041748047\n",
      "Training at step=70, batch=480, train loss = 0.06828097999095917, train acc = 0.9700000286102295, time = 0.3766186237335205\n",
      "Testing at step=70, batch=0, test loss = 0.1839868426322937, test acc = 0.9599999785423279, time = 0.14090323448181152\n",
      "Testing at step=70, batch=20, test loss = 0.09033549576997757, test acc = 0.9599999785423279, time = 0.1354050636291504\n",
      "Testing at step=70, batch=40, test loss = 0.03583137318491936, test acc = 1.0, time = 0.13472700119018555\n",
      "Testing at step=70, batch=60, test loss = 0.08863646537065506, test acc = 0.9599999785423279, time = 0.13403034210205078\n",
      "Testing at step=70, batch=80, test loss = 0.09384625405073166, test acc = 0.9700000286102295, time = 0.1347362995147705\n",
      "Step 70 finished in 258.2150683403015, Train loss = 0.08334301800622294, Test loss = 0.11001002460718155; Train Acc = 0.9756666756669681, Test Acc = 0.9669000047445298\n",
      "Training at step=71, batch=0, train loss = 0.035943448543548584, train acc = 0.9900000095367432, time = 0.37707018852233887\n",
      "Training at step=71, batch=120, train loss = 0.1273321658372879, train acc = 0.9599999785423279, time = 0.37702512741088867\n",
      "Training at step=71, batch=240, train loss = 0.10155634582042694, train acc = 0.9800000190734863, time = 0.3762650489807129\n",
      "Training at step=71, batch=360, train loss = 0.039940088987350464, train acc = 0.9800000190734863, time = 0.37581968307495117\n",
      "Training at step=71, batch=480, train loss = 0.04950935021042824, train acc = 0.9800000190734863, time = 0.37772274017333984\n",
      "Testing at step=71, batch=0, test loss = 0.1466779261827469, test acc = 0.949999988079071, time = 0.1340961456298828\n",
      "Testing at step=71, batch=20, test loss = 0.06716027110815048, test acc = 0.9800000190734863, time = 0.13449788093566895\n",
      "Testing at step=71, batch=40, test loss = 0.0715356320142746, test acc = 0.9700000286102295, time = 0.13376283645629883\n",
      "Testing at step=71, batch=60, test loss = 0.09432438015937805, test acc = 0.9599999785423279, time = 0.13387346267700195\n",
      "Testing at step=71, batch=80, test loss = 0.10269234329462051, test acc = 0.9700000286102295, time = 0.13313722610473633\n",
      "Step 71 finished in 257.74225902557373, Train loss = 0.08489460841131707, Test loss = 0.12291509442031384; Train Acc = 0.9739333427945773, Test Acc = 0.9615000057220459\n",
      "Training at step=72, batch=0, train loss = 0.09430568665266037, train acc = 0.9700000286102295, time = 0.3799436092376709\n",
      "Training at step=72, batch=120, train loss = 0.04368322342634201, train acc = 0.9900000095367432, time = 0.37747788429260254\n",
      "Training at step=72, batch=240, train loss = 0.0401923842728138, train acc = 0.9800000190734863, time = 0.3776552677154541\n",
      "Training at step=72, batch=360, train loss = 0.10632535070180893, train acc = 0.9599999785423279, time = 0.3740875720977783\n",
      "Training at step=72, batch=480, train loss = 0.038202036172151566, train acc = 0.9800000190734863, time = 0.3751058578491211\n",
      "Testing at step=72, batch=0, test loss = 0.16266252100467682, test acc = 0.9700000286102295, time = 0.1342318058013916\n",
      "Testing at step=72, batch=20, test loss = 0.060696981847286224, test acc = 0.9800000190734863, time = 0.13318562507629395\n",
      "Testing at step=72, batch=40, test loss = 0.1426219642162323, test acc = 0.949999988079071, time = 0.13329863548278809\n",
      "Testing at step=72, batch=60, test loss = 0.11465027183294296, test acc = 0.949999988079071, time = 0.13373303413391113\n",
      "Testing at step=72, batch=80, test loss = 0.07396559417247772, test acc = 0.9800000190734863, time = 0.13316845893859863\n",
      "Step 72 finished in 257.3113832473755, Train loss = 0.08318645401702573, Test loss = 0.10960075539536775; Train Acc = 0.9748666766285896, Test Acc = 0.9653000056743621\n",
      "Training at step=73, batch=0, train loss = 0.03048361837863922, train acc = 0.9900000095367432, time = 0.37787389755249023\n",
      "Training at step=73, batch=120, train loss = 0.06048723682761192, train acc = 0.9800000190734863, time = 0.3806912899017334\n",
      "Training at step=73, batch=240, train loss = 0.10033809393644333, train acc = 0.9599999785423279, time = 0.37729978561401367\n",
      "Training at step=73, batch=360, train loss = 0.17118529975414276, train acc = 0.9599999785423279, time = 0.37668919563293457\n",
      "Training at step=73, batch=480, train loss = 0.08721587061882019, train acc = 0.9800000190734863, time = 0.3747594356536865\n",
      "Testing at step=73, batch=0, test loss = 0.06126732379198074, test acc = 0.9800000190734863, time = 0.1334528923034668\n",
      "Testing at step=73, batch=20, test loss = 0.0700225755572319, test acc = 0.9900000095367432, time = 0.133742094039917\n",
      "Testing at step=73, batch=40, test loss = 0.06363920867443085, test acc = 0.9800000190734863, time = 0.13398170471191406\n",
      "Testing at step=73, batch=60, test loss = 0.09700098633766174, test acc = 0.9800000190734863, time = 0.133758544921875\n",
      "Testing at step=73, batch=80, test loss = 0.042380671948194504, test acc = 0.9900000095367432, time = 0.13328933715820312\n",
      "Step 73 finished in 256.9626262187958, Train loss = 0.08249885980660716, Test loss = 0.108770971596241; Train Acc = 0.9754500089089075, Test Acc = 0.966900006532669\n",
      "Training at step=74, batch=0, train loss = 0.02528131566941738, train acc = 0.9900000095367432, time = 0.37865471839904785\n",
      "Training at step=74, batch=120, train loss = 0.054952457547187805, train acc = 0.9700000286102295, time = 0.3758883476257324\n",
      "Training at step=74, batch=240, train loss = 0.05622308701276779, train acc = 0.9800000190734863, time = 0.3759610652923584\n",
      "Training at step=74, batch=360, train loss = 0.03983010724186897, train acc = 0.9900000095367432, time = 0.37413835525512695\n",
      "Training at step=74, batch=480, train loss = 0.1695091277360916, train acc = 0.949999988079071, time = 0.37804436683654785\n",
      "Testing at step=74, batch=0, test loss = 0.08026115596294403, test acc = 0.9599999785423279, time = 0.13344764709472656\n",
      "Testing at step=74, batch=20, test loss = 0.17994800209999084, test acc = 0.9599999785423279, time = 0.13314414024353027\n",
      "Testing at step=74, batch=40, test loss = 0.09389369934797287, test acc = 0.9700000286102295, time = 0.13495445251464844\n",
      "Testing at step=74, batch=60, test loss = 0.07336422055959702, test acc = 0.9700000286102295, time = 0.13466334342956543\n",
      "Testing at step=74, batch=80, test loss = 0.21678809821605682, test acc = 0.9200000166893005, time = 0.1330578327178955\n",
      "Step 74 finished in 256.72978138923645, Train loss = 0.0818469599991416, Test loss = 0.11947867387905717; Train Acc = 0.9750833421945572, Test Acc = 0.9624000054597854\n",
      "Training at step=75, batch=0, train loss = 0.0802435427904129, train acc = 0.9700000286102295, time = 0.3770773410797119\n",
      "Training at step=75, batch=120, train loss = 0.022426843643188477, train acc = 1.0, time = 0.37509918212890625\n",
      "Training at step=75, batch=240, train loss = 0.15040555596351624, train acc = 0.9700000286102295, time = 0.3798236846923828\n",
      "Training at step=75, batch=360, train loss = 0.0312933512032032, train acc = 0.9900000095367432, time = 0.3790121078491211\n",
      "Training at step=75, batch=480, train loss = 0.07490891218185425, train acc = 0.9700000286102295, time = 0.3756065368652344\n",
      "Testing at step=75, batch=0, test loss = 0.15513098239898682, test acc = 0.9700000286102295, time = 0.1343379020690918\n",
      "Testing at step=75, batch=20, test loss = 0.0852394849061966, test acc = 0.9700000286102295, time = 0.13382530212402344\n",
      "Testing at step=75, batch=40, test loss = 0.044936250895261765, test acc = 0.9800000190734863, time = 0.13460445404052734\n",
      "Testing at step=75, batch=60, test loss = 0.12067792564630508, test acc = 0.9800000190734863, time = 0.1335163116455078\n",
      "Testing at step=75, batch=80, test loss = 0.08382275700569153, test acc = 0.9599999785423279, time = 0.1335132122039795\n",
      "Step 75 finished in 257.10590744018555, Train loss = 0.08054382076874997, Test loss = 0.10000320311635733; Train Acc = 0.9754500097036362, Test Acc = 0.968600007891655\n",
      "Training at step=76, batch=0, train loss = 0.0960070863366127, train acc = 0.9599999785423279, time = 0.37947678565979004\n",
      "Training at step=76, batch=120, train loss = 0.08078887313604355, train acc = 0.9800000190734863, time = 0.3759946823120117\n",
      "Training at step=76, batch=240, train loss = 0.07134011387825012, train acc = 0.9700000286102295, time = 0.37526941299438477\n",
      "Training at step=76, batch=360, train loss = 0.07729874551296234, train acc = 0.9800000190734863, time = 0.37393689155578613\n",
      "Training at step=76, batch=480, train loss = 0.08398059010505676, train acc = 0.9800000190734863, time = 0.3737003803253174\n",
      "Testing at step=76, batch=0, test loss = 0.06639280915260315, test acc = 0.9700000286102295, time = 0.1368100643157959\n",
      "Testing at step=76, batch=20, test loss = 0.11777786910533905, test acc = 0.9700000286102295, time = 0.1376323699951172\n",
      "Testing at step=76, batch=40, test loss = 0.14673247933387756, test acc = 0.9599999785423279, time = 0.13640332221984863\n",
      "Testing at step=76, batch=60, test loss = 0.12749439477920532, test acc = 0.9800000190734863, time = 0.13618159294128418\n",
      "Testing at step=76, batch=80, test loss = 0.21212364733219147, test acc = 0.9300000071525574, time = 0.13467979431152344\n",
      "Step 76 finished in 257.75264525413513, Train loss = 0.08096118740271777, Test loss = 0.11370670599862934; Train Acc = 0.9752500094970067, Test Acc = 0.9650000071525574\n",
      "Training at step=77, batch=0, train loss = 0.04049430042505264, train acc = 0.9800000190734863, time = 0.3851490020751953\n",
      "Training at step=77, batch=120, train loss = 0.024058112874627113, train acc = 0.9900000095367432, time = 0.37716221809387207\n",
      "Training at step=77, batch=240, train loss = 0.07389471679925919, train acc = 0.9599999785423279, time = 0.3807845115661621\n",
      "Training at step=77, batch=360, train loss = 0.07969208061695099, train acc = 0.9700000286102295, time = 0.37629079818725586\n",
      "Training at step=77, batch=480, train loss = 0.021215172484517097, train acc = 0.9900000095367432, time = 0.37930917739868164\n",
      "Testing at step=77, batch=0, test loss = 0.15618717670440674, test acc = 0.949999988079071, time = 0.13576126098632812\n",
      "Testing at step=77, batch=20, test loss = 0.10948508232831955, test acc = 0.9599999785423279, time = 0.13383936882019043\n",
      "Testing at step=77, batch=40, test loss = 0.14205172657966614, test acc = 0.949999988079071, time = 0.13449501991271973\n",
      "Testing at step=77, batch=60, test loss = 0.054337404668331146, test acc = 0.9800000190734863, time = 0.1361401081085205\n",
      "Testing at step=77, batch=80, test loss = 0.11153990775346756, test acc = 0.9900000095367432, time = 0.1341228485107422\n",
      "Step 77 finished in 258.5343818664551, Train loss = 0.08011110901366919, Test loss = 0.10527683924883605; Train Acc = 0.975833343565464, Test Acc = 0.9682000052928924\n",
      "Training at step=78, batch=0, train loss = 0.011239678598940372, train acc = 1.0, time = 0.37863612174987793\n",
      "Training at step=78, batch=120, train loss = 0.11684026569128036, train acc = 0.949999988079071, time = 0.37752413749694824\n",
      "Training at step=78, batch=240, train loss = 0.11652252078056335, train acc = 0.9900000095367432, time = 0.3777658939361572\n",
      "Training at step=78, batch=360, train loss = 0.09332998096942902, train acc = 0.9700000286102295, time = 0.3755757808685303\n",
      "Training at step=78, batch=480, train loss = 0.025157125666737556, train acc = 0.9900000095367432, time = 0.3838777542114258\n",
      "Testing at step=78, batch=0, test loss = 0.18650037050247192, test acc = 0.949999988079071, time = 0.1344442367553711\n",
      "Testing at step=78, batch=20, test loss = 0.06119001284241676, test acc = 0.9800000190734863, time = 0.13632655143737793\n",
      "Testing at step=78, batch=40, test loss = 0.23177161812782288, test acc = 0.9599999785423279, time = 0.1349339485168457\n",
      "Testing at step=78, batch=60, test loss = 0.08759205788373947, test acc = 0.9599999785423279, time = 0.13419651985168457\n",
      "Testing at step=78, batch=80, test loss = 0.06601417809724808, test acc = 0.9700000286102295, time = 0.13481497764587402\n",
      "Step 78 finished in 258.1170151233673, Train loss = 0.07818760177896668, Test loss = 0.10250752748921514; Train Acc = 0.976100010573864, Test Acc = 0.9681000083684921\n",
      "Training at step=79, batch=0, train loss = 0.06955835223197937, train acc = 0.9800000190734863, time = 0.3799617290496826\n",
      "Training at step=79, batch=120, train loss = 0.04328276216983795, train acc = 0.9900000095367432, time = 0.3829622268676758\n",
      "Training at step=79, batch=240, train loss = 0.05505691468715668, train acc = 0.9800000190734863, time = 0.37660932540893555\n",
      "Training at step=79, batch=360, train loss = 0.030413739383220673, train acc = 0.9900000095367432, time = 0.37481117248535156\n",
      "Training at step=79, batch=480, train loss = 0.13671907782554626, train acc = 0.9700000286102295, time = 0.375903844833374\n",
      "Testing at step=79, batch=0, test loss = 0.09848928451538086, test acc = 0.9800000190734863, time = 0.13466668128967285\n",
      "Testing at step=79, batch=20, test loss = 0.18421246111392975, test acc = 0.9200000166893005, time = 0.1336531639099121\n",
      "Testing at step=79, batch=40, test loss = 0.06915411353111267, test acc = 0.9800000190734863, time = 0.13510847091674805\n",
      "Testing at step=79, batch=60, test loss = 0.09056241810321808, test acc = 0.9599999785423279, time = 0.13512420654296875\n",
      "Testing at step=79, batch=80, test loss = 0.10364601761102676, test acc = 0.949999988079071, time = 0.13606929779052734\n",
      "Step 79 finished in 258.08302450180054, Train loss = 0.07783564081881195, Test loss = 0.10541963385418057; Train Acc = 0.9759833440184593, Test Acc = 0.9672000062465668\n",
      "Training at step=80, batch=0, train loss = 0.16725321114063263, train acc = 0.9599999785423279, time = 0.37758827209472656\n",
      "Training at step=80, batch=120, train loss = 0.08738542348146439, train acc = 0.9599999785423279, time = 0.3774881362915039\n",
      "Training at step=80, batch=240, train loss = 0.05797258019447327, train acc = 0.9800000190734863, time = 0.37583327293395996\n",
      "Training at step=80, batch=360, train loss = 0.0738404393196106, train acc = 0.9700000286102295, time = 0.37551093101501465\n",
      "Training at step=80, batch=480, train loss = 0.16821856796741486, train acc = 0.9399999976158142, time = 0.37520623207092285\n",
      "Testing at step=80, batch=0, test loss = 0.06234241649508476, test acc = 0.9900000095367432, time = 0.13503360748291016\n",
      "Testing at step=80, batch=20, test loss = 0.2738144099712372, test acc = 0.949999988079071, time = 0.13468551635742188\n",
      "Testing at step=80, batch=40, test loss = 0.08073247969150543, test acc = 0.949999988079071, time = 0.13279294967651367\n",
      "Testing at step=80, batch=60, test loss = 0.20985718071460724, test acc = 0.9399999976158142, time = 0.13457226753234863\n",
      "Testing at step=80, batch=80, test loss = 0.09458794444799423, test acc = 0.9700000286102295, time = 0.13601446151733398\n",
      "Step 80 finished in 257.5514302253723, Train loss = 0.07629479194215188, Test loss = 0.11014348406344653; Train Acc = 0.9769333428144455, Test Acc = 0.9663000065088272\n",
      "Training at step=81, batch=0, train loss = 0.06869829446077347, train acc = 0.9800000190734863, time = 0.38001251220703125\n",
      "Training at step=81, batch=120, train loss = 0.06755301356315613, train acc = 0.9800000190734863, time = 0.3714933395385742\n",
      "Training at step=81, batch=240, train loss = 0.10917463898658752, train acc = 0.9599999785423279, time = 0.3783714771270752\n",
      "Training at step=81, batch=360, train loss = 0.0435483418405056, train acc = 0.9900000095367432, time = 0.3758280277252197\n",
      "Training at step=81, batch=480, train loss = 0.03870677202939987, train acc = 0.9800000190734863, time = 0.3729281425476074\n",
      "Testing at step=81, batch=0, test loss = 0.12438756972551346, test acc = 0.949999988079071, time = 0.13431906700134277\n",
      "Testing at step=81, batch=20, test loss = 0.11031167209148407, test acc = 0.9700000286102295, time = 0.13609576225280762\n",
      "Testing at step=81, batch=40, test loss = 0.042669929563999176, test acc = 0.9800000190734863, time = 0.1342172622680664\n",
      "Testing at step=81, batch=60, test loss = 0.11892475187778473, test acc = 0.9700000286102295, time = 0.1336052417755127\n",
      "Testing at step=81, batch=80, test loss = 0.11010098457336426, test acc = 0.9599999785423279, time = 0.13339662551879883\n",
      "Step 81 finished in 256.2516188621521, Train loss = 0.07738923687643061, Test loss = 0.10929231425747275; Train Acc = 0.9758666758735974, Test Acc = 0.966900006532669\n",
      "Training at step=82, batch=0, train loss = 0.06908991187810898, train acc = 0.9599999785423279, time = 0.3759303092956543\n",
      "Training at step=82, batch=120, train loss = 0.07633574306964874, train acc = 0.9700000286102295, time = 0.3737459182739258\n",
      "Training at step=82, batch=240, train loss = 0.06680391728878021, train acc = 0.9900000095367432, time = 0.37359118461608887\n",
      "Training at step=82, batch=360, train loss = 0.03523118421435356, train acc = 0.9900000095367432, time = 0.3735320568084717\n",
      "Training at step=82, batch=480, train loss = 0.05510134622454643, train acc = 0.9800000190734863, time = 0.3732130527496338\n",
      "Testing at step=82, batch=0, test loss = 0.11451958864927292, test acc = 0.9599999785423279, time = 0.13437509536743164\n",
      "Testing at step=82, batch=20, test loss = 0.18051519989967346, test acc = 0.9100000262260437, time = 0.13354206085205078\n",
      "Testing at step=82, batch=40, test loss = 0.0363144725561142, test acc = 0.9800000190734863, time = 0.13377857208251953\n",
      "Testing at step=82, batch=60, test loss = 0.1440279334783554, test acc = 0.9599999785423279, time = 0.13389897346496582\n",
      "Testing at step=82, batch=80, test loss = 0.09760969877243042, test acc = 0.9700000286102295, time = 0.1320047378540039\n",
      "Step 82 finished in 255.9488697052002, Train loss = 0.07591909767438967, Test loss = 0.10831591254100204; Train Acc = 0.976750009059906, Test Acc = 0.9682000106573105\n",
      "Training at step=83, batch=0, train loss = 0.026505768299102783, train acc = 0.9800000190734863, time = 0.37657761573791504\n",
      "Training at step=83, batch=120, train loss = 0.07107070088386536, train acc = 0.9800000190734863, time = 0.3770115375518799\n",
      "Training at step=83, batch=240, train loss = 0.11040899157524109, train acc = 0.9599999785423279, time = 0.3761143684387207\n",
      "Training at step=83, batch=360, train loss = 0.027732783928513527, train acc = 1.0, time = 0.3749253749847412\n",
      "Training at step=83, batch=480, train loss = 0.11848810315132141, train acc = 0.9800000190734863, time = 0.3755760192871094\n",
      "Testing at step=83, batch=0, test loss = 0.06334707140922546, test acc = 0.9900000095367432, time = 0.13346195220947266\n",
      "Testing at step=83, batch=20, test loss = 0.14688758552074432, test acc = 0.9599999785423279, time = 0.1325690746307373\n",
      "Testing at step=83, batch=40, test loss = 0.11056885123252869, test acc = 0.9800000190734863, time = 0.1334395408630371\n",
      "Testing at step=83, batch=60, test loss = 0.2113286256790161, test acc = 0.9300000071525574, time = 0.1335310935974121\n",
      "Testing at step=83, batch=80, test loss = 0.062076255679130554, test acc = 0.9900000095367432, time = 0.13269662857055664\n",
      "Step 83 finished in 256.5670053958893, Train loss = 0.07569421614985913, Test loss = 0.1141404072754085; Train Acc = 0.9768333439032236, Test Acc = 0.9654000055789947\n",
      "Training at step=84, batch=0, train loss = 0.07413055747747421, train acc = 0.9800000190734863, time = 0.37638092041015625\n",
      "Training at step=84, batch=120, train loss = 0.30721694231033325, train acc = 0.949999988079071, time = 0.37540698051452637\n",
      "Training at step=84, batch=240, train loss = 0.08982743322849274, train acc = 0.9700000286102295, time = 0.37435269355773926\n",
      "Training at step=84, batch=360, train loss = 0.015975967049598694, train acc = 1.0, time = 0.3795511722564697\n",
      "Training at step=84, batch=480, train loss = 0.038158416748046875, train acc = 0.9900000095367432, time = 0.3742513656616211\n",
      "Testing at step=84, batch=0, test loss = 0.1546287089586258, test acc = 0.9599999785423279, time = 0.13303399085998535\n",
      "Testing at step=84, batch=20, test loss = 0.06608134508132935, test acc = 0.9800000190734863, time = 0.1335766315460205\n",
      "Testing at step=84, batch=40, test loss = 0.06334153562784195, test acc = 0.9700000286102295, time = 0.13222002983093262\n",
      "Testing at step=84, batch=60, test loss = 0.14750269055366516, test acc = 0.949999988079071, time = 0.13333988189697266\n",
      "Testing at step=84, batch=80, test loss = 0.11631976813077927, test acc = 0.9700000286102295, time = 0.13295578956604004\n",
      "Step 84 finished in 256.31875681877136, Train loss = 0.07515930537134409, Test loss = 0.10457250506617129; Train Acc = 0.9761500100294749, Test Acc = 0.9672000050544739\n",
      "Training at step=85, batch=0, train loss = 0.05314995348453522, train acc = 0.9800000190734863, time = 0.37886929512023926\n",
      "Training at step=85, batch=120, train loss = 0.04408002272248268, train acc = 0.9700000286102295, time = 0.3747522830963135\n",
      "Training at step=85, batch=240, train loss = 0.014232797548174858, train acc = 1.0, time = 0.3758881092071533\n",
      "Training at step=85, batch=360, train loss = 0.020359618589282036, train acc = 1.0, time = 0.37517285346984863\n",
      "Training at step=85, batch=480, train loss = 0.03337734937667847, train acc = 0.9900000095367432, time = 0.3724637031555176\n",
      "Testing at step=85, batch=0, test loss = 0.1097632348537445, test acc = 0.9599999785423279, time = 0.13477635383605957\n",
      "Testing at step=85, batch=20, test loss = 0.0418594628572464, test acc = 0.9800000190734863, time = 0.13373780250549316\n",
      "Testing at step=85, batch=40, test loss = 0.1375291794538498, test acc = 0.949999988079071, time = 0.13230323791503906\n",
      "Testing at step=85, batch=60, test loss = 0.025993473827838898, test acc = 0.9900000095367432, time = 0.13417506217956543\n",
      "Testing at step=85, batch=80, test loss = 0.10239284485578537, test acc = 0.9599999785423279, time = 0.13278627395629883\n",
      "Step 85 finished in 256.50847482681274, Train loss = 0.07312758615706116, Test loss = 0.10137127944268286; Train Acc = 0.9776500104864438, Test Acc = 0.9698000067472458\n",
      "Training at step=86, batch=0, train loss = 0.026379413902759552, train acc = 0.9900000095367432, time = 0.37623119354248047\n",
      "Training at step=86, batch=120, train loss = 0.05330198258161545, train acc = 0.9700000286102295, time = 0.3732185363769531\n",
      "Training at step=86, batch=240, train loss = 0.024808432906866074, train acc = 1.0, time = 0.37379026412963867\n",
      "Training at step=86, batch=360, train loss = 0.09939402341842651, train acc = 0.9599999785423279, time = 0.3771090507507324\n",
      "Training at step=86, batch=480, train loss = 0.06544197350740433, train acc = 0.9800000190734863, time = 0.373767614364624\n",
      "Testing at step=86, batch=0, test loss = 0.03218475729227066, test acc = 0.9800000190734863, time = 0.13251972198486328\n",
      "Testing at step=86, batch=20, test loss = 0.03236568719148636, test acc = 1.0, time = 0.1342604160308838\n",
      "Testing at step=86, batch=40, test loss = 0.033139802515506744, test acc = 0.9900000095367432, time = 0.13312029838562012\n",
      "Testing at step=86, batch=60, test loss = 0.04854274168610573, test acc = 0.9900000095367432, time = 0.13304686546325684\n",
      "Testing at step=86, batch=80, test loss = 0.10474845767021179, test acc = 0.9700000286102295, time = 0.13275718688964844\n",
      "Step 86 finished in 255.92842435836792, Train loss = 0.07358194426167756, Test loss = 0.10000370603054762; Train Acc = 0.9775500104824701, Test Acc = 0.9703000074625016\n",
      "Training at step=87, batch=0, train loss = 0.07370633631944656, train acc = 0.9800000190734863, time = 0.38353991508483887\n",
      "Training at step=87, batch=120, train loss = 0.04546792805194855, train acc = 0.9900000095367432, time = 0.37577104568481445\n",
      "Training at step=87, batch=240, train loss = 0.06489376723766327, train acc = 0.9700000286102295, time = 0.3790771961212158\n",
      "Training at step=87, batch=360, train loss = 0.1467050015926361, train acc = 0.9700000286102295, time = 0.37402796745300293\n",
      "Training at step=87, batch=480, train loss = 0.03223039209842682, train acc = 0.9900000095367432, time = 0.3738090991973877\n",
      "Testing at step=87, batch=0, test loss = 0.055756401270627975, test acc = 0.9700000286102295, time = 0.1330108642578125\n",
      "Testing at step=87, batch=20, test loss = 0.06873536854982376, test acc = 0.9800000190734863, time = 0.134779691696167\n",
      "Testing at step=87, batch=40, test loss = 0.1536116749048233, test acc = 0.9300000071525574, time = 0.1333165168762207\n",
      "Testing at step=87, batch=60, test loss = 0.08264145255088806, test acc = 0.9599999785423279, time = 0.1334853172302246\n",
      "Testing at step=87, batch=80, test loss = 0.2300083190202713, test acc = 0.9200000166893005, time = 0.13308048248291016\n",
      "Step 87 finished in 256.28462862968445, Train loss = 0.07540099552366883, Test loss = 0.1161201718263328; Train Acc = 0.9768333431084951, Test Acc = 0.9635000056028367\n",
      "Training at step=88, batch=0, train loss = 0.05375010147690773, train acc = 0.9700000286102295, time = 0.3798375129699707\n",
      "Training at step=88, batch=120, train loss = 0.06776276230812073, train acc = 0.9700000286102295, time = 0.37749814987182617\n",
      "Training at step=88, batch=240, train loss = 0.22159937024116516, train acc = 0.9300000071525574, time = 0.37421226501464844\n",
      "Training at step=88, batch=360, train loss = 0.09264136105775833, train acc = 0.9800000190734863, time = 0.37429094314575195\n",
      "Training at step=88, batch=480, train loss = 0.06059121713042259, train acc = 0.9800000190734863, time = 0.37499213218688965\n",
      "Testing at step=88, batch=0, test loss = 0.037520911544561386, test acc = 1.0, time = 0.13309478759765625\n",
      "Testing at step=88, batch=20, test loss = 0.05370162054896355, test acc = 0.9800000190734863, time = 0.1339719295501709\n",
      "Testing at step=88, batch=40, test loss = 0.052002061158418655, test acc = 0.9900000095367432, time = 0.1374976634979248\n",
      "Testing at step=88, batch=60, test loss = 0.2181757688522339, test acc = 0.9599999785423279, time = 0.1335761547088623\n",
      "Testing at step=88, batch=80, test loss = 0.047104790806770325, test acc = 0.9800000190734863, time = 0.13377976417541504\n",
      "Step 88 finished in 256.33213329315186, Train loss = 0.07385744847512493, Test loss = 0.0996952638681978; Train Acc = 0.9771333425243696, Test Acc = 0.9692000061273575\n",
      "Training at step=89, batch=0, train loss = 0.03002128377556801, train acc = 0.9900000095367432, time = 0.3785061836242676\n",
      "Training at step=89, batch=120, train loss = 0.024037575349211693, train acc = 1.0, time = 0.3747377395629883\n",
      "Training at step=89, batch=240, train loss = 0.13601431250572205, train acc = 0.9900000095367432, time = 0.38313937187194824\n",
      "Training at step=89, batch=360, train loss = 0.07062333077192307, train acc = 0.9599999785423279, time = 0.3746323585510254\n",
      "Training at step=89, batch=480, train loss = 0.06236753985285759, train acc = 0.9700000286102295, time = 0.3783252239227295\n",
      "Testing at step=89, batch=0, test loss = 0.16518695652484894, test acc = 0.9599999785423279, time = 0.13231825828552246\n",
      "Testing at step=89, batch=20, test loss = 0.14544463157653809, test acc = 0.9700000286102295, time = 0.13275551795959473\n",
      "Testing at step=89, batch=40, test loss = 0.10433609038591385, test acc = 0.9700000286102295, time = 0.132476806640625\n",
      "Testing at step=89, batch=60, test loss = 0.027095062658190727, test acc = 0.9900000095367432, time = 0.13315987586975098\n",
      "Testing at step=89, batch=80, test loss = 0.30985599756240845, test acc = 0.9200000166893005, time = 0.14342474937438965\n",
      "Step 89 finished in 255.84320211410522, Train loss = 0.07336546209718411, Test loss = 0.1042583892121911; Train Acc = 0.9777000106374423, Test Acc = 0.9678000062704086\n",
      "Training at step=90, batch=0, train loss = 0.028941042721271515, train acc = 0.9900000095367432, time = 0.37664198875427246\n",
      "Training at step=90, batch=120, train loss = 0.07070577144622803, train acc = 0.9800000190734863, time = 0.37727808952331543\n",
      "Training at step=90, batch=240, train loss = 0.10754559189081192, train acc = 0.9700000286102295, time = 0.37304067611694336\n",
      "Training at step=90, batch=360, train loss = 0.0998075008392334, train acc = 0.949999988079071, time = 0.37465929985046387\n",
      "Training at step=90, batch=480, train loss = 0.18984980881214142, train acc = 0.949999988079071, time = 0.3729710578918457\n",
      "Testing at step=90, batch=0, test loss = 0.18448998034000397, test acc = 0.949999988079071, time = 0.1330416202545166\n",
      "Testing at step=90, batch=20, test loss = 0.07957243174314499, test acc = 0.9700000286102295, time = 0.13173127174377441\n",
      "Testing at step=90, batch=40, test loss = 0.029245242476463318, test acc = 1.0, time = 0.13216257095336914\n",
      "Testing at step=90, batch=60, test loss = 0.1216781884431839, test acc = 0.9800000190734863, time = 0.13253521919250488\n",
      "Testing at step=90, batch=80, test loss = 0.05693192407488823, test acc = 0.9900000095367432, time = 0.1344470977783203\n",
      "Step 90 finished in 255.56467866897583, Train loss = 0.07107280885257448, Test loss = 0.11418420283123851; Train Acc = 0.9780333432555198, Test Acc = 0.964100005030632\n",
      "Training at step=91, batch=0, train loss = 0.05351235345005989, train acc = 0.9800000190734863, time = 0.37432265281677246\n",
      "Training at step=91, batch=120, train loss = 0.15584231913089752, train acc = 0.9399999976158142, time = 0.3746922016143799\n",
      "Training at step=91, batch=240, train loss = 0.09184448421001434, train acc = 0.9599999785423279, time = 0.3716728687286377\n",
      "Training at step=91, batch=360, train loss = 0.0749531015753746, train acc = 0.9800000190734863, time = 0.3738842010498047\n",
      "Training at step=91, batch=480, train loss = 0.015866506844758987, train acc = 1.0, time = 0.3724660873413086\n",
      "Testing at step=91, batch=0, test loss = 0.011107277125120163, test acc = 1.0, time = 0.1321272850036621\n",
      "Testing at step=91, batch=20, test loss = 0.11564254015684128, test acc = 0.9599999785423279, time = 0.1322793960571289\n",
      "Testing at step=91, batch=40, test loss = 0.09058506041765213, test acc = 0.9700000286102295, time = 0.13246655464172363\n",
      "Testing at step=91, batch=60, test loss = 0.14795327186584473, test acc = 0.9300000071525574, time = 0.1319277286529541\n",
      "Testing at step=91, batch=80, test loss = 0.05690754950046539, test acc = 0.9800000190734863, time = 0.13366389274597168\n",
      "Step 91 finished in 255.1947832107544, Train loss = 0.07090819719868402, Test loss = 0.10435462524183095; Train Acc = 0.9783666774630546, Test Acc = 0.9686000055074692\n",
      "Training at step=92, batch=0, train loss = 0.030664492398500443, train acc = 0.9900000095367432, time = 0.37496304512023926\n",
      "Training at step=92, batch=120, train loss = 0.0551207959651947, train acc = 0.9900000095367432, time = 0.3733556270599365\n",
      "Training at step=92, batch=240, train loss = 0.037986870855093, train acc = 0.9900000095367432, time = 0.37407588958740234\n",
      "Training at step=92, batch=360, train loss = 0.039853885769844055, train acc = 0.9800000190734863, time = 0.3741128444671631\n",
      "Training at step=92, batch=480, train loss = 0.024427928030490875, train acc = 0.9900000095367432, time = 0.3735778331756592\n",
      "Testing at step=92, batch=0, test loss = 0.03412375599145889, test acc = 1.0, time = 0.13301587104797363\n",
      "Testing at step=92, batch=20, test loss = 0.17450988292694092, test acc = 0.949999988079071, time = 0.1324601173400879\n",
      "Testing at step=92, batch=40, test loss = 0.009504440240561962, test acc = 1.0, time = 0.13188552856445312\n",
      "Testing at step=92, batch=60, test loss = 0.021191556006669998, test acc = 0.9800000190734863, time = 0.13342523574829102\n",
      "Testing at step=92, batch=80, test loss = 0.058840252459049225, test acc = 0.9800000190734863, time = 0.13460922241210938\n",
      "Step 92 finished in 255.41173219680786, Train loss = 0.06878945273036759, Test loss = 0.10617960790172219; Train Acc = 0.9789333427945773, Test Acc = 0.9681000071763992\n",
      "Training at step=93, batch=0, train loss = 0.06376536935567856, train acc = 0.9800000190734863, time = 0.37311267852783203\n",
      "Training at step=93, batch=120, train loss = 0.08976238965988159, train acc = 0.9700000286102295, time = 0.3762245178222656\n",
      "Training at step=93, batch=240, train loss = 0.04323921725153923, train acc = 0.9900000095367432, time = 0.3738718032836914\n",
      "Training at step=93, batch=360, train loss = 0.055295638740062714, train acc = 0.9700000286102295, time = 0.370908260345459\n",
      "Training at step=93, batch=480, train loss = 0.043732352554798126, train acc = 0.9900000095367432, time = 0.3729214668273926\n",
      "Testing at step=93, batch=0, test loss = 0.11784649640321732, test acc = 0.949999988079071, time = 0.1323084831237793\n",
      "Testing at step=93, batch=20, test loss = 0.18424896895885468, test acc = 0.949999988079071, time = 0.13387465476989746\n",
      "Testing at step=93, batch=40, test loss = 0.11324268579483032, test acc = 0.9599999785423279, time = 0.13185358047485352\n",
      "Testing at step=93, batch=60, test loss = 0.13078583776950836, test acc = 0.9599999785423279, time = 0.13250470161437988\n",
      "Testing at step=93, batch=80, test loss = 0.06753885000944138, test acc = 0.9800000190734863, time = 0.13449764251708984\n",
      "Step 93 finished in 254.67233443260193, Train loss = 0.069359815766414, Test loss = 0.101630506683141; Train Acc = 0.978200011253357, Test Acc = 0.9683000081777573\n",
      "Training at step=94, batch=0, train loss = 0.04256020858883858, train acc = 0.9900000095367432, time = 0.38074731826782227\n",
      "Training at step=94, batch=120, train loss = 0.04118884727358818, train acc = 0.9800000190734863, time = 0.3729531764984131\n",
      "Training at step=94, batch=240, train loss = 0.02234503999352455, train acc = 0.9900000095367432, time = 0.3707253932952881\n",
      "Training at step=94, batch=360, train loss = 0.02561693824827671, train acc = 0.9900000095367432, time = 0.37256836891174316\n",
      "Training at step=94, batch=480, train loss = 0.06488114595413208, train acc = 0.9800000190734863, time = 0.37340664863586426\n",
      "Testing at step=94, batch=0, test loss = 0.0804562196135521, test acc = 0.9599999785423279, time = 0.13251781463623047\n",
      "Testing at step=94, batch=20, test loss = 0.1650988608598709, test acc = 0.9399999976158142, time = 0.13293981552124023\n",
      "Testing at step=94, batch=40, test loss = 0.060003116726875305, test acc = 0.9800000190734863, time = 0.1320188045501709\n",
      "Testing at step=94, batch=60, test loss = 0.07455161213874817, test acc = 0.9900000095367432, time = 0.13233470916748047\n",
      "Testing at step=94, batch=80, test loss = 0.059232357889413834, test acc = 0.9700000286102295, time = 0.13199305534362793\n",
      "Step 94 finished in 254.88484907150269, Train loss = 0.06997632837466275, Test loss = 0.10540309492498637; Train Acc = 0.9777166764934858, Test Acc = 0.9673000073432922\n",
      "Training at step=95, batch=0, train loss = 0.08554206043481827, train acc = 0.9599999785423279, time = 0.37244677543640137\n",
      "Training at step=95, batch=120, train loss = 0.009047629311680794, train acc = 1.0, time = 0.37061524391174316\n",
      "Training at step=95, batch=240, train loss = 0.03688932582736015, train acc = 0.9900000095367432, time = 0.3709728717803955\n",
      "Training at step=95, batch=360, train loss = 0.14931204915046692, train acc = 0.949999988079071, time = 0.3752150535583496\n",
      "Training at step=95, batch=480, train loss = 0.07037916779518127, train acc = 0.9800000190734863, time = 0.37120771408081055\n",
      "Testing at step=95, batch=0, test loss = 0.15628494322299957, test acc = 0.949999988079071, time = 0.13218307495117188\n",
      "Testing at step=95, batch=20, test loss = 0.08631553500890732, test acc = 0.9800000190734863, time = 0.1323409080505371\n",
      "Testing at step=95, batch=40, test loss = 0.05133187398314476, test acc = 0.9900000095367432, time = 0.13315892219543457\n",
      "Testing at step=95, batch=60, test loss = 0.07445596903562546, test acc = 0.9599999785423279, time = 0.13178372383117676\n",
      "Testing at step=95, batch=80, test loss = 0.03107268363237381, test acc = 0.9900000095367432, time = 0.131638765335083\n",
      "Step 95 finished in 254.84478902816772, Train loss = 0.06897244304884226, Test loss = 0.09768379190936685; Train Acc = 0.9785333437720934, Test Acc = 0.9697000062465668\n",
      "Training at step=96, batch=0, train loss = 0.10031117498874664, train acc = 0.9599999785423279, time = 0.376497745513916\n",
      "Training at step=96, batch=120, train loss = 0.01343440916389227, train acc = 1.0, time = 0.3704972267150879\n",
      "Training at step=96, batch=240, train loss = 0.04578124359250069, train acc = 0.9700000286102295, time = 0.37241601943969727\n",
      "Training at step=96, batch=360, train loss = 0.038854870945215225, train acc = 0.9900000095367432, time = 0.374570369720459\n",
      "Training at step=96, batch=480, train loss = 0.028155798092484474, train acc = 1.0, time = 0.3702085018157959\n",
      "Testing at step=96, batch=0, test loss = 0.09530840069055557, test acc = 0.949999988079071, time = 0.13189148902893066\n",
      "Testing at step=96, batch=20, test loss = 0.05426651984453201, test acc = 0.9900000095367432, time = 0.13236451148986816\n",
      "Testing at step=96, batch=40, test loss = 0.14037607610225677, test acc = 0.949999988079071, time = 0.13475251197814941\n",
      "Testing at step=96, batch=60, test loss = 0.046947550028562546, test acc = 0.9800000190734863, time = 0.13186359405517578\n",
      "Testing at step=96, batch=80, test loss = 0.07214711606502533, test acc = 0.9599999785423279, time = 0.1322157382965088\n",
      "Step 96 finished in 254.41326379776, Train loss = 0.06849622402340173, Test loss = 0.1018504507932812; Train Acc = 0.9786666771769523, Test Acc = 0.9687000048160553\n",
      "Training at step=97, batch=0, train loss = 0.12856686115264893, train acc = 0.9599999785423279, time = 0.37627267837524414\n",
      "Training at step=97, batch=120, train loss = 0.11112639307975769, train acc = 0.9599999785423279, time = 0.3723917007446289\n",
      "Training at step=97, batch=240, train loss = 0.07549341022968292, train acc = 0.9800000190734863, time = 0.3743307590484619\n",
      "Training at step=97, batch=360, train loss = 0.04548158496618271, train acc = 0.9900000095367432, time = 0.3721752166748047\n",
      "Training at step=97, batch=480, train loss = 0.11590363830327988, train acc = 0.949999988079071, time = 0.37111425399780273\n",
      "Testing at step=97, batch=0, test loss = 0.1944642961025238, test acc = 0.9700000286102295, time = 0.13253402709960938\n",
      "Testing at step=97, batch=20, test loss = 0.07429478317499161, test acc = 0.9700000286102295, time = 0.13171839714050293\n",
      "Testing at step=97, batch=40, test loss = 0.07301072031259537, test acc = 0.9599999785423279, time = 0.13282346725463867\n",
      "Testing at step=97, batch=60, test loss = 0.14084318280220032, test acc = 0.949999988079071, time = 0.13244009017944336\n",
      "Testing at step=97, batch=80, test loss = 0.0334022156894207, test acc = 0.9900000095367432, time = 0.13117289543151855\n",
      "Step 97 finished in 254.5573182106018, Train loss = 0.06800879407208413, Test loss = 0.11183319833129644; Train Acc = 0.9789166768391927, Test Acc = 0.9653000050783157\n",
      "Training at step=98, batch=0, train loss = 0.029916390776634216, train acc = 0.9900000095367432, time = 0.37603092193603516\n",
      "Training at step=98, batch=120, train loss = 0.09811367839574814, train acc = 0.949999988079071, time = 0.37355971336364746\n",
      "Training at step=98, batch=240, train loss = 0.01970808394253254, train acc = 1.0, time = 0.3705482482910156\n",
      "Training at step=98, batch=360, train loss = 0.0629051998257637, train acc = 0.9800000190734863, time = 0.37653136253356934\n",
      "Training at step=98, batch=480, train loss = 0.12884603440761566, train acc = 0.949999988079071, time = 0.37557435035705566\n",
      "Testing at step=98, batch=0, test loss = 0.15366752445697784, test acc = 0.949999988079071, time = 0.13210582733154297\n",
      "Testing at step=98, batch=20, test loss = 0.11087226867675781, test acc = 0.9700000286102295, time = 0.13184070587158203\n",
      "Testing at step=98, batch=40, test loss = 0.14206060767173767, test acc = 0.949999988079071, time = 0.13238954544067383\n",
      "Testing at step=98, batch=60, test loss = 0.15315879881381989, test acc = 0.9599999785423279, time = 0.1315631866455078\n",
      "Testing at step=98, batch=80, test loss = 0.02981761284172535, test acc = 1.0, time = 0.13187885284423828\n",
      "Step 98 finished in 255.00714826583862, Train loss = 0.0664906597804899, Test loss = 0.09812575164251029; Train Acc = 0.9793166768550873, Test Acc = 0.969400007724762\n",
      "Training at step=99, batch=0, train loss = 0.03868844360113144, train acc = 0.9800000190734863, time = 0.37484216690063477\n",
      "Training at step=99, batch=120, train loss = 0.056211646646261215, train acc = 0.9900000095367432, time = 0.37252330780029297\n",
      "Training at step=99, batch=240, train loss = 0.03968096897006035, train acc = 0.9900000095367432, time = 0.38001227378845215\n",
      "Training at step=99, batch=360, train loss = 0.11081302911043167, train acc = 0.9399999976158142, time = 0.37201476097106934\n",
      "Training at step=99, batch=480, train loss = 0.05214349180459976, train acc = 0.9900000095367432, time = 0.37869977951049805\n",
      "Testing at step=99, batch=0, test loss = 0.08258309215307236, test acc = 0.9900000095367432, time = 0.13286042213439941\n",
      "Testing at step=99, batch=20, test loss = 0.14898928999900818, test acc = 0.9599999785423279, time = 0.13188672065734863\n",
      "Testing at step=99, batch=40, test loss = 0.027025196701288223, test acc = 0.9900000095367432, time = 0.13191461563110352\n",
      "Testing at step=99, batch=60, test loss = 0.12204312533140182, test acc = 0.949999988079071, time = 0.1321547031402588\n",
      "Testing at step=99, batch=80, test loss = 0.12720178067684174, test acc = 0.949999988079071, time = 0.13199687004089355\n",
      "Step 99 finished in 254.5851285457611, Train loss = 0.06588231119288442, Test loss = 0.10568061825819314; Train Acc = 0.9796166769663492, Test Acc = 0.9670000088214874\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"1-mnist_quanv_classical_linear.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "U0Q0vFm7B6cg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711489773116,
     "user_tz": -660,
     "elapsed": 1144,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "42ddda3e-3765-4cd9-f7af-6eac61c51da8"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"1-mnist_quanv_classical_linear.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ]
  }
 ]
}
