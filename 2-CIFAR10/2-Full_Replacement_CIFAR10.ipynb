{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:22:52.948872Z",
     "iopub.status.busy": "2024-04-03T08:22:52.948409Z",
     "iopub.status.idle": "2024-04-03T08:24:31.832368Z",
     "shell.execute_reply": "2024-04-03T08:24:31.831751Z",
     "shell.execute_reply.started": "2024-04-03T08:22:52.948838Z"
    },
    "id": "5ebnFsErY0EM",
    "outputId": "e8932b66-715c-467f-912a-8b449c518214",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pennylane in /usr/local/lib/python3.9/dist-packages (0.35.1)\n",
      "Requirement already satisfied: cotengra in /usr/local/lib/python3.9/dist-packages (0.5.6)\n",
      "Requirement already satisfied: quimb in /usr/local/lib/python3.9/dist-packages (1.7.3)\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (1.3.2)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.6.9)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.23.4)\n",
      "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\n",
      "Requirement already satisfied: autograd in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.6.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pennylane) (4.11.0)\n",
      "Requirement already satisfied: rustworkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.14.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (3.0)\n",
      "Requirement already satisfied: pennylane-lightning>=0.35 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.35.1)\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\n",
      "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\n",
      "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.59.1)\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.12.3)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (0.11.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.2.2)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.39->quimb) (0.42.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2023.1.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (3.9.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.127)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!pip install torch torchvision torchaudio --upgrade\n",
    "!pip install pennylane cotengra quimb torchmetrics --upgrade\n",
    "#!pip install -U \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:25:45.093069Z",
     "iopub.status.busy": "2024-04-03T08:25:45.092790Z",
     "iopub.status.idle": "2024-04-03T08:26:54.096113Z",
     "shell.execute_reply": "2024-04-03T08:26:54.095258Z",
     "shell.execute_reply.started": "2024-04-03T08:25:45.093044Z"
    },
    "id": "VN2wD-U7bGse",
    "outputId": "efc73948-0832-402c-eafc-b0e6adce4a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "\n",
    "\n",
    "#import pennylane as qml\n",
    "#import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "#prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "#torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WmQPQuLbSFw"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:54.097750Z",
     "iopub.status.busy": "2024-04-03T08:26:54.097431Z",
     "iopub.status.idle": "2024-04-03T08:26:54.405968Z",
     "shell.execute_reply": "2024-04-03T08:26:54.405273Z",
     "shell.execute_reply.started": "2024-04-03T08:26:54.097729Z"
    },
    "id": "rUhQUP2dbTja",
    "outputId": "0d3641db-a2ff-4689-8b7e-32e0e8b313d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32])\n",
      "tensor([7, 2, 7, 6, 4, 5, 7, 1, 8, 1, 1, 0, 8, 9, 7, 3, 1, 4, 6, 8, 2, 6, 5, 1,\n",
      "        5, 9, 7, 2, 6, 5, 5, 9])\n",
      "tensor([ 0.0275+0.j, -0.0196+0.j, -0.0353+0.j, -0.6078+0.j, -0.7961+0.j, -0.8118+0.j,\n",
      "        -0.8196+0.j, -0.7569+0.j, -0.7490+0.j, -0.7176+0.j, -0.7490+0.j, -0.7569+0.j,\n",
      "        -0.7647+0.j, -0.8118+0.j, -0.7569+0.j, -0.7412+0.j, -0.7804+0.j, -0.6157+0.j,\n",
      "         0.0118+0.j,  0.1294+0.j,  0.0902+0.j,  0.0902+0.j,  0.0196+0.j,  0.0196+0.j,\n",
      "         0.0745+0.j,  0.0745+0.j,  0.0667+0.j,  0.0824+0.j,  0.0431+0.j,  0.0039+0.j,\n",
      "        -0.0118+0.j,  0.0118+0.j])\n"
     ]
    }
   ],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQI8WWY6byQq"
   },
   "source": [
    "# Some Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:54.407720Z",
     "iopub.status.busy": "2024-04-03T08:26:54.407518Z",
     "iopub.status.idle": "2024-04-03T08:26:56.071027Z",
     "shell.execute_reply": "2024-04-03T08:26:56.069162Z",
     "shell.execute_reply.started": "2024-04-03T08:26:54.407700Z"
    },
    "id": "7B4DTncWbwQl",
    "outputId": "38975b7c-66c4-49f5-e5fd-158e55028cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-0.j, 0.+0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j,  0.+0.j, -1.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-0.j, 0.+0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j,  0.+0.j, -1.+0.j],\n",
      "        [ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.-0.j, 0.-1.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  0.-0.j,  0.-0.j, -1.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j,  1.-0.j,  0.-0.j],\n",
      "        [ 0.+0.j,  1.-0.j,  0.+0.j,  0.-0.j],\n",
      "        [-1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, -0.+0.j, 0.-0.j, 0.+1.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, -0.-1.j, 0.+0.j, -0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -0.+0.j, -1.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -0.+0.j, -1.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-0.j, -0.+0.j, 0.+1.j],\n",
      "        [0.+0.j, 0.+0.j, -0.-1.j, -0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j, -0.+0.j,  1.-0.j]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "ket = {\n",
    "    '0':torch.tensor([1.,0.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '1':torch.tensor([0.,1.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '+':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) + torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device)),\n",
    "    '-':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) - torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device))\n",
    "}\n",
    "\n",
    "pauli = {\n",
    "    'I':torch.tensor([[1.,0.],[0.,1.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'X':torch.tensor([[0.,1.],[1.,0.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Y':torch.tensor([[0., -1.j],[1.j, 0]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Z':torch.tensor([[1.,0.],[0.,-1.]], dtype = COMPLEX_DTYPE, device=device)\n",
    "}\n",
    "\n",
    "def tensor_product(*args):\n",
    "  input_list = [a for a in args]\n",
    "  return functools.reduce(torch.kron, input_list)\n",
    "\n",
    "def multi_qubit_identity(n_qubits:int):\n",
    "  assert n_qubits>0\n",
    "  if n_qubits == 1:\n",
    "    return pauli['I']\n",
    "  else:\n",
    "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
    "\n",
    "pauli_words_su4 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
    "      #print(key1, key2)\n",
    "      #print(pauli[key1])\n",
    "      #print(pauli[key2])\n",
    "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
    "      #print(pauli_words_su4[key1+key2])\n",
    "\n",
    "pauli_words_su8 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      if not key1+key2+key3 == 'III':\n",
    "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
    "\n",
    "pauli_words_su16 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        if not key1+key2+key3+key4 == 'IIII':\n",
    "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
    "              pauli[key1],\n",
    "              pauli[key2],\n",
    "              pauli[key3],\n",
    "              pauli[key4]\n",
    "          )\n",
    "\n",
    "pauli_words_su32 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        for key5 in pauli.keys():\n",
    "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
    "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
    "                pauli[key1],\n",
    "                pauli[key2],\n",
    "                pauli[key3],\n",
    "                pauli[key4],\n",
    "                pauli[key5]\n",
    "            )\n",
    "\n",
    "def su32_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = list(pauli_words_su32.values())\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, torch.stack(dict_values))\n",
    "  #print(generator.shape)\n",
    "  return torch.matrix_exp(1j*generator)\n",
    "\n",
    "def su4_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = torch.stack(list(pauli_words_su4.values()))\n",
    "  #print(dict_values.shape)\n",
    "  generator = torch.einsum(\"i, ijk -> jk\", params, dict_values)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "def pauli_dict_func(key):\n",
    "    return pauli[key]\n",
    "\n",
    "def pauli_dict_func_multiple_keys(keys):\n",
    "    return list(map(pauli_dict_func, keys))\n",
    "\n",
    "def pauli_string_tensor_prod(pauli_string:str):\n",
    "    paulis_char = list(pauli_string)\n",
    "    paulis_mat = pauli_dict_func_multiple_keys(paulis_char)\n",
    "    return tensor_product(*paulis_mat)\n",
    "\n",
    "def generate_nqubit_pauli_strings(n_qubits:int):\n",
    "    assert n_qubits>0\n",
    "    pauli_labels = ['I', 'X', 'Y', 'Z']\n",
    "    pauli_strings = []\n",
    "    for labels in itertools.product(pauli_labels, repeat=n_qubits):\n",
    "        pauli_str = \"\".join(labels)\n",
    "        if pauli_str != 'I'*n_qubits:\n",
    "            pauli_strings.append(pauli_str)\n",
    "    return pauli_strings\n",
    "\n",
    "def generate_pauli_tensor_list(pauli_strings:list):\n",
    "    return list(map(pauli_string_tensor_prod, pauli_strings))\n",
    "\n",
    "\n",
    "print(\n",
    "    generate_pauli_tensor_list(generate_nqubit_pauli_strings(2))\n",
    ")\n",
    "\n",
    "\n",
    "#test_params = torch.randn(4**2-1,  device=device).type(COMPLEX_DTYPE)\n",
    "#print(test_params.shape)\n",
    "#test_op = su4_op(test_params)\n",
    "#print(test_op)\n",
    "#print(qml.is_unitary(qml.QubitUnitary(test_op.cpu().numpy(), wires=[0,1]))) # will be false if not torch.cdouble\n",
    "\n",
    "#rx = torch.linalg.matrix_exp(1j*torch.pi*pauli[\"X\"]/2)\n",
    "#print(qml.is_unitary(qml.QubitUnitary(rx.cpu().numpy(), wires=[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.073077Z",
     "iopub.status.busy": "2024-04-03T08:26:56.072615Z",
     "iopub.status.idle": "2024-04-03T08:26:56.079092Z",
     "shell.execute_reply": "2024-04-03T08:26:56.078390Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.073053Z"
    },
    "id": "Xs0c2F1eBnGc"
   },
   "outputs": [],
   "source": [
    "def measure_sv(\n",
    "    state,\n",
    "    observable\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Measure a statevector with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not\n",
    "  \"\"\"\n",
    "  expectation_value = torch.conj(state)@observable@state\n",
    "  return torch.real(expectation_value)\n",
    "\n",
    "def measure_dm(\n",
    "    rho,\n",
    "    observable\n",
    "):\n",
    "  \"\"\"\n",
    "  Measure a density matrix with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not.\n",
    "  \"\"\"\n",
    "  product = torch.matmul(rho, observable)\n",
    "\n",
    "  # Calculate the trace, which is the sum of diagonal elements\n",
    "  trace = torch.trace(product)\n",
    "\n",
    "  # The expectation value should be real for physical observables\n",
    "  return torch.real(trace)\n",
    "\n",
    "vmap_measure_sv = torch.vmap(measure_sv, in_dims=(None, 0), out_dims=0)\n",
    "vmap_measure_dm = torch.vmap(measure_dm, in_dims=(None, 0), out_dims=0)\n",
    "\n",
    "# assuming the input patch observables (hermitianized) has shape (batchsize,n_patches, c, h, w)\n",
    "# assuming the input set statevectors has shape (c, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_sv_batched_ob = torch.vmap(measure_sv, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "# assuming the input set desnity matrices has shape (batchsize, n_patches, c, 2**n, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_dm_batched_ob = torch.vmap(measure_dm, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "def bitstring_to_state(bitstring:str):\n",
    "  \"\"\"\n",
    "  Convert a bit string, like '0101001' or '+-+-101'\n",
    "  to a statevector. Each character in the bitstring must be among\n",
    "  0, 1, + and -\n",
    "  \"\"\"\n",
    "  assert len(bitstring)>0\n",
    "  for c in bitstring:\n",
    "    assert c in ['0', '1', '+', '-']\n",
    "  single_qubit_states = [ket[c] for c in bitstring]\n",
    "  return tensor_product(*single_qubit_states)\n",
    "\n",
    "\n",
    "#test_patch = torch.randn(size=[5, 2, 3, 4,4], dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_herm_patch = (torch.einsum(\"...jk->...kj\", test_patch)+test_patch)/2\n",
    "#print(test_herm_patch)\n",
    "#print(\"all patches shape\", test_herm_patch.shape)\n",
    "#test_sv = torch.stack([bitstring_to_state('++')]*3, axis = 0)\n",
    "#print(test_sv)\n",
    "#print(\"all sv shape\", test_sv.shape)\n",
    "#res = vmap_measure_channel_sv_batched_ob(test_sv, test_herm_patch)\n",
    "#print(res)\n",
    "#print(\"res shape\",res.shape)\n",
    "\n",
    "#for batchid in range(test_patch.shape[0]):\n",
    "#  batchitem = test_patch[batchid]\n",
    "#  for patch_idx in range(batchitem.shape[0]):\n",
    "#    patch = batchitem[patch_idx]\n",
    "#    for ch_idx in range(patch.shape[0]):\n",
    "#      print(f\"batchid={batchid}; patchid = {patch_idx}; chid = {ch_idx}\")\n",
    "#      ch = patch[ch_idx]\n",
    "#      #print(ch.shape)\n",
    "#      sv_ch = test_sv[ch_idx]\n",
    "#      #print(sv_ch.shape)\n",
    "#      print(measure_sv(sv_ch, ch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFZqT6bpElaL"
   },
   "source": [
    "# Flipped Quanvolution Kernel\n",
    "\n",
    "First, let's define the function to extract patches for a single kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.080443Z",
     "iopub.status.busy": "2024-04-03T08:26:56.079871Z",
     "iopub.status.idle": "2024-04-03T08:26:56.085801Z",
     "shell.execute_reply": "2024-04-03T08:26:56.085133Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.080420Z"
    },
    "id": "He4HdMRHC7T6"
   },
   "outputs": [],
   "source": [
    "def extract_patches(image, patch_size, stride, padding=None):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image with multiple input channels and optional custom padding.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (in_channels, height, width).\n",
    "        patch_size (int): Size of the square patches to extract.\n",
    "        stride (int): Stride between patches.\n",
    "        padding (tuple): Padding value(s) for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n",
    "    pad_l, pad_r, pad_t, pad_b = padding if padding is not None else (0,0,0,0)\n",
    "\n",
    "    if padding is not None:\n",
    "        image = torch.nn.functional.pad(image, (pad_l, pad_r, pad_t, pad_b), mode='constant')\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    height, width = image.shape[-2],  image.shape[-1]\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = image[..., i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    patches = torch.stack(patches, dim=-4)\n",
    "    return patches # has shape (batchsize, n_patches, channel, h, w)\n",
    "\n",
    "\n",
    "#single_img = torch.stack([torch.arange(32*32*1,dtype = torch.double, device=device).reshape((32,32)).type(torch.cdouble)]*3)\n",
    "\n",
    "#test_img = torch.stack([single_img]*2)\n",
    "#print(test_img[0][...,:4,:4])\n",
    "#patches = extract_patches(test_img, patch_size=4, stride=2,padding=(0,0,0,0))\n",
    "#print(patches.shape)\n",
    "#print(patches[0].shape)\n",
    "#print(patches[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRqflooEqKN"
   },
   "source": [
    "## A Three-by-Three Quanv Kernel Function\n",
    "\n",
    "For 3 by 3 patches, we only need a 2-qubit parameterised state, generated by the SU4 gate applied to the $|00\\rangle$ state. The first thing to do is to pad the 3 by 3 patches to 4 by 4. However, for input with size\n",
    "\n",
    "$$\n",
    "D_1\\times H_1 \\times W_1\n",
    "$$\n",
    "\n",
    "where $D_1$ is the number of input channels, we will need $D_1$ different two-qubit states, generated by $D_1$ different parameterised circuits. These circuits will produce $D_1$ expectation values. Suppose each circuit is parameterised by $\\theta_i, i\\in \\{1,2,\\cdots, D_1\\}$, the parameterised (pure) state can be written as\n",
    "\n",
    "$$\n",
    "|\\psi (\\theta_i)⟩ = SU4(\\theta_i)|00⟩\n",
    "$$\n",
    "\n",
    "For a channel $M_i$ of patch $M$ with shape $4\\times 4$, the expectation value is calculated by\n",
    "\n",
    "$$\n",
    "⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$\n",
    "\n",
    "Then, a single pixel value of the output channel corresponding this single kernel is\n",
    "\n",
    "$$\n",
    "\\frac{1}{D_1}\\sum_{i=1}^{D_1}⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.086697Z",
     "iopub.status.busy": "2024-04-03T08:26:56.086517Z",
     "iopub.status.idle": "2024-04-03T08:26:56.091796Z",
     "shell.execute_reply": "2024-04-03T08:26:56.091178Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.086679Z"
    },
    "id": "Yzn4KEt5ErG7"
   },
   "outputs": [],
   "source": [
    "def generate_2q_param_state(theta):\n",
    "  state = bitstring_to_state('00')\n",
    "  state = torch.matmul(su4_op(theta), state)\n",
    "  return state\n",
    "\n",
    "vmap_generate_2q_param_state = torch.vmap(generate_2q_param_state, in_dims=0, out_dims = 0)\n",
    "\n",
    "def single_kernel_op_over_batched_patches(thetas, patch):\n",
    "  # patch has shape (c_in, h, w)\n",
    "  # thetas has shape (c_in, 4^2-1) for SU4 gates\n",
    "  n_theta = thetas.shape[-2]\n",
    "  n_channel = patch.shape[-3]\n",
    "  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n",
    "  states = vmap_generate_2q_param_state(thetas)\n",
    "  #print(\"States shape\", states.shape)\n",
    "  patch = torch.nn.functional.pad(patch, (0, 1, 0, 1), mode='constant')\n",
    "  patch_t = torch.einsum(\"...jk->...kj\", patch)\n",
    "  herm_patch = (patch_t+patch)/2 # has dim (batchsize, num_patches, c, h, w)\n",
    "  #print(\"Herm patch shape\", herm_patch.shape)\n",
    "  channel_out = vmap_measure_channel_sv_batched_ob(states, herm_patch) # has dim (batchsize,n_patches, c)\n",
    "  return torch.sum(channel_out, axis = -1)/n_theta # has dim (batchszie, n_patches)\n",
    "\n",
    "\n",
    "#test_params = torch.randn((1,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_img = dummy_x.to(device)#.type(torch.cdouble)\n",
    "#print(test_img.shape)\n",
    "#test_patches = extract_patches(test_img, patch_size=3, stride=1,padding=(1,1,1,1))\n",
    "#print(test_patches.shape)\n",
    "#print(test_params.shape)\n",
    "#print(vmap_generate_2q_param_state(test_params))\n",
    "#test_out = single_kernel_op_over_batched_patches(test_params, test_patches)\n",
    "#print(test_out.shape)\n",
    "#h = (32-3+1*2)//1 +1\n",
    "#h**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4BXEKd8I67_"
   },
   "source": [
    "## Multiple Output Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.092668Z",
     "iopub.status.busy": "2024-04-03T08:26:56.092483Z",
     "iopub.status.idle": "2024-04-03T08:26:56.095952Z",
     "shell.execute_reply": "2024-04-03T08:26:56.095319Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.092650Z"
    },
    "id": "72vkHV_BI80l"
   },
   "outputs": [],
   "source": [
    "# For multiple channel output\n",
    "# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n",
    "vmap_vmap_single_kernel_op_through_extracted_patches = torch.vmap(single_kernel_op_over_batched_patches, in_dims=(0, None), out_dims=-2) # output has dim (batchsize, c_out, n_patches)\n",
    "\n",
    "#c_in = 1\n",
    "#c_out = 2\n",
    "\n",
    "#test_params2 = torch.randn((c_out, c_in,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_out2 = vmap_vmap_single_kernel_op_through_extracted_patches(test_params2, test_patches)\n",
    "#print(test_out2.shape)\n",
    "#test_out_2_features = test_out2.reshape((-1,c_out, 32, 32))\n",
    "#print(test_out_2_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuZ6G5-sKK_J"
   },
   "source": [
    "## Pytorch Module for Three-by-Three Flipped Quanv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.096785Z",
     "iopub.status.busy": "2024-04-03T08:26:56.096604Z",
     "iopub.status.idle": "2024-04-03T08:26:56.103091Z",
     "shell.execute_reply": "2024-04-03T08:26:56.102542Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.096768Z"
    },
    "id": "Gww_XdJ5KPJt"
   },
   "outputs": [],
   "source": [
    "class FlippedQuanv3x3(torch.nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, padding):\n",
    "    super(FlippedQuanv3x3, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.pad_l, self.pad_r, self.pad_t, self.pad_b = padding if padding is not None else (0,0,0,0)\n",
    "    self.pad = (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n",
    "    self.weight= torch.nn.Parameter(torch.randn((out_channels, in_channels, 4**2-1)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn((out_channels, 1)).type(COMPLEX_DTYPE))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batchsize ,c_in, h, w)\n",
    "    # weight has shape (c_out, c_in, 15)\n",
    "    # bias has shape (c_out, 1)\n",
    "    x = x.type(COMPLEX_DTYPE)\n",
    "    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n",
    "    h_out = (h_in-3+(self.pad_t+self.pad_b))//self.stride +1\n",
    "    w_out = (w_in-3+(self.pad_l+self.pad_r))//self.stride +1\n",
    "\n",
    "    #print(self.weight.shape)\n",
    "    #print(patches.shape)\n",
    "\n",
    "    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n",
    "\n",
    "\n",
    "    out = out + self.bias\n",
    "    return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_channels={self.in_channels}, out_channels={self.out_channels}, stride={self.stride}, padding={self.padding}'\n",
    "\n",
    "#test_module = FlippedQuanv3x3(in_channels=1, out_channels=2, stride=1, padding=(1,1,1,1)).to(device)\n",
    "#print(dummy_x.shape)\n",
    "#test_out = test_module(dummy_x.to(device))\n",
    "#print(test_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jY9sQZ7Ynla"
   },
   "source": [
    "# Linear Layer\n",
    "\n",
    "For input dimension $D$, the quantum linear layer is a $n = \\lceil \\log_4(D+1)⌉$-qubit quantum circuit. Both the data encoding and parameterised circuit is achieved via the $SU(2^n)$ unitary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.103883Z",
     "iopub.status.busy": "2024-04-03T08:26:56.103708Z",
     "iopub.status.idle": "2024-04-03T08:26:57.562457Z",
     "shell.execute_reply": "2024-04-03T08:26:57.561682Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.103865Z"
    },
    "id": "AXxNIObFYnPW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0847, 0.6649, 0.0675, 0.1829], device='cuda:0')\n",
      "tensor(1.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def data_encode_unitary(padded_data, t):\n",
    "  original_dim = padded_data.shape[-1]\n",
    "  new_dim = torch.sqrt(torch.tensor(original_dim)).type(torch.int)\n",
    "  data = torch.reshape(padded_data, (new_dim, new_dim))\n",
    "  generator = (data + torch.einsum(\"...jk->...kj\", data))/2\n",
    "  return torch.linalg.matrix_exp(1.0j*generator*t)\n",
    "\n",
    "def su_n(params, pauli_string_tensor_list):\n",
    "  # params has dim 4**n-1\n",
    "  paulis = torch.stack(pauli_string_tensor_list)\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, paulis)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "def linear_layer_func(padded_data, params, pauli_string_tensor_list, observables, n_qubits):\n",
    "  n_rep = params.shape[0]\n",
    "  state = bitstring_to_state(\"+\"*n_qubits)\n",
    "  for i in range(n_rep):\n",
    "    data_unitary = data_encode_unitary(padded_data, 1/n_rep)\n",
    "    #print(data_unitary.shape)\n",
    "    #print(state.shape)\n",
    "    state = torch.matmul(\n",
    "      data_unitary,\n",
    "      state\n",
    "    )\n",
    "    sun_gate = su_n(params[i], pauli_string_tensor_list)\n",
    "    #print(sun_gate.shape)\n",
    "    state = torch.matmul(\n",
    "      sun_gate,\n",
    "      state\n",
    "    )\n",
    "  return vmap_measure_sv(state, observables)\n",
    "\n",
    "test_params = torch.randn((3, 4**2-1),device=device).type(COMPLEX_DTYPE)\n",
    "test_data = torch.randn(4**2, device=device).type(COMPLEX_DTYPE)\n",
    "test_obs = torch.stack([torch.outer(bitstring_to_state('00'), bitstring_to_state('00')),\n",
    "                        torch.outer(bitstring_to_state('01'), bitstring_to_state('01')),\n",
    "                        torch.outer(bitstring_to_state('10'), bitstring_to_state('10')),\n",
    "                         torch.outer(bitstring_to_state('11'), bitstring_to_state('11'))])\n",
    "test_pauli_string_tensor_list = generate_pauli_tensor_list(generate_nqubit_pauli_strings(2))\n",
    "\n",
    "\n",
    "test_out = linear_layer_func(test_data, test_params, test_pauli_string_tensor_list, test_obs, 2)\n",
    "print(test_out)\n",
    "print(torch.sum(test_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:57.564844Z",
     "iopub.status.busy": "2024-04-03T08:26:57.564417Z",
     "iopub.status.idle": "2024-04-03T08:26:57.576557Z",
     "shell.execute_reply": "2024-04-03T08:26:57.575906Z",
     "shell.execute_reply.started": "2024-04-03T08:26:57.564823Z"
    },
    "id": "2F4_SBgIYnMv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3414, 0.4328, 0.0295, 0.1964],\n",
      "        [0.4175, 0.1313, 0.2182, 0.2330],\n",
      "        [0.1889, 0.5187, 0.1613, 0.1311]], device='cuda:0')\n",
      "tensor(3.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "vmap_batch_linear_layer_func = torch.vmap(linear_layer_func, in_dims=(0, None, None, None, None), out_dims=0)\n",
    "\n",
    "test_data = torch.randn((3, 4**2), device=device).type(COMPLEX_DTYPE)\n",
    "test_out = vmap_batch_linear_layer_func(test_data, test_params, test_pauli_string_tensor_list, test_obs, 2)\n",
    "print(test_out)\n",
    "print(torch.sum(test_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryROGtnC_3po"
   },
   "source": [
    "## PyTorch Module for the Quantum Version of Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:57.577462Z",
     "iopub.status.busy": "2024-04-03T08:26:57.577280Z",
     "iopub.status.idle": "2024-04-03T08:26:57.605899Z",
     "shell.execute_reply": "2024-04-03T08:26:57.605187Z",
     "shell.execute_reply.started": "2024-04-03T08:26:57.577443Z"
    },
    "id": "RlTC952w_8VW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 45])\n",
      "torch.Size([16, 6])\n",
      "DataReUploadingLinear(in_dim=45, out_dim=6, n_qubits=3, n_reps=10)\n"
     ]
    }
   ],
   "source": [
    "class DataReUploadingLinear(torch.nn.Module):\n",
    "  def __init__(self, in_dim, out_dim, n_qubits, n_reps):\n",
    "    super(DataReUploadingLinear, self).__init__()\n",
    "    assert 2**n_qubits >= out_dim\n",
    "    assert 4**n_qubits >= in_dim\n",
    "    self.in_dim = in_dim\n",
    "    self.out_dim = out_dim\n",
    "    self.n_qubits = n_qubits\n",
    "    self.n_reps = n_reps\n",
    "    self.pauli_string_tensor_list = generate_pauli_tensor_list(generate_nqubit_pauli_strings(n_qubits))\n",
    "    self.param_dim = 4**n_qubits-1\n",
    "    self.params = torch.nn.Parameter(torch.randn((self.n_reps,self.param_dim)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn(out_dim).type(REAL_DTYPE))\n",
    "    self.observables = self.generate_observables()\n",
    "    self.pad_size = 4**n_qubits-self.in_dim\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has size (batchsize, in_dim)\n",
    "    # pad x\n",
    "    x = torch.nn.functional.pad(x, (0, self.pad_size)).type(COMPLEX_DTYPE)\n",
    "    out = vmap_batch_linear_layer_func(x, self.params, self.pauli_string_tensor_list, self.observables, self.n_qubits)\n",
    "    out = out.type(REAL_DTYPE) + self.bias\n",
    "    return out\n",
    "\n",
    "  def generate_observables(self):\n",
    "    observables = []\n",
    "    for i in range(self.out_dim):\n",
    "      temp_bitstring = '{0:b}'.format(i).zfill(self.n_qubits)\n",
    "      ob = torch.outer(bitstring_to_state(temp_bitstring), bitstring_to_state(temp_bitstring))\n",
    "      observables.append(ob)\n",
    "    return torch.stack(observables)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_dim={self.in_dim}, out_dim={self.out_dim}, n_qubits={self.n_qubits}, n_reps={self.n_reps}'\n",
    "\n",
    "test_linear_module = DataReUploadingLinear(in_dim=45, out_dim=6, n_qubits=3, n_reps = 10).to(device)\n",
    "test_data = torch.randn((16, 45), device=device).type(COMPLEX_DTYPE)\n",
    "print(test_data.shape)\n",
    "test_out = test_linear_module(test_data.to(device))\n",
    "print(test_out.shape)\n",
    "print(test_linear_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yumZTjDVu63"
   },
   "source": [
    "# Replacing Classical Layers with Quantum Layers\n",
    "\n",
    "Let's replace `Conv2d` with `FlippedQuanv3x3`, and `Linear` with `QuantLinear`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:57.606924Z",
     "iopub.status.busy": "2024-04-03T08:26:57.606727Z",
     "iopub.status.idle": "2024-04-03T08:26:59.000203Z",
     "shell.execute_reply": "2024-04-03T08:26:58.999449Z",
     "shell.execute_reply.started": "2024-04-03T08:26:57.606905Z"
    },
    "id": "W4RC5ou-VzbE",
    "outputId": "9eb0b35d-ed18-431b-8a53-ba5216d3fd06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n",
      "HybridNet(\n",
      "  (layers): Sequential(\n",
      "    (0): FlippedQuanv3x3(in_channels=3, out_channels=32, stride=1, padding=None)\n",
      "    (1): FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): DataReUploadingLinear(in_dim=12544, out_dim=10, n_qubits=7, n_reps=10)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_509/4168075707.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n",
      "/tmp/ipykernel_509/1684779817.py:30: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:301.)\n",
      "  return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "class HybridNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HybridNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        FlippedQuanv3x3(in_channels=3, out_channels=32, stride=1, padding=None), # 32->30\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None), # 30->28\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        DataReUploadingLinear(16*28*28, 10, 7, 10)\n",
    "        #torch.nn.Linear(32*14*14, 10).type(REAL_DTYPE)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "\n",
    "net = HybridNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:59.001280Z",
     "iopub.status.busy": "2024-04-03T08:26:59.001080Z",
     "iopub.status.idle": "2024-04-03T17:06:02.379965Z",
     "shell.execute_reply": "2024-04-03T17:06:02.379200Z",
     "shell.execute_reply.started": "2024-04-03T08:26:59.001260Z"
    },
    "id": "x4ZY59q1WS4x",
    "outputId": "6532aea1-47a3-49f3-fe36-b774aee1905f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 250, Number of test batches = 50\n",
      "Print every train batch = 25, Print every test batch = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_509/4168075707.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at step=0, batch=0, train loss = 2.482003688812256, train acc = 0.11500000208616257, time = 1.5793821811676025\n",
      "Training at step=0, batch=25, train loss = 2.5056371688842773, train acc = 0.09000000357627869, time = 0.9443156719207764\n",
      "Training at step=0, batch=50, train loss = 2.5312769412994385, train acc = 0.06499999761581421, time = 0.9501950740814209\n",
      "Training at step=0, batch=75, train loss = 2.4974148273468018, train acc = 0.10999999940395355, time = 0.954216480255127\n",
      "Training at step=0, batch=100, train loss = 2.4977495670318604, train acc = 0.05999999865889549, time = 0.9341530799865723\n",
      "Training at step=0, batch=125, train loss = 2.4864206314086914, train acc = 0.08500000089406967, time = 0.9513311386108398\n",
      "Training at step=0, batch=150, train loss = 2.518573045730591, train acc = 0.08500000089406967, time = 0.9657611846923828\n",
      "Training at step=0, batch=175, train loss = 2.4448115825653076, train acc = 0.09000000357627869, time = 0.954348087310791\n",
      "Training at step=0, batch=200, train loss = 2.4454076290130615, train acc = 0.08500000089406967, time = 0.959580659866333\n",
      "Training at step=0, batch=225, train loss = 2.4599859714508057, train acc = 0.10499999672174454, time = 0.9625117778778076\n",
      "Testing at step=0, batch=0, test loss = 2.4198813438415527, test acc = 0.11999999731779099, time = 0.36157703399658203\n",
      "Testing at step=0, batch=5, test loss = 2.3729348182678223, test acc = 0.12999999523162842, time = 0.35395026206970215\n",
      "Testing at step=0, batch=10, test loss = 2.515612840652466, test acc = 0.04500000178813934, time = 0.3585829734802246\n",
      "Testing at step=0, batch=15, test loss = 2.411358118057251, test acc = 0.10999999940395355, time = 0.3696939945220947\n",
      "Testing at step=0, batch=20, test loss = 2.3988778591156006, test acc = 0.1550000011920929, time = 0.35257411003112793\n",
      "Testing at step=0, batch=25, test loss = 2.43394136428833, test acc = 0.11500000208616257, time = 0.3572959899902344\n",
      "Testing at step=0, batch=30, test loss = 2.4694883823394775, test acc = 0.08500000089406967, time = 0.36751651763916016\n",
      "Testing at step=0, batch=35, test loss = 2.4145820140838623, test acc = 0.10499999672174454, time = 0.35398077964782715\n",
      "Testing at step=0, batch=40, test loss = 2.4289565086364746, test acc = 0.11500000208616257, time = 0.3595879077911377\n",
      "Testing at step=0, batch=45, test loss = 2.478095293045044, test acc = 0.07999999821186066, time = 0.36161351203918457\n",
      "Step 0 finished in 264.75222635269165, Train loss = 2.4676976175308227, Test loss = 2.4440946912765504; Train Acc = 0.10000000017881393, Test Acc = 0.09999999985098838\n",
      "Training at step=1, batch=0, train loss = 2.3971002101898193, train acc = 0.11500000208616257, time = 0.9394400119781494\n",
      "Training at step=1, batch=25, train loss = 2.4058775901794434, train acc = 0.10999999940395355, time = 0.9511370658874512\n",
      "Training at step=1, batch=50, train loss = 2.4294021129608154, train acc = 0.10000000149011612, time = 0.9491384029388428\n",
      "Training at step=1, batch=75, train loss = 2.3687543869018555, train acc = 0.125, time = 0.9317367076873779\n",
      "Training at step=1, batch=100, train loss = 2.4466629028320312, train acc = 0.08500000089406967, time = 0.9380366802215576\n",
      "Training at step=1, batch=125, train loss = 2.407461643218994, train acc = 0.0949999988079071, time = 0.9399480819702148\n",
      "Training at step=1, batch=150, train loss = 2.3741326332092285, train acc = 0.125, time = 0.9355251789093018\n",
      "Training at step=1, batch=175, train loss = 2.374891519546509, train acc = 0.07999999821186066, time = 0.940415620803833\n",
      "Training at step=1, batch=200, train loss = 2.338127851486206, train acc = 0.0949999988079071, time = 0.9319720268249512\n",
      "Training at step=1, batch=225, train loss = 2.337405204772949, train acc = 0.10000000149011612, time = 0.9457602500915527\n",
      "Testing at step=1, batch=0, test loss = 2.290987014770508, test acc = 0.09000000357627869, time = 0.3716757297515869\n",
      "Testing at step=1, batch=5, test loss = 2.33966064453125, test acc = 0.09000000357627869, time = 0.3554720878601074\n",
      "Testing at step=1, batch=10, test loss = 2.303344964981079, test acc = 0.10499999672174454, time = 0.35009002685546875\n",
      "Testing at step=1, batch=15, test loss = 2.33920955657959, test acc = 0.08500000089406967, time = 0.34735560417175293\n",
      "Testing at step=1, batch=20, test loss = 2.3628830909729004, test acc = 0.07500000298023224, time = 0.35738420486450195\n",
      "Testing at step=1, batch=25, test loss = 2.2938544750213623, test acc = 0.14499999582767487, time = 0.36461496353149414\n",
      "Testing at step=1, batch=30, test loss = 2.31693959236145, test acc = 0.11999999731779099, time = 0.35480332374572754\n",
      "Testing at step=1, batch=35, test loss = 2.3454558849334717, test acc = 0.05999999865889549, time = 0.36466431617736816\n",
      "Testing at step=1, batch=40, test loss = 2.310751438140869, test acc = 0.10499999672174454, time = 0.36528778076171875\n",
      "Testing at step=1, batch=45, test loss = 2.319082021713257, test acc = 0.11500000208616257, time = 0.3572413921356201\n",
      "Step 1 finished in 263.6706483364105, Train loss = 2.396209144592285, Test loss = 2.326269655227661; Train Acc = 0.10000000007450581, Test Acc = 0.09999999962747097\n",
      "Training at step=2, batch=0, train loss = 2.266371726989746, train acc = 0.15000000596046448, time = 0.9440875053405762\n",
      "Training at step=2, batch=25, train loss = 2.296004056930542, train acc = 0.10999999940395355, time = 0.9393808841705322\n",
      "Training at step=2, batch=50, train loss = 2.3199193477630615, train acc = 0.10000000149011612, time = 0.9404287338256836\n",
      "Training at step=2, batch=75, train loss = 2.2706854343414307, train acc = 0.11500000208616257, time = 0.9359831809997559\n",
      "Training at step=2, batch=100, train loss = 2.284825325012207, train acc = 0.11999999731779099, time = 0.9419424533843994\n",
      "Training at step=2, batch=125, train loss = 2.3130877017974854, train acc = 0.13500000536441803, time = 0.9488921165466309\n",
      "Training at step=2, batch=150, train loss = 2.31557297706604, train acc = 0.10000000149011612, time = 0.9487507343292236\n",
      "Training at step=2, batch=175, train loss = 2.2918670177459717, train acc = 0.12999999523162842, time = 0.9535341262817383\n",
      "Training at step=2, batch=200, train loss = 2.239683151245117, train acc = 0.1550000011920929, time = 0.9427423477172852\n",
      "Training at step=2, batch=225, train loss = 2.2689032554626465, train acc = 0.13500000536441803, time = 0.9428260326385498\n",
      "Testing at step=2, batch=0, test loss = 2.2602577209472656, test acc = 0.1599999964237213, time = 0.35889196395874023\n",
      "Testing at step=2, batch=5, test loss = 2.2767419815063477, test acc = 0.10499999672174454, time = 0.3545238971710205\n",
      "Testing at step=2, batch=10, test loss = 2.2509899139404297, test acc = 0.125, time = 0.3524937629699707\n",
      "Testing at step=2, batch=15, test loss = 2.2172603607177734, test acc = 0.15000000596046448, time = 0.35172009468078613\n",
      "Testing at step=2, batch=20, test loss = 2.166656732559204, test acc = 0.22499999403953552, time = 0.35228419303894043\n",
      "Testing at step=2, batch=25, test loss = 2.2786240577697754, test acc = 0.125, time = 0.35279107093811035\n",
      "Testing at step=2, batch=30, test loss = 2.2840938568115234, test acc = 0.15000000596046448, time = 0.3538215160369873\n",
      "Testing at step=2, batch=35, test loss = 2.2276673316955566, test acc = 0.15000000596046448, time = 0.35033106803894043\n",
      "Testing at step=2, batch=40, test loss = 2.231388568878174, test acc = 0.1599999964237213, time = 0.3483095169067383\n",
      "Testing at step=2, batch=45, test loss = 2.216672658920288, test acc = 0.1899999976158142, time = 0.35878920555114746\n",
      "Step 2 finished in 263.26112842559814, Train loss = 2.292599104881287, Test loss = 2.244076657295227; Train Acc = 0.12148000031709671, Test Acc = 0.15500000044703482\n",
      "Training at step=3, batch=0, train loss = 2.240493059158325, train acc = 0.1550000011920929, time = 1.005474328994751\n",
      "Training at step=3, batch=25, train loss = 2.254366874694824, train acc = 0.14499999582767487, time = 0.9415292739868164\n",
      "Training at step=3, batch=50, train loss = 2.213803768157959, train acc = 0.20499999821186066, time = 0.9492177963256836\n",
      "Training at step=3, batch=75, train loss = 2.223625421524048, train acc = 0.1899999976158142, time = 0.943934440612793\n",
      "Training at step=3, batch=100, train loss = 2.159566640853882, train acc = 0.22499999403953552, time = 0.9368338584899902\n",
      "Training at step=3, batch=125, train loss = 2.1815567016601562, train acc = 0.18000000715255737, time = 0.9388344287872314\n",
      "Training at step=3, batch=150, train loss = 2.12229061126709, train acc = 0.24500000476837158, time = 0.9473879337310791\n",
      "Training at step=3, batch=175, train loss = 2.2115161418914795, train acc = 0.17000000178813934, time = 0.9458467960357666\n",
      "Training at step=3, batch=200, train loss = 2.0981221199035645, train acc = 0.25, time = 0.9383702278137207\n",
      "Training at step=3, batch=225, train loss = 2.070051670074463, train acc = 0.30000001192092896, time = 0.9425625801086426\n",
      "Testing at step=3, batch=0, test loss = 2.0277862548828125, test acc = 0.2750000059604645, time = 0.36126208305358887\n",
      "Testing at step=3, batch=5, test loss = 2.1107733249664307, test acc = 0.23499999940395355, time = 0.35646581649780273\n",
      "Testing at step=3, batch=10, test loss = 2.129338026046753, test acc = 0.23999999463558197, time = 0.35587120056152344\n",
      "Testing at step=3, batch=15, test loss = 2.10616397857666, test acc = 0.2549999952316284, time = 0.3602759838104248\n",
      "Testing at step=3, batch=20, test loss = 2.141113042831421, test acc = 0.2199999988079071, time = 0.35427403450012207\n",
      "Testing at step=3, batch=25, test loss = 2.012638568878174, test acc = 0.2549999952316284, time = 0.3619377613067627\n",
      "Testing at step=3, batch=30, test loss = 2.0862226486206055, test acc = 0.26499998569488525, time = 0.35146141052246094\n",
      "Testing at step=3, batch=35, test loss = 2.1128015518188477, test acc = 0.2800000011920929, time = 0.3625972270965576\n",
      "Testing at step=3, batch=40, test loss = 2.1246840953826904, test acc = 0.23499999940395355, time = 0.3492114543914795\n",
      "Testing at step=3, batch=45, test loss = 2.0794179439544678, test acc = 0.23499999940395355, time = 0.3495349884033203\n",
      "Step 3 finished in 263.7848906517029, Train loss = 2.1750702919960023, Test loss = 2.0936432695388794; Train Acc = 0.21095999884605407, Test Acc = 0.2496999990940094\n",
      "Training at step=4, batch=0, train loss = 2.1101410388946533, train acc = 0.23499999940395355, time = 0.9484250545501709\n",
      "Training at step=4, batch=25, train loss = 2.0955183506011963, train acc = 0.25, time = 0.945124626159668\n",
      "Training at step=4, batch=50, train loss = 2.088956832885742, train acc = 0.2549999952316284, time = 0.9539399147033691\n",
      "Training at step=4, batch=75, train loss = 2.0561373233795166, train acc = 0.2750000059604645, time = 0.9643993377685547\n",
      "Training at step=4, batch=100, train loss = 2.1482338905334473, train acc = 0.22499999403953552, time = 0.9574215412139893\n",
      "Training at step=4, batch=125, train loss = 2.1014516353607178, train acc = 0.26499998569488525, time = 0.9397549629211426\n",
      "Training at step=4, batch=150, train loss = 2.1152541637420654, train acc = 0.22499999403953552, time = 0.9522731304168701\n",
      "Training at step=4, batch=175, train loss = 2.0476458072662354, train acc = 0.20999999344348907, time = 0.9524867534637451\n",
      "Training at step=4, batch=200, train loss = 1.978918433189392, train acc = 0.28999999165534973, time = 1.0274462699890137\n",
      "Training at step=4, batch=225, train loss = 1.9855902194976807, train acc = 0.28999999165534973, time = 0.9433567523956299\n",
      "Testing at step=4, batch=0, test loss = 1.9550046920776367, test acc = 0.32499998807907104, time = 0.3505978584289551\n",
      "Testing at step=4, batch=5, test loss = 2.0075933933258057, test acc = 0.28999999165534973, time = 0.3780248165130615\n",
      "Testing at step=4, batch=10, test loss = 1.9783568382263184, test acc = 0.2849999964237213, time = 0.36313438415527344\n",
      "Testing at step=4, batch=15, test loss = 2.0368313789367676, test acc = 0.29499998688697815, time = 0.35430312156677246\n",
      "Testing at step=4, batch=20, test loss = 2.0258994102478027, test acc = 0.2549999952316284, time = 0.3668966293334961\n",
      "Testing at step=4, batch=25, test loss = 2.0181195735931396, test acc = 0.26499998569488525, time = 0.3642549514770508\n",
      "Testing at step=4, batch=30, test loss = 2.0028820037841797, test acc = 0.27000001072883606, time = 0.36103105545043945\n",
      "Testing at step=4, batch=35, test loss = 1.9844857454299927, test acc = 0.26499998569488525, time = 0.35770082473754883\n",
      "Testing at step=4, batch=40, test loss = 1.8885000944137573, test acc = 0.3700000047683716, time = 0.3684499263763428\n",
      "Testing at step=4, batch=45, test loss = 2.021162271499634, test acc = 0.23499999940395355, time = 0.36429357528686523\n",
      "Step 4 finished in 264.0896739959717, Train loss = 2.038031951904297, Test loss = 1.988630039691925; Train Acc = 0.26919999837875364, Test Acc = 0.2932999995350838\n",
      "Training at step=5, batch=0, train loss = 2.020305633544922, train acc = 0.23999999463558197, time = 0.9456851482391357\n",
      "Training at step=5, batch=25, train loss = 2.0620615482330322, train acc = 0.27000001072883606, time = 0.9399914741516113\n",
      "Training at step=5, batch=50, train loss = 1.984819769859314, train acc = 0.2849999964237213, time = 0.9368979930877686\n",
      "Training at step=5, batch=75, train loss = 1.971153736114502, train acc = 0.3100000023841858, time = 0.9432613849639893\n",
      "Training at step=5, batch=100, train loss = 2.0263686180114746, train acc = 0.2849999964237213, time = 0.9411089420318604\n",
      "Training at step=5, batch=125, train loss = 1.9022095203399658, train acc = 0.3100000023841858, time = 0.9501516819000244\n",
      "Training at step=5, batch=150, train loss = 1.9969754219055176, train acc = 0.32499998807907104, time = 0.9656085968017578\n",
      "Training at step=5, batch=175, train loss = 2.0096468925476074, train acc = 0.3149999976158142, time = 0.9536676406860352\n",
      "Training at step=5, batch=200, train loss = 1.9710408449172974, train acc = 0.3149999976158142, time = 0.9601655006408691\n",
      "Training at step=5, batch=225, train loss = 1.9071764945983887, train acc = 0.29499998688697815, time = 0.9412932395935059\n",
      "Testing at step=5, batch=0, test loss = 2.053877830505371, test acc = 0.2800000011920929, time = 0.3507266044616699\n",
      "Testing at step=5, batch=5, test loss = 1.9681897163391113, test acc = 0.2849999964237213, time = 0.34861063957214355\n",
      "Testing at step=5, batch=10, test loss = 2.0025529861450195, test acc = 0.33000001311302185, time = 0.3596527576446533\n",
      "Testing at step=5, batch=15, test loss = 1.9284454584121704, test acc = 0.28999999165534973, time = 0.35523414611816406\n",
      "Testing at step=5, batch=20, test loss = 2.030045509338379, test acc = 0.22499999403953552, time = 0.35689663887023926\n",
      "Testing at step=5, batch=25, test loss = 1.935867428779602, test acc = 0.29499998688697815, time = 0.355334997177124\n",
      "Testing at step=5, batch=30, test loss = 1.9908514022827148, test acc = 0.29499998688697815, time = 0.3601961135864258\n",
      "Testing at step=5, batch=35, test loss = 1.9044886827468872, test acc = 0.33500000834465027, time = 0.35642313957214355\n",
      "Testing at step=5, batch=40, test loss = 1.8485773801803589, test acc = 0.36500000953674316, time = 0.36478352546691895\n",
      "Testing at step=5, batch=45, test loss = 1.9718328714370728, test acc = 0.3100000023841858, time = 0.3556697368621826\n",
      "Step 5 finished in 263.978440284729, Train loss = 1.9692030191421508, Test loss = 1.9540439677238464; Train Acc = 0.2993399980664253, Test Acc = 0.3\n",
      "Training at step=6, batch=0, train loss = 1.9080703258514404, train acc = 0.3100000023841858, time = 0.9389197826385498\n",
      "Training at step=6, batch=25, train loss = 1.8876930475234985, train acc = 0.3449999988079071, time = 0.9396193027496338\n",
      "Training at step=6, batch=50, train loss = 2.0505852699279785, train acc = 0.23999999463558197, time = 0.9429566860198975\n",
      "Training at step=6, batch=75, train loss = 1.9127615690231323, train acc = 0.3499999940395355, time = 0.9432339668273926\n",
      "Training at step=6, batch=100, train loss = 1.8905390501022339, train acc = 0.3449999988079071, time = 0.9378092288970947\n",
      "Training at step=6, batch=125, train loss = 1.8590797185897827, train acc = 0.3449999988079071, time = 0.9481451511383057\n",
      "Training at step=6, batch=150, train loss = 1.983060598373413, train acc = 0.33000001311302185, time = 0.9506494998931885\n",
      "Training at step=6, batch=175, train loss = 1.9155845642089844, train acc = 0.30000001192092896, time = 0.943840503692627\n",
      "Training at step=6, batch=200, train loss = 1.867435336112976, train acc = 0.2800000011920929, time = 0.9562256336212158\n",
      "Training at step=6, batch=225, train loss = 1.9456312656402588, train acc = 0.3050000071525574, time = 0.9576823711395264\n",
      "Testing at step=6, batch=0, test loss = 1.9845664501190186, test acc = 0.27000001072883606, time = 0.36203932762145996\n",
      "Testing at step=6, batch=5, test loss = 1.937516689300537, test acc = 0.2849999964237213, time = 0.3628709316253662\n",
      "Testing at step=6, batch=10, test loss = 1.8974031209945679, test acc = 0.33500000834465027, time = 0.3688223361968994\n",
      "Testing at step=6, batch=15, test loss = 1.8925029039382935, test acc = 0.32499998807907104, time = 0.3714265823364258\n",
      "Testing at step=6, batch=20, test loss = 1.9188495874404907, test acc = 0.38999998569488525, time = 0.3569648265838623\n",
      "Testing at step=6, batch=25, test loss = 1.9088183641433716, test acc = 0.3199999928474426, time = 0.3688316345214844\n",
      "Testing at step=6, batch=30, test loss = 1.9823524951934814, test acc = 0.2849999964237213, time = 0.36061525344848633\n",
      "Testing at step=6, batch=35, test loss = 1.987200140953064, test acc = 0.30000001192092896, time = 0.3542442321777344\n",
      "Testing at step=6, batch=40, test loss = 1.8968915939331055, test acc = 0.3050000071525574, time = 0.35613250732421875\n",
      "Testing at step=6, batch=45, test loss = 1.9189825057983398, test acc = 0.29499998688697815, time = 0.35530662536621094\n",
      "Step 6 finished in 265.0530478954315, Train loss = 1.9259148144721985, Test loss = 1.9117099833488465; Train Acc = 0.31546000003814695, Test Acc = 0.3225\n",
      "Training at step=7, batch=0, train loss = 1.9210827350616455, train acc = 0.29499998688697815, time = 0.9485814571380615\n",
      "Training at step=7, batch=25, train loss = 1.799135684967041, train acc = 0.32499998807907104, time = 1.0377216339111328\n",
      "Training at step=7, batch=50, train loss = 1.8764829635620117, train acc = 0.36500000953674316, time = 0.939589262008667\n",
      "Training at step=7, batch=75, train loss = 1.9971497058868408, train acc = 0.28999999165534973, time = 0.9423465728759766\n",
      "Training at step=7, batch=100, train loss = 1.8818317651748657, train acc = 0.3149999976158142, time = 0.937748908996582\n",
      "Training at step=7, batch=125, train loss = 1.8877862691879272, train acc = 0.3799999952316284, time = 1.0067331790924072\n",
      "Training at step=7, batch=150, train loss = 1.9225146770477295, train acc = 0.3050000071525574, time = 0.9425849914550781\n",
      "Training at step=7, batch=175, train loss = 1.784774661064148, train acc = 0.38499999046325684, time = 0.949561357498169\n",
      "Training at step=7, batch=200, train loss = 1.9491676092147827, train acc = 0.3100000023841858, time = 0.9412767887115479\n",
      "Training at step=7, batch=225, train loss = 1.8184154033660889, train acc = 0.3499999940395355, time = 0.9615168571472168\n",
      "Testing at step=7, batch=0, test loss = 1.9405391216278076, test acc = 0.3050000071525574, time = 0.3544631004333496\n",
      "Testing at step=7, batch=5, test loss = 1.9360791444778442, test acc = 0.3400000035762787, time = 0.3671083450317383\n",
      "Testing at step=7, batch=10, test loss = 1.8719966411590576, test acc = 0.3449999988079071, time = 0.3830239772796631\n",
      "Testing at step=7, batch=15, test loss = 1.841088891029358, test acc = 0.36500000953674316, time = 0.3607752323150635\n",
      "Testing at step=7, batch=20, test loss = 1.9092174768447876, test acc = 0.3050000071525574, time = 0.35642147064208984\n",
      "Testing at step=7, batch=25, test loss = 1.7960249185562134, test acc = 0.3499999940395355, time = 0.35819315910339355\n",
      "Testing at step=7, batch=30, test loss = 1.8114393949508667, test acc = 0.4000000059604645, time = 0.37215471267700195\n",
      "Testing at step=7, batch=35, test loss = 1.8312655687332153, test acc = 0.35499998927116394, time = 0.3682413101196289\n",
      "Testing at step=7, batch=40, test loss = 1.896918773651123, test acc = 0.3149999976158142, time = 0.3527646064758301\n",
      "Testing at step=7, batch=45, test loss = 1.8347140550613403, test acc = 0.36000001430511475, time = 0.3698239326477051\n",
      "Step 7 finished in 265.6194052696228, Train loss = 1.8920394682884216, Test loss = 1.8763657236099243; Train Acc = 0.3296199991703033, Test Acc = 0.3342000013589859\n",
      "Training at step=8, batch=0, train loss = 1.8845847845077515, train acc = 0.35499998927116394, time = 0.9632344245910645\n",
      "Training at step=8, batch=25, train loss = 2.0070769786834717, train acc = 0.30000001192092896, time = 0.9423747062683105\n",
      "Training at step=8, batch=50, train loss = 1.8904962539672852, train acc = 0.30000001192092896, time = 0.9465720653533936\n",
      "Training at step=8, batch=75, train loss = 1.8674514293670654, train acc = 0.35499998927116394, time = 0.9415714740753174\n",
      "Training at step=8, batch=100, train loss = 1.789526343345642, train acc = 0.39500001072883606, time = 0.9454283714294434\n",
      "Training at step=8, batch=125, train loss = 1.8769891262054443, train acc = 0.3449999988079071, time = 0.9494860172271729\n",
      "Training at step=8, batch=150, train loss = 1.9154490232467651, train acc = 0.33000001311302185, time = 0.9518682956695557\n",
      "Training at step=8, batch=175, train loss = 1.7611066102981567, train acc = 0.4000000059604645, time = 0.9434969425201416\n",
      "Training at step=8, batch=200, train loss = 1.7828694581985474, train acc = 0.36000001430511475, time = 0.9385452270507812\n",
      "Training at step=8, batch=225, train loss = 1.8882142305374146, train acc = 0.3050000071525574, time = 0.9453098773956299\n",
      "Testing at step=8, batch=0, test loss = 1.8251540660858154, test acc = 0.3449999988079071, time = 0.34869885444641113\n",
      "Testing at step=8, batch=5, test loss = 1.8712801933288574, test acc = 0.3799999952316284, time = 0.35790014266967773\n",
      "Testing at step=8, batch=10, test loss = 1.757340431213379, test acc = 0.4099999964237213, time = 0.35041165351867676\n",
      "Testing at step=8, batch=15, test loss = 1.9564610719680786, test acc = 0.33000001311302185, time = 0.34744930267333984\n",
      "Testing at step=8, batch=20, test loss = 1.776796579360962, test acc = 0.3799999952316284, time = 0.34794187545776367\n",
      "Testing at step=8, batch=25, test loss = 1.8663312196731567, test acc = 0.3499999940395355, time = 0.3728771209716797\n",
      "Testing at step=8, batch=30, test loss = 1.8167288303375244, test acc = 0.33000001311302185, time = 0.35037899017333984\n",
      "Testing at step=8, batch=35, test loss = 1.9692862033843994, test acc = 0.26499998569488525, time = 0.35520076751708984\n",
      "Testing at step=8, batch=40, test loss = 1.9229081869125366, test acc = 0.3199999928474426, time = 0.36509227752685547\n",
      "Testing at step=8, batch=45, test loss = 2.0029923915863037, test acc = 0.3100000023841858, time = 0.3655269145965576\n",
      "Step 8 finished in 264.3163616657257, Train loss = 1.858884045124054, Test loss = 1.8728119564056396; Train Acc = 0.33968000042438506, Test Acc = 0.3375999999046326\n",
      "Training at step=9, batch=0, train loss = 1.8691527843475342, train acc = 0.33500000834465027, time = 0.9502542018890381\n",
      "Training at step=9, batch=25, train loss = 1.8624769449234009, train acc = 0.3400000035762787, time = 0.9372580051422119\n",
      "Training at step=9, batch=50, train loss = 1.8378844261169434, train acc = 0.33000001311302185, time = 0.9528374671936035\n",
      "Training at step=9, batch=75, train loss = 1.8421218395233154, train acc = 0.3149999976158142, time = 0.9400959014892578\n",
      "Training at step=9, batch=100, train loss = 1.8726807832717896, train acc = 0.3199999928474426, time = 0.9368288516998291\n",
      "Training at step=9, batch=125, train loss = 1.8142906427383423, train acc = 0.32499998807907104, time = 0.9394567012786865\n",
      "Training at step=9, batch=150, train loss = 1.820370078086853, train acc = 0.33500000834465027, time = 0.9519407749176025\n",
      "Training at step=9, batch=175, train loss = 1.7515056133270264, train acc = 0.4300000071525574, time = 0.9489624500274658\n",
      "Training at step=9, batch=200, train loss = 1.8308978080749512, train acc = 0.39500001072883606, time = 0.9514756202697754\n",
      "Training at step=9, batch=225, train loss = 1.8898106813430786, train acc = 0.32499998807907104, time = 0.947474479675293\n",
      "Testing at step=9, batch=0, test loss = 1.8554768562316895, test acc = 0.29499998688697815, time = 0.3540980815887451\n",
      "Testing at step=9, batch=5, test loss = 1.8543685674667358, test acc = 0.3499999940395355, time = 0.35417771339416504\n",
      "Testing at step=9, batch=10, test loss = 1.8553509712219238, test acc = 0.36000001430511475, time = 0.35185837745666504\n",
      "Testing at step=9, batch=15, test loss = 1.8663129806518555, test acc = 0.38499999046325684, time = 0.3566875457763672\n",
      "Testing at step=9, batch=20, test loss = 1.784056544303894, test acc = 0.375, time = 0.35846924781799316\n",
      "Testing at step=9, batch=25, test loss = 1.7593705654144287, test acc = 0.39500001072883606, time = 0.34787821769714355\n",
      "Testing at step=9, batch=30, test loss = 1.87776517868042, test acc = 0.3199999928474426, time = 0.35389280319213867\n",
      "Testing at step=9, batch=35, test loss = 1.760701298713684, test acc = 0.38999998569488525, time = 0.35217714309692383\n",
      "Testing at step=9, batch=40, test loss = 1.7425950765609741, test acc = 0.4050000011920929, time = 0.3567237854003906\n",
      "Testing at step=9, batch=45, test loss = 1.8220610618591309, test acc = 0.3700000047683716, time = 0.3495216369628906\n",
      "Step 9 finished in 263.9021027088165, Train loss = 1.8335071716308593, Test loss = 1.8143751001358033; Train Acc = 0.3488000003695488, Test Acc = 0.36359999895095824\n",
      "Training at step=10, batch=0, train loss = 1.7508413791656494, train acc = 0.3499999940395355, time = 0.9396231174468994\n",
      "Training at step=10, batch=25, train loss = 1.8969541788101196, train acc = 0.30000001192092896, time = 0.9539952278137207\n",
      "Training at step=10, batch=50, train loss = 1.8656392097473145, train acc = 0.3700000047683716, time = 0.9611470699310303\n",
      "Training at step=10, batch=75, train loss = 1.7778046131134033, train acc = 0.35499998927116394, time = 0.940178632736206\n",
      "Training at step=10, batch=100, train loss = 1.773421049118042, train acc = 0.36500000953674316, time = 0.9447135925292969\n",
      "Training at step=10, batch=125, train loss = 1.8010284900665283, train acc = 0.32499998807907104, time = 0.9775078296661377\n",
      "Training at step=10, batch=150, train loss = 1.8182721138000488, train acc = 0.3499999940395355, time = 0.9468438625335693\n",
      "Training at step=10, batch=175, train loss = 1.7851202487945557, train acc = 0.3449999988079071, time = 0.9422504901885986\n",
      "Training at step=10, batch=200, train loss = 1.7152289152145386, train acc = 0.4099999964237213, time = 0.9408836364746094\n",
      "Training at step=10, batch=225, train loss = 1.7485709190368652, train acc = 0.4099999964237213, time = 0.9431860446929932\n",
      "Testing at step=10, batch=0, test loss = 1.7277165651321411, test acc = 0.41999998688697815, time = 0.34991979598999023\n",
      "Testing at step=10, batch=5, test loss = 1.841455101966858, test acc = 0.33000001311302185, time = 0.3646688461303711\n",
      "Testing at step=10, batch=10, test loss = 1.8044800758361816, test acc = 0.3799999952316284, time = 0.3515176773071289\n",
      "Testing at step=10, batch=15, test loss = 1.8327925205230713, test acc = 0.36500000953674316, time = 0.3486785888671875\n",
      "Testing at step=10, batch=20, test loss = 1.847288966178894, test acc = 0.35499998927116394, time = 0.36072492599487305\n",
      "Testing at step=10, batch=25, test loss = 1.675628662109375, test acc = 0.4300000071525574, time = 0.36124134063720703\n",
      "Testing at step=10, batch=30, test loss = 1.8048977851867676, test acc = 0.3499999940395355, time = 0.3598616123199463\n",
      "Testing at step=10, batch=35, test loss = 1.7968394756317139, test acc = 0.3199999928474426, time = 0.3525400161743164\n",
      "Testing at step=10, batch=40, test loss = 1.76080322265625, test acc = 0.38999998569488525, time = 0.35476016998291016\n",
      "Testing at step=10, batch=45, test loss = 1.8153965473175049, test acc = 0.3449999988079071, time = 0.36261558532714844\n",
      "Step 10 finished in 263.734210729599, Train loss = 1.8051041378974915, Test loss = 1.7837292957305908; Train Acc = 0.36003999948501586, Test Acc = 0.3698000001907349\n",
      "Training at step=11, batch=0, train loss = 1.7862434387207031, train acc = 0.3400000035762787, time = 0.9517619609832764\n",
      "Training at step=11, batch=25, train loss = 1.7849297523498535, train acc = 0.33500000834465027, time = 0.9431562423706055\n",
      "Training at step=11, batch=50, train loss = 1.8136178255081177, train acc = 0.36000001430511475, time = 1.0659377574920654\n",
      "Training at step=11, batch=75, train loss = 1.681566596031189, train acc = 0.4300000071525574, time = 0.943181037902832\n",
      "Training at step=11, batch=100, train loss = 1.7378981113433838, train acc = 0.39500001072883606, time = 0.9547202587127686\n",
      "Training at step=11, batch=125, train loss = 1.7741334438323975, train acc = 0.36000001430511475, time = 0.9505736827850342\n",
      "Training at step=11, batch=150, train loss = 1.8459181785583496, train acc = 0.3700000047683716, time = 1.0089800357818604\n",
      "Training at step=11, batch=175, train loss = 1.8086280822753906, train acc = 0.4050000011920929, time = 0.9431536197662354\n",
      "Training at step=11, batch=200, train loss = 1.8752025365829468, train acc = 0.35499998927116394, time = 0.9502665996551514\n",
      "Training at step=11, batch=225, train loss = 1.8097586631774902, train acc = 0.3700000047683716, time = 0.9424922466278076\n",
      "Testing at step=11, batch=0, test loss = 1.7798913717269897, test acc = 0.38499999046325684, time = 0.3593144416809082\n",
      "Testing at step=11, batch=5, test loss = 1.866304874420166, test acc = 0.35499998927116394, time = 0.36885976791381836\n",
      "Testing at step=11, batch=10, test loss = 1.7809268236160278, test acc = 0.3499999940395355, time = 0.35102105140686035\n",
      "Testing at step=11, batch=15, test loss = 1.8385766744613647, test acc = 0.3149999976158142, time = 0.36545276641845703\n",
      "Testing at step=11, batch=20, test loss = 1.6762778759002686, test acc = 0.4350000023841858, time = 0.3484528064727783\n",
      "Testing at step=11, batch=25, test loss = 1.674443244934082, test acc = 0.4050000011920929, time = 0.35155272483825684\n",
      "Testing at step=11, batch=30, test loss = 1.6697306632995605, test acc = 0.41499999165534973, time = 0.36461949348449707\n",
      "Testing at step=11, batch=35, test loss = 1.683005928993225, test acc = 0.4300000071525574, time = 0.35540103912353516\n",
      "Testing at step=11, batch=40, test loss = 1.859554648399353, test acc = 0.33000001311302185, time = 0.3478562831878662\n",
      "Testing at step=11, batch=45, test loss = 1.7820324897766113, test acc = 0.4000000059604645, time = 0.3639254570007324\n",
      "Step 11 finished in 263.6204640865326, Train loss = 1.7797206363677978, Test loss = 1.7950274515151978; Train Acc = 0.36890000104904175, Test Acc = 0.3681000006198883\n",
      "Training at step=12, batch=0, train loss = 1.8634397983551025, train acc = 0.3449999988079071, time = 0.9432022571563721\n",
      "Training at step=12, batch=25, train loss = 1.8031105995178223, train acc = 0.3400000035762787, time = 0.9442217350006104\n",
      "Training at step=12, batch=50, train loss = 1.7934449911117554, train acc = 0.36500000953674316, time = 0.9521417617797852\n",
      "Training at step=12, batch=75, train loss = 1.7507967948913574, train acc = 0.3499999940395355, time = 0.945502758026123\n",
      "Training at step=12, batch=100, train loss = 1.8192218542099, train acc = 0.32499998807907104, time = 0.9374656677246094\n",
      "Training at step=12, batch=125, train loss = 1.7442924976348877, train acc = 0.375, time = 0.9356904029846191\n",
      "Training at step=12, batch=150, train loss = 1.7156176567077637, train acc = 0.41999998688697815, time = 0.9405136108398438\n",
      "Training at step=12, batch=175, train loss = 1.7646883726119995, train acc = 0.36000001430511475, time = 0.9396154880523682\n",
      "Training at step=12, batch=200, train loss = 1.627943754196167, train acc = 0.4050000011920929, time = 0.9453730583190918\n",
      "Training at step=12, batch=225, train loss = 1.7622283697128296, train acc = 0.38999998569488525, time = 0.9436919689178467\n",
      "Testing at step=12, batch=0, test loss = 1.7449157238006592, test acc = 0.3050000071525574, time = 0.52663254737854\n",
      "Testing at step=12, batch=5, test loss = 1.706235647201538, test acc = 0.4000000059604645, time = 0.3516535758972168\n",
      "Testing at step=12, batch=10, test loss = 1.7942805290222168, test acc = 0.3499999940395355, time = 0.3654768466949463\n",
      "Testing at step=12, batch=15, test loss = 1.7013369798660278, test acc = 0.3799999952316284, time = 0.36740899085998535\n",
      "Testing at step=12, batch=20, test loss = 1.6999698877334595, test acc = 0.39500001072883606, time = 0.371488094329834\n",
      "Testing at step=12, batch=25, test loss = 1.7937082052230835, test acc = 0.3799999952316284, time = 0.3663344383239746\n",
      "Testing at step=12, batch=30, test loss = 1.652033805847168, test acc = 0.4050000011920929, time = 0.36282992362976074\n",
      "Testing at step=12, batch=35, test loss = 1.7076127529144287, test acc = 0.32499998807907104, time = 0.35561108589172363\n",
      "Testing at step=12, batch=40, test loss = 1.7846157550811768, test acc = 0.39500001072883606, time = 0.36817359924316406\n",
      "Testing at step=12, batch=45, test loss = 1.7650728225708008, test acc = 0.4050000011920929, time = 0.37502264976501465\n",
      "Step 12 finished in 263.9546048641205, Train loss = 1.7643143520355224, Test loss = 1.7450505447387696; Train Acc = 0.3752600008249283, Test Acc = 0.3774000006914139\n",
      "Training at step=13, batch=0, train loss = 1.6809440851211548, train acc = 0.41499999165534973, time = 0.955132246017456\n",
      "Training at step=13, batch=25, train loss = 1.6717852354049683, train acc = 0.4300000071525574, time = 0.947845458984375\n",
      "Training at step=13, batch=50, train loss = 1.7237571477890015, train acc = 0.4099999964237213, time = 0.950178861618042\n",
      "Training at step=13, batch=75, train loss = 1.8217653036117554, train acc = 0.375, time = 0.9485640525817871\n",
      "Training at step=13, batch=100, train loss = 1.658603549003601, train acc = 0.38499999046325684, time = 0.9412946701049805\n",
      "Training at step=13, batch=125, train loss = 1.8311160802841187, train acc = 0.3400000035762787, time = 0.9507300853729248\n",
      "Training at step=13, batch=150, train loss = 1.7200188636779785, train acc = 0.42500001192092896, time = 0.9461367130279541\n",
      "Training at step=13, batch=175, train loss = 1.6719225645065308, train acc = 0.4050000011920929, time = 0.9440350532531738\n",
      "Training at step=13, batch=200, train loss = 1.7164406776428223, train acc = 0.46000000834465027, time = 0.9508204460144043\n",
      "Training at step=13, batch=225, train loss = 1.724525809288025, train acc = 0.3799999952316284, time = 0.9328324794769287\n",
      "Testing at step=13, batch=0, test loss = 1.7512071132659912, test acc = 0.375, time = 0.35837626457214355\n",
      "Testing at step=13, batch=5, test loss = 1.7308564186096191, test acc = 0.38999998569488525, time = 0.36606764793395996\n",
      "Testing at step=13, batch=10, test loss = 1.7487658262252808, test acc = 0.41999998688697815, time = 0.35279130935668945\n",
      "Testing at step=13, batch=15, test loss = 1.831830620765686, test acc = 0.33500000834465027, time = 0.3526930809020996\n",
      "Testing at step=13, batch=20, test loss = 1.697494626045227, test acc = 0.41499999165534973, time = 0.3503074645996094\n",
      "Testing at step=13, batch=25, test loss = 1.7984083890914917, test acc = 0.35499998927116394, time = 0.3466639518737793\n",
      "Testing at step=13, batch=30, test loss = 1.763537883758545, test acc = 0.38999998569488525, time = 0.34841227531433105\n",
      "Testing at step=13, batch=35, test loss = 1.7974138259887695, test acc = 0.3700000047683716, time = 0.35178041458129883\n",
      "Testing at step=13, batch=40, test loss = 1.865403652191162, test acc = 0.35499998927116394, time = 0.3513469696044922\n",
      "Testing at step=13, batch=45, test loss = 1.753705620765686, test acc = 0.4350000023841858, time = 0.3605763912200928\n",
      "Step 13 finished in 263.77495670318604, Train loss = 1.7487258257865905, Test loss = 1.7834693741798402; Train Acc = 0.38106000059843065, Test Acc = 0.36839999973773957\n",
      "Training at step=14, batch=0, train loss = 1.7611613273620605, train acc = 0.41999998688697815, time = 0.9419870376586914\n",
      "Training at step=14, batch=25, train loss = 1.71881103515625, train acc = 0.38499999046325684, time = 0.9465928077697754\n",
      "Training at step=14, batch=50, train loss = 1.6729910373687744, train acc = 0.42500001192092896, time = 0.9354851245880127\n",
      "Training at step=14, batch=75, train loss = 1.6745747327804565, train acc = 0.4050000011920929, time = 0.9553816318511963\n",
      "Training at step=14, batch=100, train loss = 1.7935662269592285, train acc = 0.3700000047683716, time = 0.9602105617523193\n",
      "Training at step=14, batch=125, train loss = 1.6580880880355835, train acc = 0.4300000071525574, time = 0.941504716873169\n",
      "Training at step=14, batch=150, train loss = 1.7566022872924805, train acc = 0.3400000035762787, time = 0.9497482776641846\n",
      "Training at step=14, batch=175, train loss = 1.7607461214065552, train acc = 0.3700000047683716, time = 0.9413979053497314\n",
      "Training at step=14, batch=200, train loss = 1.6986057758331299, train acc = 0.41499999165534973, time = 0.9439985752105713\n",
      "Training at step=14, batch=225, train loss = 1.836276650428772, train acc = 0.3700000047683716, time = 0.9461891651153564\n",
      "Testing at step=14, batch=0, test loss = 1.7277276515960693, test acc = 0.4050000011920929, time = 0.3532860279083252\n",
      "Testing at step=14, batch=5, test loss = 1.7354298830032349, test acc = 0.36500000953674316, time = 0.3648509979248047\n",
      "Testing at step=14, batch=10, test loss = 1.7548829317092896, test acc = 0.36000001430511475, time = 0.35236239433288574\n",
      "Testing at step=14, batch=15, test loss = 1.856697678565979, test acc = 0.3449999988079071, time = 0.3569066524505615\n",
      "Testing at step=14, batch=20, test loss = 1.6231480836868286, test acc = 0.41499999165534973, time = 0.3560147285461426\n",
      "Testing at step=14, batch=25, test loss = 1.6392062902450562, test acc = 0.4699999988079071, time = 0.3518834114074707\n",
      "Testing at step=14, batch=30, test loss = 1.7528423070907593, test acc = 0.3700000047683716, time = 0.3510768413543701\n",
      "Testing at step=14, batch=35, test loss = 1.7056198120117188, test acc = 0.41499999165534973, time = 0.3502938747406006\n",
      "Testing at step=14, batch=40, test loss = 1.7387762069702148, test acc = 0.4050000011920929, time = 0.3732106685638428\n",
      "Testing at step=14, batch=45, test loss = 1.750266671180725, test acc = 0.38999998569488525, time = 0.34375643730163574\n",
      "Step 14 finished in 263.5457730293274, Train loss = 1.7367006692886353, Test loss = 1.7378918051719665; Train Acc = 0.3838199999332428, Test Acc = 0.3818999981880188\n",
      "Training at step=15, batch=0, train loss = 1.6060950756072998, train acc = 0.41999998688697815, time = 0.9500763416290283\n",
      "Training at step=15, batch=25, train loss = 1.7021507024765015, train acc = 0.38999998569488525, time = 0.9395081996917725\n",
      "Training at step=15, batch=50, train loss = 1.7163474559783936, train acc = 0.39500001072883606, time = 0.9427106380462646\n",
      "Training at step=15, batch=75, train loss = 1.638403058052063, train acc = 0.39500001072883606, time = 0.9341869354248047\n",
      "Training at step=15, batch=100, train loss = 1.6282695531845093, train acc = 0.41499999165534973, time = 0.9486541748046875\n",
      "Training at step=15, batch=125, train loss = 1.7089240550994873, train acc = 0.38499999046325684, time = 0.9538874626159668\n",
      "Training at step=15, batch=150, train loss = 1.6614428758621216, train acc = 0.42500001192092896, time = 0.951725959777832\n",
      "Training at step=15, batch=175, train loss = 1.778149127960205, train acc = 0.3400000035762787, time = 0.9550466537475586\n",
      "Training at step=15, batch=200, train loss = 1.6704270839691162, train acc = 0.4050000011920929, time = 0.9497132301330566\n",
      "Training at step=15, batch=225, train loss = 1.6533998250961304, train acc = 0.38999998569488525, time = 0.9403262138366699\n",
      "Testing at step=15, batch=0, test loss = 1.8001527786254883, test acc = 0.3799999952316284, time = 0.3559303283691406\n",
      "Testing at step=15, batch=5, test loss = 1.6588777303695679, test acc = 0.41499999165534973, time = 0.35588932037353516\n",
      "Testing at step=15, batch=10, test loss = 1.7591102123260498, test acc = 0.3499999940395355, time = 0.35585832595825195\n",
      "Testing at step=15, batch=15, test loss = 1.9297113418579102, test acc = 0.375, time = 0.3547508716583252\n",
      "Testing at step=15, batch=20, test loss = 1.7382694482803345, test acc = 0.38999998569488525, time = 0.35134148597717285\n",
      "Testing at step=15, batch=25, test loss = 1.6690869331359863, test acc = 0.42500001192092896, time = 0.35820698738098145\n",
      "Testing at step=15, batch=30, test loss = 1.778633713722229, test acc = 0.3400000035762787, time = 0.35369014739990234\n",
      "Testing at step=15, batch=35, test loss = 1.7556109428405762, test acc = 0.4000000059604645, time = 0.35175418853759766\n",
      "Testing at step=15, batch=40, test loss = 1.7686476707458496, test acc = 0.38999998569488525, time = 0.35294437408447266\n",
      "Testing at step=15, batch=45, test loss = 1.7046711444854736, test acc = 0.3700000047683716, time = 0.3523843288421631\n",
      "Step 15 finished in 263.6799352169037, Train loss = 1.7138587470054627, Test loss = 1.716925482749939; Train Acc = 0.39251999962329864, Test Acc = 0.38790000081062315\n",
      "Training at step=16, batch=0, train loss = 1.6946535110473633, train acc = 0.4300000071525574, time = 0.9457252025604248\n",
      "Training at step=16, batch=25, train loss = 1.7832746505737305, train acc = 0.33500000834465027, time = 0.9511551856994629\n",
      "Training at step=16, batch=50, train loss = 1.6564216613769531, train acc = 0.3799999952316284, time = 0.9397878646850586\n",
      "Training at step=16, batch=75, train loss = 1.6294796466827393, train acc = 0.42500001192092896, time = 0.9871432781219482\n",
      "Training at step=16, batch=100, train loss = 1.689133644104004, train acc = 0.4000000059604645, time = 0.9519460201263428\n",
      "Training at step=16, batch=125, train loss = 1.784967064857483, train acc = 0.3499999940395355, time = 0.9430053234100342\n",
      "Training at step=16, batch=150, train loss = 1.6343175172805786, train acc = 0.38999998569488525, time = 0.951056718826294\n",
      "Training at step=16, batch=175, train loss = 1.7642954587936401, train acc = 0.3449999988079071, time = 0.9687392711639404\n",
      "Training at step=16, batch=200, train loss = 1.7791575193405151, train acc = 0.33000001311302185, time = 0.9465973377227783\n",
      "Training at step=16, batch=225, train loss = 1.6263684034347534, train acc = 0.45500001311302185, time = 0.9411256313323975\n",
      "Testing at step=16, batch=0, test loss = 1.6724541187286377, test acc = 0.39500001072883606, time = 0.3518526554107666\n",
      "Testing at step=16, batch=5, test loss = 1.751583218574524, test acc = 0.3499999940395355, time = 0.358184814453125\n",
      "Testing at step=16, batch=10, test loss = 1.6795027256011963, test acc = 0.39500001072883606, time = 0.34859347343444824\n",
      "Testing at step=16, batch=15, test loss = 1.5402100086212158, test acc = 0.4350000023841858, time = 0.35007667541503906\n",
      "Testing at step=16, batch=20, test loss = 1.7048717737197876, test acc = 0.3799999952316284, time = 0.35464048385620117\n",
      "Testing at step=16, batch=25, test loss = 1.764082908630371, test acc = 0.42500001192092896, time = 0.3557124137878418\n",
      "Testing at step=16, batch=30, test loss = 1.6458075046539307, test acc = 0.3799999952316284, time = 0.34978485107421875\n",
      "Testing at step=16, batch=35, test loss = 1.6841984987258911, test acc = 0.39500001072883606, time = 0.35881948471069336\n",
      "Testing at step=16, batch=40, test loss = 1.7057647705078125, test acc = 0.35499998927116394, time = 0.34633660316467285\n",
      "Testing at step=16, batch=45, test loss = 1.756066918373108, test acc = 0.38499999046325684, time = 0.3607301712036133\n",
      "Step 16 finished in 263.31865668296814, Train loss = 1.7005628995895385, Test loss = 1.7110478329658507; Train Acc = 0.3981399995088577, Test Acc = 0.3916999989748001\n",
      "Training at step=17, batch=0, train loss = 1.7258039712905884, train acc = 0.41499999165534973, time = 0.9513914585113525\n",
      "Training at step=17, batch=25, train loss = 1.7476310729980469, train acc = 0.38999998569488525, time = 0.9435689449310303\n",
      "Training at step=17, batch=50, train loss = 1.766579270362854, train acc = 0.3400000035762787, time = 0.9746768474578857\n",
      "Training at step=17, batch=75, train loss = 1.7915825843811035, train acc = 0.38999998569488525, time = 0.9325547218322754\n",
      "Training at step=17, batch=100, train loss = 1.7029008865356445, train acc = 0.375, time = 0.9347398281097412\n",
      "Training at step=17, batch=125, train loss = 1.6625752449035645, train acc = 0.4000000059604645, time = 0.9385437965393066\n",
      "Training at step=17, batch=150, train loss = 1.6775437593460083, train acc = 0.4749999940395355, time = 0.9363555908203125\n",
      "Training at step=17, batch=175, train loss = 1.743082880973816, train acc = 0.36500000953674316, time = 1.0121350288391113\n",
      "Training at step=17, batch=200, train loss = 1.7743864059448242, train acc = 0.3700000047683716, time = 0.9325494766235352\n",
      "Training at step=17, batch=225, train loss = 1.74898362159729, train acc = 0.375, time = 0.9605152606964111\n",
      "Testing at step=17, batch=0, test loss = 1.6910884380340576, test acc = 0.38499999046325684, time = 0.3666675090789795\n",
      "Testing at step=17, batch=5, test loss = 1.6312501430511475, test acc = 0.41499999165534973, time = 0.35369181632995605\n",
      "Testing at step=17, batch=10, test loss = 1.713447093963623, test acc = 0.4099999964237213, time = 0.360992431640625\n",
      "Testing at step=17, batch=15, test loss = 1.703584909439087, test acc = 0.39500001072883606, time = 0.351456880569458\n",
      "Testing at step=17, batch=20, test loss = 1.70499587059021, test acc = 0.3799999952316284, time = 0.3590717315673828\n",
      "Testing at step=17, batch=25, test loss = 1.6834030151367188, test acc = 0.39500001072883606, time = 0.35800695419311523\n",
      "Testing at step=17, batch=30, test loss = 1.7339900732040405, test acc = 0.38999998569488525, time = 0.3603835105895996\n",
      "Testing at step=17, batch=35, test loss = 1.6918237209320068, test acc = 0.4300000071525574, time = 0.35268354415893555\n",
      "Testing at step=17, batch=40, test loss = 1.863335371017456, test acc = 0.3149999976158142, time = 0.35569214820861816\n",
      "Testing at step=17, batch=45, test loss = 1.6591566801071167, test acc = 0.42500001192092896, time = 0.3555736541748047\n",
      "Step 17 finished in 262.80333399772644, Train loss = 1.69364346075058, Test loss = 1.6882777643203735; Train Acc = 0.40173999893665313, Test Acc = 0.4047000002861023\n",
      "Training at step=18, batch=0, train loss = 1.5771479606628418, train acc = 0.41499999165534973, time = 0.9403913021087646\n",
      "Training at step=18, batch=25, train loss = 1.5940301418304443, train acc = 0.4650000035762787, time = 0.9397962093353271\n",
      "Training at step=18, batch=50, train loss = 1.7584805488586426, train acc = 0.41499999165534973, time = 0.9462125301361084\n",
      "Training at step=18, batch=75, train loss = 1.6135964393615723, train acc = 0.41999998688697815, time = 0.9462831020355225\n",
      "Training at step=18, batch=100, train loss = 1.7201461791992188, train acc = 0.36500000953674316, time = 0.9458503723144531\n",
      "Training at step=18, batch=125, train loss = 1.7643458843231201, train acc = 0.38499999046325684, time = 0.9370150566101074\n",
      "Training at step=18, batch=150, train loss = 1.7450366020202637, train acc = 0.35499998927116394, time = 0.9456000328063965\n",
      "Training at step=18, batch=175, train loss = 1.6703811883926392, train acc = 0.4000000059604645, time = 0.9393510818481445\n",
      "Training at step=18, batch=200, train loss = 1.6045043468475342, train acc = 0.45500001311302185, time = 0.9609479904174805\n",
      "Training at step=18, batch=225, train loss = 1.6768786907196045, train acc = 0.41499999165534973, time = 0.9490232467651367\n",
      "Testing at step=18, batch=0, test loss = 1.6454963684082031, test acc = 0.3700000047683716, time = 0.36440420150756836\n",
      "Testing at step=18, batch=5, test loss = 1.8271510601043701, test acc = 0.35499998927116394, time = 0.3477046489715576\n",
      "Testing at step=18, batch=10, test loss = 1.5671298503875732, test acc = 0.4399999976158142, time = 0.35610151290893555\n",
      "Testing at step=18, batch=15, test loss = 1.772347092628479, test acc = 0.3700000047683716, time = 0.35276055335998535\n",
      "Testing at step=18, batch=20, test loss = 1.7510796785354614, test acc = 0.3700000047683716, time = 0.35617828369140625\n",
      "Testing at step=18, batch=25, test loss = 1.6344962120056152, test acc = 0.4050000011920929, time = 0.3579857349395752\n",
      "Testing at step=18, batch=30, test loss = 1.655092716217041, test acc = 0.4000000059604645, time = 0.359346866607666\n",
      "Testing at step=18, batch=35, test loss = 1.5756924152374268, test acc = 0.4449999928474426, time = 0.35720038414001465\n",
      "Testing at step=18, batch=40, test loss = 1.5645222663879395, test acc = 0.4350000023841858, time = 0.36216068267822266\n",
      "Testing at step=18, batch=45, test loss = 1.6649574041366577, test acc = 0.39500001072883606, time = 0.3499631881713867\n",
      "Step 18 finished in 263.38514256477356, Train loss = 1.674844769001007, Test loss = 1.6880758309364319; Train Acc = 0.40663999879360196, Test Acc = 0.39990000128746034\n",
      "Training at step=19, batch=0, train loss = 1.744583010673523, train acc = 0.4000000059604645, time = 0.9524486064910889\n",
      "Training at step=19, batch=25, train loss = 1.6629406213760376, train acc = 0.4300000071525574, time = 0.9344432353973389\n",
      "Training at step=19, batch=50, train loss = 1.5833433866500854, train acc = 0.4350000023841858, time = 0.9515950679779053\n",
      "Training at step=19, batch=75, train loss = 1.539429783821106, train acc = 0.5, time = 0.9326786994934082\n",
      "Training at step=19, batch=100, train loss = 1.6814639568328857, train acc = 0.38999998569488525, time = 0.9385476112365723\n",
      "Training at step=19, batch=125, train loss = 1.7083830833435059, train acc = 0.39500001072883606, time = 0.9483270645141602\n",
      "Training at step=19, batch=150, train loss = 1.6549632549285889, train acc = 0.4350000023841858, time = 0.9383320808410645\n",
      "Training at step=19, batch=175, train loss = 1.7155506610870361, train acc = 0.3499999940395355, time = 0.932478666305542\n",
      "Training at step=19, batch=200, train loss = 1.6408473253250122, train acc = 0.4099999964237213, time = 0.9460909366607666\n",
      "Training at step=19, batch=225, train loss = 1.6449748277664185, train acc = 0.4099999964237213, time = 0.9506502151489258\n",
      "Testing at step=19, batch=0, test loss = 1.7096426486968994, test acc = 0.3499999940395355, time = 0.36128973960876465\n",
      "Testing at step=19, batch=5, test loss = 1.7214590311050415, test acc = 0.41999998688697815, time = 0.35877490043640137\n",
      "Testing at step=19, batch=10, test loss = 1.717461347579956, test acc = 0.4050000011920929, time = 0.3498561382293701\n",
      "Testing at step=19, batch=15, test loss = 1.566494345664978, test acc = 0.41499999165534973, time = 0.3552584648132324\n",
      "Testing at step=19, batch=20, test loss = 1.7209830284118652, test acc = 0.3400000035762787, time = 0.3538789749145508\n",
      "Testing at step=19, batch=25, test loss = 1.6936583518981934, test acc = 0.41499999165534973, time = 0.3498554229736328\n",
      "Testing at step=19, batch=30, test loss = 1.5981773138046265, test acc = 0.42500001192092896, time = 0.3546774387359619\n",
      "Testing at step=19, batch=35, test loss = 1.5576211214065552, test acc = 0.41499999165534973, time = 0.35738158226013184\n",
      "Testing at step=19, batch=40, test loss = 1.6962612867355347, test acc = 0.39500001072883606, time = 0.3510427474975586\n",
      "Testing at step=19, batch=45, test loss = 1.662206768989563, test acc = 0.39500001072883606, time = 0.34838056564331055\n",
      "Step 19 finished in 263.3919925689697, Train loss = 1.668829981803894, Test loss = 1.6768689155578613; Train Acc = 0.40694000041484835, Test Acc = 0.403199999332428\n",
      "Training at step=20, batch=0, train loss = 1.629449486732483, train acc = 0.4050000011920929, time = 0.9475970268249512\n",
      "Training at step=20, batch=25, train loss = 1.6041357517242432, train acc = 0.41499999165534973, time = 0.9368805885314941\n",
      "Training at step=20, batch=50, train loss = 1.6096516847610474, train acc = 0.45500001311302185, time = 0.9588124752044678\n",
      "Training at step=20, batch=75, train loss = 1.6930891275405884, train acc = 0.38499999046325684, time = 0.9599297046661377\n",
      "Training at step=20, batch=100, train loss = 1.5120600461959839, train acc = 0.4749999940395355, time = 0.9527385234832764\n",
      "Training at step=20, batch=125, train loss = 1.6936944723129272, train acc = 0.4000000059604645, time = 0.9559934139251709\n",
      "Training at step=20, batch=150, train loss = 1.4726868867874146, train acc = 0.4449999928474426, time = 0.9423580169677734\n",
      "Training at step=20, batch=175, train loss = 1.7190978527069092, train acc = 0.41999998688697815, time = 0.9434647560119629\n",
      "Training at step=20, batch=200, train loss = 1.6560026407241821, train acc = 0.4000000059604645, time = 0.9450056552886963\n",
      "Training at step=20, batch=225, train loss = 1.6671985387802124, train acc = 0.4099999964237213, time = 0.9594323635101318\n",
      "Testing at step=20, batch=0, test loss = 1.6973377466201782, test acc = 0.41499999165534973, time = 0.35933828353881836\n",
      "Testing at step=20, batch=5, test loss = 1.6035891771316528, test acc = 0.4350000023841858, time = 0.3538081645965576\n",
      "Testing at step=20, batch=10, test loss = 1.6921989917755127, test acc = 0.38999998569488525, time = 0.3549060821533203\n",
      "Testing at step=20, batch=15, test loss = 1.6470730304718018, test acc = 0.3799999952316284, time = 0.34879231452941895\n",
      "Testing at step=20, batch=20, test loss = 1.716895341873169, test acc = 0.38999998569488525, time = 0.35227203369140625\n",
      "Testing at step=20, batch=25, test loss = 1.7650818824768066, test acc = 0.35499998927116394, time = 0.3542613983154297\n",
      "Testing at step=20, batch=30, test loss = 1.6743760108947754, test acc = 0.4050000011920929, time = 0.35066986083984375\n",
      "Testing at step=20, batch=35, test loss = 1.6374484300613403, test acc = 0.3700000047683716, time = 0.3497030735015869\n",
      "Testing at step=20, batch=40, test loss = 1.795823097229004, test acc = 0.4050000011920929, time = 0.3541295528411865\n",
      "Testing at step=20, batch=45, test loss = 1.4958947896957397, test acc = 0.5, time = 0.350266695022583\n",
      "Step 20 finished in 263.7455174922943, Train loss = 1.6595758743286133, Test loss = 1.6675587487220764; Train Acc = 0.4122199990749359, Test Acc = 0.4054999965429306\n",
      "Training at step=21, batch=0, train loss = 1.6109544038772583, train acc = 0.41499999165534973, time = 0.9438958168029785\n",
      "Training at step=21, batch=25, train loss = 1.6552472114562988, train acc = 0.4050000011920929, time = 0.9513635635375977\n",
      "Training at step=21, batch=50, train loss = 1.671034574508667, train acc = 0.41999998688697815, time = 0.9424126148223877\n",
      "Training at step=21, batch=75, train loss = 1.689988613128662, train acc = 0.41499999165534973, time = 0.9541606903076172\n",
      "Training at step=21, batch=100, train loss = 1.7357237339019775, train acc = 0.38499999046325684, time = 0.9479398727416992\n",
      "Training at step=21, batch=125, train loss = 1.6656767129898071, train acc = 0.4000000059604645, time = 0.941002607345581\n",
      "Training at step=21, batch=150, train loss = 1.6885476112365723, train acc = 0.41499999165534973, time = 0.9615597724914551\n",
      "Training at step=21, batch=175, train loss = 1.6139171123504639, train acc = 0.4749999940395355, time = 0.937509298324585\n",
      "Training at step=21, batch=200, train loss = 1.6060503721237183, train acc = 0.41499999165534973, time = 0.9461636543273926\n",
      "Training at step=21, batch=225, train loss = 1.603452444076538, train acc = 0.41499999165534973, time = 0.9544112682342529\n",
      "Testing at step=21, batch=0, test loss = 1.5744199752807617, test acc = 0.4300000071525574, time = 0.35166239738464355\n",
      "Testing at step=21, batch=5, test loss = 1.6940900087356567, test acc = 0.39500001072883606, time = 0.3519597053527832\n",
      "Testing at step=21, batch=10, test loss = 1.6253423690795898, test acc = 0.4399999976158142, time = 0.3658158779144287\n",
      "Testing at step=21, batch=15, test loss = 1.683385968208313, test acc = 0.375, time = 0.36635518074035645\n",
      "Testing at step=21, batch=20, test loss = 1.6528161764144897, test acc = 0.4000000059604645, time = 0.36197733879089355\n",
      "Testing at step=21, batch=25, test loss = 1.5918588638305664, test acc = 0.4399999976158142, time = 0.36051249504089355\n",
      "Testing at step=21, batch=30, test loss = 1.6658830642700195, test acc = 0.4399999976158142, time = 0.35630154609680176\n",
      "Testing at step=21, batch=35, test loss = 1.5172041654586792, test acc = 0.4650000035762787, time = 0.35753822326660156\n",
      "Testing at step=21, batch=40, test loss = 1.6908197402954102, test acc = 0.44999998807907104, time = 0.3511481285095215\n",
      "Testing at step=21, batch=45, test loss = 1.7189794778823853, test acc = 0.3700000047683716, time = 0.35282039642333984\n",
      "Step 21 finished in 264.0886995792389, Train loss = 1.6425236206054687, Test loss = 1.6403304839134216; Train Acc = 0.41748000025749205, Test Acc = 0.4168999993801117\n",
      "Training at step=22, batch=0, train loss = 1.6502970457077026, train acc = 0.44999998807907104, time = 0.9892652034759521\n",
      "Training at step=22, batch=25, train loss = 1.5750319957733154, train acc = 0.47999998927116394, time = 0.9516751766204834\n",
      "Training at step=22, batch=50, train loss = 1.6105103492736816, train acc = 0.41499999165534973, time = 0.9359574317932129\n",
      "Training at step=22, batch=75, train loss = 1.6494622230529785, train acc = 0.4449999928474426, time = 0.936286211013794\n",
      "Training at step=22, batch=100, train loss = 1.676939845085144, train acc = 0.36000001430511475, time = 0.9401810169219971\n",
      "Training at step=22, batch=125, train loss = 1.738233208656311, train acc = 0.36500000953674316, time = 0.9583945274353027\n",
      "Training at step=22, batch=150, train loss = 1.519356369972229, train acc = 0.46000000834465027, time = 0.9593150615692139\n",
      "Training at step=22, batch=175, train loss = 1.6606857776641846, train acc = 0.4399999976158142, time = 0.9509947299957275\n",
      "Training at step=22, batch=200, train loss = 1.7099380493164062, train acc = 0.36500000953674316, time = 0.939542293548584\n",
      "Training at step=22, batch=225, train loss = 1.653703212738037, train acc = 0.4099999964237213, time = 0.9487597942352295\n",
      "Testing at step=22, batch=0, test loss = 1.5131027698516846, test acc = 0.4449999928474426, time = 0.35588884353637695\n",
      "Testing at step=22, batch=5, test loss = 1.7142268419265747, test acc = 0.36500000953674316, time = 0.35176825523376465\n",
      "Testing at step=22, batch=10, test loss = 1.7091485261917114, test acc = 0.4000000059604645, time = 0.3724675178527832\n",
      "Testing at step=22, batch=15, test loss = 1.7791128158569336, test acc = 0.375, time = 0.3640754222869873\n",
      "Testing at step=22, batch=20, test loss = 1.5878599882125854, test acc = 0.4050000011920929, time = 0.3627321720123291\n",
      "Testing at step=22, batch=25, test loss = 1.6869145631790161, test acc = 0.38499999046325684, time = 0.36104869842529297\n",
      "Testing at step=22, batch=30, test loss = 1.6484266519546509, test acc = 0.4650000035762787, time = 0.3665435314178467\n",
      "Testing at step=22, batch=35, test loss = 1.5649524927139282, test acc = 0.44999998807907104, time = 0.36469197273254395\n",
      "Testing at step=22, batch=40, test loss = 1.624334692955017, test acc = 0.41999998688697815, time = 0.3618481159210205\n",
      "Testing at step=22, batch=45, test loss = 1.532088041305542, test acc = 0.48500001430511475, time = 0.36458492279052734\n",
      "Step 22 finished in 264.2734396457672, Train loss = 1.6338165435791017, Test loss = 1.656495943069458; Train Acc = 0.41866000080108645, Test Acc = 0.40989999830722806\n",
      "Training at step=23, batch=0, train loss = 1.663101315498352, train acc = 0.42500001192092896, time = 0.9457392692565918\n",
      "Training at step=23, batch=25, train loss = 1.579525351524353, train acc = 0.4699999988079071, time = 0.9366958141326904\n",
      "Training at step=23, batch=50, train loss = 1.6769410371780396, train acc = 0.41999998688697815, time = 0.9482560157775879\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.1\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T17:06:02.381493Z",
     "iopub.status.busy": "2024-04-03T17:06:02.381278Z",
     "iopub.status.idle": "2024-04-03T17:06:02.758413Z",
     "shell.execute_reply": "2024-04-03T17:06:02.757799Z",
     "shell.execute_reply.started": "2024-04-03T17:06:02.381473Z"
    },
    "id": "U0Q0vFm7B6cg"
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2-cifar10_full_repacement.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"2-cifar10_full_replacement.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
