{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:37:57.029039Z",
     "start_time": "2024-04-08T15:37:50.711923Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.17.2)\r\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: pennylane in /usr/local/lib/python3.9/dist-packages (0.35.1)\r\n",
      "Requirement already satisfied: pennylane-lightning in /usr/local/lib/python3.9/dist-packages (0.35.1)\r\n",
      "Requirement already satisfied: cotengra in /usr/local/lib/python3.9/dist-packages (0.5.6)\r\n",
      "Requirement already satisfied: quimb in /usr/local/lib/python3.9/dist-packages (1.7.3)\r\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (1.3.2)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.2.0)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.3.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch) (2.19.3)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch) (11.4.5.107)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from torch) (4.11.0)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\r\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.10.2)\r\n",
      "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.6.9)\r\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.4.4)\r\n",
      "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.10.0)\r\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\r\n",
      "Requirement already satisfied: autograd in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.6.2)\r\n",
      "Requirement already satisfied: rustworkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.14.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\r\n",
      "Requirement already satisfied: pennylane-lightning-gpu in /usr/local/lib/python3.9/dist-packages (from pennylane-lightning) (0.35.1)\r\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.12.3)\r\n",
      "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.59.1)\r\n",
      "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\r\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (0.11.2)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\r\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\r\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.39->quimb) (0.42.0)\r\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio pennylane pennylane-lightning pennylane-lightning[gpu] cotengra quimb torchmetrics --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876f9bf5f783a98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:39:05.951298Z",
     "start_time": "2024-04-08T15:37:57.031382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import pennylane as qml\n",
    "import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "#prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# torch.cfloat won't pass the unitary check, but faster\n",
    "COMPLEX_DTYPE = torch.cfloat\n",
    "REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd024814e00e5b3a",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47f4da37e1ff9389",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:39:08.747982Z",
     "start_time": "2024-04-08T15:39:05.952529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n",
      "tensor([7, 2, 7, 6, 4, 5, 7, 1, 8, 1, 1, 0, 8, 9, 7, 3, 1, 4, 6, 8, 2, 6, 5, 1,\n",
      "        5, 9, 7, 2, 6, 5, 5, 9, 3, 6, 1, 9, 7, 6, 2, 8, 1, 8, 4, 8, 9, 0, 1, 5,\n",
      "        2, 3, 3, 2, 6, 2, 5, 7, 6, 1, 8, 7, 7, 1, 5, 6])\n",
      "tensor([ 0.0275+0.j, -0.0196+0.j, -0.0353+0.j, -0.6078+0.j, -0.7961+0.j, -0.8118+0.j,\n",
      "        -0.8196+0.j, -0.7569+0.j, -0.7490+0.j, -0.7176+0.j, -0.7490+0.j, -0.7569+0.j,\n",
      "        -0.7647+0.j, -0.8118+0.j, -0.7569+0.j, -0.7412+0.j, -0.7804+0.j, -0.6157+0.j,\n",
      "         0.0118+0.j,  0.1294+0.j,  0.0902+0.j,  0.0902+0.j,  0.0196+0.j,  0.0196+0.j,\n",
      "         0.0745+0.j,  0.0745+0.j,  0.0667+0.j,  0.0824+0.j,  0.0431+0.j,  0.0039+0.j,\n",
      "        -0.0118+0.j,  0.0118+0.j], dtype=torch.complex64)\n"
     ]
    }
   ],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1859d8f04faeca",
   "metadata": {},
   "source": [
    "# Some Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5154a6074eb8e839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:39:10.489330Z",
     "start_time": "2024-04-08T15:39:08.749789Z"
    }
   },
   "outputs": [],
   "source": [
    "ket = {\n",
    "    '0':torch.tensor([1.,0.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '1':torch.tensor([0.,1.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '+':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) + torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device)),\n",
    "    '-':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) - torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device))\n",
    "}\n",
    "\n",
    "pauli = {\n",
    "    'I':torch.tensor([[1.,0.],[0.,1.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'X':torch.tensor([[0.,1.],[1.,0.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Y':torch.tensor([[0., -1.j],[1.j, 0]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Z':torch.tensor([[1.,0.],[0.,-1.]], dtype = COMPLEX_DTYPE, device=device)\n",
    "}\n",
    "\n",
    "def tensor_product(*args):\n",
    "  input_list = [a for a in args]\n",
    "  return functools.reduce(torch.kron, input_list)\n",
    "\n",
    "def multi_qubit_identity(n_qubits:int):\n",
    "  assert n_qubits>0\n",
    "  if n_qubits == 1:\n",
    "    return pauli['I']\n",
    "  else:\n",
    "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
    "\n",
    "pauli_words_su4 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
    "      #print(key1, key2)\n",
    "      #print(pauli[key1])\n",
    "      #print(pauli[key2])\n",
    "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
    "      #print(pauli_words_su4[key1+key2])\n",
    "\n",
    "pauli_words_su8 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      if not key1+key2+key3 == 'III':\n",
    "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
    "\n",
    "pauli_words_su16 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        if not key1+key2+key3+key4 == 'IIII':\n",
    "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
    "              pauli[key1],\n",
    "              pauli[key2],\n",
    "              pauli[key3],\n",
    "              pauli[key4]\n",
    "          )\n",
    "\n",
    "pauli_words_su32 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        for key5 in pauli.keys():\n",
    "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
    "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
    "                pauli[key1],\n",
    "                pauli[key2],\n",
    "                pauli[key3],\n",
    "                pauli[key4],\n",
    "                pauli[key5]\n",
    "            )\n",
    "\n",
    "def su32_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = list(pauli_words_su32.values())\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, torch.stack(dict_values))\n",
    "  #print(generator.shape)\n",
    "  return torch.matrix_exp(1j*generator)\n",
    "\n",
    "def su4_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = torch.stack(list(pauli_words_su4.values()))\n",
    "  #print(dict_values.shape)\n",
    "  generator = torch.einsum(\"i, ijk -> jk\", params, dict_values)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "def measure_sv(\n",
    "    state,\n",
    "    observable\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Measure a statevector with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not\n",
    "  \"\"\"\n",
    "  expectation_value = torch.conj(state)@observable@state\n",
    "  return torch.real(expectation_value)\n",
    "\n",
    "def measure_dm(\n",
    "    rho,\n",
    "    observable\n",
    "):\n",
    "  \"\"\"\n",
    "  Measure a density matrix with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not.\n",
    "  \"\"\"\n",
    "  product = torch.matmul(rho, observable)\n",
    "\n",
    "  # Calculate the trace, which is the sum of diagonal elements\n",
    "  trace = torch.trace(product)\n",
    "\n",
    "  # The expectation value should be real for physical observables\n",
    "  return torch.real(trace)\n",
    "\n",
    "vmap_measure_sv = torch.vmap(measure_sv, in_dims=(None, 0), out_dims=0)\n",
    "vmap_measure_dm = torch.vmap(measure_dm, in_dims=(None, 0), out_dims=0)\n",
    "\n",
    "# assuming the input patch observables (hermitianized) has shape (batchsize,n_patches, c, h, w)\n",
    "# assuming the input set statevectors has shape (c, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_sv_batched_ob = torch.vmap(measure_sv, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "# assuming the input set desnity matrices has shape (batchsize, n_patches, c, 2**n, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_dm_batched_ob = torch.vmap(measure_dm, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "def bitstring_to_state(bitstring:str):\n",
    "  \"\"\"\n",
    "  Convert a bit string, like '0101001' or '+-+-101'\n",
    "  to a statevector. Each character in the bitstring must be among\n",
    "  0, 1, + and -\n",
    "  \"\"\"\n",
    "  assert len(bitstring)>0\n",
    "  for c in bitstring:\n",
    "    assert c in ['0', '1', '+', '-']\n",
    "  single_qubit_states = [ket[c] for c in bitstring]\n",
    "  return tensor_product(*single_qubit_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45db97191b5f524",
   "metadata": {},
   "source": [
    "# Flipped Quanvolution Kernel\n",
    "\n",
    "First, let's define the function to extract patches for a single kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f163b765840d20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:39:10.495355Z",
     "start_time": "2024-04-08T15:39:10.490450Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_patches(image, patch_size, stride, padding=None):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image with multiple input channels and optional custom padding.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (in_channels, height, width).\n",
    "        patch_size (int): Size of the square patches to extract.\n",
    "        stride (int): Stride between patches.\n",
    "        padding (tuple): Padding value(s) for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n",
    "    pad_l, pad_r, pad_t, pad_b = padding if padding is not None else (0,0,0,0)\n",
    "\n",
    "    if padding is not None:\n",
    "        image = torch.nn.functional.pad(image, (pad_l, pad_r, pad_t, pad_b), mode='constant')\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    height, width = image.shape[-2],  image.shape[-1]\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = image[..., i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    patches = torch.stack(patches, dim=-4)\n",
    "    return patches # has shape (batchsize, n_patches, channel, h, w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc79120a6354fad",
   "metadata": {},
   "source": [
    "## A Three-by-Three Quanv Kernel Function\n",
    "\n",
    "For 3 by 3 patches, we only need a 2-qubit parameterised state, generated by the SU4 gate applied to the $|00\\rangle$ state. The first thing to do is to pad the 3 by 3 patches to 4 by 4. However, for input with size\n",
    "\n",
    "$$\n",
    "D_1\\times H_1 \\times W_1\n",
    "$$\n",
    "\n",
    "where $D_1$ is the number of input channels, we will need $D_1$ different two-qubit states, generated by $D_1$ different parameterised circuits. These circuits will produce $D_1$ expectation values. Suppose each circuit is parameterised by $\\theta_i, i\\in \\{1,2,\\cdots, D_1\\}$, the parameterised (pure) state can be written as\n",
    "\n",
    "$$\n",
    "|\\psi (\\theta_i)⟩ = SU4(\\theta_i)|00⟩\n",
    "$$\n",
    "\n",
    "For a channel $M_i$ of patch $M$ with shape $4\\times 4$, the expectation value is calculated by\n",
    "\n",
    "$$\n",
    "⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$\n",
    "\n",
    "Then, a single pixel value of the output channel corresponding this single kernel is\n",
    "\n",
    "$$\n",
    "\\frac{1}{D_1}\\sum_{i=1}^{D_1}⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62382dc385ee011f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:39:10.500561Z",
     "start_time": "2024-04-08T15:39:10.496285Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_2q_param_state(theta):\n",
    "  state = bitstring_to_state('00')\n",
    "  state = torch.matmul(su4_op(theta), state)\n",
    "  return state\n",
    "\n",
    "vmap_generate_2q_param_state = torch.vmap(generate_2q_param_state, in_dims=0, out_dims = 0)\n",
    "\n",
    "def single_kernel_op_over_batched_patches(thetas, patch):\n",
    "  # patch has shape (c_in, h, w)\n",
    "  # thetas has shape (c_in, 4^2-1) for SU4 gates\n",
    "  n_theta = thetas.shape[-2]\n",
    "  n_channel = patch.shape[-3]\n",
    "  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n",
    "  states = vmap_generate_2q_param_state(thetas)\n",
    "  #print(\"States shape\", states.shape)\n",
    "  patch = torch.nn.functional.pad(patch, (0, 1, 0, 1), mode='constant')\n",
    "  patch_t = torch.einsum(\"...jk->...kj\", patch)\n",
    "  herm_patch = (patch_t+patch)/2 # has dim (batchsize, num_patches, c, h, w)\n",
    "  #print(\"Herm patch shape\", herm_patch.shape)\n",
    "  channel_out = vmap_measure_channel_sv_batched_ob(states, herm_patch) # has dim (batchsize,n_patches, c)\n",
    "  return torch.sum(channel_out, axis = -1)/n_theta # has dim (batchszie, n_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cfe372503a53d",
   "metadata": {},
   "source": [
    "## Multiple Output Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "607e2ae8d2276426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:39:10.504127Z",
     "start_time": "2024-04-08T15:39:10.501465Z"
    }
   },
   "outputs": [],
   "source": [
    "# For multiple channel output\n",
    "# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n",
    "vmap_vmap_single_kernel_op_through_extracted_patches = torch.vmap(single_kernel_op_over_batched_patches, in_dims=(0, None), out_dims=-2) # output has dim (batchsize, c_out, n_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c5565702fbbe0e",
   "metadata": {},
   "source": [
    "## Pytorch Module for Three-by-Three Flipped Quanv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445bee46ba979317",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:39:10.510429Z",
     "start_time": "2024-04-08T15:39:10.504959Z"
    }
   },
   "outputs": [],
   "source": [
    "class FlippedQuanv3x3(torch.nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, padding):\n",
    "    super(FlippedQuanv3x3, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.pad_l, self.pad_r, self.pad_t, self.pad_b = padding if padding is not None else (0,0,0,0)\n",
    "    self.pad = (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n",
    "    self.weight= torch.nn.Parameter(torch.randn((out_channels, in_channels, 4**2-1)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn((out_channels, 1)).type(COMPLEX_DTYPE))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batchsize ,c_in, h, w)\n",
    "    # weight has shape (c_out, c_in, 15)\n",
    "    # bias has shape (c_out, 1)\n",
    "    x = x.type(COMPLEX_DTYPE)\n",
    "    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n",
    "    h_out = (h_in-3+(self.pad_t+self.pad_b))//self.stride +1\n",
    "    w_out = (w_in-3+(self.pad_l+self.pad_r))//self.stride +1\n",
    "\n",
    "    #print(self.weight.shape)\n",
    "    #print(patches.shape)\n",
    "\n",
    "    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n",
    "\n",
    "\n",
    "    out = out + self.bias\n",
    "    return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n",
    "  \n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_channels={self.in_channels}, out_channels={self.out_channels}, stride={self.stride}, padding={self.padding}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e122fa72e1920f",
   "metadata": {},
   "source": [
    "## Flipped Quanvolution Replacing Conv2d\n",
    "\n",
    "First, let's just replace `Conv2d` with `FlippedQuanv3x3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "449ce05ae658b652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:39:12.115423Z",
     "start_time": "2024-04-08T15:39:10.511278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "HybridNet(\n",
      "  (layers): Sequential(\n",
      "    (0): FlippedQuanv3x3()\n",
      "    (1): FlippedQuanv3x3()\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=12544, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115/323720799.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n",
      "/tmp/ipykernel_115/3852604809.py:30: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:301.)\n",
      "  return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n"
     ]
    }
   ],
   "source": [
    "class HybridNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HybridNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        FlippedQuanv3x3(in_channels=3, out_channels=32, stride=1, padding=None),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(16*28*28, 10).type(REAL_DTYPE)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "net = HybridNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838a04ff194b0a02",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-08T15:39:12.117337Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 500, Number of test batches = 100\n",
      "Print every train batch = 100, Print every test batch = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115/323720799.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at step=0, batch=0, train loss = 2.441441059112549, train acc = 0.05999999865889549, time = 0.35552048683166504\n",
      "Training at step=0, batch=100, train loss = 2.2596826553344727, train acc = 0.1599999964237213, time = 0.32975339889526367\n",
      "Training at step=0, batch=200, train loss = 2.057340145111084, train acc = 0.33000001311302185, time = 0.3284156322479248\n",
      "Training at step=0, batch=300, train loss = 2.177661418914795, train acc = 0.20000000298023224, time = 0.32393336296081543\n",
      "Training at step=0, batch=400, train loss = 2.189079523086548, train acc = 0.23000000417232513, time = 0.32784008979797363\n",
      "Testing at step=0, batch=0, test loss = 2.105168104171753, test acc = 0.28999999165534973, time = 0.10919547080993652\n",
      "Testing at step=0, batch=20, test loss = 2.1038401126861572, test acc = 0.20999999344348907, time = 0.10751962661743164\n",
      "Testing at step=0, batch=40, test loss = 2.004384756088257, test acc = 0.2800000011920929, time = 0.1080327033996582\n",
      "Testing at step=0, batch=60, test loss = 2.052694082260132, test acc = 0.25, time = 0.10806798934936523\n",
      "Testing at step=0, batch=80, test loss = 2.0501062870025635, test acc = 0.25999999046325684, time = 0.10683178901672363\n",
      "Step 0 finished in 187.57645893096924, Train loss = 2.1537411370277404, Test loss = 2.070364043712616; Train Acc = 0.22657999949157237, Test Acc = 0.26140000015497206\n",
      "Training at step=1, batch=0, train loss = 2.1015684604644775, train acc = 0.23000000417232513, time = 0.32726240158081055\n",
      "Training at step=1, batch=100, train loss = 2.0003321170806885, train acc = 0.25999999046325684, time = 0.3264493942260742\n",
      "Training at step=1, batch=200, train loss = 2.151282548904419, train acc = 0.2800000011920929, time = 0.33026647567749023\n",
      "Training at step=1, batch=300, train loss = 1.9836963415145874, train acc = 0.3100000023841858, time = 0.33193159103393555\n",
      "Training at step=1, batch=400, train loss = 1.9626028537750244, train acc = 0.33000001311302185, time = 0.329805850982666\n",
      "Testing at step=1, batch=0, test loss = 2.082218885421753, test acc = 0.23999999463558197, time = 0.10667729377746582\n",
      "Testing at step=1, batch=20, test loss = 2.0963456630706787, test acc = 0.27000001072883606, time = 0.10662579536437988\n",
      "Testing at step=1, batch=40, test loss = 1.947011113166809, test acc = 0.3199999928474426, time = 0.1111762523651123\n",
      "Testing at step=1, batch=60, test loss = 1.9809738397598267, test acc = 0.33000001311302185, time = 0.10981488227844238\n",
      "Testing at step=1, batch=80, test loss = 2.026486873626709, test acc = 0.27000001072883606, time = 0.11013412475585938\n",
      "Step 1 finished in 185.114399433136, Train loss = 2.091826896905899, Test loss = 2.066450922489166; Train Acc = 0.2541400001198053, Test Acc = 0.2683000014722347\n",
      "Training at step=2, batch=0, train loss = 2.0780375003814697, train acc = 0.30000001192092896, time = 0.32741236686706543\n",
      "Training at step=2, batch=100, train loss = 2.026594877243042, train acc = 0.2800000011920929, time = 0.32976555824279785\n",
      "Training at step=2, batch=200, train loss = 2.1287078857421875, train acc = 0.2800000011920929, time = 0.32903075218200684\n",
      "Training at step=2, batch=300, train loss = 2.1231632232666016, train acc = 0.27000001072883606, time = 0.32792115211486816\n",
      "Training at step=2, batch=400, train loss = 2.1066112518310547, train acc = 0.27000001072883606, time = 0.32810354232788086\n",
      "Testing at step=2, batch=0, test loss = 2.1138994693756104, test acc = 0.2800000011920929, time = 0.10755133628845215\n",
      "Testing at step=2, batch=20, test loss = 2.145869016647339, test acc = 0.23000000417232513, time = 0.1097707748413086\n",
      "Testing at step=2, batch=40, test loss = 2.081749439239502, test acc = 0.27000001072883606, time = 0.11038708686828613\n",
      "Testing at step=2, batch=60, test loss = 2.0810647010803223, test acc = 0.3700000047683716, time = 0.10851025581359863\n",
      "Testing at step=2, batch=80, test loss = 2.017242193222046, test acc = 0.3100000023841858, time = 0.10770750045776367\n",
      "Step 2 finished in 184.71348595619202, Train loss = 2.073362636566162, Test loss = 2.0698165118694307; Train Acc = 0.2661800001114607, Test Acc = 0.27240000054240227\n",
      "Training at step=3, batch=0, train loss = 2.2171380519866943, train acc = 0.23000000417232513, time = 0.33011484146118164\n",
      "Training at step=3, batch=100, train loss = 2.0671842098236084, train acc = 0.2199999988079071, time = 0.32881903648376465\n",
      "Training at step=3, batch=200, train loss = 2.1934258937835693, train acc = 0.23000000417232513, time = 0.32906508445739746\n",
      "Training at step=3, batch=300, train loss = 2.221061944961548, train acc = 0.2199999988079071, time = 0.32561779022216797\n",
      "Training at step=3, batch=400, train loss = 2.000108003616333, train acc = 0.28999999165534973, time = 0.3333561420440674\n",
      "Testing at step=3, batch=0, test loss = 2.1335527896881104, test acc = 0.23999999463558197, time = 0.1094064712524414\n",
      "Testing at step=3, batch=20, test loss = 2.101254463195801, test acc = 0.20999999344348907, time = 0.1080634593963623\n",
      "Testing at step=3, batch=40, test loss = 2.014408826828003, test acc = 0.28999999165534973, time = 0.1082468032836914\n",
      "Testing at step=3, batch=60, test loss = 2.046893835067749, test acc = 0.23999999463558197, time = 0.1082303524017334\n",
      "Testing at step=3, batch=80, test loss = 2.1267173290252686, test acc = 0.20999999344348907, time = 0.10767817497253418\n",
      "Step 3 finished in 184.775812625885, Train loss = 2.0558902149200438, Test loss = 2.046396405696869; Train Acc = 0.27164000102877617, Test Acc = 0.2640999987721443\n",
      "Training at step=4, batch=0, train loss = 2.0198814868927, train acc = 0.27000001072883606, time = 0.3262596130371094\n",
      "Training at step=4, batch=100, train loss = 2.0185484886169434, train acc = 0.2800000011920929, time = 0.3306009769439697\n",
      "Training at step=4, batch=200, train loss = 1.9512306451797485, train acc = 0.33000001311302185, time = 0.3255033493041992\n",
      "Training at step=4, batch=300, train loss = 2.0942318439483643, train acc = 0.23000000417232513, time = 0.33059120178222656\n",
      "Training at step=4, batch=400, train loss = 1.8923583030700684, train acc = 0.30000001192092896, time = 0.4394867420196533\n",
      "Testing at step=4, batch=0, test loss = 2.0153801441192627, test acc = 0.2800000011920929, time = 0.11051392555236816\n",
      "Testing at step=4, batch=20, test loss = 1.964139699935913, test acc = 0.3199999928474426, time = 0.10747075080871582\n",
      "Testing at step=4, batch=40, test loss = 2.1636199951171875, test acc = 0.25999999046325684, time = 0.10664248466491699\n",
      "Testing at step=4, batch=60, test loss = 2.1327552795410156, test acc = 0.27000001072883606, time = 0.10792756080627441\n",
      "Testing at step=4, batch=80, test loss = 2.0920276641845703, test acc = 0.30000001192092896, time = 0.10755372047424316\n",
      "Step 4 finished in 185.1462914943695, Train loss = 2.0427571461200715, Test loss = 2.0010504269599916; Train Acc = 0.27910000094771387, Test Acc = 0.3027000014483929\n",
      "Training at step=5, batch=0, train loss = 1.803624153137207, train acc = 0.36000001430511475, time = 0.328808069229126\n",
      "Training at step=5, batch=100, train loss = 2.0828850269317627, train acc = 0.27000001072883606, time = 0.33855175971984863\n",
      "Training at step=5, batch=200, train loss = 2.10707426071167, train acc = 0.23999999463558197, time = 0.3269059658050537\n",
      "Training at step=5, batch=300, train loss = 1.9318848848342896, train acc = 0.3199999928474426, time = 0.33006930351257324\n",
      "Training at step=5, batch=400, train loss = 2.0053327083587646, train acc = 0.33000001311302185, time = 0.32764554023742676\n",
      "Testing at step=5, batch=0, test loss = 1.862505316734314, test acc = 0.36000001430511475, time = 0.10903525352478027\n",
      "Testing at step=5, batch=20, test loss = 1.9791358709335327, test acc = 0.33000001311302185, time = 0.10721611976623535\n",
      "Testing at step=5, batch=40, test loss = 1.9209076166152954, test acc = 0.3100000023841858, time = 0.10746240615844727\n",
      "Testing at step=5, batch=60, test loss = 1.970927119255066, test acc = 0.3700000047683716, time = 0.1092979907989502\n",
      "Testing at step=5, batch=80, test loss = 2.0502169132232666, test acc = 0.25999999046325684, time = 0.10820841789245605\n",
      "Step 5 finished in 184.70939445495605, Train loss = 2.0216552810668946, Test loss = 2.01757435798645; Train Acc = 0.2874600004553795, Test Acc = 0.2764000007510185\n",
      "Training at step=6, batch=0, train loss = 1.9171823263168335, train acc = 0.2800000011920929, time = 0.32686901092529297\n",
      "Training at step=6, batch=100, train loss = 2.082709550857544, train acc = 0.30000001192092896, time = 0.32733750343322754\n",
      "Training at step=6, batch=200, train loss = 1.9310723543167114, train acc = 0.3400000035762787, time = 0.32878541946411133\n",
      "Training at step=6, batch=300, train loss = 2.0851144790649414, train acc = 0.20999999344348907, time = 0.3269193172454834\n",
      "Training at step=6, batch=400, train loss = 2.015131950378418, train acc = 0.23999999463558197, time = 0.32891392707824707\n",
      "Testing at step=6, batch=0, test loss = 1.9145911931991577, test acc = 0.3700000047683716, time = 0.10840153694152832\n",
      "Testing at step=6, batch=20, test loss = 1.8664392232894897, test acc = 0.25999999046325684, time = 0.10834407806396484\n",
      "Testing at step=6, batch=40, test loss = 1.989254117012024, test acc = 0.2199999988079071, time = 0.10773849487304688\n",
      "Testing at step=6, batch=60, test loss = 1.9737744331359863, test acc = 0.25999999046325684, time = 0.1082921028137207\n",
      "Testing at step=6, batch=80, test loss = 2.0552327632904053, test acc = 0.3100000023841858, time = 0.10734438896179199\n",
      "Step 6 finished in 184.8177101612091, Train loss = 2.0030154111385348, Test loss = 2.0319847536087035; Train Acc = 0.2918200008869171, Test Acc = 0.2610999993979931\n",
      "Training at step=7, batch=0, train loss = 2.0602636337280273, train acc = 0.25, time = 0.3279426097869873\n",
      "Training at step=7, batch=100, train loss = 1.8747116327285767, train acc = 0.33000001311302185, time = 0.32553577423095703\n",
      "Training at step=7, batch=200, train loss = 2.0444633960723877, train acc = 0.23000000417232513, time = 0.32828211784362793\n",
      "Training at step=7, batch=300, train loss = 1.9184784889221191, train acc = 0.36000001430511475, time = 0.32712841033935547\n",
      "Training at step=7, batch=400, train loss = 2.036970615386963, train acc = 0.33000001311302185, time = 0.32564616203308105\n",
      "Testing at step=7, batch=0, test loss = 1.8738195896148682, test acc = 0.3400000035762787, time = 0.11186790466308594\n",
      "Testing at step=7, batch=20, test loss = 2.061314344406128, test acc = 0.27000001072883606, time = 0.10888481140136719\n",
      "Testing at step=7, batch=40, test loss = 2.055464744567871, test acc = 0.3100000023841858, time = 0.10798859596252441\n",
      "Testing at step=7, batch=60, test loss = 2.108441114425659, test acc = 0.23000000417232513, time = 0.1081230640411377\n",
      "Testing at step=7, batch=80, test loss = 1.9373565912246704, test acc = 0.27000001072883606, time = 0.10966897010803223\n",
      "Step 7 finished in 184.9353802204132, Train loss = 1.9779999268054962, Test loss = 1.992473108768463; Train Acc = 0.30374000102281573, Test Acc = 0.2935000006854534\n",
      "Training at step=8, batch=0, train loss = 2.080681562423706, train acc = 0.20999999344348907, time = 0.32885146141052246\n",
      "Training at step=8, batch=100, train loss = 2.0091054439544678, train acc = 0.20999999344348907, time = 0.32797884941101074\n",
      "Training at step=8, batch=200, train loss = 2.0264787673950195, train acc = 0.3199999928474426, time = 0.3285818099975586\n",
      "Training at step=8, batch=300, train loss = 1.9068961143493652, train acc = 0.3400000035762787, time = 0.3279838562011719\n",
      "Training at step=8, batch=400, train loss = 1.894932746887207, train acc = 0.3400000035762787, time = 0.33214831352233887\n",
      "Testing at step=8, batch=0, test loss = 2.0809640884399414, test acc = 0.3100000023841858, time = 0.10875558853149414\n",
      "Testing at step=8, batch=20, test loss = 1.9551316499710083, test acc = 0.33000001311302185, time = 0.10934662818908691\n",
      "Testing at step=8, batch=40, test loss = 1.8822218179702759, test acc = 0.36000001430511475, time = 0.10810184478759766\n",
      "Testing at step=8, batch=60, test loss = 1.8501884937286377, test acc = 0.41999998688697815, time = 0.10886716842651367\n",
      "Testing at step=8, batch=80, test loss = 2.114898204803467, test acc = 0.2800000011920929, time = 0.10750913619995117\n",
      "Step 8 finished in 184.89884638786316, Train loss = 1.9538076286315917, Test loss = 1.9653106355667114; Train Acc = 0.3110200010538101, Test Acc = 0.3071000026166439\n",
      "Training at step=9, batch=0, train loss = 1.8439691066741943, train acc = 0.3499999940395355, time = 0.4425849914550781\n",
      "Training at step=9, batch=100, train loss = 1.9192395210266113, train acc = 0.3400000035762787, time = 0.3343236446380615\n",
      "Training at step=9, batch=200, train loss = 1.9174373149871826, train acc = 0.3499999940395355, time = 0.3276176452636719\n",
      "Training at step=9, batch=300, train loss = 1.8606230020523071, train acc = 0.3799999952316284, time = 0.32926154136657715\n",
      "Training at step=9, batch=400, train loss = 1.8976916074752808, train acc = 0.2800000011920929, time = 0.3279445171356201\n",
      "Testing at step=9, batch=0, test loss = 1.772263765335083, test acc = 0.3799999952316284, time = 0.11114263534545898\n",
      "Testing at step=9, batch=20, test loss = 2.0117452144622803, test acc = 0.28999999165534973, time = 0.10970449447631836\n",
      "Testing at step=9, batch=40, test loss = 1.9212524890899658, test acc = 0.3100000023841858, time = 0.10881328582763672\n",
      "Testing at step=9, batch=60, test loss = 1.8670440912246704, test acc = 0.3499999940395355, time = 0.2792203426361084\n",
      "Testing at step=9, batch=80, test loss = 1.898083209991455, test acc = 0.36000001430511475, time = 0.10889577865600586\n",
      "Step 9 finished in 185.1147177219391, Train loss = 1.9409110996723176, Test loss = 1.914078838825226; Train Acc = 0.3164600013494492, Test Acc = 0.33290000140666964\n",
      "Training at step=10, batch=0, train loss = 1.931331992149353, train acc = 0.3400000035762787, time = 0.3332529067993164\n",
      "Training at step=10, batch=100, train loss = 1.8659263849258423, train acc = 0.3700000047683716, time = 0.3274071216583252\n",
      "Training at step=10, batch=200, train loss = 1.8510360717773438, train acc = 0.3700000047683716, time = 0.32828521728515625\n",
      "Training at step=10, batch=300, train loss = 2.0699448585510254, train acc = 0.23999999463558197, time = 0.3275642395019531\n",
      "Training at step=10, batch=400, train loss = 1.8962315320968628, train acc = 0.36000001430511475, time = 0.32802343368530273\n",
      "Testing at step=10, batch=0, test loss = 1.8846012353897095, test acc = 0.27000001072883606, time = 0.10808110237121582\n",
      "Testing at step=10, batch=20, test loss = 1.8504457473754883, test acc = 0.3499999940395355, time = 0.1075446605682373\n",
      "Testing at step=10, batch=40, test loss = 1.8705250024795532, test acc = 0.3100000023841858, time = 0.1109762191772461\n",
      "Testing at step=10, batch=60, test loss = 1.8539559841156006, test acc = 0.25999999046325684, time = 0.11034464836120605\n",
      "Testing at step=10, batch=80, test loss = 1.7755643129348755, test acc = 0.30000001192092896, time = 0.10663771629333496\n",
      "Step 10 finished in 184.77299427986145, Train loss = 1.9191294839382171, Test loss = 1.9135529935359954; Train Acc = 0.3242600019276142, Test Acc = 0.312300001680851\n",
      "Training at step=11, batch=0, train loss = 1.8414497375488281, train acc = 0.3199999928474426, time = 0.33032751083374023\n",
      "Training at step=11, batch=100, train loss = 1.9521678686141968, train acc = 0.30000001192092896, time = 0.3259410858154297\n",
      "Training at step=11, batch=200, train loss = 1.863187313079834, train acc = 0.33000001311302185, time = 0.3295295238494873\n",
      "Training at step=11, batch=300, train loss = 1.9499517679214478, train acc = 0.28999999165534973, time = 0.3290441036224365\n",
      "Training at step=11, batch=400, train loss = 2.0559165477752686, train acc = 0.23000000417232513, time = 0.32946133613586426\n",
      "Testing at step=11, batch=0, test loss = 1.8936527967453003, test acc = 0.3799999952316284, time = 0.10942935943603516\n",
      "Testing at step=11, batch=20, test loss = 1.8124966621398926, test acc = 0.33000001311302185, time = 0.11074090003967285\n",
      "Testing at step=11, batch=40, test loss = 1.8492305278778076, test acc = 0.36000001430511475, time = 0.11161923408508301\n",
      "Testing at step=11, batch=60, test loss = 1.9144755601882935, test acc = 0.27000001072883606, time = 0.10748457908630371\n",
      "Testing at step=11, batch=80, test loss = 2.0817184448242188, test acc = 0.25999999046325684, time = 0.11027002334594727\n",
      "Step 11 finished in 185.08725714683533, Train loss = 1.9178070871829986, Test loss = 1.9007661712169648; Train Acc = 0.3246600012481213, Test Acc = 0.33360000148415564\n",
      "Training at step=12, batch=0, train loss = 1.7311748266220093, train acc = 0.38999998569488525, time = 0.3295929431915283\n",
      "Training at step=12, batch=100, train loss = 1.818058967590332, train acc = 0.4000000059604645, time = 0.3302795886993408\n",
      "Training at step=12, batch=200, train loss = 2.0213937759399414, train acc = 0.30000001192092896, time = 0.32921862602233887\n",
      "Training at step=12, batch=300, train loss = 1.9343806505203247, train acc = 0.3199999928474426, time = 0.32640767097473145\n",
      "Training at step=12, batch=400, train loss = 1.9851206541061401, train acc = 0.2800000011920929, time = 0.3320012092590332\n",
      "Testing at step=12, batch=0, test loss = 2.143993854522705, test acc = 0.25999999046325684, time = 0.1081235408782959\n",
      "Testing at step=12, batch=20, test loss = 2.070603132247925, test acc = 0.3100000023841858, time = 0.10774922370910645\n",
      "Testing at step=12, batch=40, test loss = 1.957848072052002, test acc = 0.25999999046325684, time = 0.11019062995910645\n",
      "Testing at step=12, batch=60, test loss = 1.9174662828445435, test acc = 0.3700000047683716, time = 0.1082923412322998\n",
      "Testing at step=12, batch=80, test loss = 1.863968849182129, test acc = 0.36000001430511475, time = 0.11067509651184082\n",
      "Step 12 finished in 185.67014360427856, Train loss = 1.902018874168396, Test loss = 1.9272363841533662; Train Acc = 0.331860001206398, Test Acc = 0.3291000024974346\n",
      "Training at step=13, batch=0, train loss = 1.9653397798538208, train acc = 0.3400000035762787, time = 0.3290140628814697\n",
      "Training at step=13, batch=100, train loss = 1.8131413459777832, train acc = 0.3700000047683716, time = 0.3336968421936035\n",
      "Training at step=13, batch=200, train loss = 1.796877145767212, train acc = 0.3199999928474426, time = 0.3283383846282959\n",
      "Training at step=13, batch=300, train loss = 1.757300615310669, train acc = 0.38999998569488525, time = 0.33081674575805664\n",
      "Training at step=13, batch=400, train loss = 1.8680511713027954, train acc = 0.27000001072883606, time = 0.32642102241516113\n",
      "Testing at step=13, batch=0, test loss = 1.7940990924835205, test acc = 0.4300000071525574, time = 0.10882186889648438\n",
      "Testing at step=13, batch=20, test loss = 1.7940486669540405, test acc = 0.2800000011920929, time = 0.10820341110229492\n",
      "Testing at step=13, batch=40, test loss = 1.9973113536834717, test acc = 0.3400000035762787, time = 0.10791492462158203\n",
      "Testing at step=13, batch=60, test loss = 1.9236502647399902, test acc = 0.3199999928474426, time = 0.10814690589904785\n",
      "Testing at step=13, batch=80, test loss = 1.8378541469573975, test acc = 0.46000000834465027, time = 0.11063313484191895\n",
      "Step 13 finished in 186.06273293495178, Train loss = 1.895341407775879, Test loss = 1.8827086675167084; Train Acc = 0.3355800005197525, Test Acc = 0.34250000163912775\n",
      "Training at step=14, batch=0, train loss = 1.9602100849151611, train acc = 0.36000001430511475, time = 0.329517126083374\n",
      "Training at step=14, batch=100, train loss = 1.8194385766983032, train acc = 0.3799999952316284, time = 0.3321654796600342\n",
      "Training at step=14, batch=200, train loss = 1.7860140800476074, train acc = 0.46000000834465027, time = 0.3411061763763428\n",
      "Training at step=14, batch=300, train loss = 1.7993396520614624, train acc = 0.33000001311302185, time = 0.32881760597229004\n",
      "Training at step=14, batch=400, train loss = 1.8321223258972168, train acc = 0.36000001430511475, time = 0.32978296279907227\n",
      "Testing at step=14, batch=0, test loss = 2.023841381072998, test acc = 0.2199999988079071, time = 0.10842013359069824\n",
      "Testing at step=14, batch=20, test loss = 2.015742540359497, test acc = 0.25999999046325684, time = 0.10892796516418457\n",
      "Testing at step=14, batch=40, test loss = 1.9188647270202637, test acc = 0.3499999940395355, time = 0.10808777809143066\n",
      "Testing at step=14, batch=60, test loss = 1.8394476175308228, test acc = 0.33000001311302185, time = 0.10831069946289062\n",
      "Testing at step=14, batch=80, test loss = 1.8184558153152466, test acc = 0.3400000035762787, time = 0.10802102088928223\n",
      "Step 14 finished in 185.79275035858154, Train loss = 1.8883898119926452, Test loss = 1.9121468877792358; Train Acc = 0.33640000188350677, Test Acc = 0.3148000004887581\n",
      "Training at step=15, batch=0, train loss = 1.8670082092285156, train acc = 0.30000001192092896, time = 0.33228611946105957\n",
      "Training at step=15, batch=100, train loss = 1.891385555267334, train acc = 0.33000001311302185, time = 0.3667728900909424\n",
      "Training at step=15, batch=200, train loss = 1.8290317058563232, train acc = 0.3400000035762787, time = 0.3296825885772705\n",
      "Training at step=15, batch=300, train loss = 1.8488539457321167, train acc = 0.3799999952316284, time = 0.328397274017334\n",
      "Training at step=15, batch=400, train loss = 1.8939568996429443, train acc = 0.3400000035762787, time = 0.32865452766418457\n",
      "Testing at step=15, batch=0, test loss = 1.8197529315948486, test acc = 0.3499999940395355, time = 0.11108851432800293\n",
      "Testing at step=15, batch=20, test loss = 1.7957991361618042, test acc = 0.3799999952316284, time = 0.10895442962646484\n",
      "Testing at step=15, batch=40, test loss = 1.9097720384597778, test acc = 0.33000001311302185, time = 0.1077876091003418\n",
      "Testing at step=15, batch=60, test loss = 1.7693471908569336, test acc = 0.38999998569488525, time = 0.10804438591003418\n",
      "Testing at step=15, batch=80, test loss = 1.898139238357544, test acc = 0.2800000011920929, time = 0.1086275577545166\n",
      "Step 15 finished in 185.39863562583923, Train loss = 1.874153198003769, Test loss = 1.8799505007267; Train Acc = 0.344320001244545, Test Acc = 0.3388000002503395\n",
      "Training at step=16, batch=0, train loss = 1.9218814373016357, train acc = 0.30000001192092896, time = 0.3311188220977783\n",
      "Training at step=16, batch=100, train loss = 1.7844908237457275, train acc = 0.38999998569488525, time = 0.3310856819152832\n",
      "Training at step=16, batch=200, train loss = 1.7848150730133057, train acc = 0.33000001311302185, time = 0.33206677436828613\n",
      "Training at step=16, batch=300, train loss = 1.752761960029602, train acc = 0.3700000047683716, time = 0.3290262222290039\n",
      "Training at step=16, batch=400, train loss = 1.926577091217041, train acc = 0.3100000023841858, time = 0.3294849395751953\n",
      "Testing at step=16, batch=0, test loss = 1.8696768283843994, test acc = 0.3700000047683716, time = 0.10959744453430176\n",
      "Testing at step=16, batch=20, test loss = 1.8189458847045898, test acc = 0.33000001311302185, time = 0.10803985595703125\n",
      "Testing at step=16, batch=40, test loss = 1.9379103183746338, test acc = 0.3199999928474426, time = 0.11127758026123047\n",
      "Testing at step=16, batch=60, test loss = 1.8700546026229858, test acc = 0.27000001072883606, time = 0.11006736755371094\n",
      "Testing at step=16, batch=80, test loss = 1.8851295709609985, test acc = 0.3499999940395355, time = 0.11118125915527344\n",
      "Step 16 finished in 185.467027425766, Train loss = 1.8695494077205659, Test loss = 1.8950444769859314; Train Acc = 0.3460400012433529, Test Acc = 0.3306000018119812\n",
      "Training at step=17, batch=0, train loss = 1.6029716730117798, train acc = 0.4699999988079071, time = 0.33259081840515137\n",
      "Training at step=17, batch=100, train loss = 1.7506229877471924, train acc = 0.38999998569488525, time = 0.3276803493499756\n",
      "Training at step=17, batch=200, train loss = 1.8088316917419434, train acc = 0.36000001430511475, time = 0.3287060260772705\n",
      "Training at step=17, batch=300, train loss = 1.979965329170227, train acc = 0.28999999165534973, time = 0.3267936706542969\n",
      "Training at step=17, batch=400, train loss = 1.8997471332550049, train acc = 0.3799999952316284, time = 0.3299398422241211\n",
      "Testing at step=17, batch=0, test loss = 1.9733117818832397, test acc = 0.33000001311302185, time = 0.10888862609863281\n",
      "Testing at step=17, batch=20, test loss = 1.9063431024551392, test acc = 0.36000001430511475, time = 0.10867810249328613\n",
      "Testing at step=17, batch=40, test loss = 1.949741244316101, test acc = 0.33000001311302185, time = 0.10830116271972656\n",
      "Testing at step=17, batch=60, test loss = 1.8860499858856201, test acc = 0.3400000035762787, time = 0.10853433609008789\n",
      "Testing at step=17, batch=80, test loss = 1.7156667709350586, test acc = 0.3700000047683716, time = 0.10897326469421387\n",
      "Step 17 finished in 185.37580180168152, Train loss = 1.8672696347236633, Test loss = 1.8944121372699738; Train Acc = 0.35042000153660774, Test Acc = 0.3237999992072582\n",
      "Training at step=18, batch=0, train loss = 1.8097683191299438, train acc = 0.3700000047683716, time = 0.3365936279296875\n",
      "Training at step=18, batch=100, train loss = 1.7807927131652832, train acc = 0.36000001430511475, time = 0.33744382858276367\n",
      "Training at step=18, batch=200, train loss = 1.7482599020004272, train acc = 0.41999998688697815, time = 0.3291327953338623\n",
      "Training at step=18, batch=300, train loss = 1.714475393295288, train acc = 0.4000000059604645, time = 0.3343315124511719\n",
      "Training at step=18, batch=400, train loss = 2.073030471801758, train acc = 0.3499999940395355, time = 0.33071374893188477\n",
      "Testing at step=18, batch=0, test loss = 1.8750654458999634, test acc = 0.30000001192092896, time = 0.11051416397094727\n",
      "Testing at step=18, batch=20, test loss = 1.8303256034851074, test acc = 0.33000001311302185, time = 0.10853195190429688\n",
      "Testing at step=18, batch=40, test loss = 1.7864772081375122, test acc = 0.3400000035762787, time = 0.10970735549926758\n",
      "Testing at step=18, batch=60, test loss = 1.7855972051620483, test acc = 0.38999998569488525, time = 0.11223530769348145\n",
      "Testing at step=18, batch=80, test loss = 1.9172310829162598, test acc = 0.3799999952316284, time = 0.10820674896240234\n",
      "Step 18 finished in 185.61920166015625, Train loss = 1.8558330731391908, Test loss = 1.8758031523227692; Train Acc = 0.3532800005674362, Test Acc = 0.34450000077486037\n",
      "Training at step=19, batch=0, train loss = 1.9038234949111938, train acc = 0.30000001192092896, time = 0.32793712615966797\n",
      "Training at step=19, batch=100, train loss = 1.9340019226074219, train acc = 0.3400000035762787, time = 0.3306443691253662\n",
      "Training at step=19, batch=200, train loss = 1.942500114440918, train acc = 0.3100000023841858, time = 0.32703661918640137\n",
      "Training at step=19, batch=300, train loss = 1.9163610935211182, train acc = 0.25999999046325684, time = 0.3311767578125\n",
      "Training at step=19, batch=400, train loss = 1.8385751247406006, train acc = 0.30000001192092896, time = 0.33068394660949707\n",
      "Testing at step=19, batch=0, test loss = 1.8542275428771973, test acc = 0.33000001311302185, time = 0.1090385913848877\n",
      "Testing at step=19, batch=20, test loss = 1.921839952468872, test acc = 0.3400000035762787, time = 0.10828423500061035\n",
      "Testing at step=19, batch=40, test loss = 1.8374907970428467, test acc = 0.4000000059604645, time = 0.10944151878356934\n",
      "Testing at step=19, batch=60, test loss = 1.9541444778442383, test acc = 0.23999999463558197, time = 0.11035561561584473\n",
      "Testing at step=19, batch=80, test loss = 1.9580258131027222, test acc = 0.33000001311302185, time = 0.11152958869934082\n",
      "Step 19 finished in 185.45453786849976, Train loss = 1.8539996664524079, Test loss = 1.8569977700710296; Train Acc = 0.3551800015866756, Test Acc = 0.35090000107884406\n",
      "Training at step=20, batch=0, train loss = 1.860310673713684, train acc = 0.3400000035762787, time = 0.33473920822143555\n",
      "Training at step=20, batch=100, train loss = 1.8406422138214111, train acc = 0.3499999940395355, time = 0.3284883499145508\n",
      "Training at step=20, batch=200, train loss = 1.855972170829773, train acc = 0.3700000047683716, time = 0.3309292793273926\n",
      "Training at step=20, batch=300, train loss = 1.7524380683898926, train acc = 0.36000001430511475, time = 0.33089447021484375\n",
      "Training at step=20, batch=400, train loss = 1.8862414360046387, train acc = 0.33000001311302185, time = 0.3272700309753418\n",
      "Testing at step=20, batch=0, test loss = 1.9582164287567139, test acc = 0.28999999165534973, time = 0.10982871055603027\n",
      "Testing at step=20, batch=20, test loss = 1.6988723278045654, test acc = 0.41999998688697815, time = 0.11160516738891602\n",
      "Testing at step=20, batch=40, test loss = 1.8385765552520752, test acc = 0.38999998569488525, time = 0.11230874061584473\n",
      "Testing at step=20, batch=60, test loss = 1.8341403007507324, test acc = 0.3199999928474426, time = 0.10873532295227051\n",
      "Testing at step=20, batch=80, test loss = 1.8943437337875366, test acc = 0.3400000035762787, time = 0.10776925086975098\n",
      "Step 20 finished in 185.51647925376892, Train loss = 1.8465506765842439, Test loss = 1.8429569339752196; Train Acc = 0.3573200004696846, Test Acc = 0.355\n",
      "Training at step=21, batch=0, train loss = 1.8281140327453613, train acc = 0.3700000047683716, time = 0.32776379585266113\n",
      "Training at step=21, batch=100, train loss = 1.7586337327957153, train acc = 0.33000001311302185, time = 0.3274369239807129\n",
      "Training at step=21, batch=200, train loss = 1.7541066408157349, train acc = 0.3700000047683716, time = 0.3305964469909668\n",
      "Training at step=21, batch=300, train loss = 1.8310989141464233, train acc = 0.36000001430511475, time = 0.32964205741882324\n",
      "Training at step=21, batch=400, train loss = 1.7497575283050537, train acc = 0.3799999952316284, time = 0.3272111415863037\n",
      "Testing at step=21, batch=0, test loss = 1.699353814125061, test acc = 0.3799999952316284, time = 0.10850930213928223\n",
      "Testing at step=21, batch=20, test loss = 1.990006685256958, test acc = 0.30000001192092896, time = 0.1091313362121582\n",
      "Testing at step=21, batch=40, test loss = 1.8121627569198608, test acc = 0.33000001311302185, time = 0.10921502113342285\n",
      "Testing at step=21, batch=60, test loss = 1.8561336994171143, test acc = 0.3100000023841858, time = 0.11136317253112793\n",
      "Testing at step=21, batch=80, test loss = 1.7528257369995117, test acc = 0.3199999928474426, time = 0.10843062400817871\n",
      "Step 21 finished in 185.7305724620819, Train loss = 1.8412449674606324, Test loss = 1.8652043390274047; Train Acc = 0.3610000002682209, Test Acc = 0.3424000018835068\n",
      "Training at step=22, batch=0, train loss = 1.7895643711090088, train acc = 0.3400000035762787, time = 0.3285970687866211\n",
      "Training at step=22, batch=100, train loss = 1.8326934576034546, train acc = 0.4099999964237213, time = 0.32929491996765137\n",
      "Training at step=22, batch=200, train loss = 1.8577429056167603, train acc = 0.4000000059604645, time = 0.328141450881958\n",
      "Training at step=22, batch=300, train loss = 1.6387022733688354, train acc = 0.3799999952316284, time = 0.32682299613952637\n",
      "Training at step=22, batch=400, train loss = 1.8999888896942139, train acc = 0.3100000023841858, time = 0.32680726051330566\n",
      "Testing at step=22, batch=0, test loss = 1.7121243476867676, test acc = 0.4099999964237213, time = 0.10873651504516602\n",
      "Testing at step=22, batch=20, test loss = 1.763291597366333, test acc = 0.44999998807907104, time = 0.10839509963989258\n",
      "Testing at step=22, batch=40, test loss = 1.9104048013687134, test acc = 0.3100000023841858, time = 0.1064002513885498\n",
      "Testing at step=22, batch=60, test loss = 1.819839358329773, test acc = 0.3400000035762787, time = 0.10697007179260254\n",
      "Testing at step=22, batch=80, test loss = 1.8468658924102783, test acc = 0.3400000035762787, time = 0.10817122459411621\n",
      "Step 22 finished in 184.69509887695312, Train loss = 1.8366545877456666, Test loss = 1.8403289580345155; Train Acc = 0.36161999994516375, Test Acc = 0.35240000158548357\n",
      "Training at step=23, batch=0, train loss = 1.8959742784500122, train acc = 0.36000001430511475, time = 0.32660531997680664\n",
      "Training at step=23, batch=100, train loss = 1.7659498453140259, train acc = 0.38999998569488525, time = 0.3286728858947754\n",
      "Training at step=23, batch=200, train loss = 1.8323769569396973, train acc = 0.4099999964237213, time = 0.3245363235473633\n",
      "Training at step=23, batch=300, train loss = 1.892336368560791, train acc = 0.30000001192092896, time = 0.3315455913543701\n",
      "Training at step=23, batch=400, train loss = 1.873939871788025, train acc = 0.38999998569488525, time = 0.3285660743713379\n",
      "Testing at step=23, batch=0, test loss = 1.7753688097000122, test acc = 0.38999998569488525, time = 0.11098575592041016\n",
      "Testing at step=23, batch=20, test loss = 1.7009251117706299, test acc = 0.3799999952316284, time = 0.10699009895324707\n",
      "Testing at step=23, batch=40, test loss = 1.7612398862838745, test acc = 0.38999998569488525, time = 0.10737371444702148\n",
      "Testing at step=23, batch=60, test loss = 1.8434104919433594, test acc = 0.3700000047683716, time = 0.1078038215637207\n",
      "Testing at step=23, batch=80, test loss = 1.8464868068695068, test acc = 0.25, time = 0.10796236991882324\n",
      "Step 23 finished in 184.65238547325134, Train loss = 1.828485297203064, Test loss = 1.8366244852542877; Train Acc = 0.36574000045657157, Test Acc = 0.3538000012934208\n",
      "Training at step=24, batch=0, train loss = 1.7872394323349, train acc = 0.38999998569488525, time = 0.32792043685913086\n",
      "Training at step=24, batch=100, train loss = 1.8436977863311768, train acc = 0.38999998569488525, time = 0.3263733386993408\n",
      "Training at step=24, batch=200, train loss = 1.7126514911651611, train acc = 0.3499999940395355, time = 0.3308250904083252\n",
      "Training at step=24, batch=300, train loss = 1.9686219692230225, train acc = 0.23999999463558197, time = 0.3241734504699707\n",
      "Training at step=24, batch=400, train loss = 1.5994747877120972, train acc = 0.41999998688697815, time = 0.32626819610595703\n",
      "Testing at step=24, batch=0, test loss = 1.9192416667938232, test acc = 0.3100000023841858, time = 0.10768342018127441\n",
      "Testing at step=24, batch=20, test loss = 1.7875806093215942, test acc = 0.28999999165534973, time = 0.1071617603302002\n",
      "Testing at step=24, batch=40, test loss = 1.765845775604248, test acc = 0.33000001311302185, time = 0.10819578170776367\n",
      "Testing at step=24, batch=60, test loss = 1.7320339679718018, test acc = 0.4000000059604645, time = 0.10813641548156738\n",
      "Testing at step=24, batch=80, test loss = 1.8876512050628662, test acc = 0.33000001311302185, time = 0.11192917823791504\n",
      "Step 24 finished in 184.87080264091492, Train loss = 1.820104655265808, Test loss = 1.8304851233959198; Train Acc = 0.3694000001549721, Test Acc = 0.35780000150203706\n",
      "Training at step=25, batch=0, train loss = 1.8843733072280884, train acc = 0.3400000035762787, time = 0.32619285583496094\n",
      "Training at step=25, batch=100, train loss = 1.859529733657837, train acc = 0.4099999964237213, time = 0.32654309272766113\n",
      "Training at step=25, batch=200, train loss = 1.7736819982528687, train acc = 0.36000001430511475, time = 0.32865214347839355\n",
      "Training at step=25, batch=300, train loss = 1.7907453775405884, train acc = 0.3499999940395355, time = 0.33002758026123047\n",
      "Training at step=25, batch=400, train loss = 1.7546249628067017, train acc = 0.3799999952316284, time = 0.32592010498046875\n",
      "Testing at step=25, batch=0, test loss = 1.8301575183868408, test acc = 0.3700000047683716, time = 0.10822248458862305\n",
      "Testing at step=25, batch=20, test loss = 1.6970447301864624, test acc = 0.38999998569488525, time = 0.10780787467956543\n",
      "Testing at step=25, batch=40, test loss = 1.9823459386825562, test acc = 0.36000001430511475, time = 0.10848093032836914\n",
      "Testing at step=25, batch=60, test loss = 1.7451412677764893, test acc = 0.36000001430511475, time = 0.10803818702697754\n",
      "Testing at step=25, batch=80, test loss = 1.7007880210876465, test acc = 0.4399999976158142, time = 0.1079864501953125\n",
      "Step 25 finished in 185.00791311264038, Train loss = 1.8164599153995513, Test loss = 1.8248508739471436; Train Acc = 0.36957999974489214, Test Acc = 0.3553999997675419\n",
      "Training at step=26, batch=0, train loss = 1.7755088806152344, train acc = 0.4300000071525574, time = 0.3299679756164551\n",
      "Training at step=26, batch=100, train loss = 1.8839061260223389, train acc = 0.3799999952316284, time = 0.32635998725891113\n",
      "Training at step=26, batch=200, train loss = 1.6775356531143188, train acc = 0.4099999964237213, time = 0.3302159309387207\n",
      "Training at step=26, batch=300, train loss = 1.9224238395690918, train acc = 0.33000001311302185, time = 0.3307516574859619\n",
      "Training at step=26, batch=400, train loss = 1.822108507156372, train acc = 0.4000000059604645, time = 0.329679012298584\n",
      "Testing at step=26, batch=0, test loss = 1.7474675178527832, test acc = 0.4099999964237213, time = 0.10882735252380371\n",
      "Testing at step=26, batch=20, test loss = 1.6982933282852173, test acc = 0.4000000059604645, time = 0.10727667808532715\n",
      "Testing at step=26, batch=40, test loss = 1.8132573366165161, test acc = 0.3400000035762787, time = 0.10885047912597656\n",
      "Testing at step=26, batch=60, test loss = 1.894493579864502, test acc = 0.3499999940395355, time = 0.10693573951721191\n",
      "Testing at step=26, batch=80, test loss = 1.864382266998291, test acc = 0.41999998688697815, time = 0.10797715187072754\n",
      "Step 26 finished in 185.06816339492798, Train loss = 1.8105028150081635, Test loss = 1.8188120317459107; Train Acc = 0.37262000000476836, Test Acc = 0.36530000194907186\n",
      "Training at step=27, batch=0, train loss = 1.8097517490386963, train acc = 0.3100000023841858, time = 0.32778000831604004\n",
      "Training at step=27, batch=100, train loss = 1.8166683912277222, train acc = 0.3199999928474426, time = 0.3257608413696289\n",
      "Training at step=27, batch=200, train loss = 1.8128045797348022, train acc = 0.4000000059604645, time = 0.3261585235595703\n",
      "Training at step=27, batch=300, train loss = 1.8555103540420532, train acc = 0.38999998569488525, time = 0.3254714012145996\n",
      "Training at step=27, batch=400, train loss = 1.8645453453063965, train acc = 0.33000001311302185, time = 0.32774853706359863\n",
      "Testing at step=27, batch=0, test loss = 1.83780837059021, test acc = 0.3400000035762787, time = 0.10893511772155762\n",
      "Testing at step=27, batch=20, test loss = 1.9020037651062012, test acc = 0.30000001192092896, time = 0.10779452323913574\n",
      "Testing at step=27, batch=40, test loss = 1.843356966972351, test acc = 0.38999998569488525, time = 0.10760855674743652\n",
      "Testing at step=27, batch=60, test loss = 1.8240002393722534, test acc = 0.3199999928474426, time = 0.10979342460632324\n",
      "Testing at step=27, batch=80, test loss = 1.7295887470245361, test acc = 0.4099999964237213, time = 0.10944509506225586\n",
      "Step 27 finished in 184.704523563385, Train loss = 1.800655415058136, Test loss = 1.813242676258087; Train Acc = 0.37592000025510786, Test Acc = 0.36449999958276746\n",
      "Training at step=28, batch=0, train loss = 1.6672794818878174, train acc = 0.33000001311302185, time = 0.3300142288208008\n",
      "Training at step=28, batch=100, train loss = 1.6922674179077148, train acc = 0.3799999952316284, time = 0.32390475273132324\n",
      "Training at step=28, batch=200, train loss = 1.7729214429855347, train acc = 0.4300000071525574, time = 0.32805848121643066\n",
      "Training at step=28, batch=300, train loss = 1.841047763824463, train acc = 0.3499999940395355, time = 0.32939720153808594\n",
      "Training at step=28, batch=400, train loss = 1.7357723712921143, train acc = 0.3700000047683716, time = 0.3282043933868408\n",
      "Testing at step=28, batch=0, test loss = 1.7516461610794067, test acc = 0.38999998569488525, time = 0.10734105110168457\n",
      "Testing at step=28, batch=20, test loss = 1.7594993114471436, test acc = 0.3799999952316284, time = 0.10980701446533203\n",
      "Testing at step=28, batch=40, test loss = 1.8793128728866577, test acc = 0.33000001311302185, time = 0.10756754875183105\n",
      "Testing at step=28, batch=60, test loss = 1.7814370393753052, test acc = 0.44999998807907104, time = 0.11013269424438477\n",
      "Testing at step=28, batch=80, test loss = 1.792660117149353, test acc = 0.41999998688697815, time = 0.10896158218383789\n",
      "Step 28 finished in 184.94832348823547, Train loss = 1.7952246243953704, Test loss = 1.8064590907096862; Train Acc = 0.38018000015616416, Test Acc = 0.3664000014960766\n",
      "Training at step=29, batch=0, train loss = 1.8025892972946167, train acc = 0.41999998688697815, time = 0.3281683921813965\n",
      "Training at step=29, batch=100, train loss = 1.7519171237945557, train acc = 0.33000001311302185, time = 0.32683420181274414\n",
      "Training at step=29, batch=200, train loss = 1.6769288778305054, train acc = 0.38999998569488525, time = 0.33205556869506836\n",
      "Training at step=29, batch=300, train loss = 1.6885039806365967, train acc = 0.4000000059604645, time = 0.32968592643737793\n",
      "Training at step=29, batch=400, train loss = 1.8859128952026367, train acc = 0.3499999940395355, time = 0.3264126777648926\n",
      "Testing at step=29, batch=0, test loss = 1.7382214069366455, test acc = 0.36000001430511475, time = 0.10856842994689941\n",
      "Testing at step=29, batch=20, test loss = 1.7860348224639893, test acc = 0.3499999940395355, time = 0.11024260520935059\n",
      "Testing at step=29, batch=40, test loss = 1.8786215782165527, test acc = 0.36000001430511475, time = 0.11035776138305664\n",
      "Testing at step=29, batch=60, test loss = 1.7215572595596313, test acc = 0.41999998688697815, time = 0.10791921615600586\n",
      "Testing at step=29, batch=80, test loss = 1.8703640699386597, test acc = 0.3199999928474426, time = 0.10737323760986328\n",
      "Step 29 finished in 184.79648399353027, Train loss = 1.7865538091659545, Test loss = 1.809765487909317; Train Acc = 0.3790399998724461, Test Acc = 0.3593999999761581\n",
      "Training at step=30, batch=0, train loss = 1.7991832494735718, train acc = 0.41999998688697815, time = 0.32949280738830566\n",
      "Training at step=30, batch=100, train loss = 1.8437814712524414, train acc = 0.38999998569488525, time = 0.32579922676086426\n",
      "Training at step=30, batch=200, train loss = 1.7984461784362793, train acc = 0.3799999952316284, time = 0.3287692070007324\n",
      "Training at step=30, batch=300, train loss = 1.7957427501678467, train acc = 0.4000000059604645, time = 0.33133888244628906\n",
      "Training at step=30, batch=400, train loss = 1.8926430940628052, train acc = 0.2800000011920929, time = 0.3277246952056885\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.2\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa08391e0f4a851",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"1-cifar10_quanv_classical_linear.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a547ef2530ddda",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"1-cifar10_quanv_classical_linear.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
