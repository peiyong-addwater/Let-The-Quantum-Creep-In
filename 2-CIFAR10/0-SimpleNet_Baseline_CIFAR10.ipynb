{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T15:09:43.742362Z",
     "start_time": "2024-04-08T15:09:40.389895Z"
    }
   },
   "source": "!pip install torch torchvision torchaudio pennylane cotengra quimb torchmetrics --upgrade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.17.2)\r\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: pennylane in /usr/local/lib/python3.9/dist-packages (0.35.1)\r\n",
      "Requirement already satisfied: cotengra in /usr/local/lib/python3.9/dist-packages (0.5.6)\r\n",
      "Requirement already satisfied: quimb in /usr/local/lib/python3.9/dist-packages (1.7.3)\r\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (1.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch) (2.19.3)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.2.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.0.106)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from torch) (4.11.0)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch) (8.9.2.26)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch) (11.0.2.54)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\r\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\r\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.10.2)\r\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.4.4)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\r\n",
      "Requirement already satisfied: pennylane-lightning>=0.35 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.35.1)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\r\n",
      "Requirement already satisfied: autograd in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.6.2)\r\n",
      "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.6.9)\r\n",
      "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.10.0)\r\n",
      "Requirement already satisfied: rustworkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.14.2)\r\n",
      "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\r\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\r\n",
      "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.59.1)\r\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.12.3)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (0.11.2)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\r\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\r\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.39->quimb) (0.42.0)\r\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:09:47.712672Z",
     "start_time": "2024-04-08T15:09:43.744061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "\n",
    "\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "#REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ],
   "id": "f4efb6d39bdc04a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:09:49.333308Z",
     "start_time": "2024-04-08T15:09:47.714194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ],
   "id": "b724edf448668f3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n",
      "tensor([7, 2, 7, 6, 4, 5, 7, 1, 8, 1, 1, 0, 8, 9, 7, 3, 1, 4, 6, 8, 2, 6, 5, 1,\n",
      "        5, 9, 7, 2, 6, 5, 5, 9, 3, 6, 1, 9, 7, 6, 2, 8, 1, 8, 4, 8, 9, 0, 1, 5,\n",
      "        2, 3, 3, 2, 6, 2, 5, 7, 6, 1, 8, 7, 7, 1, 5, 6])\n",
      "tensor([ 0.0275, -0.0196, -0.0353, -0.6078, -0.7961, -0.8118, -0.8196, -0.7569,\n",
      "        -0.7490, -0.7176, -0.7490, -0.7569, -0.7647, -0.8118, -0.7569, -0.7412,\n",
      "        -0.7804, -0.6157,  0.0118,  0.1294,  0.0902,  0.0902,  0.0196,  0.0196,\n",
      "         0.0745,  0.0745,  0.0667,  0.0824,  0.0431,  0.0039, -0.0118,  0.0118])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:09:53.339406Z",
     "start_time": "2024-04-08T15:09:49.335406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SimpleNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(3, 32, kernel_size=3), # 32x30x30\n",
    "        torch.nn.Conv2d(32, 16, kernel_size=3), # 64x28x28\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(16*28*28, 10),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "net = SimpleNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ],
   "id": "97cae9496e4a1ff2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "SimpleNet(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=12544, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:31:18.758903Z",
     "start_time": "2024-04-08T15:09:53.340703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchmetrics\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.2\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ],
   "id": "a960b36c56c8cd28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 500, Number of test batches = 100\n",
      "Print every train batch = 100, Print every test batch = 20\n",
      "Training at step=0, batch=0, train loss = 2.3259717360768293, train acc = 0.05000000074505806, time = 0.18309521675109863\n",
      "Training at step=0, batch=100, train loss = 1.9358004381582534, train acc = 0.28999999165534973, time = 0.007676601409912109\n",
      "Training at step=0, batch=200, train loss = 2.0321000498319157, train acc = 0.30000001192092896, time = 0.0077359676361083984\n",
      "Training at step=0, batch=300, train loss = 1.9192864594149852, train acc = 0.3499999940395355, time = 0.007567405700683594\n",
      "Training at step=0, batch=400, train loss = 1.8045777115350372, train acc = 0.3499999940395355, time = 0.007400989532470703\n",
      "Testing at step=0, batch=0, test loss = 1.8605875564158711, test acc = 0.3799999952316284, time = 0.0018236637115478516\n",
      "Testing at step=0, batch=20, test loss = 1.9151761059015364, test acc = 0.28999999165534973, time = 0.0017616748809814453\n",
      "Testing at step=0, batch=40, test loss = 1.7476384853301152, test acc = 0.41999998688697815, time = 0.001863718032836914\n",
      "Testing at step=0, batch=60, test loss = 1.9257349724965622, test acc = 0.28999999165534973, time = 0.0017309188842773438\n",
      "Testing at step=0, batch=80, test loss = 1.9849781687700792, test acc = 0.2800000011920929, time = 0.0016961097717285156\n",
      "Step 0 finished in 12.955395698547363, Train loss = 1.964271524081626, Test loss = 1.8514617409960317; Train Acc = 0.31422000055760146, Test Acc = 0.3616000011563301\n",
      "Training at step=1, batch=0, train loss = 1.84394899124286, train acc = 0.3400000035762787, time = 0.007509946823120117\n",
      "Training at step=1, batch=100, train loss = 1.7484910540866292, train acc = 0.3799999952316284, time = 0.0076634883880615234\n",
      "Training at step=1, batch=200, train loss = 1.839368672361382, train acc = 0.3199999928474426, time = 0.007615089416503906\n",
      "Training at step=1, batch=300, train loss = 1.7356640988240875, train acc = 0.46000000834465027, time = 0.007593393325805664\n",
      "Training at step=1, batch=400, train loss = 1.7078258582323078, train acc = 0.36000001430511475, time = 0.007622957229614258\n",
      "Testing at step=1, batch=0, test loss = 1.7279759866125102, test acc = 0.3700000047683716, time = 0.0017933845520019531\n",
      "Testing at step=1, batch=20, test loss = 1.7490795195099624, test acc = 0.4300000071525574, time = 0.0017910003662109375\n",
      "Testing at step=1, batch=40, test loss = 1.8713872878519158, test acc = 0.3100000023841858, time = 0.0018589496612548828\n",
      "Testing at step=1, batch=60, test loss = 1.7637052037687198, test acc = 0.38999998569488525, time = 0.0017969608306884766\n",
      "Testing at step=1, batch=80, test loss = 1.8094300088544397, test acc = 0.4300000071525574, time = 0.0017130374908447266\n",
      "Step 1 finished in 12.763970136642456, Train loss = 1.8104111594929702, Test loss = 1.7773664336575528; Train Acc = 0.3764400006532669, Test Acc = 0.38459999948740003\n",
      "Training at step=2, batch=0, train loss = 1.7990971895941643, train acc = 0.38999998569488525, time = 0.007657527923583984\n",
      "Training at step=2, batch=100, train loss = 1.739263392491194, train acc = 0.41999998688697815, time = 0.00748896598815918\n",
      "Training at step=2, batch=200, train loss = 1.7207629003889213, train acc = 0.4300000071525574, time = 0.007709026336669922\n",
      "Training at step=2, batch=300, train loss = 1.819789953160927, train acc = 0.4300000071525574, time = 0.007578372955322266\n",
      "Training at step=2, batch=400, train loss = 1.7654096905914016, train acc = 0.4099999964237213, time = 0.007534027099609375\n",
      "Testing at step=2, batch=0, test loss = 1.7484572396284255, test acc = 0.47999998927116394, time = 0.00177001953125\n",
      "Testing at step=2, batch=20, test loss = 1.6449119928926317, test acc = 0.4099999964237213, time = 0.0016820430755615234\n",
      "Testing at step=2, batch=40, test loss = 1.7586028308962625, test acc = 0.4399999976158142, time = 0.0017342567443847656\n",
      "Testing at step=2, batch=60, test loss = 1.667340538909857, test acc = 0.46000000834465027, time = 0.0016655921936035156\n",
      "Testing at step=2, batch=80, test loss = 1.7981346728051433, test acc = 0.3700000047683716, time = 0.0017652511596679688\n",
      "Step 2 finished in 12.875286102294922, Train loss = 1.764765625893487, Test loss = 1.7477829104644902; Train Acc = 0.3928599991798401, Test Acc = 0.39479999870061877\n",
      "Training at step=3, batch=0, train loss = 1.6324656807624052, train acc = 0.41999998688697815, time = 0.008178949356079102\n",
      "Training at step=3, batch=100, train loss = 1.844255989166469, train acc = 0.4000000059604645, time = 0.0077092647552490234\n",
      "Training at step=3, batch=200, train loss = 1.5801642185843248, train acc = 0.46000000834465027, time = 0.007575273513793945\n",
      "Training at step=3, batch=300, train loss = 1.776162313586785, train acc = 0.4000000059604645, time = 0.007564067840576172\n",
      "Training at step=3, batch=400, train loss = 1.644734849435308, train acc = 0.4399999976158142, time = 0.007596492767333984\n",
      "Testing at step=3, batch=0, test loss = 1.8044899866446193, test acc = 0.3700000047683716, time = 0.0017735958099365234\n",
      "Testing at step=3, batch=20, test loss = 1.5916457347411817, test acc = 0.49000000953674316, time = 0.001684427261352539\n",
      "Testing at step=3, batch=40, test loss = 1.6732190442913817, test acc = 0.44999998807907104, time = 0.0017647743225097656\n",
      "Testing at step=3, batch=60, test loss = 1.7483087543983644, test acc = 0.3199999928474426, time = 0.0017805099487304688\n",
      "Testing at step=3, batch=80, test loss = 1.820071273132293, test acc = 0.3499999940395355, time = 0.001870870590209961\n",
      "Step 3 finished in 12.755370378494263, Train loss = 1.7422287550134892, Test loss = 1.7328158398564113; Train Acc = 0.4021999988555908, Test Acc = 0.40129999846220016\n",
      "Training at step=4, batch=0, train loss = 1.9380984725849222, train acc = 0.3700000047683716, time = 0.0076825618743896484\n",
      "Training at step=4, batch=100, train loss = 1.6253302865033517, train acc = 0.41999998688697815, time = 0.007513761520385742\n",
      "Training at step=4, batch=200, train loss = 1.8078780470986542, train acc = 0.3799999952316284, time = 0.007413387298583984\n",
      "Training at step=4, batch=300, train loss = 1.758711214290868, train acc = 0.3799999952316284, time = 0.0075473785400390625\n",
      "Training at step=4, batch=400, train loss = 1.5693873235088787, train acc = 0.47999998927116394, time = 0.007547140121459961\n",
      "Testing at step=4, batch=0, test loss = 1.7059625459546701, test acc = 0.4099999964237213, time = 0.0018804073333740234\n",
      "Testing at step=4, batch=20, test loss = 1.741863806372051, test acc = 0.4000000059604645, time = 0.0017211437225341797\n",
      "Testing at step=4, batch=40, test loss = 1.7013158228345173, test acc = 0.3100000023841858, time = 0.0017137527465820312\n",
      "Testing at step=4, batch=60, test loss = 1.4850819252927343, test acc = 0.5400000214576721, time = 0.0017380714416503906\n",
      "Testing at step=4, batch=80, test loss = 1.8575018828477092, test acc = 0.36000001430511475, time = 0.0018367767333984375\n",
      "Step 4 finished in 12.782121658325195, Train loss = 1.727440268211089, Test loss = 1.727597542117411; Train Acc = 0.4093799977004528, Test Acc = 0.4046000003814697\n",
      "Training at step=5, batch=0, train loss = 1.7255396556386509, train acc = 0.44999998807907104, time = 0.0075304508209228516\n",
      "Training at step=5, batch=100, train loss = 1.6082543094834356, train acc = 0.44999998807907104, time = 0.007431507110595703\n",
      "Training at step=5, batch=200, train loss = 1.6378738833746747, train acc = 0.46000000834465027, time = 0.007471561431884766\n",
      "Training at step=5, batch=300, train loss = 1.887898915202607, train acc = 0.4000000059604645, time = 0.0075871944427490234\n",
      "Training at step=5, batch=400, train loss = 1.5727136452875559, train acc = 0.47999998927116394, time = 0.007476091384887695\n",
      "Testing at step=5, batch=0, test loss = 1.8154146494517636, test acc = 0.33000001311302185, time = 0.0020074844360351562\n",
      "Testing at step=5, batch=20, test loss = 1.9363691707883421, test acc = 0.4099999964237213, time = 0.0016849040985107422\n",
      "Testing at step=5, batch=40, test loss = 1.7349835720596518, test acc = 0.3799999952316284, time = 0.001745462417602539\n",
      "Testing at step=5, batch=60, test loss = 1.7210852630823998, test acc = 0.4000000059604645, time = 0.0018768310546875\n",
      "Testing at step=5, batch=80, test loss = 1.885541290361199, test acc = 0.4000000059604645, time = 0.0017235279083251953\n",
      "Step 5 finished in 12.923064708709717, Train loss = 1.7183154888578551, Test loss = 1.7177484049891054; Train Acc = 0.4139599984288216, Test Acc = 0.4112999975681305\n",
      "Training at step=6, batch=0, train loss = 1.7076646299006502, train acc = 0.4000000059604645, time = 0.007676601409912109\n",
      "Training at step=6, batch=100, train loss = 1.6542451333260453, train acc = 0.5099999904632568, time = 0.007436275482177734\n",
      "Training at step=6, batch=200, train loss = 1.686736145466788, train acc = 0.47999998927116394, time = 0.007656574249267578\n",
      "Training at step=6, batch=300, train loss = 1.6441488391054326, train acc = 0.49000000953674316, time = 0.007580995559692383\n",
      "Training at step=6, batch=400, train loss = 1.8055842667906803, train acc = 0.4000000059604645, time = 0.0074193477630615234\n",
      "Testing at step=6, batch=0, test loss = 1.6165223224983007, test acc = 0.4399999976158142, time = 0.0018260478973388672\n",
      "Testing at step=6, batch=20, test loss = 1.670933845322846, test acc = 0.46000000834465027, time = 0.0018765926361083984\n",
      "Testing at step=6, batch=40, test loss = 1.6980630737084907, test acc = 0.41999998688697815, time = 0.00183868408203125\n",
      "Testing at step=6, batch=60, test loss = 1.9382673694703418, test acc = 0.3700000047683716, time = 0.0017764568328857422\n",
      "Testing at step=6, batch=80, test loss = 1.7672605217190247, test acc = 0.41999998688697815, time = 0.0017652511596679688\n",
      "Step 6 finished in 12.805217981338501, Train loss = 1.711032635250732, Test loss = 1.7153343807462247; Train Acc = 0.4166599980592728, Test Acc = 0.41189999848604203\n",
      "Training at step=7, batch=0, train loss = 1.7833439040408583, train acc = 0.3700000047683716, time = 0.007651329040527344\n",
      "Training at step=7, batch=100, train loss = 1.6701503724478315, train acc = 0.3799999952316284, time = 0.00746464729309082\n",
      "Training at step=7, batch=200, train loss = 1.7023931273866797, train acc = 0.4399999976158142, time = 0.007521152496337891\n",
      "Training at step=7, batch=300, train loss = 1.7037805294126835, train acc = 0.4000000059604645, time = 0.007635593414306641\n",
      "Training at step=7, batch=400, train loss = 1.5597001067215055, train acc = 0.46000000834465027, time = 0.007603645324707031\n",
      "Testing at step=7, batch=0, test loss = 1.5906395657998371, test acc = 0.4699999988079071, time = 0.0017828941345214844\n",
      "Testing at step=7, batch=20, test loss = 1.6078229432459352, test acc = 0.44999998807907104, time = 0.0016720294952392578\n",
      "Testing at step=7, batch=40, test loss = 1.5874156944867217, test acc = 0.38999998569488525, time = 0.0017118453979492188\n",
      "Testing at step=7, batch=60, test loss = 1.6278787306152582, test acc = 0.4300000071525574, time = 0.001786947250366211\n",
      "Testing at step=7, batch=80, test loss = 1.868856922774312, test acc = 0.4000000059604645, time = 0.001708984375\n",
      "Step 7 finished in 12.75208830833435, Train loss = 1.7055823985166023, Test loss = 1.71431579186698; Train Acc = 0.41939999800920486, Test Acc = 0.40949999809265136\n",
      "Training at step=8, batch=0, train loss = 1.7247997722245723, train acc = 0.3799999952316284, time = 0.007566213607788086\n",
      "Training at step=8, batch=100, train loss = 1.6414222264181675, train acc = 0.4699999988079071, time = 0.007572650909423828\n",
      "Training at step=8, batch=200, train loss = 1.754851393770003, train acc = 0.3400000035762787, time = 0.007625102996826172\n",
      "Training at step=8, batch=300, train loss = 1.6300885128488205, train acc = 0.4699999988079071, time = 0.007645368576049805\n",
      "Training at step=8, batch=400, train loss = 1.6167508120325789, train acc = 0.5299999713897705, time = 0.007608652114868164\n",
      "Testing at step=8, batch=0, test loss = 1.697893261242456, test acc = 0.4300000071525574, time = 0.0018260478973388672\n",
      "Testing at step=8, batch=20, test loss = 1.8605603523005398, test acc = 0.3799999952316284, time = 0.001711130142211914\n",
      "Testing at step=8, batch=40, test loss = 1.6405788275275046, test acc = 0.4399999976158142, time = 0.0017354488372802734\n",
      "Testing at step=8, batch=60, test loss = 1.7068176457010116, test acc = 0.46000000834465027, time = 0.0017538070678710938\n",
      "Testing at step=8, batch=80, test loss = 1.5697392623217103, test acc = 0.49000000953674316, time = 0.0018155574798583984\n",
      "Step 8 finished in 12.856720924377441, Train loss = 1.699820661257392, Test loss = 1.7122316900741879; Train Acc = 0.42303999772667883, Test Acc = 0.4161999985575676\n",
      "Training at step=9, batch=0, train loss = 1.569049874093841, train acc = 0.44999998807907104, time = 0.00770878791809082\n",
      "Training at step=9, batch=100, train loss = 1.5890366533690108, train acc = 0.4699999988079071, time = 0.007656574249267578\n",
      "Training at step=9, batch=200, train loss = 1.7145830576416685, train acc = 0.44999998807907104, time = 0.007565498352050781\n",
      "Training at step=9, batch=300, train loss = 1.7447387082538521, train acc = 0.4099999964237213, time = 0.007483482360839844\n",
      "Training at step=9, batch=400, train loss = 1.7644049291844737, train acc = 0.3499999940395355, time = 0.007559061050415039\n",
      "Testing at step=9, batch=0, test loss = 1.6814585293814672, test acc = 0.46000000834465027, time = 0.0018045902252197266\n",
      "Testing at step=9, batch=20, test loss = 1.7978940735711688, test acc = 0.33000001311302185, time = 0.0018160343170166016\n",
      "Testing at step=9, batch=40, test loss = 1.6174728786746693, test acc = 0.38999998569488525, time = 0.0017971992492675781\n",
      "Testing at step=9, batch=60, test loss = 1.7740891659653137, test acc = 0.3799999952316284, time = 0.0017724037170410156\n",
      "Testing at step=9, batch=80, test loss = 1.8176059772019622, test acc = 0.3400000035762787, time = 0.0017478466033935547\n",
      "Step 9 finished in 12.791708946228027, Train loss = 1.6961455581782474, Test loss = 1.7101573636883236; Train Acc = 0.4244399982094765, Test Acc = 0.41389999747276307\n",
      "Training at step=10, batch=0, train loss = 1.8325825275426657, train acc = 0.3199999928474426, time = 0.007633686065673828\n",
      "Training at step=10, batch=100, train loss = 1.6873295497903198, train acc = 0.41999998688697815, time = 0.00762939453125\n",
      "Training at step=10, batch=200, train loss = 1.7073973457342486, train acc = 0.4099999964237213, time = 0.00769495964050293\n",
      "Training at step=10, batch=300, train loss = 1.6606086757420717, train acc = 0.4300000071525574, time = 0.0074672698974609375\n",
      "Training at step=10, batch=400, train loss = 1.6855360785143383, train acc = 0.4699999988079071, time = 0.007359504699707031\n",
      "Testing at step=10, batch=0, test loss = 1.7346012068293046, test acc = 0.4399999976158142, time = 0.0019702911376953125\n",
      "Testing at step=10, batch=20, test loss = 1.6827372974438666, test acc = 0.5, time = 0.001802206039428711\n",
      "Testing at step=10, batch=40, test loss = 1.6177274216329964, test acc = 0.4000000059604645, time = 0.0017938613891601562\n",
      "Testing at step=10, batch=60, test loss = 1.9025757468797757, test acc = 0.4699999988079071, time = 0.001706838607788086\n",
      "Testing at step=10, batch=80, test loss = 1.6034575836210752, test acc = 0.4099999964237213, time = 0.0017120838165283203\n",
      "Step 10 finished in 12.869216442108154, Train loss = 1.6920018227198357, Test loss = 1.7109690258689432; Train Acc = 0.42677999845147135, Test Acc = 0.4126999962329865\n",
      "Training at step=11, batch=0, train loss = 1.8052287799062219, train acc = 0.4300000071525574, time = 0.007668972015380859\n",
      "Training at step=11, batch=100, train loss = 1.547013337990839, train acc = 0.5, time = 0.007534027099609375\n",
      "Training at step=11, batch=200, train loss = 1.563816689066919, train acc = 0.4699999988079071, time = 0.007605791091918945\n",
      "Training at step=11, batch=300, train loss = 1.6421246074681621, train acc = 0.46000000834465027, time = 0.007639408111572266\n",
      "Training at step=11, batch=400, train loss = 1.646413909996229, train acc = 0.49000000953674316, time = 0.007571697235107422\n",
      "Testing at step=11, batch=0, test loss = 1.8480571618551602, test acc = 0.41999998688697815, time = 0.0017802715301513672\n",
      "Testing at step=11, batch=20, test loss = 1.5405342153488857, test acc = 0.46000000834465027, time = 0.001707315444946289\n",
      "Testing at step=11, batch=40, test loss = 1.9899890359130308, test acc = 0.3199999928474426, time = 0.0018169879913330078\n",
      "Testing at step=11, batch=60, test loss = 1.8055594064582081, test acc = 0.3199999928474426, time = 0.0018279552459716797\n",
      "Testing at step=11, batch=80, test loss = 1.593817731548724, test acc = 0.41999998688697815, time = 0.0019109249114990234\n",
      "Step 11 finished in 12.904882907867432, Train loss = 1.689023500415557, Test loss = 1.7087930469112984; Train Acc = 0.4276599985361099, Test Acc = 0.4111999982595444\n",
      "Training at step=12, batch=0, train loss = 1.8597400361641672, train acc = 0.38999998569488525, time = 0.007513523101806641\n",
      "Training at step=12, batch=100, train loss = 1.5875776055094528, train acc = 0.4300000071525574, time = 0.007585763931274414\n",
      "Training at step=12, batch=200, train loss = 1.926947966613778, train acc = 0.38999998569488525, time = 0.007508754730224609\n",
      "Training at step=12, batch=300, train loss = 1.8134730152598544, train acc = 0.3499999940395355, time = 0.007409334182739258\n",
      "Training at step=12, batch=400, train loss = 1.7004116142140548, train acc = 0.4000000059604645, time = 0.007454395294189453\n",
      "Testing at step=12, batch=0, test loss = 1.8141198139126158, test acc = 0.4000000059604645, time = 0.0017426013946533203\n",
      "Testing at step=12, batch=20, test loss = 1.6732554580001766, test acc = 0.4399999976158142, time = 0.0017952919006347656\n",
      "Testing at step=12, batch=40, test loss = 1.6951170039043788, test acc = 0.41999998688697815, time = 0.0018072128295898438\n",
      "Testing at step=12, batch=60, test loss = 1.6380464172466074, test acc = 0.41999998688697815, time = 0.0018420219421386719\n",
      "Testing at step=12, batch=80, test loss = 1.7210562940613547, test acc = 0.41999998688697815, time = 0.0017142295837402344\n",
      "Step 12 finished in 12.79010796546936, Train loss = 1.6866695496968744, Test loss = 1.7058406706200118; Train Acc = 0.4291199977993965, Test Acc = 0.4167999985814095\n",
      "Training at step=13, batch=0, train loss = 1.7543754570161016, train acc = 0.3799999952316284, time = 0.007615804672241211\n",
      "Training at step=13, batch=100, train loss = 1.6215181408839863, train acc = 0.46000000834465027, time = 0.007668733596801758\n",
      "Training at step=13, batch=200, train loss = 1.6088057598473973, train acc = 0.4399999976158142, time = 0.007685184478759766\n",
      "Training at step=13, batch=300, train loss = 1.608052705068139, train acc = 0.4699999988079071, time = 0.007607460021972656\n",
      "Training at step=13, batch=400, train loss = 1.6366172504116268, train acc = 0.4699999988079071, time = 0.007544040679931641\n",
      "Testing at step=13, batch=0, test loss = 1.9385976769703657, test acc = 0.3400000035762787, time = 0.0018014907836914062\n",
      "Testing at step=13, batch=20, test loss = 1.8555049088502247, test acc = 0.3700000047683716, time = 0.0018012523651123047\n",
      "Testing at step=13, batch=40, test loss = 1.828273232403633, test acc = 0.36000001430511475, time = 0.0017559528350830078\n",
      "Testing at step=13, batch=60, test loss = 1.6791840289979425, test acc = 0.4099999964237213, time = 0.001756429672241211\n",
      "Testing at step=13, batch=80, test loss = 1.661090112643279, test acc = 0.47999998927116394, time = 0.0017673969268798828\n",
      "Step 13 finished in 12.819280624389648, Train loss = 1.6839085954205029, Test loss = 1.7066894051770047; Train Acc = 0.42929999727010726, Test Acc = 0.4157000005245209\n",
      "Training at step=14, batch=0, train loss = 1.6285579187091843, train acc = 0.46000000834465027, time = 0.007659912109375\n",
      "Training at step=14, batch=100, train loss = 1.574810742745616, train acc = 0.47999998927116394, time = 0.007525205612182617\n",
      "Training at step=14, batch=200, train loss = 1.5833865077452678, train acc = 0.4699999988079071, time = 0.007511615753173828\n",
      "Training at step=14, batch=300, train loss = 1.6782034738327605, train acc = 0.4000000059604645, time = 0.007575511932373047\n",
      "Training at step=14, batch=400, train loss = 1.8523612091700479, train acc = 0.3700000047683716, time = 0.007536888122558594\n",
      "Testing at step=14, batch=0, test loss = 1.6569074097926468, test acc = 0.38999998569488525, time = 0.0018355846405029297\n",
      "Testing at step=14, batch=20, test loss = 1.5220223353230862, test acc = 0.5, time = 0.0016756057739257812\n",
      "Testing at step=14, batch=40, test loss = 1.736195449526495, test acc = 0.3799999952316284, time = 0.0017750263214111328\n",
      "Testing at step=14, batch=60, test loss = 1.871850727304061, test acc = 0.3499999940395355, time = 0.0017619132995605469\n",
      "Testing at step=14, batch=80, test loss = 1.744498960349902, test acc = 0.33000001311302185, time = 0.00162506103515625\n",
      "Step 14 finished in 13.005980968475342, Train loss = 1.6808819415171898, Test loss = 1.7058273833962694; Train Acc = 0.4294799984097481, Test Acc = 0.4114999973773956\n",
      "Training at step=15, batch=0, train loss = 1.6805930591921427, train acc = 0.46000000834465027, time = 0.007475376129150391\n",
      "Training at step=15, batch=100, train loss = 1.7723508661193321, train acc = 0.3400000035762787, time = 0.007642030715942383\n",
      "Training at step=15, batch=200, train loss = 1.6466124587011624, train acc = 0.4699999988079071, time = 0.007442474365234375\n",
      "Training at step=15, batch=300, train loss = 1.66775486248919, train acc = 0.36000001430511475, time = 0.007517576217651367\n",
      "Training at step=15, batch=400, train loss = 1.679921365844912, train acc = 0.4099999964237213, time = 0.007468223571777344\n",
      "Testing at step=15, batch=0, test loss = 1.7681940877092472, test acc = 0.4099999964237213, time = 0.0017240047454833984\n",
      "Testing at step=15, batch=20, test loss = 1.7987249565187826, test acc = 0.4399999976158142, time = 0.0017769336700439453\n",
      "Testing at step=15, batch=40, test loss = 1.7236963144356798, test acc = 0.4099999964237213, time = 0.0017325878143310547\n",
      "Testing at step=15, batch=60, test loss = 1.756363777518855, test acc = 0.4099999964237213, time = 0.001786947250366211\n",
      "Testing at step=15, batch=80, test loss = 1.5666220449023527, test acc = 0.4699999988079071, time = 0.001829385757446289\n",
      "Step 15 finished in 12.71808910369873, Train loss = 1.678533503557738, Test loss = 1.7065143814085155; Train Acc = 0.4303799973726273, Test Acc = 0.41589999675750733\n",
      "Training at step=16, batch=0, train loss = 1.70261104121248, train acc = 0.5400000214576721, time = 0.007609844207763672\n",
      "Training at step=16, batch=100, train loss = 1.6704929408471696, train acc = 0.3700000047683716, time = 0.007467508316040039\n",
      "Training at step=16, batch=200, train loss = 1.6426070775198898, train acc = 0.44999998807907104, time = 0.007580757141113281\n",
      "Training at step=16, batch=300, train loss = 1.8698919701947259, train acc = 0.44999998807907104, time = 0.007668733596801758\n",
      "Training at step=16, batch=400, train loss = 1.8655930084065315, train acc = 0.3499999940395355, time = 0.007359743118286133\n",
      "Testing at step=16, batch=0, test loss = 1.7214101936273112, test acc = 0.4099999964237213, time = 0.0018057823181152344\n",
      "Testing at step=16, batch=20, test loss = 1.6373140281815708, test acc = 0.4300000071525574, time = 0.0017347335815429688\n",
      "Testing at step=16, batch=40, test loss = 1.7000967559996403, test acc = 0.4000000059604645, time = 0.0017201900482177734\n",
      "Testing at step=16, batch=60, test loss = 1.7771457887408209, test acc = 0.4300000071525574, time = 0.0017399787902832031\n",
      "Testing at step=16, batch=80, test loss = 1.566959492734853, test acc = 0.44999998807907104, time = 0.0018095970153808594\n",
      "Step 16 finished in 12.920142889022827, Train loss = 1.6771985441391934, Test loss = 1.7070766635647447; Train Acc = 0.4320799977183342, Test Acc = 0.4146999974548817\n",
      "Training at step=17, batch=0, train loss = 1.6164635492790944, train acc = 0.4399999976158142, time = 0.007698774337768555\n",
      "Training at step=17, batch=100, train loss = 1.7932990512657307, train acc = 0.38999998569488525, time = 0.007543802261352539\n",
      "Training at step=17, batch=200, train loss = 1.7660806788858943, train acc = 0.3799999952316284, time = 0.007527589797973633\n",
      "Training at step=17, batch=300, train loss = 1.8330118416317658, train acc = 0.36000001430511475, time = 0.0074007511138916016\n",
      "Training at step=17, batch=400, train loss = 1.485627084874634, train acc = 0.5099999904632568, time = 0.007546424865722656\n",
      "Testing at step=17, batch=0, test loss = 1.8004962497753538, test acc = 0.4099999964237213, time = 0.0018262863159179688\n",
      "Testing at step=17, batch=20, test loss = 1.6634154532518715, test acc = 0.5099999904632568, time = 0.0017459392547607422\n",
      "Testing at step=17, batch=40, test loss = 1.6844511152184194, test acc = 0.36000001430511475, time = 0.0017397403717041016\n",
      "Testing at step=17, batch=60, test loss = 1.9099114036840057, test acc = 0.3700000047683716, time = 0.00177001953125\n",
      "Testing at step=17, batch=80, test loss = 1.6113274923750174, test acc = 0.41999998688697815, time = 0.001821279525756836\n",
      "Step 17 finished in 12.833991765975952, Train loss = 1.6747665862720165, Test loss = 1.7058673822998573; Train Acc = 0.4332799986600876, Test Acc = 0.4159999990463257\n",
      "Training at step=18, batch=0, train loss = 1.6421223485958094, train acc = 0.46000000834465027, time = 0.0076596736907958984\n",
      "Training at step=18, batch=100, train loss = 1.8083602923070965, train acc = 0.3499999940395355, time = 0.007495880126953125\n",
      "Training at step=18, batch=200, train loss = 1.6990576768713956, train acc = 0.4000000059604645, time = 0.0075113773345947266\n",
      "Training at step=18, batch=300, train loss = 1.4852202076893564, train acc = 0.5099999904632568, time = 0.007720947265625\n",
      "Training at step=18, batch=400, train loss = 1.6679530234274125, train acc = 0.44999998807907104, time = 0.007628679275512695\n",
      "Testing at step=18, batch=0, test loss = 1.7582314367594412, test acc = 0.3400000035762787, time = 0.0017638206481933594\n",
      "Testing at step=18, batch=20, test loss = 1.7625364929700174, test acc = 0.46000000834465027, time = 0.0017023086547851562\n",
      "Testing at step=18, batch=40, test loss = 1.425838921450345, test acc = 0.5299999713897705, time = 0.0017108917236328125\n",
      "Testing at step=18, batch=60, test loss = 1.7061457727431408, test acc = 0.44999998807907104, time = 0.0016682147979736328\n",
      "Testing at step=18, batch=80, test loss = 1.7283693807397653, test acc = 0.30000001192092896, time = 0.001705169677734375\n",
      "Step 18 finished in 12.832465887069702, Train loss = 1.6728965298287448, Test loss = 1.7039473391282038; Train Acc = 0.4338399990200996, Test Acc = 0.41479999959468844\n",
      "Training at step=19, batch=0, train loss = 1.7258753863433827, train acc = 0.3799999952316284, time = 0.0074961185455322266\n",
      "Training at step=19, batch=100, train loss = 1.6497673983664058, train acc = 0.46000000834465027, time = 0.007451057434082031\n",
      "Training at step=19, batch=200, train loss = 1.6007579976546455, train acc = 0.4399999976158142, time = 0.007688283920288086\n",
      "Training at step=19, batch=300, train loss = 1.577643864583621, train acc = 0.44999998807907104, time = 0.007515907287597656\n",
      "Training at step=19, batch=400, train loss = 1.771882074710127, train acc = 0.4000000059604645, time = 0.007469892501831055\n",
      "Testing at step=19, batch=0, test loss = 1.936086522332067, test acc = 0.33000001311302185, time = 0.0017838478088378906\n",
      "Testing at step=19, batch=20, test loss = 1.6344326061155252, test acc = 0.46000000834465027, time = 0.0017554759979248047\n",
      "Testing at step=19, batch=40, test loss = 1.7180179527400063, test acc = 0.38999998569488525, time = 0.0017962455749511719\n",
      "Testing at step=19, batch=60, test loss = 1.749806780043078, test acc = 0.41999998688697815, time = 0.0017354488372802734\n",
      "Testing at step=19, batch=80, test loss = 1.8880608761358821, test acc = 0.38999998569488525, time = 0.0017566680908203125\n",
      "Step 19 finished in 12.774728059768677, Train loss = 1.670733526599158, Test loss = 1.705622132825484; Train Acc = 0.43357999742031095, Test Acc = 0.41579999834299086\n",
      "Training at step=20, batch=0, train loss = 1.6409481409536568, train acc = 0.46000000834465027, time = 0.007584810256958008\n",
      "Training at step=20, batch=100, train loss = 1.7463986474521291, train acc = 0.4300000071525574, time = 0.007703065872192383\n",
      "Training at step=20, batch=200, train loss = 1.7393590212446823, train acc = 0.4000000059604645, time = 0.007441282272338867\n",
      "Training at step=20, batch=300, train loss = 1.7111885638081588, train acc = 0.38999998569488525, time = 0.00748133659362793\n",
      "Training at step=20, batch=400, train loss = 1.6424793095654728, train acc = 0.4099999964237213, time = 0.007457733154296875\n",
      "Testing at step=20, batch=0, test loss = 1.8182781270686061, test acc = 0.3700000047683716, time = 0.001828908920288086\n",
      "Testing at step=20, batch=20, test loss = 1.5451043141988456, test acc = 0.4099999964237213, time = 0.0017452239990234375\n",
      "Testing at step=20, batch=40, test loss = 1.8622459562370313, test acc = 0.4399999976158142, time = 0.001848459243774414\n",
      "Testing at step=20, batch=60, test loss = 1.941417338733677, test acc = 0.3199999928474426, time = 0.0017180442810058594\n",
      "Testing at step=20, batch=80, test loss = 1.861648533627177, test acc = 0.3799999952316284, time = 0.0017232894897460938\n",
      "Step 20 finished in 12.745946645736694, Train loss = 1.6692461466785398, Test loss = 1.708955289496688; Train Acc = 0.4352799975275993, Test Acc = 0.41389999926090243\n",
      "Training at step=21, batch=0, train loss = 1.537680855229788, train acc = 0.4699999988079071, time = 0.007578372955322266\n",
      "Training at step=21, batch=100, train loss = 1.5603430924263244, train acc = 0.47999998927116394, time = 0.0076503753662109375\n",
      "Training at step=21, batch=200, train loss = 1.8842516465794845, train acc = 0.38999998569488525, time = 0.007545948028564453\n",
      "Training at step=21, batch=300, train loss = 1.6827682972340319, train acc = 0.41999998688697815, time = 0.00753021240234375\n",
      "Training at step=21, batch=400, train loss = 1.7600487621995908, train acc = 0.41999998688697815, time = 0.0076754093170166016\n",
      "Testing at step=21, batch=0, test loss = 1.7331920117422177, test acc = 0.38999998569488525, time = 0.0017998218536376953\n",
      "Testing at step=21, batch=20, test loss = 1.5423544648236285, test acc = 0.5099999904632568, time = 0.001781463623046875\n",
      "Testing at step=21, batch=40, test loss = 1.7266588646559597, test acc = 0.3799999952316284, time = 0.0018010139465332031\n",
      "Testing at step=21, batch=60, test loss = 1.7779515099291165, test acc = 0.36000001430511475, time = 0.0018053054809570312\n",
      "Testing at step=21, batch=80, test loss = 1.6810414999611216, test acc = 0.4300000071525574, time = 0.001767873764038086\n",
      "Step 21 finished in 12.873324871063232, Train loss = 1.6677965252366482, Test loss = 1.7084416046925646; Train Acc = 0.4366399978399277, Test Acc = 0.41419999927282336\n",
      "Training at step=22, batch=0, train loss = 1.8034539486415349, train acc = 0.41999998688697815, time = 0.00751948356628418\n",
      "Training at step=22, batch=100, train loss = 1.720515527391763, train acc = 0.3700000047683716, time = 0.0074520111083984375\n",
      "Training at step=22, batch=200, train loss = 1.5878439004757183, train acc = 0.5199999809265137, time = 0.007684946060180664\n",
      "Training at step=22, batch=300, train loss = 1.7372536315609468, train acc = 0.4399999976158142, time = 0.007683753967285156\n",
      "Training at step=22, batch=400, train loss = 1.7635863142043966, train acc = 0.3799999952316284, time = 0.007513761520385742\n",
      "Testing at step=22, batch=0, test loss = 1.6667271413426366, test acc = 0.3799999952316284, time = 0.0018398761749267578\n",
      "Testing at step=22, batch=20, test loss = 1.7605652928431617, test acc = 0.3400000035762787, time = 0.0017960071563720703\n",
      "Testing at step=22, batch=40, test loss = 1.748958746561985, test acc = 0.4300000071525574, time = 0.001714944839477539\n",
      "Testing at step=22, batch=60, test loss = 1.7329279769288533, test acc = 0.41999998688697815, time = 0.001764059066772461\n",
      "Testing at step=22, batch=80, test loss = 1.500153808742329, test acc = 0.47999998927116394, time = 0.0017960071563720703\n",
      "Step 22 finished in 12.874832153320312, Train loss = 1.666152093922592, Test loss = 1.7069567092291873; Train Acc = 0.43581999814510347, Test Acc = 0.4164999982714653\n",
      "Training at step=23, batch=0, train loss = 1.6963685835727094, train acc = 0.41999998688697815, time = 0.007601737976074219\n",
      "Training at step=23, batch=100, train loss = 1.5377132110217442, train acc = 0.5199999809265137, time = 0.007464885711669922\n",
      "Training at step=23, batch=200, train loss = 1.9137072742981112, train acc = 0.3700000047683716, time = 0.007505178451538086\n",
      "Training at step=23, batch=300, train loss = 1.6422527447162292, train acc = 0.47999998927116394, time = 0.007491588592529297\n",
      "Training at step=23, batch=400, train loss = 1.7788164102010158, train acc = 0.44999998807907104, time = 0.007501125335693359\n",
      "Testing at step=23, batch=0, test loss = 1.5934115109992282, test acc = 0.4699999988079071, time = 0.0018138885498046875\n",
      "Testing at step=23, batch=20, test loss = 1.5379003358357428, test acc = 0.4099999964237213, time = 0.001783132553100586\n",
      "Testing at step=23, batch=40, test loss = 1.6551939338181074, test acc = 0.4300000071525574, time = 0.0017566680908203125\n",
      "Testing at step=23, batch=60, test loss = 1.7281206623668037, test acc = 0.4300000071525574, time = 0.0016760826110839844\n",
      "Testing at step=23, batch=80, test loss = 1.733616458382408, test acc = 0.3700000047683716, time = 0.0017592906951904297\n",
      "Step 23 finished in 12.716656684875488, Train loss = 1.6642729425080416, Test loss = 1.71124103730152; Train Acc = 0.4361399976611137, Test Acc = 0.4115999981760979\n",
      "Training at step=24, batch=0, train loss = 1.706648718354555, train acc = 0.4099999964237213, time = 0.007622718811035156\n",
      "Training at step=24, batch=100, train loss = 1.7646743474205855, train acc = 0.38999998569488525, time = 0.007504701614379883\n",
      "Training at step=24, batch=200, train loss = 1.7888119670129543, train acc = 0.3700000047683716, time = 0.007544755935668945\n",
      "Training at step=24, batch=300, train loss = 1.6610467929106578, train acc = 0.4000000059604645, time = 0.0075719356536865234\n",
      "Training at step=24, batch=400, train loss = 1.6173151573268192, train acc = 0.4399999976158142, time = 0.007485628128051758\n",
      "Testing at step=24, batch=0, test loss = 1.6649006555753534, test acc = 0.4399999976158142, time = 0.0017485618591308594\n",
      "Testing at step=24, batch=20, test loss = 1.6930762951261593, test acc = 0.41999998688697815, time = 0.001775503158569336\n",
      "Testing at step=24, batch=40, test loss = 1.7951340178558697, test acc = 0.4099999964237213, time = 0.0017101764678955078\n",
      "Testing at step=24, batch=60, test loss = 1.6136927116548079, test acc = 0.4699999988079071, time = 0.0017452239990234375\n",
      "Testing at step=24, batch=80, test loss = 1.4895597337175799, test acc = 0.5199999809265137, time = 0.0016889572143554688\n",
      "Step 24 finished in 12.942584037780762, Train loss = 1.6629109809155511, Test loss = 1.708971017297829; Train Acc = 0.4363199977874756, Test Acc = 0.4134999969601631\n",
      "Training at step=25, batch=0, train loss = 1.7286374437742196, train acc = 0.41999998688697815, time = 0.0075664520263671875\n",
      "Training at step=25, batch=100, train loss = 1.7149048516221033, train acc = 0.41999998688697815, time = 0.007470607757568359\n",
      "Training at step=25, batch=200, train loss = 1.6276926216091323, train acc = 0.4300000071525574, time = 0.007627964019775391\n",
      "Training at step=25, batch=300, train loss = 1.740761894351004, train acc = 0.3700000047683716, time = 0.0076732635498046875\n",
      "Training at step=25, batch=400, train loss = 1.5114264342471635, train acc = 0.5, time = 0.007417201995849609\n",
      "Testing at step=25, batch=0, test loss = 1.793797628873998, test acc = 0.3799999952316284, time = 0.0018088817596435547\n",
      "Testing at step=25, batch=20, test loss = 1.6372402458517963, test acc = 0.41999998688697815, time = 0.001802206039428711\n",
      "Testing at step=25, batch=40, test loss = 1.6892448961220028, test acc = 0.4399999976158142, time = 0.0017528533935546875\n",
      "Testing at step=25, batch=60, test loss = 1.7150379524631227, test acc = 0.41999998688697815, time = 0.0018908977508544922\n",
      "Testing at step=25, batch=80, test loss = 1.5971468295680693, test acc = 0.4300000071525574, time = 0.0017690658569335938\n",
      "Step 25 finished in 12.913270473480225, Train loss = 1.6614876056398737, Test loss = 1.7125438925440826; Train Acc = 0.4377999975681305, Test Acc = 0.4145999979972839\n",
      "Training at step=26, batch=0, train loss = 1.5765546474635144, train acc = 0.44999998807907104, time = 0.007716178894042969\n",
      "Training at step=26, batch=100, train loss = 1.7678902498522169, train acc = 0.46000000834465027, time = 0.007520198822021484\n",
      "Training at step=26, batch=200, train loss = 1.59576673007791, train acc = 0.46000000834465027, time = 0.007501363754272461\n",
      "Training at step=26, batch=300, train loss = 1.6640558806423258, train acc = 0.4300000071525574, time = 0.007546901702880859\n",
      "Training at step=26, batch=400, train loss = 1.6922677530140737, train acc = 0.3400000035762787, time = 0.00754547119140625\n",
      "Testing at step=26, batch=0, test loss = 1.5993945184759955, test acc = 0.44999998807907104, time = 0.0018336772918701172\n",
      "Testing at step=26, batch=20, test loss = 1.6914006977407248, test acc = 0.4399999976158142, time = 0.0019073486328125\n",
      "Testing at step=26, batch=40, test loss = 1.5315930877979738, test acc = 0.49000000953674316, time = 0.0016415119171142578\n",
      "Testing at step=26, batch=60, test loss = 1.6101396602038136, test acc = 0.4099999964237213, time = 0.001706838607788086\n",
      "Testing at step=26, batch=80, test loss = 1.5885162186969857, test acc = 0.3400000035762787, time = 0.0017840862274169922\n",
      "Step 26 finished in 12.928883075714111, Train loss = 1.6601557621152576, Test loss = 1.7097223667831658; Train Acc = 0.43955999791622163, Test Acc = 0.41499999701976775\n",
      "Training at step=27, batch=0, train loss = 1.5399624364113467, train acc = 0.46000000834465027, time = 0.007605075836181641\n",
      "Training at step=27, batch=100, train loss = 1.6671461131761354, train acc = 0.4699999988079071, time = 0.007608890533447266\n",
      "Training at step=27, batch=200, train loss = 1.7112262716581932, train acc = 0.4099999964237213, time = 0.007519245147705078\n",
      "Training at step=27, batch=300, train loss = 1.678008331246045, train acc = 0.46000000834465027, time = 0.007575511932373047\n",
      "Training at step=27, batch=400, train loss = 1.9574903093790104, train acc = 0.33000001311302185, time = 0.0076601505279541016\n",
      "Testing at step=27, batch=0, test loss = 1.6453616558153228, test acc = 0.4099999964237213, time = 0.001875162124633789\n",
      "Testing at step=27, batch=20, test loss = 1.663187968749434, test acc = 0.46000000834465027, time = 0.0017199516296386719\n",
      "Testing at step=27, batch=40, test loss = 1.6740408992928346, test acc = 0.3799999952316284, time = 0.0017592906951904297\n",
      "Testing at step=27, batch=60, test loss = 1.654532621675612, test acc = 0.46000000834465027, time = 0.0017247200012207031\n",
      "Testing at step=27, batch=80, test loss = 1.8513740813696473, test acc = 0.3100000023841858, time = 0.0017895698547363281\n",
      "Step 27 finished in 12.75750184059143, Train loss = 1.6579842768656845, Test loss = 1.7118372285709273; Train Acc = 0.4392999972105026, Test Acc = 0.4109999969601631\n",
      "Training at step=28, batch=0, train loss = 1.748993230454231, train acc = 0.4099999964237213, time = 0.007531404495239258\n",
      "Training at step=28, batch=100, train loss = 1.8595646847652667, train acc = 0.4099999964237213, time = 0.0074536800384521484\n",
      "Training at step=28, batch=200, train loss = 1.5713363365776971, train acc = 0.4399999976158142, time = 0.007497072219848633\n",
      "Training at step=28, batch=300, train loss = 1.7658963606087588, train acc = 0.3499999940395355, time = 0.007510185241699219\n",
      "Training at step=28, batch=400, train loss = 1.5574720768595722, train acc = 0.49000000953674316, time = 0.007605075836181641\n",
      "Testing at step=28, batch=0, test loss = 1.615842957364734, test acc = 0.3700000047683716, time = 0.0017867088317871094\n",
      "Testing at step=28, batch=20, test loss = 1.7837472281523898, test acc = 0.38999998569488525, time = 0.0017499923706054688\n",
      "Testing at step=28, batch=40, test loss = 1.7511910621977114, test acc = 0.41999998688697815, time = 0.0017096996307373047\n",
      "Testing at step=28, batch=60, test loss = 1.6727409645811246, test acc = 0.41999998688697815, time = 0.0017278194427490234\n",
      "Testing at step=28, batch=80, test loss = 1.7788302585378517, test acc = 0.3700000047683716, time = 0.001720428466796875\n",
      "Step 28 finished in 12.950648784637451, Train loss = 1.6574844137026206, Test loss = 1.7090888602505485; Train Acc = 0.4382199983596802, Test Acc = 0.41449999779462815\n",
      "Training at step=29, batch=0, train loss = 1.6774139404745034, train acc = 0.4699999988079071, time = 0.007489442825317383\n",
      "Training at step=29, batch=100, train loss = 1.7975942535884601, train acc = 0.36000001430511475, time = 0.007523775100708008\n",
      "Training at step=29, batch=200, train loss = 1.8544898078882257, train acc = 0.3700000047683716, time = 0.007505178451538086\n",
      "Training at step=29, batch=300, train loss = 1.4738176279961253, train acc = 0.5099999904632568, time = 0.0075647830963134766\n",
      "Training at step=29, batch=400, train loss = 1.6528378385399694, train acc = 0.4699999988079071, time = 0.007592201232910156\n",
      "Testing at step=29, batch=0, test loss = 1.6378277910572285, test acc = 0.47999998927116394, time = 0.001802682876586914\n",
      "Testing at step=29, batch=20, test loss = 1.6914675197751035, test acc = 0.38999998569488525, time = 0.001798391342163086\n",
      "Testing at step=29, batch=40, test loss = 1.7842171166179681, test acc = 0.33000001311302185, time = 0.001661539077758789\n",
      "Testing at step=29, batch=60, test loss = 1.5447268431867591, test acc = 0.4399999976158142, time = 0.0017316341400146484\n",
      "Testing at step=29, batch=80, test loss = 1.628991746713985, test acc = 0.47999998927116394, time = 0.0017476081848144531\n",
      "Step 29 finished in 12.955596208572388, Train loss = 1.6561219721624576, Test loss = 1.7133684093996464; Train Acc = 0.43895999777317046, Test Acc = 0.41239999800920485\n",
      "Training at step=30, batch=0, train loss = 1.5220453608418867, train acc = 0.49000000953674316, time = 0.007492780685424805\n",
      "Training at step=30, batch=100, train loss = 1.634600102072018, train acc = 0.47999998927116394, time = 0.0074613094329833984\n",
      "Training at step=30, batch=200, train loss = 1.654057513079806, train acc = 0.46000000834465027, time = 0.007517814636230469\n",
      "Training at step=30, batch=300, train loss = 1.684014475049694, train acc = 0.46000000834465027, time = 0.007497072219848633\n",
      "Training at step=30, batch=400, train loss = 1.69235584501607, train acc = 0.4399999976158142, time = 0.0076751708984375\n",
      "Testing at step=30, batch=0, test loss = 1.6576597848612493, test acc = 0.4699999988079071, time = 0.0017628669738769531\n",
      "Testing at step=30, batch=20, test loss = 1.9574140379226455, test acc = 0.3199999928474426, time = 0.001767873764038086\n",
      "Testing at step=30, batch=40, test loss = 1.6283135466694347, test acc = 0.44999998807907104, time = 0.0018382072448730469\n",
      "Testing at step=30, batch=60, test loss = 1.5120672085051472, test acc = 0.550000011920929, time = 0.0017993450164794922\n",
      "Testing at step=30, batch=80, test loss = 1.6811014435964478, test acc = 0.4099999964237213, time = 0.0018351078033447266\n",
      "Step 30 finished in 12.800663471221924, Train loss = 1.6541382888041147, Test loss = 1.7137091839702026; Train Acc = 0.4399399979710579, Test Acc = 0.4108999985456467\n",
      "Training at step=31, batch=0, train loss = 1.5504305683246755, train acc = 0.47999998927116394, time = 0.0076808929443359375\n",
      "Training at step=31, batch=100, train loss = 1.6227267618229044, train acc = 0.4099999964237213, time = 0.0074803829193115234\n",
      "Training at step=31, batch=200, train loss = 1.8476710163833454, train acc = 0.36000001430511475, time = 0.007695198059082031\n",
      "Training at step=31, batch=300, train loss = 1.678070693630686, train acc = 0.4000000059604645, time = 0.007603168487548828\n",
      "Training at step=31, batch=400, train loss = 1.6041169404937572, train acc = 0.44999998807907104, time = 0.0075225830078125\n",
      "Testing at step=31, batch=0, test loss = 1.6441371687624708, test acc = 0.44999998807907104, time = 0.001760244369506836\n",
      "Testing at step=31, batch=20, test loss = 1.6562655643056161, test acc = 0.3799999952316284, time = 0.0017392635345458984\n",
      "Testing at step=31, batch=40, test loss = 1.6756400762424435, test acc = 0.38999998569488525, time = 0.0017578601837158203\n",
      "Testing at step=31, batch=60, test loss = 1.7315392980953865, test acc = 0.44999998807907104, time = 0.0017085075378417969\n",
      "Testing at step=31, batch=80, test loss = 1.7478093893751525, test acc = 0.38999998569488525, time = 0.0016427040100097656\n",
      "Step 31 finished in 13.0207839012146, Train loss = 1.6541206409419476, Test loss = 1.710114409015314; Train Acc = 0.43953999757766726, Test Acc = 0.4125999975204468\n",
      "Training at step=32, batch=0, train loss = 1.7144823602336665, train acc = 0.4000000059604645, time = 0.00767064094543457\n",
      "Training at step=32, batch=100, train loss = 1.7563029558285337, train acc = 0.4399999976158142, time = 0.007657766342163086\n",
      "Training at step=32, batch=200, train loss = 1.5453119636325667, train acc = 0.44999998807907104, time = 0.0075833797454833984\n",
      "Training at step=32, batch=300, train loss = 1.5878323232918214, train acc = 0.47999998927116394, time = 0.007570743560791016\n",
      "Training at step=32, batch=400, train loss = 1.6605494126343234, train acc = 0.41999998688697815, time = 0.007673501968383789\n",
      "Testing at step=32, batch=0, test loss = 1.7541412299839019, test acc = 0.4000000059604645, time = 0.0018231868743896484\n",
      "Testing at step=32, batch=20, test loss = 1.6840711848673175, test acc = 0.3400000035762787, time = 0.0017826557159423828\n",
      "Testing at step=32, batch=40, test loss = 1.8162956853098589, test acc = 0.3100000023841858, time = 0.0017843246459960938\n",
      "Testing at step=32, batch=60, test loss = 1.6488086161951125, test acc = 0.4399999976158142, time = 0.0018091201782226562\n",
      "Testing at step=32, batch=80, test loss = 1.7128394187540998, test acc = 0.5, time = 0.0018377304077148438\n",
      "Step 32 finished in 13.000874280929565, Train loss = 1.6523595428768663, Test loss = 1.71333639658697; Train Acc = 0.4397399975061417, Test Acc = 0.4119999995827675\n",
      "Training at step=33, batch=0, train loss = 1.7483993018728912, train acc = 0.4300000071525574, time = 0.007650613784790039\n",
      "Training at step=33, batch=100, train loss = 1.805325254729795, train acc = 0.4300000071525574, time = 0.007442951202392578\n",
      "Training at step=33, batch=200, train loss = 1.7758437296421432, train acc = 0.4099999964237213, time = 0.007622241973876953\n",
      "Training at step=33, batch=300, train loss = 1.6794096132284861, train acc = 0.4099999964237213, time = 0.007498741149902344\n",
      "Training at step=33, batch=400, train loss = 1.7907536362733387, train acc = 0.36000001430511475, time = 0.0077402591705322266\n",
      "Testing at step=33, batch=0, test loss = 1.7459961522774758, test acc = 0.3700000047683716, time = 0.0018379688262939453\n",
      "Testing at step=33, batch=20, test loss = 1.7633887444100076, test acc = 0.36000001430511475, time = 0.0018169879913330078\n",
      "Testing at step=33, batch=40, test loss = 1.7146403712739202, test acc = 0.38999998569488525, time = 0.0017538070678710938\n",
      "Testing at step=33, batch=60, test loss = 1.764966499055958, test acc = 0.3499999940395355, time = 0.001667022705078125\n",
      "Testing at step=33, batch=80, test loss = 1.9398276607752565, test acc = 0.3499999940395355, time = 0.0016622543334960938\n",
      "Step 33 finished in 12.904667854309082, Train loss = 1.6507026392819233, Test loss = 1.7151804477464028; Train Acc = 0.4419599975347519, Test Acc = 0.4073999997973442\n",
      "Training at step=34, batch=0, train loss = 1.6585850391848347, train acc = 0.5299999713897705, time = 0.007581472396850586\n",
      "Training at step=34, batch=100, train loss = 1.747382833421436, train acc = 0.4300000071525574, time = 0.007523536682128906\n",
      "Training at step=34, batch=200, train loss = 1.5087522930834576, train acc = 0.4399999976158142, time = 0.007785320281982422\n",
      "Training at step=34, batch=300, train loss = 1.7785022440946907, train acc = 0.4099999964237213, time = 0.0074558258056640625\n",
      "Training at step=34, batch=400, train loss = 1.5977452043498612, train acc = 0.47999998927116394, time = 0.007475137710571289\n",
      "Testing at step=34, batch=0, test loss = 1.5765456161593172, test acc = 0.4300000071525574, time = 0.0018513202667236328\n",
      "Testing at step=34, batch=20, test loss = 1.810301918889475, test acc = 0.33000001311302185, time = 0.0018987655639648438\n",
      "Testing at step=34, batch=40, test loss = 1.907685026919012, test acc = 0.36000001430511475, time = 0.001867532730102539\n",
      "Testing at step=34, batch=60, test loss = 1.713750264160337, test acc = 0.3700000047683716, time = 0.001741170883178711\n",
      "Testing at step=34, batch=80, test loss = 1.825019542805743, test acc = 0.3499999940395355, time = 0.0017986297607421875\n",
      "Step 34 finished in 13.013875722885132, Train loss = 1.6508549742183514, Test loss = 1.71300241509583; Train Acc = 0.4423799974322319, Test Acc = 0.40839999973773955\n",
      "Training at step=35, batch=0, train loss = 1.6836209707300496, train acc = 0.4699999988079071, time = 0.0077855587005615234\n",
      "Training at step=35, batch=100, train loss = 1.499731225158502, train acc = 0.5600000023841858, time = 0.00751495361328125\n",
      "Training at step=35, batch=200, train loss = 1.6170698093866207, train acc = 0.47999998927116394, time = 0.0074460506439208984\n",
      "Training at step=35, batch=300, train loss = 1.6255839479239744, train acc = 0.4300000071525574, time = 0.0075337886810302734\n",
      "Training at step=35, batch=400, train loss = 1.7279926198237876, train acc = 0.4699999988079071, time = 0.007504701614379883\n",
      "Testing at step=35, batch=0, test loss = 1.8250684923017195, test acc = 0.3499999940395355, time = 0.0017278194427490234\n",
      "Testing at step=35, batch=20, test loss = 1.7348805923268338, test acc = 0.4099999964237213, time = 0.0018572807312011719\n",
      "Testing at step=35, batch=40, test loss = 1.8120880283680905, test acc = 0.4000000059604645, time = 0.0017461776733398438\n",
      "Testing at step=35, batch=60, test loss = 1.5524616773552768, test acc = 0.4399999976158142, time = 0.0017352104187011719\n",
      "Testing at step=35, batch=80, test loss = 1.8216136206464453, test acc = 0.3799999952316284, time = 0.001718759536743164\n",
      "Step 35 finished in 12.849076747894287, Train loss = 1.6491647792830417, Test loss = 1.7142012813763186; Train Acc = 0.4419399979114532, Test Acc = 0.41149999856948855\n",
      "Training at step=36, batch=0, train loss = 1.5750970098862427, train acc = 0.3799999952316284, time = 0.007493734359741211\n",
      "Training at step=36, batch=100, train loss = 1.5443727583015223, train acc = 0.4300000071525574, time = 0.007559299468994141\n",
      "Training at step=36, batch=200, train loss = 1.6135835418284323, train acc = 0.44999998807907104, time = 0.007670879364013672\n",
      "Training at step=36, batch=300, train loss = 1.7637888765849157, train acc = 0.3799999952316284, time = 0.007559776306152344\n",
      "Training at step=36, batch=400, train loss = 1.7149154666175053, train acc = 0.3700000047683716, time = 0.007631540298461914\n",
      "Testing at step=36, batch=0, test loss = 1.5948473379936707, test acc = 0.44999998807907104, time = 0.0018165111541748047\n",
      "Testing at step=36, batch=20, test loss = 1.7960991378352467, test acc = 0.3700000047683716, time = 0.0017077922821044922\n",
      "Testing at step=36, batch=40, test loss = 1.9071195713235896, test acc = 0.3499999940395355, time = 0.0017027854919433594\n",
      "Testing at step=36, batch=60, test loss = 1.8534994004436738, test acc = 0.3700000047683716, time = 0.0017359256744384766\n",
      "Testing at step=36, batch=80, test loss = 1.8322263497273246, test acc = 0.47999998927116394, time = 0.0016798973083496094\n",
      "Step 36 finished in 12.797407865524292, Train loss = 1.647972753841058, Test loss = 1.7203364238890788; Train Acc = 0.4434199975132942, Test Acc = 0.4074999985098839\n",
      "Training at step=37, batch=0, train loss = 1.7453028014593577, train acc = 0.3499999940395355, time = 0.00758671760559082\n",
      "Training at step=37, batch=100, train loss = 1.7006048597910628, train acc = 0.4399999976158142, time = 0.0074846744537353516\n",
      "Training at step=37, batch=200, train loss = 1.5988465893149408, train acc = 0.44999998807907104, time = 0.0075702667236328125\n",
      "Training at step=37, batch=300, train loss = 1.7519308763161363, train acc = 0.38999998569488525, time = 0.007436513900756836\n",
      "Training at step=37, batch=400, train loss = 1.765963652891513, train acc = 0.3700000047683716, time = 0.00765681266784668\n",
      "Testing at step=37, batch=0, test loss = 1.6965960402155116, test acc = 0.41999998688697815, time = 0.0020372867584228516\n",
      "Testing at step=37, batch=20, test loss = 1.594496981049014, test acc = 0.49000000953674316, time = 0.0017724037170410156\n",
      "Testing at step=37, batch=40, test loss = 1.6084644459432524, test acc = 0.4399999976158142, time = 0.0017173290252685547\n",
      "Testing at step=37, batch=60, test loss = 1.8154247302243252, test acc = 0.38999998569488525, time = 0.001783132553100586\n",
      "Testing at step=37, batch=80, test loss = 1.7968521337039192, test acc = 0.3499999940395355, time = 0.0016999244689941406\n",
      "Step 37 finished in 12.772813081741333, Train loss = 1.646782054475113, Test loss = 1.7195650868521921; Train Acc = 0.441739997446537, Test Acc = 0.4124999982118607\n",
      "Training at step=38, batch=0, train loss = 1.8395634199025777, train acc = 0.46000000834465027, time = 0.007717132568359375\n",
      "Training at step=38, batch=100, train loss = 1.688684355362414, train acc = 0.4000000059604645, time = 0.0076138973236083984\n",
      "Training at step=38, batch=200, train loss = 1.6731379715804442, train acc = 0.4300000071525574, time = 0.007460355758666992\n",
      "Training at step=38, batch=300, train loss = 1.54707394798137, train acc = 0.44999998807907104, time = 0.007629871368408203\n",
      "Training at step=38, batch=400, train loss = 1.6187443346037609, train acc = 0.46000000834465027, time = 0.007572174072265625\n",
      "Testing at step=38, batch=0, test loss = 1.6152966399034172, test acc = 0.4000000059604645, time = 0.0017044544219970703\n",
      "Testing at step=38, batch=20, test loss = 1.5727013905305633, test acc = 0.4399999976158142, time = 0.0017185211181640625\n",
      "Testing at step=38, batch=40, test loss = 1.6621779364492415, test acc = 0.46000000834465027, time = 0.0017211437225341797\n",
      "Testing at step=38, batch=60, test loss = 1.8283578636998448, test acc = 0.3400000035762787, time = 0.0017192363739013672\n",
      "Testing at step=38, batch=80, test loss = 1.7550260282936783, test acc = 0.36000001430511475, time = 0.0018503665924072266\n",
      "Step 38 finished in 12.84033465385437, Train loss = 1.6462491622128914, Test loss = 1.715412267954493; Train Acc = 0.4425199972987175, Test Acc = 0.41009999960660937\n",
      "Training at step=39, batch=0, train loss = 1.5177620226563162, train acc = 0.5, time = 0.0074558258056640625\n",
      "Training at step=39, batch=100, train loss = 1.6478771785647857, train acc = 0.4300000071525574, time = 0.007797956466674805\n",
      "Training at step=39, batch=200, train loss = 1.6672166698661801, train acc = 0.47999998927116394, time = 0.007478952407836914\n",
      "Training at step=39, batch=300, train loss = 1.6964693046665777, train acc = 0.3700000047683716, time = 0.0075244903564453125\n",
      "Training at step=39, batch=400, train loss = 1.4806021564422114, train acc = 0.47999998927116394, time = 0.007561683654785156\n",
      "Testing at step=39, batch=0, test loss = 1.6093445368447568, test acc = 0.4300000071525574, time = 0.0018129348754882812\n",
      "Testing at step=39, batch=20, test loss = 1.8815881457497188, test acc = 0.36000001430511475, time = 0.0017304420471191406\n",
      "Testing at step=39, batch=40, test loss = 1.5785532571974854, test acc = 0.47999998927116394, time = 0.001725912094116211\n",
      "Testing at step=39, batch=60, test loss = 1.965503165977139, test acc = 0.3499999940395355, time = 0.0018665790557861328\n",
      "Testing at step=39, batch=80, test loss = 1.4726596120727173, test acc = 0.47999998927116394, time = 0.0017805099487304688\n",
      "Step 39 finished in 12.93101692199707, Train loss = 1.6446045818213415, Test loss = 1.7178571162383343; Train Acc = 0.44363999688625333, Test Acc = 0.40979999750852586\n",
      "Training at step=40, batch=0, train loss = 1.542712863189922, train acc = 0.44999998807907104, time = 0.0076732635498046875\n",
      "Training at step=40, batch=100, train loss = 1.7420953299514093, train acc = 0.3499999940395355, time = 0.007383584976196289\n",
      "Training at step=40, batch=200, train loss = 1.7829720815807435, train acc = 0.3700000047683716, time = 0.0075643062591552734\n",
      "Training at step=40, batch=300, train loss = 1.6219804048431634, train acc = 0.4399999976158142, time = 0.0074310302734375\n",
      "Training at step=40, batch=400, train loss = 1.5598671666172785, train acc = 0.46000000834465027, time = 0.007551670074462891\n",
      "Testing at step=40, batch=0, test loss = 1.6399912550991331, test acc = 0.46000000834465027, time = 0.0018317699432373047\n",
      "Testing at step=40, batch=20, test loss = 1.8184394987074342, test acc = 0.4000000059604645, time = 0.0016651153564453125\n",
      "Testing at step=40, batch=40, test loss = 1.6844487852032677, test acc = 0.4399999976158142, time = 0.0016815662384033203\n",
      "Testing at step=40, batch=60, test loss = 1.857293984538245, test acc = 0.3199999928474426, time = 0.0017578601837158203\n",
      "Testing at step=40, batch=80, test loss = 1.7492687234366797, test acc = 0.4099999964237213, time = 0.0017971992492675781\n",
      "Step 40 finished in 12.748294591903687, Train loss = 1.6440035901534935, Test loss = 1.719967526979565; Train Acc = 0.44391999804973603, Test Acc = 0.4090999981760979\n",
      "Training at step=41, batch=0, train loss = 1.8406447851422945, train acc = 0.3499999940395355, time = 0.007571220397949219\n",
      "Training at step=41, batch=100, train loss = 1.6343487790711404, train acc = 0.4300000071525574, time = 0.0074121952056884766\n",
      "Training at step=41, batch=200, train loss = 1.5631786101919716, train acc = 0.47999998927116394, time = 0.007582664489746094\n",
      "Training at step=41, batch=300, train loss = 1.6760844955299299, train acc = 0.4300000071525574, time = 0.007507801055908203\n",
      "Training at step=41, batch=400, train loss = 1.4762381731092913, train acc = 0.44999998807907104, time = 0.00747227668762207\n",
      "Testing at step=41, batch=0, test loss = 1.4913159633319844, test acc = 0.46000000834465027, time = 0.0018265247344970703\n",
      "Testing at step=41, batch=20, test loss = 1.579313068634489, test acc = 0.44999998807907104, time = 0.0017697811126708984\n",
      "Testing at step=41, batch=40, test loss = 1.7584373747007638, test acc = 0.4699999988079071, time = 0.0017070770263671875\n",
      "Testing at step=41, batch=60, test loss = 1.723109358592626, test acc = 0.38999998569488525, time = 0.0018014907836914062\n",
      "Testing at step=41, batch=80, test loss = 1.6853328934845737, test acc = 0.41999998688697815, time = 0.0018122196197509766\n",
      "Step 41 finished in 12.771097660064697, Train loss = 1.6425623539554763, Test loss = 1.7170079550379471; Train Acc = 0.4444599975347519, Test Acc = 0.4070999982953072\n",
      "Training at step=42, batch=0, train loss = 1.7634799407210884, train acc = 0.46000000834465027, time = 0.007502555847167969\n",
      "Training at step=42, batch=100, train loss = 1.5646814702459415, train acc = 0.4099999964237213, time = 0.007443904876708984\n",
      "Training at step=42, batch=200, train loss = 1.8770872772515352, train acc = 0.4099999964237213, time = 0.007408618927001953\n",
      "Training at step=42, batch=300, train loss = 1.7487511131730753, train acc = 0.47999998927116394, time = 0.007587909698486328\n",
      "Training at step=42, batch=400, train loss = 1.827568365993016, train acc = 0.4099999964237213, time = 0.007394075393676758\n",
      "Testing at step=42, batch=0, test loss = 1.5404024968218202, test acc = 0.4300000071525574, time = 0.0017476081848144531\n",
      "Testing at step=42, batch=20, test loss = 1.7386209964291968, test acc = 0.4099999964237213, time = 0.0016667842864990234\n",
      "Testing at step=42, batch=40, test loss = 1.6576893601184535, test acc = 0.38999998569488525, time = 0.001739501953125\n",
      "Testing at step=42, batch=60, test loss = 1.4670512818498451, test acc = 0.4399999976158142, time = 0.0017952919006347656\n",
      "Testing at step=42, batch=80, test loss = 1.6328787816900787, test acc = 0.49000000953674316, time = 0.0016973018646240234\n",
      "Step 42 finished in 12.685727596282959, Train loss = 1.6416901228163991, Test loss = 1.7171260216626552; Train Acc = 0.44589999741315844, Test Acc = 0.40979999870061873\n",
      "Training at step=43, batch=0, train loss = 1.5171512902418463, train acc = 0.4699999988079071, time = 0.00762486457824707\n",
      "Training at step=43, batch=100, train loss = 1.5760141085158674, train acc = 0.5, time = 0.007447719573974609\n",
      "Training at step=43, batch=200, train loss = 1.5721165725483612, train acc = 0.4399999976158142, time = 0.007650852203369141\n",
      "Training at step=43, batch=300, train loss = 1.5934306607610478, train acc = 0.5299999713897705, time = 0.007624149322509766\n",
      "Training at step=43, batch=400, train loss = 1.5931902760621943, train acc = 0.4699999988079071, time = 0.00749516487121582\n",
      "Testing at step=43, batch=0, test loss = 1.6490114188517078, test acc = 0.4300000071525574, time = 0.001795053482055664\n",
      "Testing at step=43, batch=20, test loss = 1.7211268397481243, test acc = 0.3700000047683716, time = 0.001680135726928711\n",
      "Testing at step=43, batch=40, test loss = 1.7659616720158189, test acc = 0.41999998688697815, time = 0.0017123222351074219\n",
      "Testing at step=43, batch=60, test loss = 1.5282694515956763, test acc = 0.4300000071525574, time = 0.0017108917236328125\n",
      "Testing at step=43, batch=80, test loss = 1.7229330110196848, test acc = 0.3700000047683716, time = 0.0017688274383544922\n",
      "Step 43 finished in 12.730018854141235, Train loss = 1.6409407282407795, Test loss = 1.7208561457074376; Train Acc = 0.44509999722242355, Test Acc = 0.4100999990105629\n",
      "Training at step=44, batch=0, train loss = 1.693703388394637, train acc = 0.5099999904632568, time = 0.0076825618743896484\n",
      "Training at step=44, batch=100, train loss = 1.6882797434620582, train acc = 0.4000000059604645, time = 0.0074901580810546875\n",
      "Training at step=44, batch=200, train loss = 1.7181516409984963, train acc = 0.38999998569488525, time = 0.0074460506439208984\n",
      "Training at step=44, batch=300, train loss = 1.7343111812308343, train acc = 0.5, time = 0.007675647735595703\n",
      "Training at step=44, batch=400, train loss = 1.778928235363501, train acc = 0.36000001430511475, time = 0.0074615478515625\n",
      "Testing at step=44, batch=0, test loss = 1.5638695274055607, test acc = 0.44999998807907104, time = 0.001821279525756836\n",
      "Testing at step=44, batch=20, test loss = 1.6697215282925757, test acc = 0.4300000071525574, time = 0.0016636848449707031\n",
      "Testing at step=44, batch=40, test loss = 1.6581000057514814, test acc = 0.41999998688697815, time = 0.001728057861328125\n",
      "Testing at step=44, batch=60, test loss = 1.71029270977175, test acc = 0.38999998569488525, time = 0.001703500747680664\n",
      "Testing at step=44, batch=80, test loss = 1.790574501584591, test acc = 0.38999998569488525, time = 0.001753091812133789\n",
      "Step 44 finished in 12.769986629486084, Train loss = 1.640518547076706, Test loss = 1.7181303624239086; Train Acc = 0.44449999755620956, Test Acc = 0.4071999990940094\n",
      "Training at step=45, batch=0, train loss = 1.5448494609481613, train acc = 0.5, time = 0.007584810256958008\n",
      "Training at step=45, batch=100, train loss = 1.4161450331853258, train acc = 0.47999998927116394, time = 0.007771492004394531\n",
      "Training at step=45, batch=200, train loss = 1.6337652394455098, train acc = 0.5199999809265137, time = 0.007522106170654297\n",
      "Training at step=45, batch=300, train loss = 1.6053974313241115, train acc = 0.4399999976158142, time = 0.0075032711029052734\n",
      "Training at step=45, batch=400, train loss = 1.6202165836074363, train acc = 0.49000000953674316, time = 0.007468223571777344\n",
      "Testing at step=45, batch=0, test loss = 1.8745592142987855, test acc = 0.38999998569488525, time = 0.0018160343170166016\n",
      "Testing at step=45, batch=20, test loss = 1.5379522612891432, test acc = 0.5799999833106995, time = 0.0017671585083007812\n",
      "Testing at step=45, batch=40, test loss = 1.6034246772926146, test acc = 0.4300000071525574, time = 0.0017156600952148438\n",
      "Testing at step=45, batch=60, test loss = 1.557183754837446, test acc = 0.5099999904632568, time = 0.0017862319946289062\n",
      "Testing at step=45, batch=80, test loss = 1.758544909467573, test acc = 0.44999998807907104, time = 0.0018150806427001953\n",
      "Step 45 finished in 13.035759687423706, Train loss = 1.6388869596576336, Test loss = 1.718853084333981; Train Acc = 0.4455399980545044, Test Acc = 0.41009999841451644\n",
      "Training at step=46, batch=0, train loss = 1.697742018764624, train acc = 0.4300000071525574, time = 0.007713794708251953\n",
      "Training at step=46, batch=100, train loss = 1.750129879331771, train acc = 0.4099999964237213, time = 0.007325172424316406\n",
      "Training at step=46, batch=200, train loss = 1.671199234416361, train acc = 0.4699999988079071, time = 0.007650136947631836\n",
      "Training at step=46, batch=300, train loss = 1.5798016226676657, train acc = 0.4000000059604645, time = 0.0076906681060791016\n",
      "Training at step=46, batch=400, train loss = 1.6693008361296993, train acc = 0.47999998927116394, time = 0.007441997528076172\n",
      "Testing at step=46, batch=0, test loss = 1.6164322381244898, test acc = 0.41999998688697815, time = 0.0018320083618164062\n",
      "Testing at step=46, batch=20, test loss = 1.6925444830710894, test acc = 0.41999998688697815, time = 0.001748800277709961\n",
      "Testing at step=46, batch=40, test loss = 1.779612774677227, test acc = 0.3799999952316284, time = 0.0016481876373291016\n",
      "Testing at step=46, batch=60, test loss = 1.7403105602705204, test acc = 0.41999998688697815, time = 0.0017936229705810547\n",
      "Testing at step=46, batch=80, test loss = 1.936824774611768, test acc = 0.38999998569488525, time = 0.0018494129180908203\n",
      "Step 46 finished in 12.827440977096558, Train loss = 1.638402719024241, Test loss = 1.721786793808636; Train Acc = 0.4457999974489212, Test Acc = 0.40479999721050264\n",
      "Training at step=47, batch=0, train loss = 1.6697912079118031, train acc = 0.44999998807907104, time = 0.007601022720336914\n",
      "Training at step=47, batch=100, train loss = 1.6232413902771077, train acc = 0.5, time = 0.007443428039550781\n",
      "Training at step=47, batch=200, train loss = 1.7603967419334134, train acc = 0.47999998927116394, time = 0.007404327392578125\n",
      "Training at step=47, batch=300, train loss = 1.5437823480463964, train acc = 0.4399999976158142, time = 0.007757663726806641\n",
      "Training at step=47, batch=400, train loss = 1.5162079606537846, train acc = 0.5, time = 0.007470130920410156\n",
      "Testing at step=47, batch=0, test loss = 1.7946680620720454, test acc = 0.3799999952316284, time = 0.0017719268798828125\n",
      "Testing at step=47, batch=20, test loss = 1.7039679668134209, test acc = 0.4000000059604645, time = 0.0018053054809570312\n",
      "Testing at step=47, batch=40, test loss = 1.7628484611024537, test acc = 0.3799999952316284, time = 0.0016863346099853516\n",
      "Testing at step=47, batch=60, test loss = 1.660842633063811, test acc = 0.4300000071525574, time = 0.0017223358154296875\n",
      "Testing at step=47, batch=80, test loss = 1.7327635760552464, test acc = 0.3799999952316284, time = 0.0018720626831054688\n",
      "Step 47 finished in 12.844853639602661, Train loss = 1.6372274363275148, Test loss = 1.7219971343808587; Train Acc = 0.4463599972128868, Test Acc = 0.404899999499321\n",
      "Training at step=48, batch=0, train loss = 1.5924071282194285, train acc = 0.49000000953674316, time = 0.007570028305053711\n",
      "Training at step=48, batch=100, train loss = 1.6552824896151472, train acc = 0.46000000834465027, time = 0.007615089416503906\n",
      "Training at step=48, batch=200, train loss = 1.5494655449149475, train acc = 0.4399999976158142, time = 0.007576465606689453\n",
      "Training at step=48, batch=300, train loss = 1.6565484663949508, train acc = 0.4099999964237213, time = 0.007595062255859375\n",
      "Training at step=48, batch=400, train loss = 1.686237191357449, train acc = 0.4099999964237213, time = 0.007344245910644531\n",
      "Testing at step=48, batch=0, test loss = 1.6023269917364678, test acc = 0.49000000953674316, time = 0.0018320083618164062\n",
      "Testing at step=48, batch=20, test loss = 1.762916715857288, test acc = 0.3700000047683716, time = 0.0017631053924560547\n",
      "Testing at step=48, batch=40, test loss = 1.6099734704890534, test acc = 0.44999998807907104, time = 0.0017600059509277344\n",
      "Testing at step=48, batch=60, test loss = 1.6473107760604606, test acc = 0.4300000071525574, time = 0.0017282962799072266\n",
      "Testing at step=48, batch=80, test loss = 1.735420761516618, test acc = 0.38999998569488525, time = 0.001741170883178711\n",
      "Step 48 finished in 12.839929580688477, Train loss = 1.6356316576350665, Test loss = 1.724699381897827; Train Acc = 0.44645999699831007, Test Acc = 0.4058999988436699\n",
      "Training at step=49, batch=0, train loss = 1.7651497200108244, train acc = 0.41999998688697815, time = 0.007624387741088867\n",
      "Training at step=49, batch=100, train loss = 1.598404075173959, train acc = 0.3799999952316284, time = 0.007451534271240234\n",
      "Training at step=49, batch=200, train loss = 1.6584962269660355, train acc = 0.4399999976158142, time = 0.007488250732421875\n",
      "Training at step=49, batch=300, train loss = 1.5864798414784496, train acc = 0.44999998807907104, time = 0.0075092315673828125\n",
      "Training at step=49, batch=400, train loss = 1.6018498224645585, train acc = 0.49000000953674316, time = 0.007489919662475586\n",
      "Testing at step=49, batch=0, test loss = 1.7920423567080421, test acc = 0.4399999976158142, time = 0.0017845630645751953\n",
      "Testing at step=49, batch=20, test loss = 1.8005508965749957, test acc = 0.47999998927116394, time = 0.0017557144165039062\n",
      "Testing at step=49, batch=40, test loss = 1.7520911619733643, test acc = 0.3799999952316284, time = 0.0017924308776855469\n",
      "Testing at step=49, batch=60, test loss = 1.7473950662294158, test acc = 0.38999998569488525, time = 0.0018138885498046875\n",
      "Testing at step=49, batch=80, test loss = 1.6145534146120173, test acc = 0.4699999988079071, time = 0.0016756057739257812\n",
      "Step 49 finished in 12.78029179573059, Train loss = 1.635350004237157, Test loss = 1.7259390773928527; Train Acc = 0.44621999794244765, Test Acc = 0.4081999987363815\n",
      "Training at step=50, batch=0, train loss = 1.656695796233854, train acc = 0.5199999809265137, time = 0.007581949234008789\n",
      "Training at step=50, batch=100, train loss = 1.610721387497439, train acc = 0.46000000834465027, time = 0.007612466812133789\n",
      "Training at step=50, batch=200, train loss = 1.59900777583684, train acc = 0.4300000071525574, time = 0.007569789886474609\n",
      "Training at step=50, batch=300, train loss = 1.6403593174970659, train acc = 0.47999998927116394, time = 0.007507801055908203\n",
      "Training at step=50, batch=400, train loss = 1.6937671428470644, train acc = 0.41999998688697815, time = 0.007642269134521484\n",
      "Testing at step=50, batch=0, test loss = 1.7768953176654403, test acc = 0.4399999976158142, time = 0.0017802715301513672\n",
      "Testing at step=50, batch=20, test loss = 1.7835191816163114, test acc = 0.4300000071525574, time = 0.0017979145050048828\n",
      "Testing at step=50, batch=40, test loss = 1.7172132572162015, test acc = 0.44999998807907104, time = 0.0017778873443603516\n",
      "Testing at step=50, batch=60, test loss = 1.7653035365311038, test acc = 0.36000001430511475, time = 0.0017199516296386719\n",
      "Testing at step=50, batch=80, test loss = 1.8523232899755373, test acc = 0.3499999940395355, time = 0.0017132759094238281\n",
      "Step 50 finished in 12.828760862350464, Train loss = 1.6342132818675081, Test loss = 1.726755608850701; Train Acc = 0.4454999977350235, Test Acc = 0.4061999982595444\n",
      "Training at step=51, batch=0, train loss = 1.498133359434264, train acc = 0.5299999713897705, time = 0.007573127746582031\n",
      "Training at step=51, batch=100, train loss = 1.5617262049184606, train acc = 0.44999998807907104, time = 0.007489919662475586\n",
      "Training at step=51, batch=200, train loss = 1.7279036149098255, train acc = 0.41999998688697815, time = 0.007465839385986328\n",
      "Training at step=51, batch=300, train loss = 1.8494171234344323, train acc = 0.33000001311302185, time = 0.0073435306549072266\n",
      "Training at step=51, batch=400, train loss = 1.5613663298828981, train acc = 0.550000011920929, time = 0.007493734359741211\n",
      "Testing at step=51, batch=0, test loss = 1.6797928955169863, test acc = 0.41999998688697815, time = 0.0017578601837158203\n",
      "Testing at step=51, batch=20, test loss = 1.7311855546233341, test acc = 0.36000001430511475, time = 0.0016407966613769531\n",
      "Testing at step=51, batch=40, test loss = 1.6689690395377292, test acc = 0.47999998927116394, time = 0.001775503158569336\n",
      "Testing at step=51, batch=60, test loss = 1.556337040131882, test acc = 0.5, time = 0.0017354488372802734\n",
      "Testing at step=51, batch=80, test loss = 1.840345524302194, test acc = 0.41999998688697815, time = 0.0017025470733642578\n",
      "Step 51 finished in 12.83257246017456, Train loss = 1.6331166797156815, Test loss = 1.7236775099881398; Train Acc = 0.4482199977040291, Test Acc = 0.4071999976038933\n",
      "Training at step=52, batch=0, train loss = 1.7615553934785229, train acc = 0.4000000059604645, time = 0.0076923370361328125\n",
      "Training at step=52, batch=100, train loss = 1.590914768938207, train acc = 0.41999998688697815, time = 0.007627725601196289\n",
      "Training at step=52, batch=200, train loss = 1.6291429131023512, train acc = 0.4399999976158142, time = 0.007517337799072266\n",
      "Training at step=52, batch=300, train loss = 1.8060736811719909, train acc = 0.44999998807907104, time = 0.007542133331298828\n",
      "Training at step=52, batch=400, train loss = 1.627815886706537, train acc = 0.4300000071525574, time = 0.007532596588134766\n",
      "Testing at step=52, batch=0, test loss = 1.767099663486337, test acc = 0.3700000047683716, time = 0.0018720626831054688\n",
      "Testing at step=52, batch=20, test loss = 1.8879306805754112, test acc = 0.3700000047683716, time = 0.0016608238220214844\n",
      "Testing at step=52, batch=40, test loss = 1.7308826532040218, test acc = 0.3799999952316284, time = 0.0017287731170654297\n",
      "Testing at step=52, batch=60, test loss = 1.9013751694484284, test acc = 0.3199999928474426, time = 0.0016362667083740234\n",
      "Testing at step=52, batch=80, test loss = 1.8294713913892675, test acc = 0.36000001430511475, time = 0.0017123222351074219\n",
      "Step 52 finished in 12.729523658752441, Train loss = 1.632889212924268, Test loss = 1.7240737876405212; Train Acc = 0.44823999744653703, Test Acc = 0.40619999915361404\n",
      "Training at step=53, batch=0, train loss = 1.5840290520034483, train acc = 0.47999998927116394, time = 0.007561922073364258\n",
      "Training at step=53, batch=100, train loss = 1.332314886370742, train acc = 0.5400000214576721, time = 0.007571220397949219\n",
      "Training at step=53, batch=200, train loss = 1.7640840374821394, train acc = 0.4399999976158142, time = 0.007573604583740234\n",
      "Training at step=53, batch=300, train loss = 1.6772522393391869, train acc = 0.44999998807907104, time = 0.007611751556396484\n",
      "Training at step=53, batch=400, train loss = 1.4721435052190566, train acc = 0.4699999988079071, time = 0.007483959197998047\n",
      "Testing at step=53, batch=0, test loss = 1.5934581540251063, test acc = 0.46000000834465027, time = 0.001918792724609375\n",
      "Testing at step=53, batch=20, test loss = 1.776134242857657, test acc = 0.4300000071525574, time = 0.0018596649169921875\n",
      "Testing at step=53, batch=40, test loss = 1.745038101332915, test acc = 0.36000001430511475, time = 0.0018086433410644531\n",
      "Testing at step=53, batch=60, test loss = 1.7442023553687873, test acc = 0.4300000071525574, time = 0.0017101764678955078\n",
      "Testing at step=53, batch=80, test loss = 1.5531474668399665, test acc = 0.4000000059604645, time = 0.0018796920776367188\n",
      "Step 53 finished in 12.929878950119019, Train loss = 1.6316976514065307, Test loss = 1.7271080983769838; Train Acc = 0.44793999671936036, Test Acc = 0.4039000004529953\n",
      "Training at step=54, batch=0, train loss = 1.7299335469008759, train acc = 0.44999998807907104, time = 0.007658243179321289\n",
      "Training at step=54, batch=100, train loss = 1.524460337710385, train acc = 0.4399999976158142, time = 0.0076007843017578125\n",
      "Training at step=54, batch=200, train loss = 1.590692510254147, train acc = 0.4099999964237213, time = 0.007549762725830078\n",
      "Training at step=54, batch=300, train loss = 1.640031516403604, train acc = 0.44999998807907104, time = 0.007521867752075195\n",
      "Training at step=54, batch=400, train loss = 1.6482421390398159, train acc = 0.49000000953674316, time = 0.00737309455871582\n",
      "Testing at step=54, batch=0, test loss = 1.6467726673129135, test acc = 0.4699999988079071, time = 0.0018367767333984375\n",
      "Testing at step=54, batch=20, test loss = 1.4888570056030865, test acc = 0.49000000953674316, time = 0.0017614364624023438\n",
      "Testing at step=54, batch=40, test loss = 1.8291280602945497, test acc = 0.38999998569488525, time = 0.0018796920776367188\n",
      "Testing at step=54, batch=60, test loss = 1.714177756166126, test acc = 0.47999998927116394, time = 0.0017886161804199219\n",
      "Testing at step=54, batch=80, test loss = 1.8645982380247939, test acc = 0.33000001311302185, time = 0.0019614696502685547\n",
      "Step 54 finished in 13.00190019607544, Train loss = 1.6309646992691291, Test loss = 1.7290526708154672; Train Acc = 0.4480799977183342, Test Acc = 0.4047999981045723\n",
      "Training at step=55, batch=0, train loss = 1.6791448215738785, train acc = 0.4099999964237213, time = 0.0077817440032958984\n",
      "Training at step=55, batch=100, train loss = 1.6954558415716892, train acc = 0.5199999809265137, time = 0.007508993148803711\n",
      "Training at step=55, batch=200, train loss = 1.6185984856384759, train acc = 0.4300000071525574, time = 0.007839679718017578\n",
      "Training at step=55, batch=300, train loss = 1.562332420665889, train acc = 0.4699999988079071, time = 0.007500171661376953\n",
      "Training at step=55, batch=400, train loss = 1.6229278414917478, train acc = 0.4699999988079071, time = 0.007565021514892578\n",
      "Testing at step=55, batch=0, test loss = 1.6983970527859267, test acc = 0.4000000059604645, time = 0.001834869384765625\n",
      "Testing at step=55, batch=20, test loss = 1.804400373740644, test acc = 0.36000001430511475, time = 0.0017485618591308594\n",
      "Testing at step=55, batch=40, test loss = 1.6162862590933746, test acc = 0.5099999904632568, time = 0.0017364025115966797\n",
      "Testing at step=55, batch=60, test loss = 1.7744338785644005, test acc = 0.41999998688697815, time = 0.0018095970153808594\n",
      "Testing at step=55, batch=80, test loss = 1.7855759887527378, test acc = 0.38999998569488525, time = 0.0017430782318115234\n",
      "Step 55 finished in 13.04126501083374, Train loss = 1.6298896629189086, Test loss = 1.7266603453593325; Train Acc = 0.44755999666452406, Test Acc = 0.40729999899864194\n",
      "Training at step=56, batch=0, train loss = 1.646330428565511, train acc = 0.4300000071525574, time = 0.007682323455810547\n",
      "Training at step=56, batch=100, train loss = 1.6786868250819251, train acc = 0.41999998688697815, time = 0.007379055023193359\n",
      "Training at step=56, batch=200, train loss = 1.568206935052405, train acc = 0.4399999976158142, time = 0.007503509521484375\n",
      "Training at step=56, batch=300, train loss = 1.781608697777679, train acc = 0.3700000047683716, time = 0.007616519927978516\n",
      "Training at step=56, batch=400, train loss = 1.7864408212699778, train acc = 0.3199999928474426, time = 0.007615804672241211\n",
      "Testing at step=56, batch=0, test loss = 1.7551803595922002, test acc = 0.44999998807907104, time = 0.0019462108612060547\n",
      "Testing at step=56, batch=20, test loss = 1.7430185075155133, test acc = 0.36000001430511475, time = 0.0017693042755126953\n",
      "Testing at step=56, batch=40, test loss = 1.7229910937562074, test acc = 0.3700000047683716, time = 0.0017404556274414062\n",
      "Testing at step=56, batch=60, test loss = 1.7299396240459664, test acc = 0.38999998569488525, time = 0.0016393661499023438\n",
      "Testing at step=56, batch=80, test loss = 2.0573479698676977, test acc = 0.33000001311302185, time = 0.0017037391662597656\n",
      "Step 56 finished in 12.786619424819946, Train loss = 1.629392752375376, Test loss = 1.7268686106133695; Train Acc = 0.447379998087883, Test Acc = 0.40439999729394915\n",
      "Training at step=57, batch=0, train loss = 1.2888012100529023, train acc = 0.6600000262260437, time = 0.007517099380493164\n",
      "Training at step=57, batch=100, train loss = 1.6437958178155392, train acc = 0.4300000071525574, time = 0.007783651351928711\n",
      "Training at step=57, batch=200, train loss = 1.6484495768991305, train acc = 0.49000000953674316, time = 0.0075130462646484375\n",
      "Training at step=57, batch=300, train loss = 1.624045000310681, train acc = 0.46000000834465027, time = 0.007491350173950195\n",
      "Training at step=57, batch=400, train loss = 1.675717520176974, train acc = 0.5, time = 0.007669687271118164\n",
      "Testing at step=57, batch=0, test loss = 1.6816951890138145, test acc = 0.38999998569488525, time = 0.0018749237060546875\n",
      "Testing at step=57, batch=20, test loss = 1.6112362997727225, test acc = 0.49000000953674316, time = 0.001703023910522461\n",
      "Testing at step=57, batch=40, test loss = 1.7482229941894207, test acc = 0.3700000047683716, time = 0.0017762184143066406\n",
      "Testing at step=57, batch=60, test loss = 1.6521920937381767, test acc = 0.4300000071525574, time = 0.0018205642700195312\n",
      "Testing at step=57, batch=80, test loss = 1.8564509024620461, test acc = 0.3499999940395355, time = 0.0016493797302246094\n",
      "Step 57 finished in 12.76745343208313, Train loss = 1.6281028312970154, Test loss = 1.7282432602926443; Train Acc = 0.4490399975180626, Test Acc = 0.4065999984741211\n",
      "Training at step=58, batch=0, train loss = 1.5381625490743684, train acc = 0.5099999904632568, time = 0.007630586624145508\n",
      "Training at step=58, batch=100, train loss = 1.735200189730638, train acc = 0.4099999964237213, time = 0.007391691207885742\n",
      "Training at step=58, batch=200, train loss = 1.5457278299647714, train acc = 0.49000000953674316, time = 0.007609128952026367\n",
      "Training at step=58, batch=300, train loss = 1.6183844029269383, train acc = 0.4300000071525574, time = 0.007524013519287109\n",
      "Training at step=58, batch=400, train loss = 1.477372413607604, train acc = 0.5299999713897705, time = 0.007487058639526367\n",
      "Testing at step=58, batch=0, test loss = 1.8216374975039766, test acc = 0.3400000035762787, time = 0.001772165298461914\n",
      "Testing at step=58, batch=20, test loss = 1.63824244173565, test acc = 0.4399999976158142, time = 0.0017938613891601562\n",
      "Testing at step=58, batch=40, test loss = 1.7213945807564353, test acc = 0.46000000834465027, time = 0.0018055438995361328\n",
      "Testing at step=58, batch=60, test loss = 1.8514219441061461, test acc = 0.4099999964237213, time = 0.0017244815826416016\n",
      "Testing at step=58, batch=80, test loss = 1.6196457398458757, test acc = 0.3799999952316284, time = 0.0016856193542480469\n",
      "Step 58 finished in 12.825524091720581, Train loss = 1.6279677369186136, Test loss = 1.731358096745142; Train Acc = 0.4484399973154068, Test Acc = 0.40169999867677686\n",
      "Training at step=59, batch=0, train loss = 1.7746866307021234, train acc = 0.3799999952316284, time = 0.007603883743286133\n",
      "Training at step=59, batch=100, train loss = 1.4783556993413358, train acc = 0.49000000953674316, time = 0.007487297058105469\n",
      "Training at step=59, batch=200, train loss = 1.645512469332179, train acc = 0.44999998807907104, time = 0.007666587829589844\n",
      "Training at step=59, batch=300, train loss = 1.6397622896329156, train acc = 0.3700000047683716, time = 0.007548093795776367\n",
      "Training at step=59, batch=400, train loss = 1.6326776878209333, train acc = 0.41999998688697815, time = 0.007515668869018555\n",
      "Testing at step=59, batch=0, test loss = 1.7747220280970517, test acc = 0.3700000047683716, time = 0.001779317855834961\n",
      "Testing at step=59, batch=20, test loss = 1.732715064276578, test acc = 0.4000000059604645, time = 0.001741647720336914\n",
      "Testing at step=59, batch=40, test loss = 1.6826768018616989, test acc = 0.36000001430511475, time = 0.0017914772033691406\n",
      "Testing at step=59, batch=60, test loss = 1.881990483881222, test acc = 0.33000001311302185, time = 0.0017478466033935547\n",
      "Testing at step=59, batch=80, test loss = 1.6944300112186936, test acc = 0.4000000059604645, time = 0.0016334056854248047\n",
      "Step 59 finished in 12.839082479476929, Train loss = 1.6266997359208208, Test loss = 1.7292861020859718; Train Acc = 0.45031999808549883, Test Acc = 0.40340000003576276\n",
      "Training at step=60, batch=0, train loss = 1.7067200300928838, train acc = 0.3799999952316284, time = 0.007579326629638672\n",
      "Training at step=60, batch=100, train loss = 1.5857293955141305, train acc = 0.4699999988079071, time = 0.007632255554199219\n",
      "Training at step=60, batch=200, train loss = 1.6167515280576747, train acc = 0.44999998807907104, time = 0.007611751556396484\n",
      "Training at step=60, batch=300, train loss = 1.7181787609755756, train acc = 0.46000000834465027, time = 0.007486581802368164\n",
      "Training at step=60, batch=400, train loss = 1.63851231014711, train acc = 0.3799999952316284, time = 0.007485866546630859\n",
      "Testing at step=60, batch=0, test loss = 1.8238458939921118, test acc = 0.36000001430511475, time = 0.0018582344055175781\n",
      "Testing at step=60, batch=20, test loss = 1.9716463029775806, test acc = 0.3199999928474426, time = 0.0016787052154541016\n",
      "Testing at step=60, batch=40, test loss = 1.6815018808841697, test acc = 0.4000000059604645, time = 0.0016553401947021484\n",
      "Testing at step=60, batch=60, test loss = 1.8378067718551336, test acc = 0.2800000011920929, time = 0.0017085075378417969\n",
      "Testing at step=60, batch=80, test loss = 1.6229368887633207, test acc = 0.44999998807907104, time = 0.0016753673553466797\n",
      "Step 60 finished in 12.830454349517822, Train loss = 1.6261677710008051, Test loss = 1.7333424045159247; Train Acc = 0.45005999755859377, Test Acc = 0.40219999819993973\n",
      "Training at step=61, batch=0, train loss = 1.5328516884883718, train acc = 0.47999998927116394, time = 0.007524967193603516\n",
      "Training at step=61, batch=100, train loss = 1.5257340753785682, train acc = 0.47999998927116394, time = 0.007585763931274414\n",
      "Training at step=61, batch=200, train loss = 1.5320495361645958, train acc = 0.5099999904632568, time = 0.007490873336791992\n",
      "Training at step=61, batch=300, train loss = 1.7075034935233315, train acc = 0.4300000071525574, time = 0.00751805305480957\n",
      "Training at step=61, batch=400, train loss = 1.5996932438693212, train acc = 0.41999998688697815, time = 0.007548093795776367\n",
      "Testing at step=61, batch=0, test loss = 1.5680060094748989, test acc = 0.44999998807907104, time = 0.0018434524536132812\n",
      "Testing at step=61, batch=20, test loss = 1.7554981990309284, test acc = 0.3400000035762787, time = 0.0018622875213623047\n",
      "Testing at step=61, batch=40, test loss = 1.61849225020214, test acc = 0.3700000047683716, time = 0.0017478466033935547\n",
      "Testing at step=61, batch=60, test loss = 1.7785434503056985, test acc = 0.4099999964237213, time = 0.0016620159149169922\n",
      "Testing at step=61, batch=80, test loss = 1.5872750082053033, test acc = 0.41999998688697815, time = 0.0017087459564208984\n",
      "Step 61 finished in 13.053415775299072, Train loss = 1.625696669390503, Test loss = 1.728978276910523; Train Acc = 0.4488999980092049, Test Acc = 0.4019999995827675\n",
      "Training at step=62, batch=0, train loss = 1.5310946335407616, train acc = 0.4000000059604645, time = 0.007628440856933594\n",
      "Training at step=62, batch=100, train loss = 1.7591057637430891, train acc = 0.4399999976158142, time = 0.007697105407714844\n",
      "Training at step=62, batch=200, train loss = 1.715668149828726, train acc = 0.44999998807907104, time = 0.007300615310668945\n",
      "Training at step=62, batch=300, train loss = 1.591664422840531, train acc = 0.4300000071525574, time = 0.007527589797973633\n",
      "Training at step=62, batch=400, train loss = 1.7113211313501793, train acc = 0.3700000047683716, time = 0.009356021881103516\n",
      "Testing at step=62, batch=0, test loss = 1.7096279475972862, test acc = 0.38999998569488525, time = 0.0017004013061523438\n",
      "Testing at step=62, batch=20, test loss = 1.7757512763463392, test acc = 0.36000001430511475, time = 0.0017690658569335938\n",
      "Testing at step=62, batch=40, test loss = 1.7950756048057972, test acc = 0.47999998927116394, time = 0.0017633438110351562\n",
      "Testing at step=62, batch=60, test loss = 1.8342744161083124, test acc = 0.3499999940395355, time = 0.001695871353149414\n",
      "Testing at step=62, batch=80, test loss = 1.787634024832535, test acc = 0.3199999928474426, time = 0.00164794921875\n",
      "Step 62 finished in 12.642882823944092, Train loss = 1.6244905075116554, Test loss = 1.7324065283422956; Train Acc = 0.4498599970936775, Test Acc = 0.3991999977827072\n",
      "Training at step=63, batch=0, train loss = 1.621813630998073, train acc = 0.41999998688697815, time = 0.007552385330200195\n",
      "Training at step=63, batch=100, train loss = 1.5796948344933874, train acc = 0.44999998807907104, time = 0.007532358169555664\n",
      "Training at step=63, batch=200, train loss = 1.7618654601694288, train acc = 0.3499999940395355, time = 0.007556915283203125\n",
      "Training at step=63, batch=300, train loss = 1.5468871777460698, train acc = 0.47999998927116394, time = 0.007455348968505859\n",
      "Training at step=63, batch=400, train loss = 1.679669827431016, train acc = 0.3700000047683716, time = 0.0077037811279296875\n",
      "Testing at step=63, batch=0, test loss = 1.6393893727298916, test acc = 0.3799999952316284, time = 0.0017910003662109375\n",
      "Testing at step=63, batch=20, test loss = 1.5610345048062573, test acc = 0.4399999976158142, time = 0.0016431808471679688\n",
      "Testing at step=63, batch=40, test loss = 1.801197097133133, test acc = 0.33000001311302185, time = 0.0019888877868652344\n",
      "Testing at step=63, batch=60, test loss = 1.6914267175196562, test acc = 0.4000000059604645, time = 0.0017242431640625\n",
      "Testing at step=63, batch=80, test loss = 1.634534213859707, test acc = 0.41999998688697815, time = 0.0018265247344970703\n",
      "Step 63 finished in 12.7256600856781, Train loss = 1.6238671184458322, Test loss = 1.7332059227040977; Train Acc = 0.450419997215271, Test Acc = 0.40349999874830245\n",
      "Training at step=64, batch=0, train loss = 1.4358671273501198, train acc = 0.49000000953674316, time = 0.007578611373901367\n",
      "Training at step=64, batch=100, train loss = 1.787912379556912, train acc = 0.33000001311302185, time = 0.007599353790283203\n",
      "Training at step=64, batch=200, train loss = 1.7258811629504223, train acc = 0.36000001430511475, time = 0.007541656494140625\n",
      "Training at step=64, batch=300, train loss = 1.7229817141179584, train acc = 0.41999998688697815, time = 0.007390737533569336\n",
      "Training at step=64, batch=400, train loss = 1.5513541339574939, train acc = 0.49000000953674316, time = 0.007384777069091797\n",
      "Testing at step=64, batch=0, test loss = 1.771390606094295, test acc = 0.36000001430511475, time = 0.0017368793487548828\n",
      "Testing at step=64, batch=20, test loss = 1.5887280121722105, test acc = 0.47999998927116394, time = 0.0018165111541748047\n",
      "Testing at step=64, batch=40, test loss = 1.6717075672303334, test acc = 0.38999998569488525, time = 0.0017459392547607422\n",
      "Testing at step=64, batch=60, test loss = 1.673543412873953, test acc = 0.4000000059604645, time = 0.001720428466796875\n",
      "Testing at step=64, batch=80, test loss = 1.7322483317378485, test acc = 0.3799999952316284, time = 0.0017354488372802734\n",
      "Step 64 finished in 12.800103187561035, Train loss = 1.6230421026027644, Test loss = 1.7333532874727076; Train Acc = 0.45191999691724777, Test Acc = 0.4003999999165535\n",
      "Training at step=65, batch=0, train loss = 1.6756115623777836, train acc = 0.4099999964237213, time = 0.00766444206237793\n",
      "Training at step=65, batch=100, train loss = 1.5081276269354453, train acc = 0.5, time = 0.007498264312744141\n",
      "Training at step=65, batch=200, train loss = 1.48092562670763, train acc = 0.5400000214576721, time = 0.007415056228637695\n",
      "Training at step=65, batch=300, train loss = 1.8974255453450823, train acc = 0.3499999940395355, time = 0.007595539093017578\n",
      "Training at step=65, batch=400, train loss = 1.6829115611410403, train acc = 0.44999998807907104, time = 0.007496833801269531\n",
      "Testing at step=65, batch=0, test loss = 1.781706522778895, test acc = 0.3499999940395355, time = 0.0017769336700439453\n",
      "Testing at step=65, batch=20, test loss = 1.7426954125010063, test acc = 0.3700000047683716, time = 0.001668691635131836\n",
      "Testing at step=65, batch=40, test loss = 1.8506549427665733, test acc = 0.3499999940395355, time = 0.0016989707946777344\n",
      "Testing at step=65, batch=60, test loss = 1.7022499865892984, test acc = 0.3799999952316284, time = 0.001678466796875\n",
      "Testing at step=65, batch=80, test loss = 1.8682394648642187, test acc = 0.36000001430511475, time = 0.0016756057739257812\n",
      "Step 65 finished in 12.634056806564331, Train loss = 1.622034501441561, Test loss = 1.7327538341631512; Train Acc = 0.45149999678134917, Test Acc = 0.3998999990522861\n",
      "Training at step=66, batch=0, train loss = 1.5299820697887179, train acc = 0.47999998927116394, time = 0.0075206756591796875\n",
      "Training at step=66, batch=100, train loss = 1.7802239739792443, train acc = 0.4000000059604645, time = 0.007430315017700195\n",
      "Training at step=66, batch=200, train loss = 1.5696377801313077, train acc = 0.47999998927116394, time = 0.0075855255126953125\n",
      "Training at step=66, batch=300, train loss = 1.6298244712584844, train acc = 0.47999998927116394, time = 0.007561922073364258\n",
      "Training at step=66, batch=400, train loss = 1.3949549910370447, train acc = 0.49000000953674316, time = 0.007599353790283203\n",
      "Testing at step=66, batch=0, test loss = 1.8935833518121665, test acc = 0.4000000059604645, time = 0.0018200874328613281\n",
      "Testing at step=66, batch=20, test loss = 1.6718769196510208, test acc = 0.4099999964237213, time = 0.001644134521484375\n",
      "Testing at step=66, batch=40, test loss = 1.6221980426877942, test acc = 0.41999998688697815, time = 0.0017657279968261719\n",
      "Testing at step=66, batch=60, test loss = 1.768146150153625, test acc = 0.44999998807907104, time = 0.0017232894897460938\n",
      "Testing at step=66, batch=80, test loss = 1.6938310622562742, test acc = 0.38999998569488525, time = 0.001773834228515625\n",
      "Step 66 finished in 12.852480173110962, Train loss = 1.6217344651552672, Test loss = 1.7311280955297912; Train Acc = 0.45121999752521513, Test Acc = 0.3994999986886978\n",
      "Training at step=67, batch=0, train loss = 1.6104061910198455, train acc = 0.5, time = 0.0077245235443115234\n",
      "Training at step=67, batch=100, train loss = 1.6623687107829688, train acc = 0.4399999976158142, time = 0.007591962814331055\n",
      "Training at step=67, batch=200, train loss = 1.380802927681819, train acc = 0.550000011920929, time = 0.0075931549072265625\n",
      "Training at step=67, batch=300, train loss = 1.5852989134804063, train acc = 0.4300000071525574, time = 0.007424116134643555\n",
      "Training at step=67, batch=400, train loss = 1.7179423651206713, train acc = 0.4699999988079071, time = 0.007536888122558594\n",
      "Testing at step=67, batch=0, test loss = 1.915621256123628, test acc = 0.3700000047683716, time = 0.0017552375793457031\n",
      "Testing at step=67, batch=20, test loss = 1.8987498655074178, test acc = 0.3499999940395355, time = 0.0018084049224853516\n",
      "Testing at step=67, batch=40, test loss = 1.662333054366506, test acc = 0.3400000035762787, time = 0.0017464160919189453\n",
      "Testing at step=67, batch=60, test loss = 1.8031850124417022, test acc = 0.41999998688697815, time = 0.0016989707946777344\n",
      "Testing at step=67, batch=80, test loss = 1.6021037944241803, test acc = 0.4099999964237213, time = 0.0017213821411132812\n",
      "Step 67 finished in 12.998401880264282, Train loss = 1.6202781941464568, Test loss = 1.735808756817833; Train Acc = 0.45231999742984774, Test Acc = 0.4009999978542328\n",
      "Training at step=68, batch=0, train loss = 1.6586031381983353, train acc = 0.4000000059604645, time = 0.007521390914916992\n",
      "Training at step=68, batch=100, train loss = 1.733959547014281, train acc = 0.4099999964237213, time = 0.007655620574951172\n",
      "Training at step=68, batch=200, train loss = 1.4319472481829578, train acc = 0.5400000214576721, time = 0.007455587387084961\n",
      "Training at step=68, batch=300, train loss = 1.7890868348759807, train acc = 0.4099999964237213, time = 0.007528781890869141\n",
      "Training at step=68, batch=400, train loss = 1.7246973205617238, train acc = 0.3799999952316284, time = 0.007651090621948242\n",
      "Testing at step=68, batch=0, test loss = 1.7124356765302589, test acc = 0.41999998688697815, time = 0.0017998218536376953\n",
      "Testing at step=68, batch=20, test loss = 1.6390782420559, test acc = 0.4399999976158142, time = 0.0017125606536865234\n",
      "Testing at step=68, batch=40, test loss = 1.6663222004335072, test acc = 0.4099999964237213, time = 0.0017580986022949219\n",
      "Testing at step=68, batch=60, test loss = 1.7096074282016205, test acc = 0.41999998688697815, time = 0.0018877983093261719\n",
      "Testing at step=68, batch=80, test loss = 1.7602640317213365, test acc = 0.38999998569488525, time = 0.0017974376678466797\n",
      "Step 68 finished in 12.866400957107544, Train loss = 1.6198756921832536, Test loss = 1.736436837064733; Train Acc = 0.45207999777793884, Test Acc = 0.3982999977469444\n",
      "Training at step=69, batch=0, train loss = 1.7293023217539645, train acc = 0.4099999964237213, time = 0.007684469223022461\n",
      "Training at step=69, batch=100, train loss = 1.5467972856521022, train acc = 0.5, time = 0.007501125335693359\n",
      "Training at step=69, batch=200, train loss = 1.7941673910412883, train acc = 0.44999998807907104, time = 0.007513999938964844\n",
      "Training at step=69, batch=300, train loss = 1.5912777910363887, train acc = 0.4699999988079071, time = 0.007673740386962891\n",
      "Training at step=69, batch=400, train loss = 1.554042639852339, train acc = 0.4399999976158142, time = 0.007350444793701172\n",
      "Testing at step=69, batch=0, test loss = 1.7848870580149616, test acc = 0.36000001430511475, time = 0.0018038749694824219\n",
      "Testing at step=69, batch=20, test loss = 1.808303463640105, test acc = 0.36000001430511475, time = 0.0016932487487792969\n",
      "Testing at step=69, batch=40, test loss = 1.7245859236837133, test acc = 0.41999998688697815, time = 0.0016956329345703125\n",
      "Testing at step=69, batch=60, test loss = 1.7734812395570219, test acc = 0.3199999928474426, time = 0.0016994476318359375\n",
      "Testing at step=69, batch=80, test loss = 1.7680910432085326, test acc = 0.38999998569488525, time = 0.0017650127410888672\n",
      "Step 69 finished in 12.728438377380371, Train loss = 1.6190070130958725, Test loss = 1.7356332144751871; Train Acc = 0.4513599982261658, Test Acc = 0.40129999846220016\n",
      "Training at step=70, batch=0, train loss = 1.5381634822304528, train acc = 0.47999998927116394, time = 0.007677555084228516\n",
      "Training at step=70, batch=100, train loss = 1.6843234352165433, train acc = 0.4399999976158142, time = 0.007403373718261719\n",
      "Training at step=70, batch=200, train loss = 1.896059413863286, train acc = 0.41999998688697815, time = 0.007502079010009766\n",
      "Training at step=70, batch=300, train loss = 1.5852739882864022, train acc = 0.5, time = 0.007632017135620117\n",
      "Training at step=70, batch=400, train loss = 1.6554903657691276, train acc = 0.4300000071525574, time = 0.007516145706176758\n",
      "Testing at step=70, batch=0, test loss = 1.8842328409909532, test acc = 0.36000001430511475, time = 0.0018019676208496094\n",
      "Testing at step=70, batch=20, test loss = 1.7979889075062487, test acc = 0.36000001430511475, time = 0.0017485618591308594\n",
      "Testing at step=70, batch=40, test loss = 1.715813984067157, test acc = 0.3400000035762787, time = 0.0016803741455078125\n",
      "Testing at step=70, batch=60, test loss = 1.6617771089801243, test acc = 0.4300000071525574, time = 0.00177001953125\n",
      "Testing at step=70, batch=80, test loss = 1.7437606331656754, test acc = 0.3499999940395355, time = 0.0017871856689453125\n",
      "Step 70 finished in 12.930497169494629, Train loss = 1.6182124578490629, Test loss = 1.7370747986963664; Train Acc = 0.45215999710559845, Test Acc = 0.39789999902248385\n",
      "Training at step=71, batch=0, train loss = 1.5296502532728502, train acc = 0.44999998807907104, time = 0.0076410770416259766\n",
      "Training at step=71, batch=100, train loss = 1.7073561497494985, train acc = 0.44999998807907104, time = 0.007558107376098633\n",
      "Training at step=71, batch=200, train loss = 1.6231549107266008, train acc = 0.44999998807907104, time = 0.007398366928100586\n",
      "Training at step=71, batch=300, train loss = 1.5639957765317662, train acc = 0.4399999976158142, time = 0.0075376033782958984\n",
      "Training at step=71, batch=400, train loss = 1.683088738614325, train acc = 0.4000000059604645, time = 0.007451534271240234\n",
      "Testing at step=71, batch=0, test loss = 1.7270548355990545, test acc = 0.3799999952316284, time = 0.0017337799072265625\n",
      "Testing at step=71, batch=20, test loss = 1.7631709399316042, test acc = 0.38999998569488525, time = 0.0017709732055664062\n",
      "Testing at step=71, batch=40, test loss = 1.5233129151022704, test acc = 0.49000000953674316, time = 0.001725912094116211\n",
      "Testing at step=71, batch=60, test loss = 1.5896442565009543, test acc = 0.4300000071525574, time = 0.001665353775024414\n",
      "Testing at step=71, batch=80, test loss = 1.7555106315629345, test acc = 0.3199999928474426, time = 0.0017056465148925781\n",
      "Step 71 finished in 12.794725179672241, Train loss = 1.6177226904367785, Test loss = 1.7354990082093633; Train Acc = 0.45251999735832216, Test Acc = 0.3994999995827675\n",
      "Training at step=72, batch=0, train loss = 1.5322851979561185, train acc = 0.5199999809265137, time = 0.0074422359466552734\n",
      "Training at step=72, batch=100, train loss = 1.6614161990295395, train acc = 0.46000000834465027, time = 0.007573127746582031\n",
      "Training at step=72, batch=200, train loss = 1.4656430738798343, train acc = 0.46000000834465027, time = 0.00762176513671875\n",
      "Training at step=72, batch=300, train loss = 1.6578630561314984, train acc = 0.44999998807907104, time = 0.0074920654296875\n",
      "Training at step=72, batch=400, train loss = 1.6594225474973825, train acc = 0.4300000071525574, time = 0.007504940032958984\n",
      "Testing at step=72, batch=0, test loss = 1.746074136918754, test acc = 0.4300000071525574, time = 0.0020585060119628906\n",
      "Testing at step=72, batch=20, test loss = 1.5635571917863142, test acc = 0.4699999988079071, time = 0.0017423629760742188\n",
      "Testing at step=72, batch=40, test loss = 1.5670322876119271, test acc = 0.49000000953674316, time = 0.0017347335815429688\n",
      "Testing at step=72, batch=60, test loss = 1.9064791806353696, test acc = 0.3400000035762787, time = 0.0017788410186767578\n",
      "Testing at step=72, batch=80, test loss = 1.8064495773584417, test acc = 0.3700000047683716, time = 0.0016329288482666016\n",
      "Step 72 finished in 12.883174657821655, Train loss = 1.6168715422762305, Test loss = 1.7420683968535315; Train Acc = 0.45301999735832216, Test Acc = 0.3992000004649162\n",
      "Training at step=73, batch=0, train loss = 1.6177481602292343, train acc = 0.47999998927116394, time = 0.0077056884765625\n",
      "Training at step=73, batch=100, train loss = 1.5915227054976961, train acc = 0.41999998688697815, time = 0.0074460506439208984\n",
      "Training at step=73, batch=200, train loss = 1.5534485680982968, train acc = 0.5400000214576721, time = 0.007475614547729492\n",
      "Training at step=73, batch=300, train loss = 1.602111025240937, train acc = 0.46000000834465027, time = 0.00766754150390625\n",
      "Training at step=73, batch=400, train loss = 1.5125250097299214, train acc = 0.5099999904632568, time = 0.007483243942260742\n",
      "Testing at step=73, batch=0, test loss = 1.7905217154087805, test acc = 0.4300000071525574, time = 0.0018074512481689453\n",
      "Testing at step=73, batch=20, test loss = 1.8972831422204393, test acc = 0.28999999165534973, time = 0.0016851425170898438\n",
      "Testing at step=73, batch=40, test loss = 1.5319368867861645, test acc = 0.5, time = 0.0016808509826660156\n",
      "Testing at step=73, batch=60, test loss = 1.7240157639930431, test acc = 0.41999998688697815, time = 0.0017497539520263672\n",
      "Testing at step=73, batch=80, test loss = 1.700032719071602, test acc = 0.3799999952316284, time = 0.0017044544219970703\n",
      "Step 73 finished in 12.762274980545044, Train loss = 1.6163272321623086, Test loss = 1.7359632449731748; Train Acc = 0.45295999658107755, Test Acc = 0.4016999998688698\n",
      "Training at step=74, batch=0, train loss = 1.493831230448538, train acc = 0.44999998807907104, time = 0.007696628570556641\n",
      "Training at step=74, batch=100, train loss = 1.5851048960355882, train acc = 0.4699999988079071, time = 0.007596731185913086\n",
      "Training at step=74, batch=200, train loss = 1.6526989953269402, train acc = 0.4399999976158142, time = 0.00747227668762207\n",
      "Training at step=74, batch=300, train loss = 1.4485077768028825, train acc = 0.49000000953674316, time = 0.007412433624267578\n",
      "Training at step=74, batch=400, train loss = 1.6143025103080637, train acc = 0.4300000071525574, time = 0.007449626922607422\n",
      "Testing at step=74, batch=0, test loss = 1.6842620760392222, test acc = 0.41999998688697815, time = 0.001779794692993164\n",
      "Testing at step=74, batch=20, test loss = 1.8833300692113442, test acc = 0.3400000035762787, time = 0.0017735958099365234\n",
      "Testing at step=74, batch=40, test loss = 1.8484087185589695, test acc = 0.3400000035762787, time = 0.001634836196899414\n",
      "Testing at step=74, batch=60, test loss = 1.8724177461315181, test acc = 0.3799999952316284, time = 0.0016808509826660156\n",
      "Testing at step=74, batch=80, test loss = 1.6783988464265243, test acc = 0.49000000953674316, time = 0.0016908645629882812\n",
      "Step 74 finished in 12.700044631958008, Train loss = 1.6154439772110132, Test loss = 1.7374911799165744; Train Acc = 0.45249999779462813, Test Acc = 0.40279999881982803\n",
      "Training at step=75, batch=0, train loss = 1.7553823140854843, train acc = 0.4099999964237213, time = 0.0075266361236572266\n",
      "Training at step=75, batch=100, train loss = 1.6834964628674902, train acc = 0.4699999988079071, time = 0.00764775276184082\n",
      "Training at step=75, batch=200, train loss = 1.8206987399147174, train acc = 0.41999998688697815, time = 0.0076406002044677734\n",
      "Training at step=75, batch=300, train loss = 1.8406622733435964, train acc = 0.46000000834465027, time = 0.0075225830078125\n",
      "Training at step=75, batch=400, train loss = 1.6502804235355777, train acc = 0.41999998688697815, time = 0.007534027099609375\n",
      "Testing at step=75, batch=0, test loss = 1.7889565888482875, test acc = 0.33000001311302185, time = 0.0017554759979248047\n",
      "Testing at step=75, batch=20, test loss = 1.8400339092351887, test acc = 0.3400000035762787, time = 0.0018343925476074219\n",
      "Testing at step=75, batch=40, test loss = 1.6986194050577772, test acc = 0.46000000834465027, time = 0.0016944408416748047\n",
      "Testing at step=75, batch=60, test loss = 1.583213133218449, test acc = 0.46000000834465027, time = 0.0017964839935302734\n",
      "Testing at step=75, batch=80, test loss = 1.5876904581072138, test acc = 0.44999998807907104, time = 0.001842498779296875\n",
      "Step 75 finished in 12.843696355819702, Train loss = 1.614779143844713, Test loss = 1.7367778416137876; Train Acc = 0.4536999968290329, Test Acc = 0.4018999978899956\n",
      "Training at step=76, batch=0, train loss = 1.5442241008673034, train acc = 0.44999998807907104, time = 0.007613420486450195\n",
      "Training at step=76, batch=100, train loss = 1.5970614307316178, train acc = 0.4699999988079071, time = 0.0074727535247802734\n",
      "Training at step=76, batch=200, train loss = 1.7081127905535909, train acc = 0.4000000059604645, time = 0.007569789886474609\n",
      "Training at step=76, batch=300, train loss = 1.5234850281247103, train acc = 0.5, time = 0.007444620132446289\n",
      "Training at step=76, batch=400, train loss = 1.5057905774030507, train acc = 0.4300000071525574, time = 0.007443666458129883\n",
      "Testing at step=76, batch=0, test loss = 1.6008394237158283, test acc = 0.36000001430511475, time = 0.0017549991607666016\n",
      "Testing at step=76, batch=20, test loss = 1.6382612747528162, test acc = 0.4300000071525574, time = 0.0016946792602539062\n",
      "Testing at step=76, batch=40, test loss = 1.6057489271272283, test acc = 0.4000000059604645, time = 0.0017075538635253906\n",
      "Testing at step=76, batch=60, test loss = 1.6050715254613608, test acc = 0.4099999964237213, time = 0.001811981201171875\n",
      "Testing at step=76, batch=80, test loss = 1.7215309629671407, test acc = 0.38999998569488525, time = 0.0018391609191894531\n",
      "Step 76 finished in 12.760874271392822, Train loss = 1.6143914675530537, Test loss = 1.7380982999410588; Train Acc = 0.45301999711990354, Test Acc = 0.39849999994039537\n",
      "Training at step=77, batch=0, train loss = 1.871622034142109, train acc = 0.4099999964237213, time = 0.007798433303833008\n",
      "Training at step=77, batch=100, train loss = 1.6952858177717391, train acc = 0.4000000059604645, time = 0.007503986358642578\n",
      "Training at step=77, batch=200, train loss = 1.658345027226398, train acc = 0.44999998807907104, time = 0.007688283920288086\n",
      "Training at step=77, batch=300, train loss = 1.6860405518812496, train acc = 0.44999998807907104, time = 0.007632017135620117\n",
      "Training at step=77, batch=400, train loss = 1.7156735619460575, train acc = 0.46000000834465027, time = 0.00751948356628418\n",
      "Testing at step=77, batch=0, test loss = 1.7752159858534329, test acc = 0.38999998569488525, time = 0.0018084049224853516\n",
      "Testing at step=77, batch=20, test loss = 1.5534291487860186, test acc = 0.4699999988079071, time = 0.0017256736755371094\n",
      "Testing at step=77, batch=40, test loss = 1.9840317162097556, test acc = 0.3499999940395355, time = 0.0016932487487792969\n",
      "Testing at step=77, batch=60, test loss = 1.6058439790216026, test acc = 0.4099999964237213, time = 0.0018279552459716797\n",
      "Testing at step=77, batch=80, test loss = 1.6737064611876245, test acc = 0.38999998569488525, time = 0.0018224716186523438\n",
      "Step 77 finished in 12.979387044906616, Train loss = 1.61292415605113, Test loss = 1.7406736975249455; Train Acc = 0.454099996984005, Test Acc = 0.3989999982714653\n",
      "Training at step=78, batch=0, train loss = 1.58802489563847, train acc = 0.4300000071525574, time = 0.007631778717041016\n",
      "Training at step=78, batch=100, train loss = 1.617129804741267, train acc = 0.4000000059604645, time = 0.007514476776123047\n",
      "Training at step=78, batch=200, train loss = 1.5060313664555915, train acc = 0.5099999904632568, time = 0.0075190067291259766\n",
      "Training at step=78, batch=300, train loss = 1.6099772121146085, train acc = 0.5199999809265137, time = 0.007516384124755859\n",
      "Training at step=78, batch=400, train loss = 1.537317100046575, train acc = 0.5199999809265137, time = 0.007595062255859375\n",
      "Testing at step=78, batch=0, test loss = 1.846987581828749, test acc = 0.36000001430511475, time = 0.0016739368438720703\n",
      "Testing at step=78, batch=20, test loss = 1.7937463422720015, test acc = 0.3700000047683716, time = 0.0017316341400146484\n",
      "Testing at step=78, batch=40, test loss = 1.5778516632172819, test acc = 0.4099999964237213, time = 0.0017242431640625\n",
      "Testing at step=78, batch=60, test loss = 1.6597797903095446, test acc = 0.46000000834465027, time = 0.001695394515991211\n",
      "Testing at step=78, batch=80, test loss = 1.737329969744262, test acc = 0.38999998569488525, time = 0.0015988349914550781\n",
      "Step 78 finished in 12.792555093765259, Train loss = 1.6126220849545088, Test loss = 1.7376122540532846; Train Acc = 0.4537799972295761, Test Acc = 0.40019999861717226\n",
      "Training at step=79, batch=0, train loss = 1.636997345508537, train acc = 0.4000000059604645, time = 0.007554292678833008\n",
      "Training at step=79, batch=100, train loss = 1.534703442184588, train acc = 0.46000000834465027, time = 0.00747370719909668\n",
      "Training at step=79, batch=200, train loss = 1.7362409544212931, train acc = 0.4099999964237213, time = 0.007554054260253906\n",
      "Training at step=79, batch=300, train loss = 1.6972933358395397, train acc = 0.4699999988079071, time = 0.0074727535247802734\n",
      "Training at step=79, batch=400, train loss = 1.6023515949840874, train acc = 0.46000000834465027, time = 0.007516384124755859\n",
      "Testing at step=79, batch=0, test loss = 1.6879184143598018, test acc = 0.3700000047683716, time = 0.001753091812133789\n",
      "Testing at step=79, batch=20, test loss = 1.7252053105299785, test acc = 0.4099999964237213, time = 0.0017490386962890625\n",
      "Testing at step=79, batch=40, test loss = 1.8317822175843694, test acc = 0.4000000059604645, time = 0.0016489028930664062\n",
      "Testing at step=79, batch=60, test loss = 1.887626914708471, test acc = 0.3499999940395355, time = 0.001682281494140625\n",
      "Testing at step=79, batch=80, test loss = 1.693202845590718, test acc = 0.30000001192092896, time = 0.0017206668853759766\n",
      "Step 79 finished in 13.005652904510498, Train loss = 1.612100860924399, Test loss = 1.7397410498385184; Train Acc = 0.4537199971079826, Test Acc = 0.3991999998688698\n",
      "Training at step=80, batch=0, train loss = 1.77398149945809, train acc = 0.4099999964237213, time = 0.007691860198974609\n",
      "Training at step=80, batch=100, train loss = 1.5823450101729157, train acc = 0.4300000071525574, time = 0.007598161697387695\n",
      "Training at step=80, batch=200, train loss = 1.8254081662554953, train acc = 0.41999998688697815, time = 0.007563352584838867\n",
      "Training at step=80, batch=300, train loss = 1.5406444828519492, train acc = 0.47999998927116394, time = 0.0077517032623291016\n",
      "Training at step=80, batch=400, train loss = 1.606943990278002, train acc = 0.47999998927116394, time = 0.007483243942260742\n",
      "Testing at step=80, batch=0, test loss = 1.80842095547979, test acc = 0.4099999964237213, time = 0.0017304420471191406\n",
      "Testing at step=80, batch=20, test loss = 1.8412724281834512, test acc = 0.3799999952316284, time = 0.001718759536743164\n",
      "Testing at step=80, batch=40, test loss = 1.702969160610017, test acc = 0.3799999952316284, time = 0.0016970634460449219\n",
      "Testing at step=80, batch=60, test loss = 1.800166693532686, test acc = 0.3400000035762787, time = 0.001668691635131836\n",
      "Testing at step=80, batch=80, test loss = 1.7837830426477763, test acc = 0.4000000059604645, time = 0.0016872882843017578\n",
      "Step 80 finished in 12.748440504074097, Train loss = 1.6114006085128598, Test loss = 1.7398785337316323; Train Acc = 0.45347999727725985, Test Acc = 0.40189999997615816\n",
      "Training at step=81, batch=0, train loss = 1.5157683250875151, train acc = 0.46000000834465027, time = 0.007595062255859375\n",
      "Training at step=81, batch=100, train loss = 1.485447462672906, train acc = 0.47999998927116394, time = 0.007431983947753906\n",
      "Training at step=81, batch=200, train loss = 1.6353116731648074, train acc = 0.44999998807907104, time = 0.007692098617553711\n",
      "Training at step=81, batch=300, train loss = 1.4723607170580604, train acc = 0.5, time = 0.00746464729309082\n",
      "Training at step=81, batch=400, train loss = 1.7227658399760437, train acc = 0.38999998569488525, time = 0.007542133331298828\n",
      "Testing at step=81, batch=0, test loss = 1.7046643163606123, test acc = 0.38999998569488525, time = 0.0018384456634521484\n",
      "Testing at step=81, batch=20, test loss = 1.8981234314507685, test acc = 0.3700000047683716, time = 0.0017507076263427734\n",
      "Testing at step=81, batch=40, test loss = 1.56204263105932, test acc = 0.4699999988079071, time = 0.0017423629760742188\n",
      "Testing at step=81, batch=60, test loss = 1.6729807125106018, test acc = 0.46000000834465027, time = 0.0017578601837158203\n",
      "Testing at step=81, batch=80, test loss = 1.8520481919190517, test acc = 0.33000001311302185, time = 0.0016603469848632812\n",
      "Step 81 finished in 12.731122970581055, Train loss = 1.6104646184813796, Test loss = 1.7426210065803729; Train Acc = 0.4539799972176552, Test Acc = 0.4001999992132187\n",
      "Training at step=82, batch=0, train loss = 1.6754972441309683, train acc = 0.4399999976158142, time = 0.007665395736694336\n",
      "Training at step=82, batch=100, train loss = 1.5432564358123357, train acc = 0.5, time = 0.007394313812255859\n",
      "Training at step=82, batch=200, train loss = 1.7439294567442514, train acc = 0.5099999904632568, time = 0.007513999938964844\n",
      "Training at step=82, batch=300, train loss = 1.5446297753276281, train acc = 0.41999998688697815, time = 0.0075190067291259766\n",
      "Training at step=82, batch=400, train loss = 1.6773917453531566, train acc = 0.4399999976158142, time = 0.0074269771575927734\n",
      "Testing at step=82, batch=0, test loss = 1.555514912560377, test acc = 0.47999998927116394, time = 0.0017838478088378906\n",
      "Testing at step=82, batch=20, test loss = 1.9056908260979375, test acc = 0.4000000059604645, time = 0.0017588138580322266\n",
      "Testing at step=82, batch=40, test loss = 1.8209102288595762, test acc = 0.3799999952316284, time = 0.0016908645629882812\n",
      "Testing at step=82, batch=60, test loss = 1.9997203318310153, test acc = 0.2800000011920929, time = 0.0017077922821044922\n",
      "Testing at step=82, batch=80, test loss = 1.8463253996862512, test acc = 0.36000001430511475, time = 0.001798391342163086\n",
      "Step 82 finished in 12.724127769470215, Train loss = 1.6105861177335434, Test loss = 1.74553152740552; Train Acc = 0.4558199976682663, Test Acc = 0.39649999886751175\n",
      "Training at step=83, batch=0, train loss = 1.4928186601796027, train acc = 0.47999998927116394, time = 0.009222030639648438\n",
      "Training at step=83, batch=100, train loss = 1.5451818160126216, train acc = 0.550000011920929, time = 0.007806301116943359\n",
      "Training at step=83, batch=200, train loss = 1.5565669012747465, train acc = 0.44999998807907104, time = 0.007596492767333984\n",
      "Training at step=83, batch=300, train loss = 1.4572168035058854, train acc = 0.4399999976158142, time = 0.007523298263549805\n",
      "Training at step=83, batch=400, train loss = 1.6859090847396003, train acc = 0.3799999952316284, time = 0.007501363754272461\n",
      "Testing at step=83, batch=0, test loss = 1.7942973180560144, test acc = 0.3700000047683716, time = 0.0018563270568847656\n",
      "Testing at step=83, batch=20, test loss = 1.8851530677231114, test acc = 0.3799999952316284, time = 0.0016942024230957031\n",
      "Testing at step=83, batch=40, test loss = 1.646865564692443, test acc = 0.4399999976158142, time = 0.0017049312591552734\n",
      "Testing at step=83, batch=60, test loss = 1.8328259084902518, test acc = 0.36000001430511475, time = 0.0018057823181152344\n",
      "Testing at step=83, batch=80, test loss = 1.745993492497619, test acc = 0.4300000071525574, time = 0.0018162727355957031\n",
      "Step 83 finished in 12.967006921768188, Train loss = 1.6090412943821761, Test loss = 1.743628733120656; Train Acc = 0.45635999780893327, Test Acc = 0.39539999946951865\n",
      "Training at step=84, batch=0, train loss = 1.7515811072735383, train acc = 0.4000000059604645, time = 0.007652759552001953\n",
      "Training at step=84, batch=100, train loss = 1.5429182950935552, train acc = 0.4699999988079071, time = 0.00765538215637207\n",
      "Training at step=84, batch=200, train loss = 1.5876696101467689, train acc = 0.47999998927116394, time = 0.007582426071166992\n",
      "Training at step=84, batch=300, train loss = 1.7965553301391097, train acc = 0.4099999964237213, time = 0.0076580047607421875\n",
      "Training at step=84, batch=400, train loss = 1.6840583602318884, train acc = 0.36000001430511475, time = 0.007678985595703125\n",
      "Testing at step=84, batch=0, test loss = 1.7302740841934479, test acc = 0.3799999952316284, time = 0.001798868179321289\n",
      "Testing at step=84, batch=20, test loss = 1.7865727097434212, test acc = 0.41999998688697815, time = 0.001688241958618164\n",
      "Testing at step=84, batch=40, test loss = 1.7848777733919254, test acc = 0.36000001430511475, time = 0.0017614364624023438\n",
      "Testing at step=84, batch=60, test loss = 1.804473783970834, test acc = 0.38999998569488525, time = 0.0017237663269042969\n",
      "Testing at step=84, batch=80, test loss = 1.8056416493680996, test acc = 0.38999998569488525, time = 0.0017671585083007812\n",
      "Step 84 finished in 12.884647607803345, Train loss = 1.6085849218777446, Test loss = 1.7446195955520032; Train Acc = 0.4552999982237816, Test Acc = 0.39469999849796294\n",
      "Training at step=85, batch=0, train loss = 1.4919774902967047, train acc = 0.46000000834465027, time = 0.007621288299560547\n",
      "Training at step=85, batch=100, train loss = 1.65334605004871, train acc = 0.44999998807907104, time = 0.007655620574951172\n",
      "Training at step=85, batch=200, train loss = 1.5666139532851022, train acc = 0.47999998927116394, time = 0.007552623748779297\n",
      "Training at step=85, batch=300, train loss = 1.7152789500720302, train acc = 0.3799999952316284, time = 0.0075604915618896484\n",
      "Training at step=85, batch=400, train loss = 1.7554382440082308, train acc = 0.3499999940395355, time = 0.0073490142822265625\n",
      "Testing at step=85, batch=0, test loss = 1.714928869599449, test acc = 0.3499999940395355, time = 0.0017931461334228516\n",
      "Testing at step=85, batch=20, test loss = 1.9101659731222882, test acc = 0.33000001311302185, time = 0.0018117427825927734\n",
      "Testing at step=85, batch=40, test loss = 1.7173854674154427, test acc = 0.46000000834465027, time = 0.00182342529296875\n",
      "Testing at step=85, batch=60, test loss = 1.7692926251491692, test acc = 0.3499999940395355, time = 0.001781463623046875\n",
      "Testing at step=85, batch=80, test loss = 1.7820343260020954, test acc = 0.4000000059604645, time = 0.0017743110656738281\n",
      "Step 85 finished in 12.83300256729126, Train loss = 1.60790375611441, Test loss = 1.7450884251688583; Train Acc = 0.4557399978041649, Test Acc = 0.39919999897480013\n",
      "Training at step=86, batch=0, train loss = 1.6600995759450672, train acc = 0.46000000834465027, time = 0.008282661437988281\n",
      "Training at step=86, batch=100, train loss = 1.4317875519560936, train acc = 0.5, time = 0.0076558589935302734\n",
      "Training at step=86, batch=200, train loss = 1.6417953854098035, train acc = 0.4300000071525574, time = 0.007333517074584961\n",
      "Training at step=86, batch=300, train loss = 1.3916430791220242, train acc = 0.5699999928474426, time = 0.007664680480957031\n",
      "Training at step=86, batch=400, train loss = 1.6166237437043862, train acc = 0.4399999976158142, time = 0.007657527923583984\n",
      "Testing at step=86, batch=0, test loss = 1.5700838958678531, test acc = 0.4399999976158142, time = 0.001833200454711914\n",
      "Testing at step=86, batch=20, test loss = 1.7259007339304082, test acc = 0.4000000059604645, time = 0.001692056655883789\n",
      "Testing at step=86, batch=40, test loss = 1.7497822318430938, test acc = 0.36000001430511475, time = 0.0016808509826660156\n",
      "Testing at step=86, batch=60, test loss = 1.9793326987072333, test acc = 0.3499999940395355, time = 0.0017511844635009766\n",
      "Testing at step=86, batch=80, test loss = 1.5264274056631324, test acc = 0.4699999988079071, time = 0.001756906509399414\n",
      "Step 86 finished in 12.750762462615967, Train loss = 1.6071517685505388, Test loss = 1.7436415874715734; Train Acc = 0.4551799973845482, Test Acc = 0.3993000003695488\n",
      "Training at step=87, batch=0, train loss = 1.552881311089916, train acc = 0.46000000834465027, time = 0.007653236389160156\n",
      "Training at step=87, batch=100, train loss = 1.543668730304883, train acc = 0.5, time = 0.007611751556396484\n",
      "Training at step=87, batch=200, train loss = 1.6853057655851338, train acc = 0.3499999940395355, time = 0.007458925247192383\n",
      "Training at step=87, batch=300, train loss = 1.5351210645127984, train acc = 0.46000000834465027, time = 0.007615566253662109\n",
      "Training at step=87, batch=400, train loss = 1.6666651328458093, train acc = 0.3799999952316284, time = 0.007559537887573242\n",
      "Testing at step=87, batch=0, test loss = 1.75217731805171, test acc = 0.36000001430511475, time = 0.0018253326416015625\n",
      "Testing at step=87, batch=20, test loss = 1.9038456675093058, test acc = 0.3400000035762787, time = 0.001749277114868164\n",
      "Testing at step=87, batch=40, test loss = 1.7766002117710387, test acc = 0.3499999940395355, time = 0.0018506050109863281\n",
      "Testing at step=87, batch=60, test loss = 1.7355911395427597, test acc = 0.4099999964237213, time = 0.001683950424194336\n",
      "Testing at step=87, batch=80, test loss = 1.6238814927559588, test acc = 0.46000000834465027, time = 0.0017473697662353516\n",
      "Step 87 finished in 12.757126569747925, Train loss = 1.606561984021562, Test loss = 1.744954630038217; Train Acc = 0.45659999817609787, Test Acc = 0.3990999984741211\n",
      "Training at step=88, batch=0, train loss = 1.6460824629023536, train acc = 0.4000000059604645, time = 0.007584095001220703\n",
      "Training at step=88, batch=100, train loss = 1.5364355175321007, train acc = 0.49000000953674316, time = 0.007581472396850586\n",
      "Training at step=88, batch=200, train loss = 1.5519750913684152, train acc = 0.44999998807907104, time = 0.007611989974975586\n",
      "Training at step=88, batch=300, train loss = 1.7725166063875002, train acc = 0.28999999165534973, time = 0.0078012943267822266\n",
      "Training at step=88, batch=400, train loss = 1.6787579297689632, train acc = 0.4399999976158142, time = 0.007627725601196289\n",
      "Testing at step=88, batch=0, test loss = 1.6938878511108277, test acc = 0.38999998569488525, time = 0.0018205642700195312\n",
      "Testing at step=88, batch=20, test loss = 1.579937943162833, test acc = 0.4399999976158142, time = 0.0017080307006835938\n",
      "Testing at step=88, batch=40, test loss = 1.7198117088528162, test acc = 0.4399999976158142, time = 0.0017943382263183594\n",
      "Testing at step=88, batch=60, test loss = 1.6730675279539762, test acc = 0.4300000071525574, time = 0.00167083740234375\n",
      "Testing at step=88, batch=80, test loss = 1.8107271576408033, test acc = 0.38999998569488525, time = 0.0017445087432861328\n",
      "Step 88 finished in 13.027581453323364, Train loss = 1.6062728196981269, Test loss = 1.7445111705211247; Train Acc = 0.45479999697208406, Test Acc = 0.3997999981045723\n",
      "Training at step=89, batch=0, train loss = 1.6120033580905408, train acc = 0.4699999988079071, time = 0.007601261138916016\n",
      "Training at step=89, batch=100, train loss = 1.524788930875929, train acc = 0.5400000214576721, time = 0.007604122161865234\n",
      "Training at step=89, batch=200, train loss = 1.7051357039300152, train acc = 0.44999998807907104, time = 0.007521867752075195\n",
      "Training at step=89, batch=300, train loss = 1.6858321869425594, train acc = 0.4300000071525574, time = 0.007565021514892578\n",
      "Training at step=89, batch=400, train loss = 1.4416849307503605, train acc = 0.5199999809265137, time = 0.00759577751159668\n",
      "Testing at step=89, batch=0, test loss = 1.6122358071147067, test acc = 0.46000000834465027, time = 0.0018622875213623047\n",
      "Testing at step=89, batch=20, test loss = 1.5903532424257236, test acc = 0.46000000834465027, time = 0.0018291473388671875\n",
      "Testing at step=89, batch=40, test loss = 1.6705952120934922, test acc = 0.3700000047683716, time = 0.0018541812896728516\n",
      "Testing at step=89, batch=60, test loss = 1.517068280761241, test acc = 0.47999998927116394, time = 0.001729726791381836\n",
      "Testing at step=89, batch=80, test loss = 1.7346787390162874, test acc = 0.44999998807907104, time = 0.0017518997192382812\n",
      "Step 89 finished in 12.927778482437134, Train loss = 1.6052456486656415, Test loss = 1.7467891666263196; Train Acc = 0.4557799971103668, Test Acc = 0.39799999982118606\n",
      "Training at step=90, batch=0, train loss = 1.5863304197056831, train acc = 0.4399999976158142, time = 0.0075244903564453125\n",
      "Training at step=90, batch=100, train loss = 1.5767118233926545, train acc = 0.44999998807907104, time = 0.007489442825317383\n",
      "Training at step=90, batch=200, train loss = 1.6286283209108787, train acc = 0.4699999988079071, time = 0.007534027099609375\n",
      "Training at step=90, batch=300, train loss = 1.4783533707482404, train acc = 0.5, time = 0.007542610168457031\n",
      "Training at step=90, batch=400, train loss = 1.5599472401072652, train acc = 0.47999998927116394, time = 0.0073833465576171875\n",
      "Testing at step=90, batch=0, test loss = 1.6532259894638615, test acc = 0.38999998569488525, time = 0.001821756362915039\n",
      "Testing at step=90, batch=20, test loss = 1.5740326128790707, test acc = 0.4699999988079071, time = 0.0017094612121582031\n",
      "Testing at step=90, batch=40, test loss = 1.7249826119559493, test acc = 0.41999998688697815, time = 0.0018527507781982422\n",
      "Testing at step=90, batch=60, test loss = 1.5391014613024567, test acc = 0.4399999976158142, time = 0.0016872882843017578\n",
      "Testing at step=90, batch=80, test loss = 1.722453736883638, test acc = 0.4000000059604645, time = 0.0018203258514404297\n",
      "Step 90 finished in 12.840160131454468, Train loss = 1.6047352118380527, Test loss = 1.7478111929070164; Train Acc = 0.45643999630212784, Test Acc = 0.3956999987363815\n",
      "Training at step=91, batch=0, train loss = 1.4773805867445933, train acc = 0.4399999976158142, time = 0.00749969482421875\n",
      "Training at step=91, batch=100, train loss = 1.6282360898656185, train acc = 0.41999998688697815, time = 0.007468223571777344\n",
      "Training at step=91, batch=200, train loss = 1.6558490992575177, train acc = 0.44999998807907104, time = 0.007769107818603516\n",
      "Training at step=91, batch=300, train loss = 1.3542889395657993, train acc = 0.49000000953674316, time = 0.007671356201171875\n",
      "Training at step=91, batch=400, train loss = 1.5240485926362533, train acc = 0.4699999988079071, time = 0.007457733154296875\n",
      "Testing at step=91, batch=0, test loss = 1.5397387773074678, test acc = 0.38999998569488525, time = 0.0018711090087890625\n",
      "Testing at step=91, batch=20, test loss = 1.8401566905915714, test acc = 0.38999998569488525, time = 0.0017726421356201172\n",
      "Testing at step=91, batch=40, test loss = 1.7751688049957226, test acc = 0.36000001430511475, time = 0.0018117427825927734\n",
      "Testing at step=91, batch=60, test loss = 1.7087845894928229, test acc = 0.3700000047683716, time = 0.0017597675323486328\n",
      "Testing at step=91, batch=80, test loss = 1.6572837218323178, test acc = 0.4000000059604645, time = 0.001748800277709961\n",
      "Step 91 finished in 12.968981742858887, Train loss = 1.6038765452823578, Test loss = 1.745960031584607; Train Acc = 0.45579999643564223, Test Acc = 0.39699999898672106\n",
      "Training at step=92, batch=0, train loss = 1.4156053726084128, train acc = 0.44999998807907104, time = 0.00771641731262207\n",
      "Training at step=92, batch=100, train loss = 1.5074660518790648, train acc = 0.49000000953674316, time = 0.007585287094116211\n",
      "Training at step=92, batch=200, train loss = 1.6742764710032048, train acc = 0.44999998807907104, time = 0.007380962371826172\n",
      "Training at step=92, batch=300, train loss = 1.6525385920934186, train acc = 0.4699999988079071, time = 0.0075380802154541016\n",
      "Training at step=92, batch=400, train loss = 1.6741405333691726, train acc = 0.46000000834465027, time = 0.007516145706176758\n",
      "Testing at step=92, batch=0, test loss = 1.7660446428670893, test acc = 0.38999998569488525, time = 0.001745462417602539\n",
      "Testing at step=92, batch=20, test loss = 1.7231686830758455, test acc = 0.4399999976158142, time = 0.0017781257629394531\n",
      "Testing at step=92, batch=40, test loss = 1.6383042587382701, test acc = 0.4300000071525574, time = 0.0017271041870117188\n",
      "Testing at step=92, batch=60, test loss = 1.8930506245434162, test acc = 0.4099999964237213, time = 0.0017387866973876953\n",
      "Testing at step=92, batch=80, test loss = 1.927692118557507, test acc = 0.36000001430511475, time = 0.0016624927520751953\n",
      "Step 92 finished in 12.890091180801392, Train loss = 1.603753933147766, Test loss = 1.7457727394946587; Train Acc = 0.455779997587204, Test Acc = 0.39909999787807465\n",
      "Training at step=93, batch=0, train loss = 1.5795987969882714, train acc = 0.4000000059604645, time = 0.007514238357543945\n",
      "Training at step=93, batch=100, train loss = 1.694733803386039, train acc = 0.41999998688697815, time = 0.0075495243072509766\n",
      "Training at step=93, batch=200, train loss = 1.6600594829813666, train acc = 0.47999998927116394, time = 0.007500886917114258\n",
      "Training at step=93, batch=300, train loss = 1.5100945358890896, train acc = 0.4399999976158142, time = 0.007390737533569336\n",
      "Training at step=93, batch=400, train loss = 1.6541951620700337, train acc = 0.4399999976158142, time = 0.0075817108154296875\n",
      "Testing at step=93, batch=0, test loss = 1.6891456763114099, test acc = 0.3700000047683716, time = 0.001779317855834961\n",
      "Testing at step=93, batch=20, test loss = 1.8906699962575837, test acc = 0.38999998569488525, time = 0.0016875267028808594\n",
      "Testing at step=93, batch=40, test loss = 1.8123271552422415, test acc = 0.3700000047683716, time = 0.0016524791717529297\n",
      "Testing at step=93, batch=60, test loss = 1.819829420607516, test acc = 0.3700000047683716, time = 0.001714468002319336\n",
      "Testing at step=93, batch=80, test loss = 1.792797301555664, test acc = 0.3199999928474426, time = 0.0017075538635253906\n",
      "Step 93 finished in 12.678635120391846, Train loss = 1.6030431842207709, Test loss = 1.747150498746138; Train Acc = 0.45719999688863755, Test Acc = 0.3980000001192093\n",
      "Training at step=94, batch=0, train loss = 1.571040753061264, train acc = 0.41999998688697815, time = 0.007581949234008789\n",
      "Training at step=94, batch=100, train loss = 1.3056652849406327, train acc = 0.5799999833106995, time = 0.007623910903930664\n",
      "Training at step=94, batch=200, train loss = 1.594564994379811, train acc = 0.4099999964237213, time = 0.0076677799224853516\n",
      "Training at step=94, batch=300, train loss = 1.6530475527061859, train acc = 0.4099999964237213, time = 0.007557392120361328\n",
      "Training at step=94, batch=400, train loss = 1.8324982307060258, train acc = 0.3799999952316284, time = 0.007515430450439453\n",
      "Testing at step=94, batch=0, test loss = 1.636425641462594, test acc = 0.4399999976158142, time = 0.0018985271453857422\n",
      "Testing at step=94, batch=20, test loss = 1.7701240689378637, test acc = 0.33000001311302185, time = 0.0018208026885986328\n",
      "Testing at step=94, batch=40, test loss = 2.0237585299890215, test acc = 0.36000001430511475, time = 0.0016908645629882812\n",
      "Testing at step=94, batch=60, test loss = 1.589924148658489, test acc = 0.4000000059604645, time = 0.0017588138580322266\n",
      "Testing at step=94, batch=80, test loss = 1.6775532336850596, test acc = 0.4300000071525574, time = 0.0017993450164794922\n",
      "Step 94 finished in 12.781935691833496, Train loss = 1.6026921627753552, Test loss = 1.7488941094695127; Train Acc = 0.45861999636888506, Test Acc = 0.3956999987363815\n",
      "Training at step=95, batch=0, train loss = 1.5617155114789742, train acc = 0.5, time = 0.007724285125732422\n",
      "Training at step=95, batch=100, train loss = 1.3727812586281678, train acc = 0.49000000953674316, time = 0.007444620132446289\n",
      "Training at step=95, batch=200, train loss = 1.44141186337221, train acc = 0.5600000023841858, time = 0.007441997528076172\n",
      "Training at step=95, batch=300, train loss = 1.5007708505776043, train acc = 0.5, time = 0.0075168609619140625\n",
      "Training at step=95, batch=400, train loss = 1.730153680006212, train acc = 0.41999998688697815, time = 0.007508039474487305\n",
      "Testing at step=95, batch=0, test loss = 1.8196709010146968, test acc = 0.3499999940395355, time = 0.0017900466918945312\n",
      "Testing at step=95, batch=20, test loss = 1.7336661188417304, test acc = 0.41999998688697815, time = 0.0017561912536621094\n",
      "Testing at step=95, batch=40, test loss = 1.5940670529544283, test acc = 0.4000000059604645, time = 0.0017538070678710938\n",
      "Testing at step=95, batch=60, test loss = 1.7781757127429325, test acc = 0.38999998569488525, time = 0.0017714500427246094\n",
      "Testing at step=95, batch=80, test loss = 1.879424920601243, test acc = 0.3199999928474426, time = 0.0018260478973388672\n",
      "Step 95 finished in 12.823063373565674, Train loss = 1.601363943729811, Test loss = 1.751423876835695; Train Acc = 0.45569999814033507, Test Acc = 0.39559999853372574\n",
      "Training at step=96, batch=0, train loss = 1.7321422210507473, train acc = 0.4099999964237213, time = 0.007736921310424805\n",
      "Training at step=96, batch=100, train loss = 1.5191417546893746, train acc = 0.5199999809265137, time = 0.007495403289794922\n",
      "Training at step=96, batch=200, train loss = 1.3926339884447227, train acc = 0.5299999713897705, time = 0.0074846744537353516\n",
      "Training at step=96, batch=300, train loss = 1.6977249009467974, train acc = 0.4699999988079071, time = 0.0074689388275146484\n",
      "Training at step=96, batch=400, train loss = 1.7823655513124894, train acc = 0.4000000059604645, time = 0.0074269771575927734\n",
      "Testing at step=96, batch=0, test loss = 1.8680199261988293, test acc = 0.3199999928474426, time = 0.0018086433410644531\n",
      "Testing at step=96, batch=20, test loss = 1.7799472603938071, test acc = 0.3799999952316284, time = 0.0017321109771728516\n",
      "Testing at step=96, batch=40, test loss = 1.8815205685838736, test acc = 0.3799999952316284, time = 0.0018143653869628906\n",
      "Testing at step=96, batch=60, test loss = 1.9049089913812969, test acc = 0.3400000035762787, time = 0.0017936229705810547\n",
      "Testing at step=96, batch=80, test loss = 1.800207396297908, test acc = 0.3799999952316284, time = 0.0017595291137695312\n",
      "Step 96 finished in 12.8058180809021, Train loss = 1.6014711712065244, Test loss = 1.7517227386096201; Train Acc = 0.456159996509552, Test Acc = 0.39879999846220016\n",
      "Training at step=97, batch=0, train loss = 1.4450903571814533, train acc = 0.5899999737739563, time = 0.007604360580444336\n",
      "Training at step=97, batch=100, train loss = 1.4853383935180156, train acc = 0.46000000834465027, time = 0.007578849792480469\n",
      "Training at step=97, batch=200, train loss = 1.6029222847112172, train acc = 0.38999998569488525, time = 0.0074462890625\n",
      "Training at step=97, batch=300, train loss = 1.4179631310996939, train acc = 0.49000000953674316, time = 0.007508754730224609\n",
      "Training at step=97, batch=400, train loss = 1.5975035506177, train acc = 0.5, time = 0.007424354553222656\n",
      "Testing at step=97, batch=0, test loss = 1.7327234361886101, test acc = 0.4099999964237213, time = 0.0017447471618652344\n",
      "Testing at step=97, batch=20, test loss = 1.7066585007494464, test acc = 0.41999998688697815, time = 0.0017180442810058594\n",
      "Testing at step=97, batch=40, test loss = 1.7373826125641652, test acc = 0.38999998569488525, time = 0.0016560554504394531\n",
      "Testing at step=97, batch=60, test loss = 1.7821772802780418, test acc = 0.41999998688697815, time = 0.0017528533935546875\n",
      "Testing at step=97, batch=80, test loss = 1.7400329916044055, test acc = 0.36000001430511475, time = 0.001737833023071289\n",
      "Step 97 finished in 12.964022874832153, Train loss = 1.6000798537821534, Test loss = 1.7524897032274407; Train Acc = 0.4565599967241287, Test Acc = 0.39779999673366545\n",
      "Training at step=98, batch=0, train loss = 1.501838519516589, train acc = 0.5299999713897705, time = 0.007750988006591797\n",
      "Training at step=98, batch=100, train loss = 1.421955293289691, train acc = 0.6000000238418579, time = 0.007547855377197266\n",
      "Training at step=98, batch=200, train loss = 1.5265115686921078, train acc = 0.46000000834465027, time = 0.007524251937866211\n",
      "Training at step=98, batch=300, train loss = 1.6111614115219088, train acc = 0.4399999976158142, time = 0.007482051849365234\n",
      "Training at step=98, batch=400, train loss = 1.8571900319949475, train acc = 0.38999998569488525, time = 0.007479667663574219\n",
      "Testing at step=98, batch=0, test loss = 1.9649111029606778, test acc = 0.4000000059604645, time = 0.0018100738525390625\n",
      "Testing at step=98, batch=20, test loss = 1.8340091536517393, test acc = 0.4000000059604645, time = 0.0017435550689697266\n",
      "Testing at step=98, batch=40, test loss = 1.9242957770503866, test acc = 0.4000000059604645, time = 0.001786947250366211\n",
      "Testing at step=98, batch=60, test loss = 1.6400899772943658, test acc = 0.38999998569488525, time = 0.001756429672241211\n",
      "Testing at step=98, batch=80, test loss = 1.8341906699140091, test acc = 0.41999998688697815, time = 0.0018398761749267578\n",
      "Step 98 finished in 12.80510425567627, Train loss = 1.5997067690126865, Test loss = 1.7499777755431336; Train Acc = 0.45761999708414075, Test Acc = 0.39789999842643736\n",
      "Training at step=99, batch=0, train loss = 1.7183250725823738, train acc = 0.44999998807907104, time = 0.007558107376098633\n",
      "Training at step=99, batch=100, train loss = 1.6514477984921292, train acc = 0.4699999988079071, time = 0.007450103759765625\n",
      "Training at step=99, batch=200, train loss = 1.408989098535317, train acc = 0.47999998927116394, time = 0.007436275482177734\n",
      "Training at step=99, batch=300, train loss = 1.6659521636069414, train acc = 0.4099999964237213, time = 0.007639408111572266\n",
      "Training at step=99, batch=400, train loss = 1.3401978683796625, train acc = 0.5299999713897705, time = 0.007536888122558594\n",
      "Testing at step=99, batch=0, test loss = 1.7139252737965704, test acc = 0.3799999952316284, time = 0.0018188953399658203\n",
      "Testing at step=99, batch=20, test loss = 2.0079682757093793, test acc = 0.30000001192092896, time = 0.0017476081848144531\n",
      "Testing at step=99, batch=40, test loss = 1.768049692042997, test acc = 0.3199999928474426, time = 0.0016944408416748047\n",
      "Testing at step=99, batch=60, test loss = 1.7776467046596716, test acc = 0.3499999940395355, time = 0.0018510818481445312\n",
      "Testing at step=99, batch=80, test loss = 1.6275100477648925, test acc = 0.36000001430511475, time = 0.0018045902252197266\n",
      "Step 99 finished in 12.929145097732544, Train loss = 1.5991138342300095, Test loss = 1.7517076284168887; Train Acc = 0.4584399968385696, Test Acc = 0.3975999990105629\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:32:10.937885Z",
     "start_time": "2024-04-08T15:32:10.397284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"0-cifar10_simplenet.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ],
   "id": "c478e8fbddc2cf97",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:32:14.703503Z",
     "start_time": "2024-04-08T15:32:14.689666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"0-cifar10_simplenet.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ],
   "id": "ffa732623e4e7678",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:31:19.357934Z",
     "start_time": "2024-04-08T15:31:19.355934Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f8fd8658c38bede3",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
