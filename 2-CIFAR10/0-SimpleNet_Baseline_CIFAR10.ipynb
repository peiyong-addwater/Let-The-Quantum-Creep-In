{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-08T14:22:33.201920Z",
     "start_time": "2024-04-08T14:22:29.948184Z"
    }
   },
   "source": "!pip install torch torchvision torchaudio pennylane cotengra quimb torchmetrics --upgrade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.17.2)\r\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: pennylane in /usr/local/lib/python3.9/dist-packages (0.35.1)\r\n",
      "Requirement already satisfied: cotengra in /usr/local/lib/python3.9/dist-packages (0.5.6)\r\n",
      "Requirement already satisfied: quimb in /usr/local/lib/python3.9/dist-packages (1.7.3)\r\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (1.3.2)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch) (8.9.2.26)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch) (10.3.2.106)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from torch) (4.11.0)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.2.0)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\r\n",
      "Requirement already satisfied: autograd in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.6.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\r\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.4.4)\r\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\r\n",
      "Requirement already satisfied: pennylane-lightning>=0.35 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.35.1)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\r\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.10.2)\r\n",
      "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.6.9)\r\n",
      "Requirement already satisfied: rustworkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.14.2)\r\n",
      "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.10.0)\r\n",
      "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.59.1)\r\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.12.3)\r\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\r\n",
      "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (0.11.2)\r\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\r\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.39->quimb) (0.42.0)\r\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T14:22:36.858965Z",
     "start_time": "2024-04-08T14:22:33.204633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "\n",
    "\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "#REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ],
   "id": "f4efb6d39bdc04a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T14:22:38.484645Z",
     "start_time": "2024-04-08T14:22:36.860553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ],
   "id": "b724edf448668f3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([64, 3, 32, 32])\n",
      "torch.Size([64])\n",
      "tensor([7, 2, 7, 6, 4, 5, 7, 1, 8, 1, 1, 0, 8, 9, 7, 3, 1, 4, 6, 8, 2, 6, 5, 1,\n",
      "        5, 9, 7, 2, 6, 5, 5, 9, 3, 6, 1, 9, 7, 6, 2, 8, 1, 8, 4, 8, 9, 0, 1, 5,\n",
      "        2, 3, 3, 2, 6, 2, 5, 7, 6, 1, 8, 7, 7, 1, 5, 6])\n",
      "tensor([ 0.0275, -0.0196, -0.0353, -0.6078, -0.7961, -0.8118, -0.8196, -0.7569,\n",
      "        -0.7490, -0.7176, -0.7490, -0.7569, -0.7647, -0.8118, -0.7569, -0.7412,\n",
      "        -0.7804, -0.6157,  0.0118,  0.1294,  0.0902,  0.0902,  0.0196,  0.0196,\n",
      "         0.0745,  0.0745,  0.0667,  0.0824,  0.0431,  0.0039, -0.0118,  0.0118])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T14:22:42.597556Z",
     "start_time": "2024-04-08T14:22:38.486871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleNet2(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SimpleNet2, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(3, 32, kernel_size=3), # 32x30x30\n",
    "        torch.nn.Conv2d(32, 64, kernel_size=3), # 64x28x28\n",
    "        #torch.nn.Conv2d(64, 64, kernel_size=3), # 64x26x26\n",
    "        #torch.nn.Conv2d(64, 64, kernel_size=3), # 64x24x24\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(64*28*28, 10),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "net = SimpleNet2().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ],
   "id": "97cae9496e4a1ff2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n",
      "SimpleNet2(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=50176, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T14:44:29.481784Z",
     "start_time": "2024-04-08T14:22:42.598914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchmetrics\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.2\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ],
   "id": "a960b36c56c8cd28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 500, Number of test batches = 100\n",
      "Print every train batch = 100, Print every test batch = 20\n",
      "Training at step=0, batch=0, train loss = 2.2942078670143102, train acc = 0.09000000357627869, time = 0.1824660301208496\n",
      "Training at step=0, batch=100, train loss = 1.9711621278371314, train acc = 0.3400000035762787, time = 0.00799250602722168\n",
      "Training at step=0, batch=200, train loss = 2.019060596063117, train acc = 0.2800000011920929, time = 0.00811147689819336\n",
      "Training at step=0, batch=300, train loss = 1.8727192687278469, train acc = 0.3799999952316284, time = 0.007997751235961914\n",
      "Training at step=0, batch=400, train loss = 1.8566578837851488, train acc = 0.38999998569488525, time = 0.007956266403198242\n",
      "Testing at step=0, batch=0, test loss = 1.7454867522008264, test acc = 0.38999998569488525, time = 0.0018415451049804688\n",
      "Testing at step=0, batch=20, test loss = 1.9380294612108724, test acc = 0.3199999928474426, time = 0.0018892288208007812\n",
      "Testing at step=0, batch=40, test loss = 1.9267076721566139, test acc = 0.3100000023841858, time = 0.001857757568359375\n",
      "Testing at step=0, batch=60, test loss = 1.799467755033954, test acc = 0.38999998569488525, time = 0.0018308162689208984\n",
      "Testing at step=0, batch=80, test loss = 1.767729924773553, test acc = 0.36000001430511475, time = 0.0018494129180908203\n",
      "Step 0 finished in 13.144892692565918, Train loss = 1.8952583041900135, Test loss = 1.795421909951576; Train Acc = 0.3420000006407499, Test Acc = 0.3768999990820885\n",
      "Training at step=1, batch=0, train loss = 1.8180234975885012, train acc = 0.3400000035762787, time = 0.008271932601928711\n",
      "Training at step=1, batch=100, train loss = 1.8669876530825404, train acc = 0.3199999928474426, time = 0.008042573928833008\n",
      "Training at step=1, batch=200, train loss = 1.8577701822963275, train acc = 0.4000000059604645, time = 0.007910728454589844\n",
      "Training at step=1, batch=300, train loss = 1.8131713698617755, train acc = 0.33000001311302185, time = 0.008008241653442383\n",
      "Training at step=1, batch=400, train loss = 1.7654131574438716, train acc = 0.3700000047683716, time = 0.008161544799804688\n",
      "Testing at step=1, batch=0, test loss = 2.146675809115271, test acc = 0.3199999928474426, time = 0.0019047260284423828\n",
      "Testing at step=1, batch=20, test loss = 1.817538923882372, test acc = 0.28999999165534973, time = 0.0017652511596679688\n",
      "Testing at step=1, batch=40, test loss = 1.807419691761009, test acc = 0.3700000047683716, time = 0.0019099712371826172\n",
      "Testing at step=1, batch=60, test loss = 1.6575215794924822, test acc = 0.41999998688697815, time = 0.0018641948699951172\n",
      "Testing at step=1, batch=80, test loss = 1.773081791584508, test acc = 0.36000001430511475, time = 0.0018222332000732422\n",
      "Step 1 finished in 12.946586608886719, Train loss = 1.7749847578827047, Test loss = 1.7546786080106285; Train Acc = 0.3882599996030331, Test Acc = 0.3948999974131584\n",
      "Training at step=2, batch=0, train loss = 1.8277320842721145, train acc = 0.38999998569488525, time = 0.008133411407470703\n",
      "Training at step=2, batch=100, train loss = 1.6677291488005384, train acc = 0.47999998927116394, time = 0.007986068725585938\n",
      "Training at step=2, batch=200, train loss = 1.7341737176623362, train acc = 0.3499999940395355, time = 0.008059978485107422\n",
      "Training at step=2, batch=300, train loss = 1.7337090552558059, train acc = 0.44999998807907104, time = 0.008009910583496094\n",
      "Training at step=2, batch=400, train loss = 1.8364102891087233, train acc = 0.3700000047683716, time = 0.008148908615112305\n",
      "Testing at step=2, batch=0, test loss = 1.6638013597044141, test acc = 0.5, time = 0.0018455982208251953\n",
      "Testing at step=2, batch=20, test loss = 1.6761210891252225, test acc = 0.38999998569488525, time = 0.001779794692993164\n",
      "Testing at step=2, batch=40, test loss = 1.8698309441682728, test acc = 0.3199999928474426, time = 0.002002716064453125\n",
      "Testing at step=2, batch=60, test loss = 1.759598921770183, test acc = 0.3400000035762787, time = 0.0018804073333740234\n",
      "Testing at step=2, batch=80, test loss = 1.6627790939750249, test acc = 0.46000000834465027, time = 0.001943826675415039\n",
      "Step 2 finished in 13.08180284500122, Train loss = 1.7408895350947053, Test loss = 1.7362093489468557; Train Acc = 0.4042999991476536, Test Acc = 0.40259999930858614\n",
      "Training at step=3, batch=0, train loss = 1.7434795335954885, train acc = 0.44999998807907104, time = 0.008214950561523438\n",
      "Training at step=3, batch=100, train loss = 1.9214655623448837, train acc = 0.33000001311302185, time = 0.00792837142944336\n",
      "Training at step=3, batch=200, train loss = 1.764237866236105, train acc = 0.3700000047683716, time = 0.007987499237060547\n",
      "Training at step=3, batch=300, train loss = 1.7241562742111045, train acc = 0.41999998688697815, time = 0.008102655410766602\n",
      "Training at step=3, batch=400, train loss = 1.69814301363181, train acc = 0.4099999964237213, time = 0.00806117057800293\n",
      "Testing at step=3, batch=0, test loss = 1.664602870595767, test acc = 0.36000001430511475, time = 0.0018303394317626953\n",
      "Testing at step=3, batch=20, test loss = 1.7984978216280423, test acc = 0.4099999964237213, time = 0.0017535686492919922\n",
      "Testing at step=3, batch=40, test loss = 1.7528377732182951, test acc = 0.4000000059604645, time = 0.001749277114868164\n",
      "Testing at step=3, batch=60, test loss = 1.7661317196748485, test acc = 0.4000000059604645, time = 0.0017271041870117188\n",
      "Testing at step=3, batch=80, test loss = 1.7812707800822167, test acc = 0.38999998569488525, time = 0.0017693042755126953\n",
      "Step 3 finished in 12.805707454681396, Train loss = 1.7218197187392024, Test loss = 1.7232185475784578; Train Acc = 0.41259999781847, Test Acc = 0.4034999975562096\n",
      "Training at step=4, batch=0, train loss = 1.4738140408886544, train acc = 0.5199999809265137, time = 0.007989168167114258\n",
      "Training at step=4, batch=100, train loss = 1.6364031303856377, train acc = 0.44999998807907104, time = 0.007985591888427734\n",
      "Training at step=4, batch=200, train loss = 1.662607574031071, train acc = 0.4099999964237213, time = 0.008159875869750977\n",
      "Training at step=4, batch=300, train loss = 1.680425056402284, train acc = 0.41999998688697815, time = 0.007888555526733398\n",
      "Training at step=4, batch=400, train loss = 1.8637050729803446, train acc = 0.33000001311302185, time = 0.008210897445678711\n",
      "Testing at step=4, batch=0, test loss = 1.7158694534757044, test acc = 0.4300000071525574, time = 0.0018787384033203125\n",
      "Testing at step=4, batch=20, test loss = 1.8718882017834895, test acc = 0.3799999952316284, time = 0.001953125\n",
      "Testing at step=4, batch=40, test loss = 1.7103044001806098, test acc = 0.38999998569488525, time = 0.0018966197967529297\n",
      "Testing at step=4, batch=60, test loss = 1.5746523165953001, test acc = 0.5099999904632568, time = 0.0018434524536132812\n",
      "Testing at step=4, batch=80, test loss = 1.799127410799049, test acc = 0.41999998688697815, time = 0.0019278526306152344\n",
      "Step 4 finished in 13.008106470108032, Train loss = 1.708718362671691, Test loss = 1.7186626756784174; Train Acc = 0.4173599981069565, Test Acc = 0.41169999688863756\n",
      "Training at step=5, batch=0, train loss = 1.6313896400665966, train acc = 0.4399999976158142, time = 0.00825190544128418\n",
      "Training at step=5, batch=100, train loss = 1.6674614963415264, train acc = 0.4300000071525574, time = 0.008089303970336914\n",
      "Training at step=5, batch=200, train loss = 1.5925533706842203, train acc = 0.4699999988079071, time = 0.008207559585571289\n",
      "Training at step=5, batch=300, train loss = 1.6828832224459065, train acc = 0.44999998807907104, time = 0.007910013198852539\n",
      "Training at step=5, batch=400, train loss = 1.5679002710598826, train acc = 0.4399999976158142, time = 0.008074283599853516\n",
      "Testing at step=5, batch=0, test loss = 1.8880344862728302, test acc = 0.4099999964237213, time = 0.0018236637115478516\n",
      "Testing at step=5, batch=20, test loss = 1.5722646407329675, test acc = 0.49000000953674316, time = 0.0017943382263183594\n",
      "Testing at step=5, batch=40, test loss = 1.7511228913740413, test acc = 0.44999998807907104, time = 0.0017588138580322266\n",
      "Testing at step=5, batch=60, test loss = 1.7672716064036182, test acc = 0.4300000071525574, time = 0.0017955303192138672\n",
      "Testing at step=5, batch=80, test loss = 1.8279067337808599, test acc = 0.3400000035762787, time = 0.001867055892944336\n",
      "Step 5 finished in 12.998745203018188, Train loss = 1.700914918030041, Test loss = 1.7123732644633307; Train Acc = 0.4222399985194206, Test Acc = 0.41020000010728835\n",
      "Training at step=6, batch=0, train loss = 1.6192318583061107, train acc = 0.46000000834465027, time = 0.008079290390014648\n",
      "Training at step=6, batch=100, train loss = 1.6547673609670812, train acc = 0.3799999952316284, time = 0.007855415344238281\n",
      "Training at step=6, batch=200, train loss = 1.7066630694523282, train acc = 0.4699999988079071, time = 0.008610725402832031\n",
      "Training at step=6, batch=300, train loss = 1.6902728714064017, train acc = 0.41999998688697815, time = 0.008049726486206055\n",
      "Training at step=6, batch=400, train loss = 1.6698683816533662, train acc = 0.4300000071525574, time = 0.008025407791137695\n",
      "Testing at step=6, batch=0, test loss = 1.666196729047703, test acc = 0.41999998688697815, time = 0.0019350051879882812\n",
      "Testing at step=6, batch=20, test loss = 1.6848360241164735, test acc = 0.4399999976158142, time = 0.001750946044921875\n",
      "Testing at step=6, batch=40, test loss = 1.4124344793004446, test acc = 0.550000011920929, time = 0.0018625259399414062\n",
      "Testing at step=6, batch=60, test loss = 1.723702950725143, test acc = 0.4399999976158142, time = 0.0018188953399658203\n",
      "Testing at step=6, batch=80, test loss = 1.5697089993332305, test acc = 0.5099999904632568, time = 0.0018107891082763672\n",
      "Step 6 finished in 13.169427871704102, Train loss = 1.6931601222157429, Test loss = 1.7114471550049168; Train Acc = 0.42573999869823453, Test Acc = 0.41399999856948855\n",
      "Training at step=7, batch=0, train loss = 1.6381657758382482, train acc = 0.44999998807907104, time = 0.00815892219543457\n",
      "Training at step=7, batch=100, train loss = 1.5499940533010346, train acc = 0.46000000834465027, time = 0.00797581672668457\n",
      "Training at step=7, batch=200, train loss = 1.5001731137865943, train acc = 0.49000000953674316, time = 0.007871389389038086\n",
      "Training at step=7, batch=300, train loss = 1.7942665960955992, train acc = 0.49000000953674316, time = 0.007990837097167969\n",
      "Training at step=7, batch=400, train loss = 1.6826024929012222, train acc = 0.49000000953674316, time = 0.007926702499389648\n",
      "Testing at step=7, batch=0, test loss = 1.8184644157763814, test acc = 0.3199999928474426, time = 0.0018265247344970703\n",
      "Testing at step=7, batch=20, test loss = 1.7712376409974206, test acc = 0.3700000047683716, time = 0.0017991065979003906\n",
      "Testing at step=7, batch=40, test loss = 1.79733352978411, test acc = 0.3799999952316284, time = 0.0018050670623779297\n",
      "Testing at step=7, batch=60, test loss = 1.7782054652088348, test acc = 0.4099999964237213, time = 0.0018248558044433594\n",
      "Testing at step=7, batch=80, test loss = 1.653634563020073, test acc = 0.38999998569488525, time = 0.0018684864044189453\n",
      "Step 7 finished in 13.079195737838745, Train loss = 1.6868932241707344, Test loss = 1.7095479075387374; Train Acc = 0.428459998190403, Test Acc = 0.4130999967455864\n",
      "Training at step=8, batch=0, train loss = 1.771384212238501, train acc = 0.3700000047683716, time = 0.008185386657714844\n",
      "Training at step=8, batch=100, train loss = 1.8450164438607042, train acc = 0.46000000834465027, time = 0.008210897445678711\n",
      "Training at step=8, batch=200, train loss = 1.5582216548008174, train acc = 0.47999998927116394, time = 0.007918357849121094\n",
      "Training at step=8, batch=300, train loss = 1.6767802856722531, train acc = 0.46000000834465027, time = 0.007852315902709961\n",
      "Training at step=8, batch=400, train loss = 1.6552487575179118, train acc = 0.4699999988079071, time = 0.008026123046875\n",
      "Testing at step=8, batch=0, test loss = 1.8888831863545863, test acc = 0.33000001311302185, time = 0.0019030570983886719\n",
      "Testing at step=8, batch=20, test loss = 1.6243469038925542, test acc = 0.4300000071525574, time = 0.0017659664154052734\n",
      "Testing at step=8, batch=40, test loss = 1.8897425118895907, test acc = 0.33000001311302185, time = 0.001842498779296875\n",
      "Testing at step=8, batch=60, test loss = 1.633157475329402, test acc = 0.4699999988079071, time = 0.0018165111541748047\n",
      "Testing at step=8, batch=80, test loss = 1.824280011227668, test acc = 0.3100000023841858, time = 0.00182342529296875\n",
      "Step 8 finished in 12.921171426773071, Train loss = 1.682670728014849, Test loss = 1.7087645384914023; Train Acc = 0.4298399972319603, Test Acc = 0.41360000044107437\n",
      "Training at step=9, batch=0, train loss = 1.5290714183989609, train acc = 0.4699999988079071, time = 0.00805211067199707\n",
      "Training at step=9, batch=100, train loss = 1.8195150920986938, train acc = 0.4099999964237213, time = 0.00793600082397461\n",
      "Training at step=9, batch=200, train loss = 1.8049139028996983, train acc = 0.3799999952316284, time = 0.008103370666503906\n",
      "Training at step=9, batch=300, train loss = 1.7753462601610113, train acc = 0.3499999940395355, time = 0.007924079895019531\n",
      "Training at step=9, batch=400, train loss = 1.8182244929076024, train acc = 0.38999998569488525, time = 0.007949113845825195\n",
      "Testing at step=9, batch=0, test loss = 1.9396804182520697, test acc = 0.33000001311302185, time = 0.001924276351928711\n",
      "Testing at step=9, batch=20, test loss = 1.5698365320912542, test acc = 0.5199999809265137, time = 0.0017647743225097656\n",
      "Testing at step=9, batch=40, test loss = 1.675918077510212, test acc = 0.3499999940395355, time = 0.0018515586853027344\n",
      "Testing at step=9, batch=60, test loss = 1.656749995700328, test acc = 0.4099999964237213, time = 0.0021431446075439453\n",
      "Testing at step=9, batch=80, test loss = 1.5713350010538603, test acc = 0.46000000834465027, time = 0.0019195079803466797\n",
      "Step 9 finished in 12.957893371582031, Train loss = 1.6782570139614361, Test loss = 1.707986667836851; Train Acc = 0.43163999837636946, Test Acc = 0.41589999735355376\n",
      "Training at step=10, batch=0, train loss = 1.6985972913674896, train acc = 0.5, time = 0.008135557174682617\n",
      "Training at step=10, batch=100, train loss = 1.753042800553426, train acc = 0.44999998807907104, time = 0.007925033569335938\n",
      "Training at step=10, batch=200, train loss = 1.8291542319608245, train acc = 0.4300000071525574, time = 0.008140802383422852\n",
      "Training at step=10, batch=300, train loss = 1.6914065537023937, train acc = 0.3700000047683716, time = 0.008205175399780273\n",
      "Training at step=10, batch=400, train loss = 1.5329210125710253, train acc = 0.41999998688697815, time = 0.008127927780151367\n",
      "Testing at step=10, batch=0, test loss = 1.901494531103627, test acc = 0.4099999964237213, time = 0.0018534660339355469\n",
      "Testing at step=10, batch=20, test loss = 1.6472338638987916, test acc = 0.4300000071525574, time = 0.0018112659454345703\n",
      "Testing at step=10, batch=40, test loss = 1.461964808508439, test acc = 0.47999998927116394, time = 0.0018520355224609375\n",
      "Testing at step=10, batch=60, test loss = 1.8142193244430742, test acc = 0.4000000059604645, time = 0.0019741058349609375\n",
      "Testing at step=10, batch=80, test loss = 1.6880771289515437, test acc = 0.4099999964237213, time = 0.0018429756164550781\n",
      "Step 10 finished in 13.011709690093994, Train loss = 1.6746396967622121, Test loss = 1.7090484448234762; Train Acc = 0.4325799981355667, Test Acc = 0.4122999984025955\n",
      "Training at step=11, batch=0, train loss = 1.689946993658819, train acc = 0.4399999976158142, time = 0.008090496063232422\n",
      "Training at step=11, batch=100, train loss = 1.9606657325334904, train acc = 0.3799999952316284, time = 0.008074283599853516\n",
      "Training at step=11, batch=200, train loss = 1.6833354431314498, train acc = 0.4300000071525574, time = 0.008136510848999023\n",
      "Training at step=11, batch=300, train loss = 1.509095151273622, train acc = 0.5099999904632568, time = 0.00836181640625\n",
      "Training at step=11, batch=400, train loss = 1.7178061187207376, train acc = 0.4099999964237213, time = 0.00807642936706543\n",
      "Testing at step=11, batch=0, test loss = 1.6970816516230576, test acc = 0.4399999976158142, time = 0.0019404888153076172\n",
      "Testing at step=11, batch=20, test loss = 1.66589462741131, test acc = 0.46000000834465027, time = 0.0018835067749023438\n",
      "Testing at step=11, batch=40, test loss = 1.836333810277805, test acc = 0.4099999964237213, time = 0.0018095970153808594\n",
      "Testing at step=11, batch=60, test loss = 1.7065463997752974, test acc = 0.38999998569488525, time = 0.0018506050109863281\n",
      "Testing at step=11, batch=80, test loss = 1.6431331194623278, test acc = 0.4300000071525574, time = 0.0018024444580078125\n",
      "Step 11 finished in 13.018129825592041, Train loss = 1.6713484739507993, Test loss = 1.7071754419885545; Train Acc = 0.4348399983048439, Test Acc = 0.41380000025033953\n",
      "Training at step=12, batch=0, train loss = 1.7852015057175894, train acc = 0.38999998569488525, time = 0.008144140243530273\n",
      "Training at step=12, batch=100, train loss = 1.7311365740341529, train acc = 0.4699999988079071, time = 0.007977962493896484\n",
      "Training at step=12, batch=200, train loss = 1.6465876530717451, train acc = 0.4399999976158142, time = 0.007912635803222656\n",
      "Training at step=12, batch=300, train loss = 1.6708752005684866, train acc = 0.44999998807907104, time = 0.008062362670898438\n",
      "Training at step=12, batch=400, train loss = 1.688628941680399, train acc = 0.46000000834465027, time = 0.008091926574707031\n",
      "Testing at step=12, batch=0, test loss = 1.6608835361298748, test acc = 0.44999998807907104, time = 0.0018763542175292969\n",
      "Testing at step=12, batch=20, test loss = 1.8090214724089253, test acc = 0.3700000047683716, time = 0.0018510818481445312\n",
      "Testing at step=12, batch=40, test loss = 1.7960480642299037, test acc = 0.4099999964237213, time = 0.0017600059509277344\n",
      "Testing at step=12, batch=60, test loss = 1.8304729462478442, test acc = 0.36000001430511475, time = 0.0018222332000732422\n",
      "Testing at step=12, batch=80, test loss = 1.9134401547845454, test acc = 0.3199999928474426, time = 0.0017859935760498047\n",
      "Step 12 finished in 12.906107187271118, Train loss = 1.6685214614747965, Test loss = 1.7072602428664954; Train Acc = 0.4353599974513054, Test Acc = 0.41199999809265136\n",
      "Training at step=13, batch=0, train loss = 1.6701180538818345, train acc = 0.41999998688697815, time = 0.008287191390991211\n",
      "Training at step=13, batch=100, train loss = 1.4972192133512146, train acc = 0.5400000214576721, time = 0.008136510848999023\n",
      "Training at step=13, batch=200, train loss = 1.7039982806585008, train acc = 0.4099999964237213, time = 0.008019208908081055\n",
      "Training at step=13, batch=300, train loss = 1.7883702050288823, train acc = 0.3799999952316284, time = 0.007964134216308594\n",
      "Training at step=13, batch=400, train loss = 1.6552275802949525, train acc = 0.46000000834465027, time = 0.00799560546875\n",
      "Testing at step=13, batch=0, test loss = 1.7033708455171064, test acc = 0.4099999964237213, time = 0.0018420219421386719\n",
      "Testing at step=13, batch=20, test loss = 1.5417673907806073, test acc = 0.44999998807907104, time = 0.0018076896667480469\n",
      "Testing at step=13, batch=40, test loss = 1.7621179753423575, test acc = 0.3799999952316284, time = 0.0017971992492675781\n",
      "Testing at step=13, batch=60, test loss = 1.5733927566243142, test acc = 0.5099999904632568, time = 0.0018353462219238281\n",
      "Testing at step=13, batch=80, test loss = 1.6180436706078436, test acc = 0.3799999952316284, time = 0.0018723011016845703\n",
      "Step 13 finished in 12.90518569946289, Train loss = 1.6654672262355665, Test loss = 1.7096679262299508; Train Acc = 0.43627999806404116, Test Acc = 0.4111999961733818\n",
      "Training at step=14, batch=0, train loss = 1.595309562099613, train acc = 0.46000000834465027, time = 0.008169174194335938\n",
      "Training at step=14, batch=100, train loss = 1.818273029975179, train acc = 0.3799999952316284, time = 0.008224964141845703\n",
      "Training at step=14, batch=200, train loss = 1.5417110590544019, train acc = 0.46000000834465027, time = 0.008157491683959961\n",
      "Training at step=14, batch=300, train loss = 1.6907756483363436, train acc = 0.4300000071525574, time = 0.008089065551757812\n",
      "Training at step=14, batch=400, train loss = 1.7208890899109013, train acc = 0.4099999964237213, time = 0.008002519607543945\n",
      "Testing at step=14, batch=0, test loss = 1.7595034851043478, test acc = 0.4099999964237213, time = 0.0018329620361328125\n",
      "Testing at step=14, batch=20, test loss = 1.601998312773673, test acc = 0.49000000953674316, time = 0.0018563270568847656\n",
      "Testing at step=14, batch=40, test loss = 1.7014859036963708, test acc = 0.4300000071525574, time = 0.0018846988677978516\n",
      "Testing at step=14, batch=60, test loss = 1.5417323317765186, test acc = 0.5099999904632568, time = 0.0019216537475585938\n",
      "Testing at step=14, batch=80, test loss = 1.5341049503074482, test acc = 0.49000000953674316, time = 0.0018749237060546875\n",
      "Step 14 finished in 13.016809463500977, Train loss = 1.6639346507851724, Test loss = 1.7071851862921812; Train Acc = 0.4387199977040291, Test Acc = 0.41639999955892565\n",
      "Training at step=15, batch=0, train loss = 1.4954575668988965, train acc = 0.5400000214576721, time = 0.008101224899291992\n",
      "Training at step=15, batch=100, train loss = 1.6407835807730498, train acc = 0.4300000071525574, time = 0.00808572769165039\n",
      "Training at step=15, batch=200, train loss = 1.6549515705897548, train acc = 0.44999998807907104, time = 0.008181571960449219\n",
      "Training at step=15, batch=300, train loss = 1.5348590094046992, train acc = 0.4699999988079071, time = 0.008131027221679688\n",
      "Training at step=15, batch=400, train loss = 1.5562562471400332, train acc = 0.46000000834465027, time = 0.008013010025024414\n",
      "Testing at step=15, batch=0, test loss = 1.805946876884728, test acc = 0.38999998569488525, time = 0.0019061565399169922\n",
      "Testing at step=15, batch=20, test loss = 1.7407021481201665, test acc = 0.38999998569488525, time = 0.0017323493957519531\n",
      "Testing at step=15, batch=40, test loss = 1.7826603961816563, test acc = 0.38999998569488525, time = 0.0017328262329101562\n",
      "Testing at step=15, batch=60, test loss = 1.6519168704135663, test acc = 0.4399999976158142, time = 0.001825094223022461\n",
      "Testing at step=15, batch=80, test loss = 1.6963731674159235, test acc = 0.4399999976158142, time = 0.0018000602722167969\n",
      "Step 15 finished in 12.92711091041565, Train loss = 1.6606469282452214, Test loss = 1.7121126381162566; Train Acc = 0.4377799978852272, Test Acc = 0.41119999915361405\n",
      "Training at step=16, batch=0, train loss = 1.8675934888823462, train acc = 0.41999998688697815, time = 0.008353948593139648\n",
      "Training at step=16, batch=100, train loss = 1.7148657182909894, train acc = 0.41999998688697815, time = 0.008061885833740234\n",
      "Training at step=16, batch=200, train loss = 1.5194558574893489, train acc = 0.5199999809265137, time = 0.007943391799926758\n",
      "Training at step=16, batch=300, train loss = 1.758949639791396, train acc = 0.3799999952316284, time = 0.007964611053466797\n",
      "Training at step=16, batch=400, train loss = 1.6747572568363587, train acc = 0.3700000047683716, time = 0.007995128631591797\n",
      "Testing at step=16, batch=0, test loss = 1.7944341351127449, test acc = 0.36000001430511475, time = 0.0018954277038574219\n",
      "Testing at step=16, batch=20, test loss = 1.698834890349739, test acc = 0.4099999964237213, time = 0.0018813610076904297\n",
      "Testing at step=16, batch=40, test loss = 1.638231837085251, test acc = 0.4099999964237213, time = 0.0019137859344482422\n",
      "Testing at step=16, batch=60, test loss = 1.665368814843541, test acc = 0.49000000953674316, time = 0.0018379688262939453\n",
      "Testing at step=16, batch=80, test loss = 1.627446464822939, test acc = 0.41999998688697815, time = 0.0018835067749023438\n",
      "Step 16 finished in 13.343604326248169, Train loss = 1.6592645195691156, Test loss = 1.7084337933957194; Train Acc = 0.4383799983263016, Test Acc = 0.41469999969005583\n",
      "Training at step=17, batch=0, train loss = 1.5937782436518233, train acc = 0.5099999904632568, time = 0.008214235305786133\n",
      "Training at step=17, batch=100, train loss = 1.6805764564042391, train acc = 0.38999998569488525, time = 0.008046627044677734\n",
      "Training at step=17, batch=200, train loss = 1.4823590218468123, train acc = 0.5199999809265137, time = 0.008094072341918945\n",
      "Training at step=17, batch=300, train loss = 1.6298243657051987, train acc = 0.44999998807907104, time = 0.008062124252319336\n",
      "Training at step=17, batch=400, train loss = 1.6959549503098716, train acc = 0.44999998807907104, time = 0.008153438568115234\n",
      "Testing at step=17, batch=0, test loss = 1.4657576235182666, test acc = 0.47999998927116394, time = 0.0018091201782226562\n",
      "Testing at step=17, batch=20, test loss = 1.9020220028927282, test acc = 0.36000001430511475, time = 0.0018153190612792969\n",
      "Testing at step=17, batch=40, test loss = 1.8100620082093755, test acc = 0.3199999928474426, time = 0.0018510818481445312\n",
      "Testing at step=17, batch=60, test loss = 1.6541423005505729, test acc = 0.4000000059604645, time = 0.0017998218536376953\n",
      "Testing at step=17, batch=80, test loss = 1.6286035831402084, test acc = 0.4099999964237213, time = 0.0018002986907958984\n",
      "Step 17 finished in 13.03023099899292, Train loss = 1.6570242052716784, Test loss = 1.7134504658242502; Train Acc = 0.4382999973893166, Test Acc = 0.4085000005364418\n",
      "Training at step=18, batch=0, train loss = 1.4722079441794185, train acc = 0.4399999976158142, time = 0.008129358291625977\n",
      "Training at step=18, batch=100, train loss = 1.5885010766528105, train acc = 0.4300000071525574, time = 0.008060455322265625\n",
      "Training at step=18, batch=200, train loss = 1.6990587768385534, train acc = 0.3799999952316284, time = 0.008013248443603516\n",
      "Training at step=18, batch=300, train loss = 1.7507316810890978, train acc = 0.44999998807907104, time = 0.007975578308105469\n",
      "Training at step=18, batch=400, train loss = 1.7085611108234298, train acc = 0.46000000834465027, time = 0.008068323135375977\n",
      "Testing at step=18, batch=0, test loss = 1.8490725591506816, test acc = 0.3199999928474426, time = 0.0018503665924072266\n",
      "Testing at step=18, batch=20, test loss = 1.6867598294089268, test acc = 0.4300000071525574, time = 0.0018162727355957031\n",
      "Testing at step=18, batch=40, test loss = 1.5688843933419707, test acc = 0.47999998927116394, time = 0.0018575191497802734\n",
      "Testing at step=18, batch=60, test loss = 1.6885893067547852, test acc = 0.41999998688697815, time = 0.0019617080688476562\n",
      "Testing at step=18, batch=80, test loss = 1.738588622404759, test acc = 0.4099999964237213, time = 0.0018877983093261719\n",
      "Step 18 finished in 12.92215609550476, Train loss = 1.6550491962187643, Test loss = 1.7108947176140197; Train Acc = 0.4408199970126152, Test Acc = 0.41379999846220017\n",
      "Training at step=19, batch=0, train loss = 1.591643700403629, train acc = 0.47999998927116394, time = 0.008129358291625977\n",
      "Training at step=19, batch=100, train loss = 1.7183759937770398, train acc = 0.4099999964237213, time = 0.008179426193237305\n",
      "Training at step=19, batch=200, train loss = 1.6381379490823758, train acc = 0.38999998569488525, time = 0.00796365737915039\n",
      "Training at step=19, batch=300, train loss = 1.6731790208233266, train acc = 0.47999998927116394, time = 0.008115053176879883\n",
      "Training at step=19, batch=400, train loss = 1.6273204580899125, train acc = 0.4300000071525574, time = 0.007967710494995117\n",
      "Testing at step=19, batch=0, test loss = 1.8043877917142794, test acc = 0.4099999964237213, time = 0.001989126205444336\n",
      "Testing at step=19, batch=20, test loss = 1.684620953498615, test acc = 0.5, time = 0.001840829849243164\n",
      "Testing at step=19, batch=40, test loss = 2.045242961718224, test acc = 0.28999999165534973, time = 0.0018603801727294922\n",
      "Testing at step=19, batch=60, test loss = 1.7358375933193986, test acc = 0.3799999952316284, time = 0.0025780200958251953\n",
      "Testing at step=19, batch=80, test loss = 1.6966602445256993, test acc = 0.4099999964237213, time = 0.0018875598907470703\n",
      "Step 19 finished in 13.123726844787598, Train loss = 1.652955486618579, Test loss = 1.7133010730298723; Train Acc = 0.4409599972367287, Test Acc = 0.40919999837875365\n",
      "Training at step=20, batch=0, train loss = 1.6578463067731959, train acc = 0.41999998688697815, time = 0.008080482482910156\n",
      "Training at step=20, batch=100, train loss = 1.5112293166058584, train acc = 0.5199999809265137, time = 0.008077859878540039\n",
      "Training at step=20, batch=200, train loss = 1.807743776928244, train acc = 0.4300000071525574, time = 0.008044719696044922\n",
      "Training at step=20, batch=300, train loss = 1.6993809551690797, train acc = 0.4300000071525574, time = 0.008183479309082031\n",
      "Training at step=20, batch=400, train loss = 1.7275561150165786, train acc = 0.3499999940395355, time = 0.009491443634033203\n",
      "Testing at step=20, batch=0, test loss = 1.591315337571869, test acc = 0.4399999976158142, time = 0.0019083023071289062\n",
      "Testing at step=20, batch=20, test loss = 1.6044811979754292, test acc = 0.36000001430511475, time = 0.0017426013946533203\n",
      "Testing at step=20, batch=40, test loss = 1.7080861763930855, test acc = 0.4099999964237213, time = 0.0018656253814697266\n",
      "Testing at step=20, batch=60, test loss = 1.7974165077492565, test acc = 0.3100000023841858, time = 0.0017867088317871094\n",
      "Testing at step=20, batch=80, test loss = 1.9082260406285556, test acc = 0.3700000047683716, time = 0.0017478466033935547\n",
      "Step 20 finished in 13.09483814239502, Train loss = 1.65105656670365, Test loss = 1.714275229791169; Train Acc = 0.44193999814987184, Test Acc = 0.40999999701976775\n",
      "Training at step=21, batch=0, train loss = 1.7007536629899491, train acc = 0.46000000834465027, time = 0.008056640625\n",
      "Training at step=21, batch=100, train loss = 1.5698527618995695, train acc = 0.4099999964237213, time = 0.008239507675170898\n",
      "Training at step=21, batch=200, train loss = 1.6206588754231337, train acc = 0.4099999964237213, time = 0.008305072784423828\n",
      "Training at step=21, batch=300, train loss = 1.7182347294534726, train acc = 0.3499999940395355, time = 0.007923364639282227\n",
      "Training at step=21, batch=400, train loss = 1.5919181292172204, train acc = 0.4699999988079071, time = 0.008032798767089844\n",
      "Testing at step=21, batch=0, test loss = 1.6004561483814754, test acc = 0.4300000071525574, time = 0.0018591880798339844\n",
      "Testing at step=21, batch=20, test loss = 1.5286857255917923, test acc = 0.5099999904632568, time = 0.0018658638000488281\n",
      "Testing at step=21, batch=40, test loss = 1.7278156544420655, test acc = 0.44999998807907104, time = 0.0018520355224609375\n",
      "Testing at step=21, batch=60, test loss = 1.640974680235998, test acc = 0.4399999976158142, time = 0.001859426498413086\n",
      "Testing at step=21, batch=80, test loss = 1.7004661109521573, test acc = 0.3799999952316284, time = 0.0018644332885742188\n",
      "Step 21 finished in 13.038955688476562, Train loss = 1.6495442380415601, Test loss = 1.7153370503220515; Train Acc = 0.4421599974632263, Test Acc = 0.4127999985218048\n",
      "Training at step=22, batch=0, train loss = 1.5798577830139056, train acc = 0.49000000953674316, time = 0.008086204528808594\n",
      "Training at step=22, batch=100, train loss = 1.691127030100878, train acc = 0.41999998688697815, time = 0.007875919342041016\n",
      "Training at step=22, batch=200, train loss = 1.485987972590547, train acc = 0.4300000071525574, time = 0.008095026016235352\n",
      "Training at step=22, batch=300, train loss = 1.4940242078352355, train acc = 0.47999998927116394, time = 0.008144140243530273\n",
      "Training at step=22, batch=400, train loss = 1.7831486195357713, train acc = 0.3799999952316284, time = 0.008144855499267578\n",
      "Testing at step=22, batch=0, test loss = 1.6324623727705656, test acc = 0.5199999809265137, time = 0.001888275146484375\n",
      "Testing at step=22, batch=20, test loss = 1.7246738593605133, test acc = 0.41999998688697815, time = 0.0019369125366210938\n",
      "Testing at step=22, batch=40, test loss = 1.7302020718750213, test acc = 0.49000000953674316, time = 0.0018625259399414062\n",
      "Testing at step=22, batch=60, test loss = 1.5917106581241456, test acc = 0.4399999976158142, time = 0.0018820762634277344\n",
      "Testing at step=22, batch=80, test loss = 1.8082812402598583, test acc = 0.4000000059604645, time = 0.0018885135650634766\n",
      "Step 22 finished in 13.089905023574829, Train loss = 1.6477404554334725, Test loss = 1.714842581722864; Train Acc = 0.4422199978232384, Test Acc = 0.4125999975204468\n",
      "Training at step=23, batch=0, train loss = 1.4396610275233053, train acc = 0.5400000214576721, time = 0.008181333541870117\n",
      "Training at step=23, batch=100, train loss = 1.8655609978369012, train acc = 0.4099999964237213, time = 0.008082151412963867\n",
      "Training at step=23, batch=200, train loss = 1.4404697408530829, train acc = 0.5400000214576721, time = 0.007958650588989258\n",
      "Training at step=23, batch=300, train loss = 1.623472908948099, train acc = 0.47999998927116394, time = 0.008057355880737305\n",
      "Training at step=23, batch=400, train loss = 1.5439246493926284, train acc = 0.47999998927116394, time = 0.008007287979125977\n",
      "Testing at step=23, batch=0, test loss = 1.7536081485061306, test acc = 0.38999998569488525, time = 0.0019085407257080078\n",
      "Testing at step=23, batch=20, test loss = 1.6774468081158063, test acc = 0.4300000071525574, time = 0.0018880367279052734\n",
      "Testing at step=23, batch=40, test loss = 1.5147217564113946, test acc = 0.46000000834465027, time = 0.0017943382263183594\n",
      "Testing at step=23, batch=60, test loss = 1.8517957162196987, test acc = 0.4000000059604645, time = 0.0018529891967773438\n",
      "Testing at step=23, batch=80, test loss = 1.6356278132331261, test acc = 0.44999998807907104, time = 0.0018134117126464844\n",
      "Step 23 finished in 13.075520753860474, Train loss = 1.6461619354783115, Test loss = 1.713312138985125; Train Acc = 0.44353999859094617, Test Acc = 0.41249999791383746\n",
      "Training at step=24, batch=0, train loss = 1.559635443540252, train acc = 0.4399999976158142, time = 0.008246898651123047\n",
      "Training at step=24, batch=100, train loss = 1.7871880823346336, train acc = 0.3199999928474426, time = 0.008094549179077148\n",
      "Training at step=24, batch=200, train loss = 1.5790038671214601, train acc = 0.4300000071525574, time = 0.00803685188293457\n",
      "Training at step=24, batch=300, train loss = 1.7822114553500796, train acc = 0.3799999952316284, time = 0.008020401000976562\n",
      "Training at step=24, batch=400, train loss = 1.7105390295412943, train acc = 0.33000001311302185, time = 0.008234024047851562\n",
      "Testing at step=24, batch=0, test loss = 1.754994876427543, test acc = 0.4099999964237213, time = 0.0019092559814453125\n",
      "Testing at step=24, batch=20, test loss = 1.9683842903653628, test acc = 0.3199999928474426, time = 0.00188446044921875\n",
      "Testing at step=24, batch=40, test loss = 1.6119092798671522, test acc = 0.44999998807907104, time = 0.0017902851104736328\n",
      "Testing at step=24, batch=60, test loss = 1.750226012704783, test acc = 0.4099999964237213, time = 0.0018913745880126953\n",
      "Testing at step=24, batch=80, test loss = 1.6950591293421409, test acc = 0.38999998569488525, time = 0.0017995834350585938\n",
      "Step 24 finished in 13.101709127426147, Train loss = 1.6441718558393459, Test loss = 1.71615951000999; Train Acc = 0.4428199984431267, Test Acc = 0.4133999979496002\n",
      "Training at step=25, batch=0, train loss = 1.6427692586531828, train acc = 0.4300000071525574, time = 0.008060932159423828\n",
      "Training at step=25, batch=100, train loss = 1.6630118275614336, train acc = 0.41999998688697815, time = 0.008068323135375977\n",
      "Training at step=25, batch=200, train loss = 1.4354865200483484, train acc = 0.5400000214576721, time = 0.008099794387817383\n",
      "Training at step=25, batch=300, train loss = 1.5097245456682882, train acc = 0.5099999904632568, time = 0.008018255233764648\n",
      "Training at step=25, batch=400, train loss = 1.5770772854161856, train acc = 0.4699999988079071, time = 0.008138179779052734\n",
      "Testing at step=25, batch=0, test loss = 1.8270582564502558, test acc = 0.38999998569488525, time = 0.001897573471069336\n",
      "Testing at step=25, batch=20, test loss = 1.6587080436386088, test acc = 0.4399999976158142, time = 0.0018382072448730469\n",
      "Testing at step=25, batch=40, test loss = 1.7350869717692747, test acc = 0.41999998688697815, time = 0.0018036365509033203\n",
      "Testing at step=25, batch=60, test loss = 1.730877496507648, test acc = 0.4000000059604645, time = 0.0018587112426757812\n",
      "Testing at step=25, batch=80, test loss = 1.8433678406870198, test acc = 0.3199999928474426, time = 0.0017824172973632812\n",
      "Step 25 finished in 13.044172048568726, Train loss = 1.6427500340500096, Test loss = 1.714694690867442; Train Acc = 0.4445399973988533, Test Acc = 0.4121999990940094\n",
      "Training at step=26, batch=0, train loss = 1.529356300671936, train acc = 0.44999998807907104, time = 0.008092880249023438\n",
      "Training at step=26, batch=100, train loss = 1.499316484503963, train acc = 0.44999998807907104, time = 0.00802469253540039\n",
      "Training at step=26, batch=200, train loss = 1.8202945953124081, train acc = 0.3700000047683716, time = 0.008103132247924805\n",
      "Training at step=26, batch=300, train loss = 1.568824079250343, train acc = 0.49000000953674316, time = 0.008021354675292969\n",
      "Training at step=26, batch=400, train loss = 1.6941149347209907, train acc = 0.5400000214576721, time = 0.008345365524291992\n",
      "Testing at step=26, batch=0, test loss = 1.6952892113325617, test acc = 0.4300000071525574, time = 0.0018486976623535156\n",
      "Testing at step=26, batch=20, test loss = 1.6339511348365536, test acc = 0.44999998807907104, time = 0.0018894672393798828\n",
      "Testing at step=26, batch=40, test loss = 1.7538805593095264, test acc = 0.4399999976158142, time = 0.0018122196197509766\n",
      "Testing at step=26, batch=60, test loss = 1.8996238353241237, test acc = 0.4000000059604645, time = 0.0017826557159423828\n",
      "Testing at step=26, batch=80, test loss = 1.5304464833993092, test acc = 0.3799999952316284, time = 0.0018041133880615234\n",
      "Step 26 finished in 12.891448020935059, Train loss = 1.6412643878071107, Test loss = 1.7183541711990895; Train Acc = 0.4455599973797798, Test Acc = 0.40919999867677687\n",
      "Training at step=27, batch=0, train loss = 1.5835418251515683, train acc = 0.4699999988079071, time = 0.00814962387084961\n",
      "Training at step=27, batch=100, train loss = 1.693461399046431, train acc = 0.4399999976158142, time = 0.007988691329956055\n",
      "Training at step=27, batch=200, train loss = 1.5865581516685756, train acc = 0.4699999988079071, time = 0.008147716522216797\n",
      "Training at step=27, batch=300, train loss = 1.5557027115509527, train acc = 0.46000000834465027, time = 0.008154869079589844\n",
      "Training at step=27, batch=400, train loss = 1.6554208269793085, train acc = 0.38999998569488525, time = 0.007893800735473633\n",
      "Testing at step=27, batch=0, test loss = 1.8816494935490005, test acc = 0.4000000059604645, time = 0.0018775463104248047\n",
      "Testing at step=27, batch=20, test loss = 1.8124650657277661, test acc = 0.4399999976158142, time = 0.001749277114868164\n",
      "Testing at step=27, batch=40, test loss = 1.743615539344874, test acc = 0.38999998569488525, time = 0.0018825531005859375\n",
      "Testing at step=27, batch=60, test loss = 1.800758142167407, test acc = 0.4000000059604645, time = 0.0018413066864013672\n",
      "Testing at step=27, batch=80, test loss = 1.7842452700931692, test acc = 0.3700000047683716, time = 0.0018277168273925781\n",
      "Step 27 finished in 13.002787113189697, Train loss = 1.6400982389650758, Test loss = 1.7197037710116279; Train Acc = 0.4450999972820282, Test Acc = 0.4134999993443489\n",
      "Training at step=28, batch=0, train loss = 1.7222753914398266, train acc = 0.38999998569488525, time = 0.008084297180175781\n",
      "Training at step=28, batch=100, train loss = 1.768529854658795, train acc = 0.4099999964237213, time = 0.007994413375854492\n",
      "Training at step=28, batch=200, train loss = 1.5053250948177157, train acc = 0.46000000834465027, time = 0.008156061172485352\n",
      "Training at step=28, batch=300, train loss = 1.5462957198329002, train acc = 0.4399999976158142, time = 0.007920503616333008\n",
      "Training at step=28, batch=400, train loss = 1.5859397058467863, train acc = 0.4699999988079071, time = 0.00803685188293457\n",
      "Testing at step=28, batch=0, test loss = 1.6736976147567322, test acc = 0.4699999988079071, time = 0.0019099712371826172\n",
      "Testing at step=28, batch=20, test loss = 1.8924276603190515, test acc = 0.33000001311302185, time = 0.0018322467803955078\n",
      "Testing at step=28, batch=40, test loss = 1.6678171547519534, test acc = 0.38999998569488525, time = 0.001840353012084961\n",
      "Testing at step=28, batch=60, test loss = 1.6739302947360102, test acc = 0.4399999976158142, time = 0.0018262863159179688\n",
      "Testing at step=28, batch=80, test loss = 1.975852156023966, test acc = 0.3100000023841858, time = 0.0018126964569091797\n",
      "Step 28 finished in 13.077064514160156, Train loss = 1.6387557932804677, Test loss = 1.7200409965556702; Train Acc = 0.4460999974012375, Test Acc = 0.4056999984383583\n",
      "Training at step=29, batch=0, train loss = 1.5092058683644403, train acc = 0.4399999976158142, time = 0.008057594299316406\n",
      "Training at step=29, batch=100, train loss = 1.5696360195214314, train acc = 0.49000000953674316, time = 0.007987499237060547\n",
      "Training at step=29, batch=200, train loss = 1.6111614500817086, train acc = 0.44999998807907104, time = 0.008095264434814453\n",
      "Training at step=29, batch=300, train loss = 1.7317085435216282, train acc = 0.4300000071525574, time = 0.00814676284790039\n",
      "Training at step=29, batch=400, train loss = 1.7547558581949396, train acc = 0.41999998688697815, time = 0.007971763610839844\n",
      "Testing at step=29, batch=0, test loss = 1.6335307682914597, test acc = 0.44999998807907104, time = 0.0018885135650634766\n",
      "Testing at step=29, batch=20, test loss = 1.7846886334350136, test acc = 0.3199999928474426, time = 0.001847982406616211\n",
      "Testing at step=29, batch=40, test loss = 1.8444871977625141, test acc = 0.38999998569488525, time = 0.0018191337585449219\n",
      "Testing at step=29, batch=60, test loss = 1.6672139289044114, test acc = 0.4300000071525574, time = 0.0018649101257324219\n",
      "Testing at step=29, batch=80, test loss = 1.7500310371843169, test acc = 0.44999998807907104, time = 0.0018429756164550781\n",
      "Step 29 finished in 13.14401125907898, Train loss = 1.637565620871584, Test loss = 1.7185128309139401; Train Acc = 0.4461399976015091, Test Acc = 0.40899999633431433\n",
      "Training at step=30, batch=0, train loss = 1.6200885014881237, train acc = 0.4699999988079071, time = 0.008189201354980469\n",
      "Training at step=30, batch=100, train loss = 1.3829637924108567, train acc = 0.5400000214576721, time = 0.008173227310180664\n",
      "Training at step=30, batch=200, train loss = 1.4839571433947223, train acc = 0.4699999988079071, time = 0.008194684982299805\n",
      "Training at step=30, batch=300, train loss = 1.6571677925109825, train acc = 0.3499999940395355, time = 0.008016347885131836\n",
      "Training at step=30, batch=400, train loss = 1.6030087676795983, train acc = 0.44999998807907104, time = 0.00804448127746582\n",
      "Testing at step=30, batch=0, test loss = 1.6755385491582575, test acc = 0.3799999952316284, time = 0.0018970966339111328\n",
      "Testing at step=30, batch=20, test loss = 1.898989401841214, test acc = 0.3499999940395355, time = 0.0018007755279541016\n",
      "Testing at step=30, batch=40, test loss = 1.8516963939098894, test acc = 0.3700000047683716, time = 0.0018095970153808594\n",
      "Testing at step=30, batch=60, test loss = 1.7415711330225767, test acc = 0.38999998569488525, time = 0.0018591880798339844\n",
      "Testing at step=30, batch=80, test loss = 1.6259215719145328, test acc = 0.38999998569488525, time = 0.0018079280853271484\n",
      "Step 30 finished in 13.05277943611145, Train loss = 1.6355842896937152, Test loss = 1.7245978536027038; Train Acc = 0.4452999976873398, Test Acc = 0.4078999972343445\n",
      "Training at step=31, batch=0, train loss = 1.5796715219403907, train acc = 0.4699999988079071, time = 0.008144617080688477\n",
      "Training at step=31, batch=100, train loss = 1.7548627518714408, train acc = 0.36000001430511475, time = 0.008038520812988281\n",
      "Training at step=31, batch=200, train loss = 1.591800226929574, train acc = 0.49000000953674316, time = 0.008056879043579102\n",
      "Training at step=31, batch=300, train loss = 1.6394016989855387, train acc = 0.46000000834465027, time = 0.007995367050170898\n",
      "Training at step=31, batch=400, train loss = 1.5859782727602478, train acc = 0.5299999713897705, time = 0.007976293563842773\n",
      "Testing at step=31, batch=0, test loss = 1.7821109554856307, test acc = 0.3799999952316284, time = 0.0019485950469970703\n",
      "Testing at step=31, batch=20, test loss = 1.7048787714920675, test acc = 0.4300000071525574, time = 0.001962423324584961\n",
      "Testing at step=31, batch=40, test loss = 1.769759836475449, test acc = 0.3700000047683716, time = 0.0019021034240722656\n",
      "Testing at step=31, batch=60, test loss = 1.5654308629678082, test acc = 0.5099999904632568, time = 0.0019278526306152344\n",
      "Testing at step=31, batch=80, test loss = 1.8956906371826567, test acc = 0.4000000059604645, time = 0.0018036365509033203\n",
      "Step 31 finished in 13.090907573699951, Train loss = 1.6343512656463988, Test loss = 1.724306720750697; Train Acc = 0.4469799984693527, Test Acc = 0.408599998652935\n",
      "Training at step=32, batch=0, train loss = 1.496648917316263, train acc = 0.5199999809265137, time = 0.008102893829345703\n",
      "Training at step=32, batch=100, train loss = 1.7119255970984537, train acc = 0.46000000834465027, time = 0.008024215698242188\n",
      "Training at step=32, batch=200, train loss = 1.7394425190967484, train acc = 0.4699999988079071, time = 0.008051633834838867\n",
      "Training at step=32, batch=300, train loss = 1.4589970807147394, train acc = 0.5299999713897705, time = 0.008189678192138672\n",
      "Training at step=32, batch=400, train loss = 1.597421578931331, train acc = 0.4399999976158142, time = 0.007998466491699219\n",
      "Testing at step=32, batch=0, test loss = 1.669974605246056, test acc = 0.3700000047683716, time = 0.0018625259399414062\n",
      "Testing at step=32, batch=20, test loss = 1.5612534928124966, test acc = 0.46000000834465027, time = 0.0019178390502929688\n",
      "Testing at step=32, batch=40, test loss = 1.771312728698835, test acc = 0.36000001430511475, time = 0.0018398761749267578\n",
      "Testing at step=32, batch=60, test loss = 1.6048550677887283, test acc = 0.4000000059604645, time = 0.0018589496612548828\n",
      "Testing at step=32, batch=80, test loss = 1.7374373815763315, test acc = 0.38999998569488525, time = 0.0018646717071533203\n",
      "Step 32 finished in 12.982322454452515, Train loss = 1.633038793410512, Test loss = 1.7211541497441134; Train Acc = 0.4490999974608421, Test Acc = 0.40419999867677686\n",
      "Training at step=33, batch=0, train loss = 1.5191643280115181, train acc = 0.5099999904632568, time = 0.008078336715698242\n",
      "Training at step=33, batch=100, train loss = 1.5873260389850004, train acc = 0.4699999988079071, time = 0.007964372634887695\n",
      "Training at step=33, batch=200, train loss = 1.7875108495791983, train acc = 0.3700000047683716, time = 0.008306264877319336\n",
      "Training at step=33, batch=300, train loss = 1.5952626495905784, train acc = 0.44999998807907104, time = 0.00809025764465332\n",
      "Training at step=33, batch=400, train loss = 1.6704615895830641, train acc = 0.4699999988079071, time = 0.008041620254516602\n",
      "Testing at step=33, batch=0, test loss = 1.7150831360428065, test acc = 0.3700000047683716, time = 0.0018422603607177734\n",
      "Testing at step=33, batch=20, test loss = 1.7590906454912303, test acc = 0.3799999952316284, time = 0.001851797103881836\n",
      "Testing at step=33, batch=40, test loss = 1.7962492099416172, test acc = 0.36000001430511475, time = 0.0019674301147460938\n",
      "Testing at step=33, batch=60, test loss = 1.5610603616148573, test acc = 0.4399999976158142, time = 0.0017817020416259766\n",
      "Testing at step=33, batch=80, test loss = 1.6267489557837183, test acc = 0.4399999976158142, time = 0.0019567012786865234\n",
      "Step 33 finished in 12.969410419464111, Train loss = 1.6321389346350392, Test loss = 1.7219942860462438; Train Acc = 0.44803999781608583, Test Acc = 0.4087999993562698\n",
      "Training at step=34, batch=0, train loss = 1.4869209825295908, train acc = 0.5799999833106995, time = 0.008266687393188477\n",
      "Training at step=34, batch=100, train loss = 1.664331540524241, train acc = 0.4300000071525574, time = 0.008078575134277344\n",
      "Training at step=34, batch=200, train loss = 1.6371000551480757, train acc = 0.44999998807907104, time = 0.008037328720092773\n",
      "Training at step=34, batch=300, train loss = 1.6824754412404477, train acc = 0.4300000071525574, time = 0.007960796356201172\n",
      "Training at step=34, batch=400, train loss = 1.9227125307316189, train acc = 0.3199999928474426, time = 0.008155345916748047\n",
      "Testing at step=34, batch=0, test loss = 1.715426681755863, test acc = 0.3499999940395355, time = 0.0018665790557861328\n",
      "Testing at step=34, batch=20, test loss = 1.6694485635565368, test acc = 0.3199999928474426, time = 0.0017364025115966797\n",
      "Testing at step=34, batch=40, test loss = 1.8949556678284185, test acc = 0.4000000059604645, time = 0.0017466545104980469\n",
      "Testing at step=34, batch=60, test loss = 1.5753114352166795, test acc = 0.4300000071525574, time = 0.001804351806640625\n",
      "Testing at step=34, batch=80, test loss = 1.753424638833403, test acc = 0.44999998807907104, time = 0.0017704963684082031\n",
      "Step 34 finished in 12.94778037071228, Train loss = 1.6300229082719042, Test loss = 1.7229017053704936; Train Acc = 0.4488399980068207, Test Acc = 0.4062999972701073\n",
      "Training at step=35, batch=0, train loss = 1.8943123841368652, train acc = 0.3700000047683716, time = 0.008079051971435547\n",
      "Training at step=35, batch=100, train loss = 1.9427564165055364, train acc = 0.3700000047683716, time = 0.008020162582397461\n",
      "Training at step=35, batch=200, train loss = 1.7834787529442568, train acc = 0.4099999964237213, time = 0.008410930633544922\n",
      "Training at step=35, batch=300, train loss = 1.6874966626876242, train acc = 0.4699999988079071, time = 0.008326053619384766\n",
      "Training at step=35, batch=400, train loss = 1.56208312354187, train acc = 0.47999998927116394, time = 0.008094072341918945\n",
      "Testing at step=35, batch=0, test loss = 1.6960991313360727, test acc = 0.4000000059604645, time = 0.0018732547760009766\n",
      "Testing at step=35, batch=20, test loss = 1.8668987033799724, test acc = 0.38999998569488525, time = 0.001790761947631836\n",
      "Testing at step=35, batch=40, test loss = 1.9490318755382978, test acc = 0.38999998569488525, time = 0.0018115043640136719\n",
      "Testing at step=35, batch=60, test loss = 1.5666572385632154, test acc = 0.49000000953674316, time = 0.0018274784088134766\n",
      "Testing at step=35, batch=80, test loss = 1.7588930194960557, test acc = 0.4000000059604645, time = 0.001789093017578125\n",
      "Step 35 finished in 12.981662511825562, Train loss = 1.629134191295418, Test loss = 1.7306763576607422; Train Acc = 0.4487599974870682, Test Acc = 0.40119999706745146\n",
      "Training at step=36, batch=0, train loss = 1.461030811704819, train acc = 0.550000011920929, time = 0.008097410202026367\n",
      "Training at step=36, batch=100, train loss = 1.5550398773910812, train acc = 0.49000000953674316, time = 0.008318901062011719\n",
      "Training at step=36, batch=200, train loss = 1.4481523440385686, train acc = 0.5, time = 0.008316278457641602\n",
      "Training at step=36, batch=300, train loss = 1.6685329038466974, train acc = 0.5299999713897705, time = 0.007981061935424805\n",
      "Training at step=36, batch=400, train loss = 1.6101436162141272, train acc = 0.4300000071525574, time = 0.008170366287231445\n",
      "Testing at step=36, batch=0, test loss = 1.7194136134035158, test acc = 0.4000000059604645, time = 0.0019636154174804688\n",
      "Testing at step=36, batch=20, test loss = 1.6396949058739727, test acc = 0.44999998807907104, time = 0.0018351078033447266\n",
      "Testing at step=36, batch=40, test loss = 1.8627659027639474, test acc = 0.3799999952316284, time = 0.0018756389617919922\n",
      "Testing at step=36, batch=60, test loss = 1.6433028500753823, test acc = 0.4099999964237213, time = 0.0018100738525390625\n",
      "Testing at step=36, batch=80, test loss = 1.8914526488368344, test acc = 0.3499999940395355, time = 0.0018448829650878906\n",
      "Step 36 finished in 13.207013368606567, Train loss = 1.6275545140156122, Test loss = 1.7237923322807833; Train Acc = 0.45009999781847, Test Acc = 0.4085999992489815\n",
      "Training at step=37, batch=0, train loss = 1.7593601838577686, train acc = 0.4099999964237213, time = 0.008085012435913086\n",
      "Training at step=37, batch=100, train loss = 1.6252354301149716, train acc = 0.4699999988079071, time = 0.00790095329284668\n",
      "Training at step=37, batch=200, train loss = 1.6775389945180115, train acc = 0.49000000953674316, time = 0.008011102676391602\n",
      "Training at step=37, batch=300, train loss = 1.6778675218673271, train acc = 0.46000000834465027, time = 0.008105993270874023\n",
      "Training at step=37, batch=400, train loss = 1.6601260582304695, train acc = 0.38999998569488525, time = 0.00801229476928711\n",
      "Testing at step=37, batch=0, test loss = 1.6561743811244067, test acc = 0.4399999976158142, time = 0.0018622875213623047\n",
      "Testing at step=37, batch=20, test loss = 1.6651792507597536, test acc = 0.4300000071525574, time = 0.0018658638000488281\n",
      "Testing at step=37, batch=40, test loss = 1.6100146126641288, test acc = 0.44999998807907104, time = 0.0019214153289794922\n",
      "Testing at step=37, batch=60, test loss = 1.8980499364477612, test acc = 0.38999998569488525, time = 0.0018429756164550781\n",
      "Testing at step=37, batch=80, test loss = 1.7859759816365253, test acc = 0.4000000059604645, time = 0.001867532730102539\n",
      "Step 37 finished in 13.07540774345398, Train loss = 1.6270977182770747, Test loss = 1.7249471498313764; Train Acc = 0.44879999846220014, Test Acc = 0.4038999983668327\n",
      "Training at step=38, batch=0, train loss = 1.6284704071433773, train acc = 0.4099999964237213, time = 0.008129358291625977\n",
      "Training at step=38, batch=100, train loss = 1.7577615020466366, train acc = 0.41999998688697815, time = 0.008020162582397461\n",
      "Training at step=38, batch=200, train loss = 1.5181396935449711, train acc = 0.49000000953674316, time = 0.007915973663330078\n",
      "Training at step=38, batch=300, train loss = 1.6454025961876027, train acc = 0.4699999988079071, time = 0.008066177368164062\n",
      "Training at step=38, batch=400, train loss = 1.5911828012611258, train acc = 0.4300000071525574, time = 0.007992982864379883\n",
      "Testing at step=38, batch=0, test loss = 1.8752276560235372, test acc = 0.33000001311302185, time = 0.001967191696166992\n",
      "Testing at step=38, batch=20, test loss = 1.7698276855194695, test acc = 0.4000000059604645, time = 0.0018181800842285156\n",
      "Testing at step=38, batch=40, test loss = 1.6679859382991662, test acc = 0.4399999976158142, time = 0.0019450187683105469\n",
      "Testing at step=38, batch=60, test loss = 1.5577351055481161, test acc = 0.44999998807907104, time = 0.0018727779388427734\n",
      "Testing at step=38, batch=80, test loss = 1.6634746479045797, test acc = 0.4000000059604645, time = 0.001859426498413086\n",
      "Step 38 finished in 12.941211700439453, Train loss = 1.6257864771613642, Test loss = 1.7293451550631622; Train Acc = 0.44995999765396116, Test Acc = 0.4043999978899956\n",
      "Training at step=39, batch=0, train loss = 1.6541344874381982, train acc = 0.44999998807907104, time = 0.008055686950683594\n",
      "Training at step=39, batch=100, train loss = 1.6027076806847975, train acc = 0.41999998688697815, time = 0.008263826370239258\n",
      "Training at step=39, batch=200, train loss = 1.629216011776012, train acc = 0.4099999964237213, time = 0.00797128677368164\n",
      "Training at step=39, batch=300, train loss = 1.4330112571541518, train acc = 0.49000000953674316, time = 0.008033275604248047\n",
      "Training at step=39, batch=400, train loss = 1.562547748693514, train acc = 0.46000000834465027, time = 0.00807809829711914\n",
      "Testing at step=39, batch=0, test loss = 1.7426763063959536, test acc = 0.4000000059604645, time = 0.0019478797912597656\n",
      "Testing at step=39, batch=20, test loss = 1.5739258477775686, test acc = 0.5199999809265137, time = 0.0018126964569091797\n",
      "Testing at step=39, batch=40, test loss = 1.822726748329991, test acc = 0.3499999940395355, time = 0.001995563507080078\n",
      "Testing at step=39, batch=60, test loss = 1.5076717690372428, test acc = 0.5099999904632568, time = 0.0017676353454589844\n",
      "Testing at step=39, batch=80, test loss = 1.8084773396891258, test acc = 0.3100000023841858, time = 0.0019681453704833984\n",
      "Step 39 finished in 13.081603765487671, Train loss = 1.624618341617398, Test loss = 1.72574586487646; Train Acc = 0.4504999976158142, Test Acc = 0.40789999783039094\n",
      "Training at step=40, batch=0, train loss = 1.8579533609245886, train acc = 0.38999998569488525, time = 0.008137941360473633\n",
      "Training at step=40, batch=100, train loss = 1.444038642512132, train acc = 0.5199999809265137, time = 0.008168220520019531\n",
      "Training at step=40, batch=200, train loss = 1.640617011025537, train acc = 0.46000000834465027, time = 0.00805354118347168\n",
      "Training at step=40, batch=300, train loss = 1.6861830939426934, train acc = 0.4000000059604645, time = 0.008063316345214844\n",
      "Training at step=40, batch=400, train loss = 1.6054721043646472, train acc = 0.41999998688697815, time = 0.008054256439208984\n",
      "Testing at step=40, batch=0, test loss = 1.8060530739980902, test acc = 0.4099999964237213, time = 0.0018880367279052734\n",
      "Testing at step=40, batch=20, test loss = 1.7943770640053485, test acc = 0.4300000071525574, time = 0.0018148422241210938\n",
      "Testing at step=40, batch=40, test loss = 1.657695954925644, test acc = 0.4699999988079071, time = 0.0017919540405273438\n",
      "Testing at step=40, batch=60, test loss = 1.6633122833656315, test acc = 0.38999998569488525, time = 0.0018048286437988281\n",
      "Testing at step=40, batch=80, test loss = 1.7381220877609345, test acc = 0.4300000071525574, time = 0.0017170906066894531\n",
      "Step 40 finished in 13.190342426300049, Train loss = 1.6235369124810837, Test loss = 1.7274310382680855; Train Acc = 0.4505599967837334, Test Acc = 0.4052999979257584\n",
      "Training at step=41, batch=0, train loss = 1.5994670666959099, train acc = 0.41999998688697815, time = 0.007995128631591797\n",
      "Training at step=41, batch=100, train loss = 1.6746540703370252, train acc = 0.46000000834465027, time = 0.008054733276367188\n",
      "Training at step=41, batch=200, train loss = 1.559447279936625, train acc = 0.49000000953674316, time = 0.00809335708618164\n",
      "Training at step=41, batch=300, train loss = 1.5684918145820228, train acc = 0.3799999952316284, time = 0.008128881454467773\n",
      "Training at step=41, batch=400, train loss = 1.7047509028233825, train acc = 0.4099999964237213, time = 0.008014440536499023\n",
      "Testing at step=41, batch=0, test loss = 1.871684356009535, test acc = 0.36000001430511475, time = 0.0018987655639648438\n",
      "Testing at step=41, batch=20, test loss = 1.7597340231561918, test acc = 0.47999998927116394, time = 0.0018665790557861328\n",
      "Testing at step=41, batch=40, test loss = 1.7347675165350815, test acc = 0.49000000953674316, time = 0.0018329620361328125\n",
      "Testing at step=41, batch=60, test loss = 1.6312482817866834, test acc = 0.4699999988079071, time = 0.0018014907836914062\n",
      "Testing at step=41, batch=80, test loss = 1.8923755309372894, test acc = 0.36000001430511475, time = 0.0018010139465332031\n",
      "Step 41 finished in 13.050316572189331, Train loss = 1.6217719473717658, Test loss = 1.734050235874062; Train Acc = 0.45135999739170074, Test Acc = 0.4039999973773956\n",
      "Training at step=42, batch=0, train loss = 1.496468292191804, train acc = 0.5600000023841858, time = 0.008214473724365234\n",
      "Training at step=42, batch=100, train loss = 1.61969899870192, train acc = 0.5299999713897705, time = 0.00795435905456543\n",
      "Training at step=42, batch=200, train loss = 1.704875162835621, train acc = 0.3700000047683716, time = 0.008037805557250977\n",
      "Training at step=42, batch=300, train loss = 1.660651399027485, train acc = 0.4699999988079071, time = 0.008154153823852539\n",
      "Training at step=42, batch=400, train loss = 1.68941371782397, train acc = 0.38999998569488525, time = 0.008112192153930664\n",
      "Testing at step=42, batch=0, test loss = 1.8285728345651426, test acc = 0.3700000047683716, time = 0.0018208026885986328\n",
      "Testing at step=42, batch=20, test loss = 1.7269785880449917, test acc = 0.44999998807907104, time = 0.0017969608306884766\n",
      "Testing at step=42, batch=40, test loss = 1.784912682090762, test acc = 0.33000001311302185, time = 0.0018873214721679688\n",
      "Testing at step=42, batch=60, test loss = 1.734567297544713, test acc = 0.4099999964237213, time = 0.0018389225006103516\n",
      "Testing at step=42, batch=80, test loss = 1.653741214232768, test acc = 0.41999998688697815, time = 0.001882314682006836\n",
      "Step 42 finished in 12.928250789642334, Train loss = 1.6210104061107824, Test loss = 1.7278744063372486; Train Acc = 0.45189999693632127, Test Acc = 0.40189999938011167\n",
      "Training at step=43, batch=0, train loss = 1.6388220473286592, train acc = 0.4000000059604645, time = 0.008187055587768555\n",
      "Training at step=43, batch=100, train loss = 1.5552648892300454, train acc = 0.5299999713897705, time = 0.008117198944091797\n",
      "Training at step=43, batch=200, train loss = 1.5050663803851436, train acc = 0.550000011920929, time = 0.008096694946289062\n",
      "Training at step=43, batch=300, train loss = 1.6907103742718688, train acc = 0.41999998688697815, time = 0.008045434951782227\n",
      "Training at step=43, batch=400, train loss = 1.7691685425549906, train acc = 0.4399999976158142, time = 0.007992267608642578\n",
      "Testing at step=43, batch=0, test loss = 1.6271441270286295, test acc = 0.4399999976158142, time = 0.0018420219421386719\n",
      "Testing at step=43, batch=20, test loss = 1.663313620345894, test acc = 0.4300000071525574, time = 0.0018384456634521484\n",
      "Testing at step=43, batch=40, test loss = 1.8007177827183178, test acc = 0.4099999964237213, time = 0.0020418167114257812\n",
      "Testing at step=43, batch=60, test loss = 1.7435027558769567, test acc = 0.3799999952316284, time = 0.001811981201171875\n",
      "Testing at step=43, batch=80, test loss = 1.6767886305374389, test acc = 0.38999998569488525, time = 0.001773834228515625\n",
      "Step 43 finished in 13.007473707199097, Train loss = 1.620253815923606, Test loss = 1.729605340081752; Train Acc = 0.45059999734163286, Test Acc = 0.40179999798536303\n",
      "Training at step=44, batch=0, train loss = 1.7118298305736468, train acc = 0.4699999988079071, time = 0.00804448127746582\n",
      "Training at step=44, batch=100, train loss = 1.4685155742371026, train acc = 0.5299999713897705, time = 0.008031845092773438\n",
      "Training at step=44, batch=200, train loss = 1.8484640508910346, train acc = 0.4099999964237213, time = 0.007975578308105469\n",
      "Training at step=44, batch=300, train loss = 1.6683358277142728, train acc = 0.4399999976158142, time = 0.008005619049072266\n",
      "Training at step=44, batch=400, train loss = 1.680910570951445, train acc = 0.4300000071525574, time = 0.008071422576904297\n",
      "Testing at step=44, batch=0, test loss = 1.7878925262008984, test acc = 0.4099999964237213, time = 0.0019218921661376953\n",
      "Testing at step=44, batch=20, test loss = 1.6577418815687475, test acc = 0.41999998688697815, time = 0.0018470287322998047\n",
      "Testing at step=44, batch=40, test loss = 1.5558894937940315, test acc = 0.46000000834465027, time = 0.0019474029541015625\n",
      "Testing at step=44, batch=60, test loss = 1.7437256350522146, test acc = 0.4300000071525574, time = 0.0018057823181152344\n",
      "Testing at step=44, batch=80, test loss = 1.6665136094303044, test acc = 0.38999998569488525, time = 0.001856088638305664\n",
      "Step 44 finished in 13.054679155349731, Train loss = 1.6184347216852402, Test loss = 1.7328464194956987; Train Acc = 0.45347999715805054, Test Acc = 0.4035999998450279\n",
      "Training at step=45, batch=0, train loss = 1.6015890413239757, train acc = 0.4399999976158142, time = 0.008226156234741211\n",
      "Training at step=45, batch=100, train loss = 1.6258522705491256, train acc = 0.4099999964237213, time = 0.008168458938598633\n",
      "Training at step=45, batch=200, train loss = 1.4880758226308293, train acc = 0.49000000953674316, time = 0.008215904235839844\n",
      "Training at step=45, batch=300, train loss = 1.54122721527131, train acc = 0.4300000071525574, time = 0.008373737335205078\n",
      "Training at step=45, batch=400, train loss = 1.6854753184549494, train acc = 0.4699999988079071, time = 0.008170843124389648\n",
      "Testing at step=45, batch=0, test loss = 2.0897326044408264, test acc = 0.30000001192092896, time = 0.0019328594207763672\n",
      "Testing at step=45, batch=20, test loss = 1.7376777251283244, test acc = 0.4000000059604645, time = 0.0018932819366455078\n",
      "Testing at step=45, batch=40, test loss = 1.6273137421192105, test acc = 0.4300000071525574, time = 0.0018863677978515625\n",
      "Testing at step=45, batch=60, test loss = 1.7467304961264807, test acc = 0.3700000047683716, time = 0.0019178390502929688\n",
      "Testing at step=45, batch=80, test loss = 1.876628148488062, test acc = 0.3199999928474426, time = 0.001961231231689453\n",
      "Step 45 finished in 13.48899221420288, Train loss = 1.6180849551143828, Test loss = 1.7304019759692175; Train Acc = 0.453159996509552, Test Acc = 0.4010999992489815\n",
      "Training at step=46, batch=0, train loss = 1.3869790502332195, train acc = 0.5299999713897705, time = 0.008178234100341797\n",
      "Training at step=46, batch=100, train loss = 1.4798868788165547, train acc = 0.4699999988079071, time = 0.008273601531982422\n",
      "Training at step=46, batch=200, train loss = 1.7288729533533544, train acc = 0.4099999964237213, time = 0.008178949356079102\n",
      "Training at step=46, batch=300, train loss = 1.6386121103166946, train acc = 0.4099999964237213, time = 0.008049726486206055\n",
      "Training at step=46, batch=400, train loss = 1.6525351769980399, train acc = 0.41999998688697815, time = 0.00822591781616211\n",
      "Testing at step=46, batch=0, test loss = 2.000700705745493, test acc = 0.28999999165534973, time = 0.0019669532775878906\n",
      "Testing at step=46, batch=20, test loss = 1.7872508180792395, test acc = 0.4000000059604645, time = 0.001886129379272461\n",
      "Testing at step=46, batch=40, test loss = 1.6973791554612878, test acc = 0.4399999976158142, time = 0.0018658638000488281\n",
      "Testing at step=46, batch=60, test loss = 1.7647902226606516, test acc = 0.4399999976158142, time = 0.0018563270568847656\n",
      "Testing at step=46, batch=80, test loss = 1.8232468578940404, test acc = 0.3199999928474426, time = 0.001962900161743164\n",
      "Step 46 finished in 13.528876543045044, Train loss = 1.616902203439336, Test loss = 1.7343388752408293; Train Acc = 0.4526399977207184, Test Acc = 0.4031999984383583\n",
      "Training at step=47, batch=0, train loss = 1.5498528363590143, train acc = 0.5, time = 0.008099794387817383\n",
      "Training at step=47, batch=100, train loss = 1.6573572834064825, train acc = 0.4699999988079071, time = 0.00809025764465332\n",
      "Training at step=47, batch=200, train loss = 1.5006038124966514, train acc = 0.4699999988079071, time = 0.008142709732055664\n",
      "Training at step=47, batch=300, train loss = 1.7100680940971644, train acc = 0.4099999964237213, time = 0.008058786392211914\n",
      "Training at step=47, batch=400, train loss = 1.6119212254467905, train acc = 0.49000000953674316, time = 0.008377790451049805\n",
      "Testing at step=47, batch=0, test loss = 1.938513392099909, test acc = 0.3799999952316284, time = 0.001928091049194336\n",
      "Testing at step=47, batch=20, test loss = 1.8982241526368937, test acc = 0.3100000023841858, time = 0.0018639564514160156\n",
      "Testing at step=47, batch=40, test loss = 1.7022282028781948, test acc = 0.4699999988079071, time = 0.0018787384033203125\n",
      "Testing at step=47, batch=60, test loss = 1.7068728938790165, test acc = 0.3799999952316284, time = 0.0019047260284423828\n",
      "Testing at step=47, batch=80, test loss = 1.639689036303904, test acc = 0.38999998569488525, time = 0.0018591880798339844\n",
      "Step 47 finished in 13.244013786315918, Train loss = 1.6157585132220902, Test loss = 1.7336629801116705; Train Acc = 0.4528999974131584, Test Acc = 0.4041999977827072\n",
      "Training at step=48, batch=0, train loss = 1.6153059431487997, train acc = 0.4000000059604645, time = 0.008249998092651367\n",
      "Training at step=48, batch=100, train loss = 1.5631317979093353, train acc = 0.47999998927116394, time = 0.008073091506958008\n",
      "Training at step=48, batch=200, train loss = 1.6410026505378548, train acc = 0.4399999976158142, time = 0.008103609085083008\n",
      "Training at step=48, batch=300, train loss = 1.5825423041564886, train acc = 0.4699999988079071, time = 0.00793147087097168\n",
      "Training at step=48, batch=400, train loss = 1.6621285839551354, train acc = 0.4399999976158142, time = 0.008165121078491211\n",
      "Testing at step=48, batch=0, test loss = 1.7140978226773642, test acc = 0.36000001430511475, time = 0.001970529556274414\n",
      "Testing at step=48, batch=20, test loss = 1.6755926993833337, test acc = 0.3400000035762787, time = 0.001943349838256836\n",
      "Testing at step=48, batch=40, test loss = 1.6631040178680165, test acc = 0.4099999964237213, time = 0.002028226852416992\n",
      "Testing at step=48, batch=60, test loss = 1.69712819365154, test acc = 0.3799999952316284, time = 0.0019600391387939453\n",
      "Testing at step=48, batch=80, test loss = 1.6813585983858153, test acc = 0.46000000834465027, time = 0.001962423324584961\n",
      "Step 48 finished in 13.376276016235352, Train loss = 1.614698315567483, Test loss = 1.7324797984385254; Train Acc = 0.45337999737262724, Test Acc = 0.4043999992311001\n",
      "Training at step=49, batch=0, train loss = 1.6031706456193833, train acc = 0.49000000953674316, time = 0.008257627487182617\n",
      "Training at step=49, batch=100, train loss = 1.641144970341601, train acc = 0.4399999976158142, time = 0.008028268814086914\n",
      "Training at step=49, batch=200, train loss = 1.6084870869334094, train acc = 0.4099999964237213, time = 0.008141517639160156\n",
      "Training at step=49, batch=300, train loss = 1.6198248578566845, train acc = 0.4300000071525574, time = 0.008023262023925781\n",
      "Training at step=49, batch=400, train loss = 1.4091259216992125, train acc = 0.4699999988079071, time = 0.008096933364868164\n",
      "Testing at step=49, batch=0, test loss = 1.810358529119522, test acc = 0.3199999928474426, time = 0.0020418167114257812\n",
      "Testing at step=49, batch=20, test loss = 1.7186956195106295, test acc = 0.4099999964237213, time = 0.0018229484558105469\n",
      "Testing at step=49, batch=40, test loss = 1.6851611485993718, test acc = 0.36000001430511475, time = 0.0018579959869384766\n",
      "Testing at step=49, batch=60, test loss = 1.8615812968657295, test acc = 0.3700000047683716, time = 0.0018661022186279297\n",
      "Testing at step=49, batch=80, test loss = 1.9516923976156355, test acc = 0.3700000047683716, time = 0.0018720626831054688\n",
      "Step 49 finished in 13.201608180999756, Train loss = 1.6138761461050857, Test loss = 1.732421575617576; Train Acc = 0.4544199966788292, Test Acc = 0.40459999978542327\n",
      "Training at step=50, batch=0, train loss = 1.517402730177661, train acc = 0.5199999809265137, time = 0.008271455764770508\n",
      "Training at step=50, batch=100, train loss = 1.7221676465043803, train acc = 0.3700000047683716, time = 0.008304357528686523\n",
      "Training at step=50, batch=200, train loss = 1.6426059466984302, train acc = 0.41999998688697815, time = 0.008309602737426758\n",
      "Training at step=50, batch=300, train loss = 1.627974125413985, train acc = 0.46000000834465027, time = 0.008230209350585938\n",
      "Training at step=50, batch=400, train loss = 1.4554960827916128, train acc = 0.5299999713897705, time = 0.008173942565917969\n",
      "Testing at step=50, batch=0, test loss = 1.6438454476996764, test acc = 0.46000000834465027, time = 0.0018694400787353516\n",
      "Testing at step=50, batch=20, test loss = 1.643696641810453, test acc = 0.4000000059604645, time = 0.0018267631530761719\n",
      "Testing at step=50, batch=40, test loss = 1.6947404970149031, test acc = 0.3700000047683716, time = 0.0018889904022216797\n",
      "Testing at step=50, batch=60, test loss = 1.8330258929725307, test acc = 0.3700000047683716, time = 0.0018732547760009766\n",
      "Testing at step=50, batch=80, test loss = 1.5992243307357847, test acc = 0.4300000071525574, time = 0.00180816650390625\n",
      "Step 50 finished in 13.121092319488525, Train loss = 1.6128342596662255, Test loss = 1.7356273453046693; Train Acc = 0.4540399973988533, Test Acc = 0.40129999846220016\n",
      "Training at step=51, batch=0, train loss = 1.5660773780380124, train acc = 0.47999998927116394, time = 0.008147478103637695\n",
      "Training at step=51, batch=100, train loss = 1.562920572714546, train acc = 0.4300000071525574, time = 0.00800776481628418\n",
      "Training at step=51, batch=200, train loss = 1.6069924209579143, train acc = 0.41999998688697815, time = 0.00800466537475586\n",
      "Training at step=51, batch=300, train loss = 1.4775665852588002, train acc = 0.49000000953674316, time = 0.008403778076171875\n",
      "Training at step=51, batch=400, train loss = 1.944783164166444, train acc = 0.3799999952316284, time = 0.007940292358398438\n",
      "Testing at step=51, batch=0, test loss = 1.5019303166056566, test acc = 0.47999998927116394, time = 0.0019822120666503906\n",
      "Testing at step=51, batch=20, test loss = 1.712768699563391, test acc = 0.41999998688697815, time = 0.0020918846130371094\n",
      "Testing at step=51, batch=40, test loss = 1.7958908646060114, test acc = 0.4000000059604645, time = 0.0018830299377441406\n",
      "Testing at step=51, batch=60, test loss = 1.6568614704422122, test acc = 0.3799999952316284, time = 0.0018320083618164062\n",
      "Testing at step=51, batch=80, test loss = 1.7403612035620086, test acc = 0.3799999952316284, time = 0.0018596649169921875\n",
      "Step 51 finished in 13.269898653030396, Train loss = 1.6120367305275813, Test loss = 1.7391504276774779; Train Acc = 0.45451999801397325, Test Acc = 0.40019999802112577\n",
      "Training at step=52, batch=0, train loss = 1.4990726854231442, train acc = 0.5199999809265137, time = 0.008095502853393555\n",
      "Training at step=52, batch=100, train loss = 1.5178417034944807, train acc = 0.4399999976158142, time = 0.008231639862060547\n",
      "Training at step=52, batch=200, train loss = 1.4247569993205327, train acc = 0.5199999809265137, time = 0.008171796798706055\n",
      "Training at step=52, batch=300, train loss = 1.4652052349376274, train acc = 0.5, time = 0.008026838302612305\n",
      "Training at step=52, batch=400, train loss = 1.7423226945533659, train acc = 0.4000000059604645, time = 0.008330345153808594\n",
      "Testing at step=52, batch=0, test loss = 1.6224909450346714, test acc = 0.4399999976158142, time = 0.0019228458404541016\n",
      "Testing at step=52, batch=20, test loss = 1.778270691691092, test acc = 0.36000001430511475, time = 0.0021598339080810547\n",
      "Testing at step=52, batch=40, test loss = 1.8552548777053728, test acc = 0.36000001430511475, time = 0.0018613338470458984\n",
      "Testing at step=52, batch=60, test loss = 1.8501345412394332, test acc = 0.3799999952316284, time = 0.0019371509552001953\n",
      "Testing at step=52, batch=80, test loss = 1.813674187399808, test acc = 0.30000001192092896, time = 0.0018079280853271484\n",
      "Step 52 finished in 13.285943984985352, Train loss = 1.6110169934734262, Test loss = 1.7360770589479222; Train Acc = 0.45353999751806257, Test Acc = 0.4026999993622303\n",
      "Training at step=53, batch=0, train loss = 1.4729448392438138, train acc = 0.4300000071525574, time = 0.008110761642456055\n",
      "Training at step=53, batch=100, train loss = 1.527992231627109, train acc = 0.49000000953674316, time = 0.008114337921142578\n",
      "Training at step=53, batch=200, train loss = 1.6395594396417486, train acc = 0.4300000071525574, time = 0.007991552352905273\n",
      "Training at step=53, batch=300, train loss = 1.5602507166363133, train acc = 0.5, time = 0.007869243621826172\n",
      "Training at step=53, batch=400, train loss = 1.7150296666347984, train acc = 0.38999998569488525, time = 0.008023262023925781\n",
      "Testing at step=53, batch=0, test loss = 1.6887238017979755, test acc = 0.3400000035762787, time = 0.0018944740295410156\n",
      "Testing at step=53, batch=20, test loss = 1.7525135664320137, test acc = 0.33000001311302185, time = 0.002001523971557617\n",
      "Testing at step=53, batch=40, test loss = 1.691882120120221, test acc = 0.3799999952316284, time = 0.0019125938415527344\n",
      "Testing at step=53, batch=60, test loss = 1.7272362178737037, test acc = 0.3400000035762787, time = 0.0018620491027832031\n",
      "Testing at step=53, batch=80, test loss = 1.7026143603104964, test acc = 0.4000000059604645, time = 0.0018820762634277344\n",
      "Step 53 finished in 13.030484676361084, Train loss = 1.6098648798127804, Test loss = 1.7385376825869512; Train Acc = 0.4551599965691566, Test Acc = 0.40499999791383745\n",
      "Training at step=54, batch=0, train loss = 1.623425616392005, train acc = 0.44999998807907104, time = 0.008114814758300781\n",
      "Training at step=54, batch=100, train loss = 1.4664354012181042, train acc = 0.5299999713897705, time = 0.008183717727661133\n",
      "Training at step=54, batch=200, train loss = 1.6573883235228988, train acc = 0.5099999904632568, time = 0.008161783218383789\n",
      "Training at step=54, batch=300, train loss = 1.4866624335780818, train acc = 0.46000000834465027, time = 0.008015871047973633\n",
      "Training at step=54, batch=400, train loss = 1.4999038323139517, train acc = 0.49000000953674316, time = 0.007951498031616211\n",
      "Testing at step=54, batch=0, test loss = 1.8584492711056402, test acc = 0.38999998569488525, time = 0.001882791519165039\n",
      "Testing at step=54, batch=20, test loss = 2.0204481313725187, test acc = 0.28999999165534973, time = 0.0017991065979003906\n",
      "Testing at step=54, batch=40, test loss = 1.7084335887273445, test acc = 0.4000000059604645, time = 0.0017669200897216797\n",
      "Testing at step=54, batch=60, test loss = 1.8215807980618897, test acc = 0.3799999952316284, time = 0.001720428466796875\n",
      "Testing at step=54, batch=80, test loss = 1.6397151060107813, test acc = 0.4300000071525574, time = 0.0018157958984375\n",
      "Step 54 finished in 13.059064626693726, Train loss = 1.6092528787117009, Test loss = 1.7398352455812378; Train Acc = 0.45435999721288683, Test Acc = 0.4017999991774559\n",
      "Training at step=55, batch=0, train loss = 1.7389069603777565, train acc = 0.4000000059604645, time = 0.008076190948486328\n",
      "Training at step=55, batch=100, train loss = 1.6707365223020012, train acc = 0.41999998688697815, time = 0.008075952529907227\n",
      "Training at step=55, batch=200, train loss = 1.5370804549218686, train acc = 0.4699999988079071, time = 0.008173465728759766\n",
      "Training at step=55, batch=300, train loss = 1.5159401450019114, train acc = 0.5099999904632568, time = 0.008452892303466797\n",
      "Training at step=55, batch=400, train loss = 1.7713433190062398, train acc = 0.3700000047683716, time = 0.007948637008666992\n",
      "Testing at step=55, batch=0, test loss = 1.6543187386369618, test acc = 0.38999998569488525, time = 0.0017902851104736328\n",
      "Testing at step=55, batch=20, test loss = 1.7138899457060435, test acc = 0.4399999976158142, time = 0.0017294883728027344\n",
      "Testing at step=55, batch=40, test loss = 1.793480174149051, test acc = 0.4000000059604645, time = 0.0017552375793457031\n",
      "Testing at step=55, batch=60, test loss = 1.74800452256443, test acc = 0.38999998569488525, time = 0.0017848014831542969\n",
      "Testing at step=55, batch=80, test loss = 1.7918649569024678, test acc = 0.3700000047683716, time = 0.0018112659454345703\n",
      "Step 55 finished in 12.898013591766357, Train loss = 1.608200614980682, Test loss = 1.7391235449585412; Train Acc = 0.45589999824762345, Test Acc = 0.40280000001192096\n",
      "Training at step=56, batch=0, train loss = 1.4694858515282576, train acc = 0.5199999809265137, time = 0.007982969284057617\n",
      "Training at step=56, batch=100, train loss = 1.345399072794693, train acc = 0.5299999713897705, time = 0.008041143417358398\n",
      "Training at step=56, batch=200, train loss = 1.5314907842471592, train acc = 0.47999998927116394, time = 0.00808858871459961\n",
      "Training at step=56, batch=300, train loss = 1.7230766276922396, train acc = 0.4300000071525574, time = 0.007817983627319336\n",
      "Training at step=56, batch=400, train loss = 1.6156155859192927, train acc = 0.4399999976158142, time = 0.008043527603149414\n",
      "Testing at step=56, batch=0, test loss = 1.7802572843221183, test acc = 0.4099999964237213, time = 0.0019512176513671875\n",
      "Testing at step=56, batch=20, test loss = 2.06005094553861, test acc = 0.3100000023841858, time = 0.0018346309661865234\n",
      "Testing at step=56, batch=40, test loss = 1.7522750067902673, test acc = 0.3799999952316284, time = 0.0019068717956542969\n",
      "Testing at step=56, batch=60, test loss = 1.6643872325051283, test acc = 0.46000000834465027, time = 0.0018184185028076172\n",
      "Testing at step=56, batch=80, test loss = 1.5292856516447229, test acc = 0.5099999904632568, time = 0.001950979232788086\n",
      "Step 56 finished in 12.976620435714722, Train loss = 1.607187200611496, Test loss = 1.7403165425347797; Train Acc = 0.45619999706745146, Test Acc = 0.39739999920129776\n",
      "Training at step=57, batch=0, train loss = 1.5686132910490251, train acc = 0.44999998807907104, time = 0.008105993270874023\n",
      "Training at step=57, batch=100, train loss = 1.7140942290700198, train acc = 0.38999998569488525, time = 0.008189916610717773\n",
      "Training at step=57, batch=200, train loss = 1.6515451375155763, train acc = 0.4399999976158142, time = 0.007914304733276367\n",
      "Training at step=57, batch=300, train loss = 1.6856617826159903, train acc = 0.49000000953674316, time = 0.007905006408691406\n",
      "Training at step=57, batch=400, train loss = 1.4269865383345288, train acc = 0.47999998927116394, time = 0.008090019226074219\n",
      "Testing at step=57, batch=0, test loss = 1.6489716994283616, test acc = 0.3700000047683716, time = 0.0019578933715820312\n",
      "Testing at step=57, batch=20, test loss = 1.6297116108158176, test acc = 0.3700000047683716, time = 0.0018210411071777344\n",
      "Testing at step=57, batch=40, test loss = 1.5857250471157054, test acc = 0.46000000834465027, time = 0.0018296241760253906\n",
      "Testing at step=57, batch=60, test loss = 1.6078540871751505, test acc = 0.38999998569488525, time = 0.0018723011016845703\n",
      "Testing at step=57, batch=80, test loss = 1.7770489520826, test acc = 0.38999998569488525, time = 0.0018143653869628906\n",
      "Step 57 finished in 13.027663230895996, Train loss = 1.6064812807747166, Test loss = 1.7400156307902666; Train Acc = 0.4558799969553947, Test Acc = 0.4019999983906746\n",
      "Training at step=58, batch=0, train loss = 1.6308869257566858, train acc = 0.5, time = 0.009740114212036133\n",
      "Training at step=58, batch=100, train loss = 1.7330326978768105, train acc = 0.4099999964237213, time = 0.008138179779052734\n",
      "Training at step=58, batch=200, train loss = 1.4509523773627009, train acc = 0.5, time = 0.008047103881835938\n",
      "Training at step=58, batch=300, train loss = 1.5282922446116618, train acc = 0.4699999988079071, time = 0.008449792861938477\n",
      "Training at step=58, batch=400, train loss = 1.8054021813173113, train acc = 0.38999998569488525, time = 0.007900238037109375\n",
      "Testing at step=58, batch=0, test loss = 1.8507071752527864, test acc = 0.3799999952316284, time = 0.001871347427368164\n",
      "Testing at step=58, batch=20, test loss = 1.7588848167536968, test acc = 0.36000001430511475, time = 0.0018661022186279297\n",
      "Testing at step=58, batch=40, test loss = 1.6296626415902438, test acc = 0.49000000953674316, time = 0.0017642974853515625\n",
      "Testing at step=58, batch=60, test loss = 1.689674950842205, test acc = 0.41999998688697815, time = 0.0018384456634521484\n",
      "Testing at step=58, batch=80, test loss = 1.739454683543947, test acc = 0.3700000047683716, time = 0.0018188953399658203\n",
      "Step 58 finished in 13.07396149635315, Train loss = 1.6051027218635765, Test loss = 1.7421577668064863; Train Acc = 0.4567799977660179, Test Acc = 0.39780000030994417\n",
      "Training at step=59, batch=0, train loss = 1.580822002308092, train acc = 0.47999998927116394, time = 0.008019208908081055\n",
      "Training at step=59, batch=100, train loss = 1.5551229405800282, train acc = 0.49000000953674316, time = 0.007999420166015625\n",
      "Training at step=59, batch=200, train loss = 1.3854653104523027, train acc = 0.550000011920929, time = 0.00790858268737793\n",
      "Training at step=59, batch=300, train loss = 1.4897341969826274, train acc = 0.5099999904632568, time = 0.008077383041381836\n",
      "Training at step=59, batch=400, train loss = 1.7718464268058485, train acc = 0.3799999952316284, time = 0.007983207702636719\n",
      "Testing at step=59, batch=0, test loss = 1.8309385388129744, test acc = 0.4000000059604645, time = 0.0018761157989501953\n",
      "Testing at step=59, batch=20, test loss = 1.838213623558673, test acc = 0.4000000059604645, time = 0.001851797103881836\n",
      "Testing at step=59, batch=40, test loss = 1.772842000319005, test acc = 0.41999998688697815, time = 0.0017514228820800781\n",
      "Testing at step=59, batch=60, test loss = 1.7107029020890172, test acc = 0.4000000059604645, time = 0.0019023418426513672\n",
      "Testing at step=59, batch=80, test loss = 1.723138182751847, test acc = 0.3700000047683716, time = 0.0018696784973144531\n",
      "Step 59 finished in 13.024307012557983, Train loss = 1.6049698599327402, Test loss = 1.7390615275239305; Train Acc = 0.45625999677181245, Test Acc = 0.40009999841451643\n",
      "Training at step=60, batch=0, train loss = 1.4984044643742385, train acc = 0.47999998927116394, time = 0.008140087127685547\n",
      "Training at step=60, batch=100, train loss = 1.6844016194007816, train acc = 0.46000000834465027, time = 0.008058547973632812\n",
      "Training at step=60, batch=200, train loss = 1.5609980215315888, train acc = 0.5, time = 0.007967472076416016\n",
      "Training at step=60, batch=300, train loss = 1.6498537394025794, train acc = 0.4099999964237213, time = 0.008185148239135742\n",
      "Training at step=60, batch=400, train loss = 1.7678921658324775, train acc = 0.4300000071525574, time = 0.008118152618408203\n",
      "Testing at step=60, batch=0, test loss = 1.828458314317548, test acc = 0.33000001311302185, time = 0.0018606185913085938\n",
      "Testing at step=60, batch=20, test loss = 1.6760861458990517, test acc = 0.4000000059604645, time = 0.0018498897552490234\n",
      "Testing at step=60, batch=40, test loss = 1.9358605520111112, test acc = 0.4099999964237213, time = 0.0019567012786865234\n",
      "Testing at step=60, batch=60, test loss = 1.5706293048438653, test acc = 0.4300000071525574, time = 0.0018634796142578125\n",
      "Testing at step=60, batch=80, test loss = 1.7237379092464118, test acc = 0.3799999952316284, time = 0.0018222332000732422\n",
      "Step 60 finished in 13.165804386138916, Train loss = 1.6035084228274563, Test loss = 1.7421242966693646; Train Acc = 0.4568399971723556, Test Acc = 0.39939999908208845\n",
      "Training at step=61, batch=0, train loss = 1.4809950112776127, train acc = 0.5, time = 0.009763002395629883\n",
      "Training at step=61, batch=100, train loss = 1.460443938607417, train acc = 0.5199999809265137, time = 0.00811004638671875\n",
      "Training at step=61, batch=200, train loss = 1.7522175633116268, train acc = 0.4300000071525574, time = 0.008024454116821289\n",
      "Training at step=61, batch=300, train loss = 1.6727668438611487, train acc = 0.4699999988079071, time = 0.007961750030517578\n",
      "Training at step=61, batch=400, train loss = 1.7740805812405345, train acc = 0.36000001430511475, time = 0.008219242095947266\n",
      "Testing at step=61, batch=0, test loss = 1.6949363176912104, test acc = 0.3499999940395355, time = 0.00177001953125\n",
      "Testing at step=61, batch=20, test loss = 1.539589047023984, test acc = 0.44999998807907104, time = 0.0017518997192382812\n",
      "Testing at step=61, batch=40, test loss = 1.8634239624309326, test acc = 0.3799999952316284, time = 0.0018894672393798828\n",
      "Testing at step=61, batch=60, test loss = 1.7173694664001982, test acc = 0.38999998569488525, time = 0.0018210411071777344\n",
      "Testing at step=61, batch=80, test loss = 1.829523014174728, test acc = 0.3700000047683716, time = 0.001718282699584961\n",
      "Step 61 finished in 13.09608268737793, Train loss = 1.602581435552535, Test loss = 1.7426575727907956; Train Acc = 0.45723999744653704, Test Acc = 0.40039999932050707\n",
      "Training at step=62, batch=0, train loss = 1.5195628779489465, train acc = 0.46000000834465027, time = 0.008125543594360352\n",
      "Training at step=62, batch=100, train loss = 1.536091009749983, train acc = 0.5400000214576721, time = 0.00799560546875\n",
      "Training at step=62, batch=200, train loss = 1.5095085689866183, train acc = 0.4699999988079071, time = 0.008116722106933594\n",
      "Training at step=62, batch=300, train loss = 1.653178351163972, train acc = 0.49000000953674316, time = 0.00808262825012207\n",
      "Training at step=62, batch=400, train loss = 1.5857263452859363, train acc = 0.49000000953674316, time = 0.007962226867675781\n",
      "Testing at step=62, batch=0, test loss = 1.7498280385602483, test acc = 0.4099999964237213, time = 0.0018911361694335938\n",
      "Testing at step=62, batch=20, test loss = 1.742385563507291, test acc = 0.4000000059604645, time = 0.0018100738525390625\n",
      "Testing at step=62, batch=40, test loss = 1.64434888098057, test acc = 0.4000000059604645, time = 0.0018215179443359375\n",
      "Testing at step=62, batch=60, test loss = 1.8406166598134712, test acc = 0.33000001311302185, time = 0.0018405914306640625\n",
      "Testing at step=62, batch=80, test loss = 1.5070107805381112, test acc = 0.44999998807907104, time = 0.0018324851989746094\n",
      "Step 62 finished in 13.199628114700317, Train loss = 1.6016581641887204, Test loss = 1.7442610663021312; Train Acc = 0.4586399972438812, Test Acc = 0.40019999980926513\n",
      "Training at step=63, batch=0, train loss = 1.537910271248682, train acc = 0.5400000214576721, time = 0.00819253921508789\n",
      "Training at step=63, batch=100, train loss = 1.6673275430327685, train acc = 0.44999998807907104, time = 0.008000612258911133\n",
      "Training at step=63, batch=200, train loss = 1.5331868106226827, train acc = 0.4099999964237213, time = 0.00805044174194336\n",
      "Training at step=63, batch=300, train loss = 1.4418067396996708, train acc = 0.5600000023841858, time = 0.008190155029296875\n",
      "Training at step=63, batch=400, train loss = 1.535407490406572, train acc = 0.5, time = 0.00820612907409668\n",
      "Testing at step=63, batch=0, test loss = 1.7411995501082973, test acc = 0.4300000071525574, time = 0.0018568038940429688\n",
      "Testing at step=63, batch=20, test loss = 1.7214192801781787, test acc = 0.46000000834465027, time = 0.0018322467803955078\n",
      "Testing at step=63, batch=40, test loss = 1.7930881321231518, test acc = 0.3799999952316284, time = 0.0020508766174316406\n",
      "Testing at step=63, batch=60, test loss = 1.7148740310880402, test acc = 0.3700000047683716, time = 0.0018115043640136719\n",
      "Testing at step=63, batch=80, test loss = 1.8980823442958388, test acc = 0.3400000035762787, time = 0.0018415451049804688\n",
      "Step 63 finished in 13.129302501678467, Train loss = 1.600958481346216, Test loss = 1.74396081810548; Train Acc = 0.4590799978375435, Test Acc = 0.3983999994397163\n",
      "Training at step=64, batch=0, train loss = 1.6289277183861781, train acc = 0.44999998807907104, time = 0.008129596710205078\n",
      "Training at step=64, batch=100, train loss = 1.651352876032747, train acc = 0.38999998569488525, time = 0.00797414779663086\n",
      "Training at step=64, batch=200, train loss = 1.6854360037443383, train acc = 0.41999998688697815, time = 0.008241653442382812\n",
      "Training at step=64, batch=300, train loss = 1.5766804960932228, train acc = 0.47999998927116394, time = 0.007990121841430664\n",
      "Training at step=64, batch=400, train loss = 1.3982445654572768, train acc = 0.5600000023841858, time = 0.007888317108154297\n",
      "Testing at step=64, batch=0, test loss = 1.7596010245335194, test acc = 0.3799999952316284, time = 0.0018014907836914062\n",
      "Testing at step=64, batch=20, test loss = 1.6768078788210128, test acc = 0.3700000047683716, time = 0.0018191337585449219\n",
      "Testing at step=64, batch=40, test loss = 1.7373664508164846, test acc = 0.4000000059604645, time = 0.0017664432525634766\n",
      "Testing at step=64, batch=60, test loss = 1.523286427177378, test acc = 0.47999998927116394, time = 0.0018193721771240234\n",
      "Testing at step=64, batch=80, test loss = 1.539979952762038, test acc = 0.46000000834465027, time = 0.0018303394317626953\n",
      "Step 64 finished in 12.989630937576294, Train loss = 1.60028884672268, Test loss = 1.7453561526098968; Train Acc = 0.457539997279644, Test Acc = 0.40169999897480013\n",
      "Training at step=65, batch=0, train loss = 1.5141286364621684, train acc = 0.5600000023841858, time = 0.008105278015136719\n",
      "Training at step=65, batch=100, train loss = 1.5691718860603883, train acc = 0.5099999904632568, time = 0.007968664169311523\n",
      "Training at step=65, batch=200, train loss = 1.6366800503707353, train acc = 0.44999998807907104, time = 0.00832509994506836\n",
      "Training at step=65, batch=300, train loss = 1.6789830336214755, train acc = 0.36000001430511475, time = 0.008167743682861328\n",
      "Training at step=65, batch=400, train loss = 1.4709596477511895, train acc = 0.550000011920929, time = 0.008075237274169922\n",
      "Testing at step=65, batch=0, test loss = 1.7100629028495127, test acc = 0.3799999952316284, time = 0.0018162727355957031\n",
      "Testing at step=65, batch=20, test loss = 1.8604392273280521, test acc = 0.38999998569488525, time = 0.0018463134765625\n",
      "Testing at step=65, batch=40, test loss = 1.8319048994113438, test acc = 0.38999998569488525, time = 0.001874685287475586\n",
      "Testing at step=65, batch=60, test loss = 1.6323390570241454, test acc = 0.4399999976158142, time = 0.0019378662109375\n",
      "Testing at step=65, batch=80, test loss = 1.8333145251450085, test acc = 0.38999998569488525, time = 0.0019152164459228516\n",
      "Step 65 finished in 12.961435317993164, Train loss = 1.5991493252238553, Test loss = 1.747431672132006; Train Acc = 0.45853999781608584, Test Acc = 0.39649999916553497\n",
      "Training at step=66, batch=0, train loss = 1.568044829152473, train acc = 0.44999998807907104, time = 0.008114337921142578\n",
      "Training at step=66, batch=100, train loss = 1.4404618622838365, train acc = 0.4399999976158142, time = 0.008077144622802734\n",
      "Training at step=66, batch=200, train loss = 1.7957990734366465, train acc = 0.4399999976158142, time = 0.008007287979125977\n",
      "Training at step=66, batch=300, train loss = 1.5578589204506474, train acc = 0.47999998927116394, time = 0.008202791213989258\n",
      "Training at step=66, batch=400, train loss = 1.5449032904055295, train acc = 0.4699999988079071, time = 0.008055925369262695\n",
      "Testing at step=66, batch=0, test loss = 1.864431094084195, test acc = 0.41999998688697815, time = 0.001867055892944336\n",
      "Testing at step=66, batch=20, test loss = 1.575647618293667, test acc = 0.46000000834465027, time = 0.0018754005432128906\n",
      "Testing at step=66, batch=40, test loss = 1.7199349389381557, test acc = 0.38999998569488525, time = 0.001840829849243164\n",
      "Testing at step=66, batch=60, test loss = 1.5844335393994478, test acc = 0.4699999988079071, time = 0.0017642974853515625\n",
      "Testing at step=66, batch=80, test loss = 1.9419318673402821, test acc = 0.30000001192092896, time = 0.001856088638305664\n",
      "Step 66 finished in 13.013139486312866, Train loss = 1.5986338098404382, Test loss = 1.7443211322398173; Train Acc = 0.4560599972605705, Test Acc = 0.4011999997496605\n",
      "Training at step=67, batch=0, train loss = 1.7177855409311757, train acc = 0.3799999952316284, time = 0.008046388626098633\n",
      "Training at step=67, batch=100, train loss = 1.6795870374795079, train acc = 0.46000000834465027, time = 0.00822758674621582\n",
      "Training at step=67, batch=200, train loss = 1.6537170572531446, train acc = 0.3700000047683716, time = 0.008170604705810547\n",
      "Training at step=67, batch=300, train loss = 1.6978183045396928, train acc = 0.4300000071525574, time = 0.008062601089477539\n",
      "Training at step=67, batch=400, train loss = 1.5340249641391543, train acc = 0.4000000059604645, time = 0.007999420166015625\n",
      "Testing at step=67, batch=0, test loss = 1.8667836868439007, test acc = 0.3199999928474426, time = 0.0018818378448486328\n",
      "Testing at step=67, batch=20, test loss = 1.8618316648903137, test acc = 0.38999998569488525, time = 0.0018284320831298828\n",
      "Testing at step=67, batch=40, test loss = 1.8174234878223272, test acc = 0.3400000035762787, time = 0.00177764892578125\n",
      "Testing at step=67, batch=60, test loss = 1.7108964300431801, test acc = 0.38999998569488525, time = 0.0018129348754882812\n",
      "Testing at step=67, batch=80, test loss = 1.7231417890740213, test acc = 0.3499999940395355, time = 0.0017995834350585938\n",
      "Step 67 finished in 12.970243453979492, Train loss = 1.5977804642283806, Test loss = 1.7476040983765841; Train Acc = 0.4575799970626831, Test Acc = 0.395999998152256\n",
      "Training at step=68, batch=0, train loss = 1.4993839888511729, train acc = 0.5099999904632568, time = 0.008132457733154297\n",
      "Training at step=68, batch=100, train loss = 1.5749025286564058, train acc = 0.4399999976158142, time = 0.008086681365966797\n",
      "Training at step=68, batch=200, train loss = 1.6679426001310689, train acc = 0.47999998927116394, time = 0.008018016815185547\n",
      "Training at step=68, batch=300, train loss = 1.6444713493977576, train acc = 0.4300000071525574, time = 0.008149385452270508\n",
      "Training at step=68, batch=400, train loss = 1.6881360053137933, train acc = 0.4099999964237213, time = 0.008094072341918945\n",
      "Testing at step=68, batch=0, test loss = 1.757050483906776, test acc = 0.4099999964237213, time = 0.0019021034240722656\n",
      "Testing at step=68, batch=20, test loss = 1.7376709449825711, test acc = 0.4000000059604645, time = 0.0018863677978515625\n",
      "Testing at step=68, batch=40, test loss = 1.642158227576624, test acc = 0.3499999940395355, time = 0.0018949508666992188\n",
      "Testing at step=68, batch=60, test loss = 1.6319779138966288, test acc = 0.41999998688697815, time = 0.0019223690032958984\n",
      "Testing at step=68, batch=80, test loss = 1.686518306741272, test acc = 0.4300000071525574, time = 0.0018355846405029297\n",
      "Step 68 finished in 13.274959802627563, Train loss = 1.5966815469494244, Test loss = 1.7468578976708455; Train Acc = 0.45943999648094175, Test Acc = 0.40169999867677686\n",
      "Training at step=69, batch=0, train loss = 1.4519670534007896, train acc = 0.47999998927116394, time = 0.00824737548828125\n",
      "Training at step=69, batch=100, train loss = 1.59177132646148, train acc = 0.47999998927116394, time = 0.008002281188964844\n",
      "Training at step=69, batch=200, train loss = 1.7042233203566959, train acc = 0.44999998807907104, time = 0.00816202163696289\n",
      "Training at step=69, batch=300, train loss = 1.7771618629636876, train acc = 0.38999998569488525, time = 0.00813436508178711\n",
      "Training at step=69, batch=400, train loss = 1.5067110356187008, train acc = 0.4699999988079071, time = 0.008124351501464844\n",
      "Testing at step=69, batch=0, test loss = 1.6734187458925, test acc = 0.38999998569488525, time = 0.0018897056579589844\n",
      "Testing at step=69, batch=20, test loss = 2.008345841511136, test acc = 0.3799999952316284, time = 0.0018281936645507812\n",
      "Testing at step=69, batch=40, test loss = 1.6789006959193682, test acc = 0.4300000071525574, time = 0.0019216537475585938\n",
      "Testing at step=69, batch=60, test loss = 1.6405046060335167, test acc = 0.46000000834465027, time = 0.001886606216430664\n",
      "Testing at step=69, batch=80, test loss = 1.646491015223276, test acc = 0.44999998807907104, time = 0.0018870830535888672\n",
      "Step 69 finished in 12.968696594238281, Train loss = 1.595995644605892, Test loss = 1.7485924650665805; Train Acc = 0.45839999699592593, Test Acc = 0.39759999841451643\n",
      "Training at step=70, batch=0, train loss = 1.5839110113542776, train acc = 0.4399999976158142, time = 0.008123397827148438\n",
      "Training at step=70, batch=100, train loss = 1.5296622078213877, train acc = 0.5299999713897705, time = 0.00794672966003418\n",
      "Training at step=70, batch=200, train loss = 1.680896943553562, train acc = 0.5, time = 0.00814676284790039\n",
      "Training at step=70, batch=300, train loss = 1.5222126942624719, train acc = 0.47999998927116394, time = 0.008224725723266602\n",
      "Training at step=70, batch=400, train loss = 1.483012771543509, train acc = 0.5400000214576721, time = 0.008131265640258789\n",
      "Testing at step=70, batch=0, test loss = 1.5525232969685363, test acc = 0.44999998807907104, time = 0.0018932819366455078\n",
      "Testing at step=70, batch=20, test loss = 1.8480618300189289, test acc = 0.27000001072883606, time = 0.001909017562866211\n",
      "Testing at step=70, batch=40, test loss = 1.7410404921484481, test acc = 0.3799999952316284, time = 0.0018382072448730469\n",
      "Testing at step=70, batch=60, test loss = 1.6646476240768868, test acc = 0.4000000059604645, time = 0.0018336772918701172\n",
      "Testing at step=70, batch=80, test loss = 1.625951628593455, test acc = 0.4399999976158142, time = 0.0018372535705566406\n",
      "Step 70 finished in 13.03253722190857, Train loss = 1.5953470254738993, Test loss = 1.7499057421896709; Train Acc = 0.46041999661922456, Test Acc = 0.3989999994635582\n",
      "Training at step=71, batch=0, train loss = 1.5175480025626529, train acc = 0.5299999713897705, time = 0.008121967315673828\n",
      "Training at step=71, batch=100, train loss = 1.7078282775094593, train acc = 0.4099999964237213, time = 0.008191347122192383\n",
      "Training at step=71, batch=200, train loss = 1.6840746008272651, train acc = 0.4399999976158142, time = 0.00789785385131836\n",
      "Training at step=71, batch=300, train loss = 1.3199774352467404, train acc = 0.5299999713897705, time = 0.007930517196655273\n",
      "Training at step=71, batch=400, train loss = 1.6478943827962058, train acc = 0.41999998688697815, time = 0.009164571762084961\n",
      "Testing at step=71, batch=0, test loss = 1.912723098010712, test acc = 0.3499999940395355, time = 0.001893758773803711\n",
      "Testing at step=71, batch=20, test loss = 1.78317805522607, test acc = 0.4300000071525574, time = 0.0017881393432617188\n",
      "Testing at step=71, batch=40, test loss = 1.7612544524955487, test acc = 0.3799999952316284, time = 0.0018341541290283203\n",
      "Testing at step=71, batch=60, test loss = 1.7994456478528975, test acc = 0.3799999952316284, time = 0.0018358230590820312\n",
      "Testing at step=71, batch=80, test loss = 2.009071887616924, test acc = 0.3499999940395355, time = 0.0018215179443359375\n",
      "Step 71 finished in 13.080472946166992, Train loss = 1.594719573179308, Test loss = 1.7482185004806734; Train Acc = 0.45943999671936037, Test Acc = 0.40079999834299085\n",
      "Training at step=72, batch=0, train loss = 1.5716532646356343, train acc = 0.47999998927116394, time = 0.008061647415161133\n",
      "Training at step=72, batch=100, train loss = 1.5200593042587809, train acc = 0.5600000023841858, time = 0.00801849365234375\n",
      "Training at step=72, batch=200, train loss = 1.4842705057735934, train acc = 0.550000011920929, time = 0.007938146591186523\n",
      "Training at step=72, batch=300, train loss = 1.7774305717190424, train acc = 0.4000000059604645, time = 0.00792384147644043\n",
      "Training at step=72, batch=400, train loss = 1.8183121309669676, train acc = 0.4000000059604645, time = 0.008168935775756836\n",
      "Testing at step=72, batch=0, test loss = 1.918191803359482, test acc = 0.4099999964237213, time = 0.0018718242645263672\n",
      "Testing at step=72, batch=20, test loss = 1.6606570223013983, test acc = 0.4300000071525574, time = 0.0018281936645507812\n",
      "Testing at step=72, batch=40, test loss = 1.5400341320404487, test acc = 0.4300000071525574, time = 0.0018351078033447266\n",
      "Testing at step=72, batch=60, test loss = 1.7463744206304128, test acc = 0.44999998807907104, time = 0.0017993450164794922\n",
      "Testing at step=72, batch=80, test loss = 1.7707953362690418, test acc = 0.41999998688697815, time = 0.001867532730102539\n",
      "Step 72 finished in 12.941806554794312, Train loss = 1.5940244106038999, Test loss = 1.7521092200249802; Train Acc = 0.45999999809265135, Test Acc = 0.39689999908208845\n",
      "Training at step=73, batch=0, train loss = 1.8497235523450513, train acc = 0.3700000047683716, time = 0.008139371871948242\n",
      "Training at step=73, batch=100, train loss = 1.500503145775611, train acc = 0.4699999988079071, time = 0.008120298385620117\n",
      "Training at step=73, batch=200, train loss = 1.5402877213157238, train acc = 0.4300000071525574, time = 0.00803065299987793\n",
      "Training at step=73, batch=300, train loss = 1.5343726662356234, train acc = 0.47999998927116394, time = 0.008006572723388672\n",
      "Training at step=73, batch=400, train loss = 1.4711442940637645, train acc = 0.4300000071525574, time = 0.008220911026000977\n",
      "Testing at step=73, batch=0, test loss = 1.8232236843283658, test acc = 0.4000000059604645, time = 0.001833200454711914\n",
      "Testing at step=73, batch=20, test loss = 1.7815351384558762, test acc = 0.47999998927116394, time = 0.001821279525756836\n",
      "Testing at step=73, batch=40, test loss = 1.872715720790861, test acc = 0.3499999940395355, time = 0.001832723617553711\n",
      "Testing at step=73, batch=60, test loss = 1.624363217124296, test acc = 0.3799999952316284, time = 0.0018723011016845703\n",
      "Testing at step=73, batch=80, test loss = 1.798323545424183, test acc = 0.3499999940395355, time = 0.0018718242645263672\n",
      "Step 73 finished in 13.01111102104187, Train loss = 1.593432158457254, Test loss = 1.7514060021000588; Train Acc = 0.46021999716758727, Test Acc = 0.3980999982357025\n",
      "Training at step=74, batch=0, train loss = 1.540705533809726, train acc = 0.46000000834465027, time = 0.008185625076293945\n",
      "Training at step=74, batch=100, train loss = 1.8062101937443427, train acc = 0.3799999952316284, time = 0.008049249649047852\n",
      "Training at step=74, batch=200, train loss = 1.5705205545927055, train acc = 0.49000000953674316, time = 0.008042573928833008\n",
      "Training at step=74, batch=300, train loss = 1.5241604756780556, train acc = 0.44999998807907104, time = 0.007982730865478516\n",
      "Training at step=74, batch=400, train loss = 1.6924245041164945, train acc = 0.41999998688697815, time = 0.008156538009643555\n",
      "Testing at step=74, batch=0, test loss = 1.6381467708753328, test acc = 0.41999998688697815, time = 0.001920938491821289\n",
      "Testing at step=74, batch=20, test loss = 1.8107113880555703, test acc = 0.3799999952316284, time = 0.0018239021301269531\n",
      "Testing at step=74, batch=40, test loss = 1.6313080601041632, test acc = 0.44999998807907104, time = 0.0017614364624023438\n",
      "Testing at step=74, batch=60, test loss = 1.5919328427756476, test acc = 0.4300000071525574, time = 0.0017592906951904297\n",
      "Testing at step=74, batch=80, test loss = 1.7535892510234927, test acc = 0.4099999964237213, time = 0.0017604827880859375\n",
      "Step 74 finished in 13.011364459991455, Train loss = 1.5924017497470984, Test loss = 1.752045718669905; Train Acc = 0.45979999780654907, Test Acc = 0.39709999844431876\n",
      "Training at step=75, batch=0, train loss = 1.636127518291278, train acc = 0.4399999976158142, time = 0.008106231689453125\n",
      "Training at step=75, batch=100, train loss = 1.5529385973299707, train acc = 0.46000000834465027, time = 0.008019447326660156\n",
      "Training at step=75, batch=200, train loss = 1.6427445408827424, train acc = 0.44999998807907104, time = 0.007966995239257812\n",
      "Training at step=75, batch=300, train loss = 1.6205508478036479, train acc = 0.49000000953674316, time = 0.008244037628173828\n",
      "Training at step=75, batch=400, train loss = 1.515173240185984, train acc = 0.5400000214576721, time = 0.008032798767089844\n",
      "Testing at step=75, batch=0, test loss = 1.8006656839077444, test acc = 0.4099999964237213, time = 0.0018787384033203125\n",
      "Testing at step=75, batch=20, test loss = 1.76407646932034, test acc = 0.4300000071525574, time = 0.0018169879913330078\n",
      "Testing at step=75, batch=40, test loss = 1.7050151617081652, test acc = 0.3400000035762787, time = 0.0019145011901855469\n",
      "Testing at step=75, batch=60, test loss = 1.7241905137766818, test acc = 0.4000000059604645, time = 0.0018756389617919922\n",
      "Testing at step=75, batch=80, test loss = 1.8609448447918364, test acc = 0.4099999964237213, time = 0.0018620491027832031\n",
      "Step 75 finished in 13.248554229736328, Train loss = 1.5916232090371067, Test loss = 1.75424555712028; Train Acc = 0.4612599973678589, Test Acc = 0.39509999841451643\n",
      "Training at step=76, batch=0, train loss = 1.4332750844021398, train acc = 0.5199999809265137, time = 0.008006095886230469\n",
      "Training at step=76, batch=100, train loss = 1.5444984775491222, train acc = 0.47999998927116394, time = 0.008185863494873047\n",
      "Training at step=76, batch=200, train loss = 1.544686667270621, train acc = 0.44999998807907104, time = 0.008016109466552734\n",
      "Training at step=76, batch=300, train loss = 1.6087278773069051, train acc = 0.5199999809265137, time = 0.00816655158996582\n",
      "Training at step=76, batch=400, train loss = 1.6191860244316454, train acc = 0.4099999964237213, time = 0.008029460906982422\n",
      "Testing at step=76, batch=0, test loss = 1.7774684915695431, test acc = 0.4000000059604645, time = 0.0018355846405029297\n",
      "Testing at step=76, batch=20, test loss = 1.6312614703664412, test acc = 0.4699999988079071, time = 0.0017459392547607422\n",
      "Testing at step=76, batch=40, test loss = 2.028410213486366, test acc = 0.3799999952316284, time = 0.0018544197082519531\n",
      "Testing at step=76, batch=60, test loss = 1.640247487928545, test acc = 0.4699999988079071, time = 0.0017750263214111328\n",
      "Testing at step=76, batch=80, test loss = 1.9512491151302995, test acc = 0.3700000047683716, time = 0.001832723617553711\n",
      "Step 76 finished in 13.028589963912964, Train loss = 1.590944746199452, Test loss = 1.7526822558765238; Train Acc = 0.4600599972605705, Test Acc = 0.40130000084638595\n",
      "Training at step=77, batch=0, train loss = 1.5794296539938808, train acc = 0.46000000834465027, time = 0.008164167404174805\n",
      "Training at step=77, batch=100, train loss = 1.6381466953533645, train acc = 0.4699999988079071, time = 0.008120298385620117\n",
      "Training at step=77, batch=200, train loss = 1.5257435492462588, train acc = 0.41999998688697815, time = 0.008061408996582031\n",
      "Training at step=77, batch=300, train loss = 1.3961062166845688, train acc = 0.49000000953674316, time = 0.007989645004272461\n",
      "Training at step=77, batch=400, train loss = 1.5689656894778157, train acc = 0.550000011920929, time = 0.008033037185668945\n",
      "Testing at step=77, batch=0, test loss = 1.7113269196562273, test acc = 0.38999998569488525, time = 0.0019497871398925781\n",
      "Testing at step=77, batch=20, test loss = 1.8276620729473683, test acc = 0.4399999976158142, time = 0.0018739700317382812\n",
      "Testing at step=77, batch=40, test loss = 1.721591072369826, test acc = 0.4399999976158142, time = 0.0018689632415771484\n",
      "Testing at step=77, batch=60, test loss = 1.6717341134277688, test acc = 0.4300000071525574, time = 0.0018417835235595703\n",
      "Testing at step=77, batch=80, test loss = 1.824845661385873, test acc = 0.38999998569488525, time = 0.0019228458404541016\n",
      "Step 77 finished in 13.014479160308838, Train loss = 1.5905227140582023, Test loss = 1.7543413510039862; Train Acc = 0.4607399980425835, Test Acc = 0.39919999957084656\n",
      "Training at step=78, batch=0, train loss = 1.4855098205150186, train acc = 0.46000000834465027, time = 0.008144617080688477\n",
      "Training at step=78, batch=100, train loss = 1.5055902050912733, train acc = 0.47999998927116394, time = 0.008098840713500977\n",
      "Training at step=78, batch=200, train loss = 1.5278352646086943, train acc = 0.44999998807907104, time = 0.008040666580200195\n",
      "Training at step=78, batch=300, train loss = 1.4596968616793158, train acc = 0.5299999713897705, time = 0.008046627044677734\n",
      "Training at step=78, batch=400, train loss = 1.533316529286508, train acc = 0.5099999904632568, time = 0.007987499237060547\n",
      "Testing at step=78, batch=0, test loss = 1.6805416932273087, test acc = 0.3700000047683716, time = 0.001863241195678711\n",
      "Testing at step=78, batch=20, test loss = 1.7408785322451297, test acc = 0.4399999976158142, time = 0.0017457008361816406\n",
      "Testing at step=78, batch=40, test loss = 1.875139814905075, test acc = 0.3700000047683716, time = 0.0019390583038330078\n",
      "Testing at step=78, batch=60, test loss = 1.972683049622988, test acc = 0.27000001072883606, time = 0.0017993450164794922\n",
      "Testing at step=78, batch=80, test loss = 1.6732787422064528, test acc = 0.38999998569488525, time = 0.0017461776733398438\n",
      "Step 78 finished in 12.857062101364136, Train loss = 1.589122429656712, Test loss = 1.7558033601053402; Train Acc = 0.4620599966645241, Test Acc = 0.39719999849796295\n",
      "Training at step=79, batch=0, train loss = 1.3836485229396083, train acc = 0.5199999809265137, time = 0.008098602294921875\n",
      "Training at step=79, batch=100, train loss = 1.7101226730795447, train acc = 0.4300000071525574, time = 0.007985830307006836\n",
      "Training at step=79, batch=200, train loss = 1.4347557286361816, train acc = 0.49000000953674316, time = 0.008011341094970703\n",
      "Training at step=79, batch=300, train loss = 1.7201466671072745, train acc = 0.49000000953674316, time = 0.007968664169311523\n",
      "Training at step=79, batch=400, train loss = 1.5859924021712348, train acc = 0.41999998688697815, time = 0.008077621459960938\n",
      "Testing at step=79, batch=0, test loss = 1.6285564803349697, test acc = 0.4300000071525574, time = 0.0018687248229980469\n",
      "Testing at step=79, batch=20, test loss = 1.7389940406539148, test acc = 0.36000001430511475, time = 0.0017669200897216797\n",
      "Testing at step=79, batch=40, test loss = 1.4999506966339493, test acc = 0.4399999976158142, time = 0.0019114017486572266\n",
      "Testing at step=79, batch=60, test loss = 1.8527681955147939, test acc = 0.3199999928474426, time = 0.0018663406372070312\n",
      "Testing at step=79, batch=80, test loss = 1.7176160514319787, test acc = 0.38999998569488525, time = 0.001897573471069336\n",
      "Step 79 finished in 13.099263429641724, Train loss = 1.5884095761493469, Test loss = 1.7536083063057644; Train Acc = 0.4615399969816208, Test Acc = 0.39829999953508377\n",
      "Training at step=80, batch=0, train loss = 1.5058392727304966, train acc = 0.5400000214576721, time = 0.008321523666381836\n",
      "Training at step=80, batch=100, train loss = 1.6913804731055433, train acc = 0.41999998688697815, time = 0.008077383041381836\n",
      "Training at step=80, batch=200, train loss = 1.5056629077498116, train acc = 0.5, time = 0.007890462875366211\n",
      "Training at step=80, batch=300, train loss = 1.5461137803500205, train acc = 0.46000000834465027, time = 0.00790858268737793\n",
      "Training at step=80, batch=400, train loss = 1.5081216066169099, train acc = 0.4399999976158142, time = 0.008196592330932617\n",
      "Testing at step=80, batch=0, test loss = 1.7433585868132548, test acc = 0.4300000071525574, time = 0.001886606216430664\n",
      "Testing at step=80, batch=20, test loss = 2.0425961689223557, test acc = 0.33000001311302185, time = 0.0019235610961914062\n",
      "Testing at step=80, batch=40, test loss = 1.8042229750242902, test acc = 0.3499999940395355, time = 0.0018856525421142578\n",
      "Testing at step=80, batch=60, test loss = 1.5854827912921892, test acc = 0.46000000834465027, time = 0.0019450187683105469\n",
      "Testing at step=80, batch=80, test loss = 1.9389290378097106, test acc = 0.3100000023841858, time = 0.0018231868743896484\n",
      "Step 80 finished in 13.100320100784302, Train loss = 1.5884253552736105, Test loss = 1.754575350031725; Train Acc = 0.4609999971985817, Test Acc = 0.3983000001311302\n",
      "Training at step=81, batch=0, train loss = 1.7093758106847639, train acc = 0.3700000047683716, time = 0.007994651794433594\n",
      "Training at step=81, batch=100, train loss = 1.4602261823042983, train acc = 0.47999998927116394, time = 0.008077859878540039\n",
      "Training at step=81, batch=200, train loss = 1.624692854449263, train acc = 0.4399999976158142, time = 0.008020877838134766\n",
      "Training at step=81, batch=300, train loss = 1.4811936960202348, train acc = 0.47999998927116394, time = 0.007985115051269531\n",
      "Training at step=81, batch=400, train loss = 1.603381255744135, train acc = 0.46000000834465027, time = 0.007918596267700195\n",
      "Testing at step=81, batch=0, test loss = 1.6618996540277362, test acc = 0.4399999976158142, time = 0.0018727779388427734\n",
      "Testing at step=81, batch=20, test loss = 1.8243091808728764, test acc = 0.4099999964237213, time = 0.0017943382263183594\n",
      "Testing at step=81, batch=40, test loss = 1.7140523988957639, test acc = 0.44999998807907104, time = 0.0017626285552978516\n",
      "Testing at step=81, batch=60, test loss = 1.8569817352274678, test acc = 0.38999998569488525, time = 0.0019087791442871094\n",
      "Testing at step=81, batch=80, test loss = 1.7402447114293949, test acc = 0.4699999988079071, time = 0.0018219947814941406\n",
      "Step 81 finished in 12.867569923400879, Train loss = 1.5873612160831791, Test loss = 1.7556128726714149; Train Acc = 0.4623599978089333, Test Acc = 0.3972999987006187\n",
      "Training at step=82, batch=0, train loss = 1.5248353330335578, train acc = 0.5, time = 0.008110523223876953\n",
      "Training at step=82, batch=100, train loss = 1.6238229695977744, train acc = 0.47999998927116394, time = 0.007967472076416016\n",
      "Training at step=82, batch=200, train loss = 1.2944312342823812, train acc = 0.5199999809265137, time = 0.008015632629394531\n",
      "Training at step=82, batch=300, train loss = 1.4172085942535284, train acc = 0.46000000834465027, time = 0.008220911026000977\n",
      "Training at step=82, batch=400, train loss = 1.6287746280776847, train acc = 0.44999998807907104, time = 0.008057832717895508\n",
      "Testing at step=82, batch=0, test loss = 1.6234065717001132, test acc = 0.4399999976158142, time = 0.0017991065979003906\n",
      "Testing at step=82, batch=20, test loss = 1.6235711076107926, test acc = 0.4399999976158142, time = 0.0018308162689208984\n",
      "Testing at step=82, batch=40, test loss = 1.829727703237989, test acc = 0.46000000834465027, time = 0.0018503665924072266\n",
      "Testing at step=82, batch=60, test loss = 1.7408622668876097, test acc = 0.4000000059604645, time = 0.0018627643585205078\n",
      "Testing at step=82, batch=80, test loss = 1.7889704433333247, test acc = 0.4099999964237213, time = 0.001768350601196289\n",
      "Step 82 finished in 12.96975564956665, Train loss = 1.5867599568068316, Test loss = 1.7604679649611077; Train Acc = 0.46245999789237974, Test Acc = 0.3952999985218048\n",
      "Training at step=83, batch=0, train loss = 1.560888833530052, train acc = 0.4699999988079071, time = 0.008066654205322266\n",
      "Training at step=83, batch=100, train loss = 1.7776565125375265, train acc = 0.41999998688697815, time = 0.008186817169189453\n",
      "Training at step=83, batch=200, train loss = 1.549839948182732, train acc = 0.44999998807907104, time = 0.008005142211914062\n",
      "Training at step=83, batch=300, train loss = 1.6683773441458838, train acc = 0.4000000059604645, time = 0.007982730865478516\n",
      "Training at step=83, batch=400, train loss = 1.6142926486272628, train acc = 0.4099999964237213, time = 0.008115291595458984\n",
      "Testing at step=83, batch=0, test loss = 1.716186695700473, test acc = 0.41999998688697815, time = 0.0019431114196777344\n",
      "Testing at step=83, batch=20, test loss = 1.8831611557973615, test acc = 0.3400000035762787, time = 0.0018711090087890625\n",
      "Testing at step=83, batch=40, test loss = 1.5935450985376896, test acc = 0.41999998688697815, time = 0.0019342899322509766\n",
      "Testing at step=83, batch=60, test loss = 1.6758340447567366, test acc = 0.44999998807907104, time = 0.0018236637115478516\n",
      "Testing at step=83, batch=80, test loss = 1.826268632127241, test acc = 0.36000001430511475, time = 0.0017762184143066406\n",
      "Step 83 finished in 12.94573163986206, Train loss = 1.586129167850163, Test loss = 1.7578753056690988; Train Acc = 0.4634799964427948, Test Acc = 0.39579999893903733\n",
      "Training at step=84, batch=0, train loss = 1.4402463484892667, train acc = 0.49000000953674316, time = 0.008089780807495117\n",
      "Training at step=84, batch=100, train loss = 1.5916149007716027, train acc = 0.41999998688697815, time = 0.007966041564941406\n",
      "Training at step=84, batch=200, train loss = 1.5291465501256447, train acc = 0.44999998807907104, time = 0.007988929748535156\n",
      "Training at step=84, batch=300, train loss = 1.6502167204405744, train acc = 0.4099999964237213, time = 0.008166074752807617\n",
      "Training at step=84, batch=400, train loss = 1.4518301015810753, train acc = 0.5199999809265137, time = 0.008209466934204102\n",
      "Testing at step=84, batch=0, test loss = 1.854303504159738, test acc = 0.3799999952316284, time = 0.0019378662109375\n",
      "Testing at step=84, batch=20, test loss = 1.8550112864778643, test acc = 0.3700000047683716, time = 0.0018243789672851562\n",
      "Testing at step=84, batch=40, test loss = 1.9806143882675642, test acc = 0.3799999952316284, time = 0.0018534660339355469\n",
      "Testing at step=84, batch=60, test loss = 1.8476080045002157, test acc = 0.30000001192092896, time = 0.0017940998077392578\n",
      "Testing at step=84, batch=80, test loss = 1.5338786863057465, test acc = 0.5099999904632568, time = 0.0018010139465332031\n",
      "Step 84 finished in 13.032003402709961, Train loss = 1.5854689843173542, Test loss = 1.7575133817075674; Train Acc = 0.4625999976396561, Test Acc = 0.39689999908208845\n",
      "Training at step=85, batch=0, train loss = 1.4510662586482113, train acc = 0.5699999928474426, time = 0.008039712905883789\n",
      "Training at step=85, batch=100, train loss = 1.6961969402153454, train acc = 0.4099999964237213, time = 0.00818324089050293\n",
      "Training at step=85, batch=200, train loss = 1.6386294309104472, train acc = 0.38999998569488525, time = 0.008104324340820312\n",
      "Training at step=85, batch=300, train loss = 1.6611792113090416, train acc = 0.4099999964237213, time = 0.007997751235961914\n",
      "Training at step=85, batch=400, train loss = 1.6049349067685292, train acc = 0.4399999976158142, time = 0.008051395416259766\n",
      "Testing at step=85, batch=0, test loss = 1.970591957673846, test acc = 0.3100000023841858, time = 0.001889944076538086\n",
      "Testing at step=85, batch=20, test loss = 1.6816409699122081, test acc = 0.4399999976158142, time = 0.00189208984375\n",
      "Testing at step=85, batch=40, test loss = 1.669525186732259, test acc = 0.44999998807907104, time = 0.0018687248229980469\n",
      "Testing at step=85, batch=60, test loss = 1.6615910328060326, test acc = 0.4099999964237213, time = 0.0019047260284423828\n",
      "Testing at step=85, batch=80, test loss = 1.6869067151182444, test acc = 0.3799999952316284, time = 0.0019023418426513672\n",
      "Step 85 finished in 13.003275871276855, Train loss = 1.585075475401151, Test loss = 1.7584181862212722; Train Acc = 0.4629599960446358, Test Acc = 0.39889999955892563\n",
      "Training at step=86, batch=0, train loss = 1.5022509624951454, train acc = 0.5199999809265137, time = 0.008064985275268555\n",
      "Training at step=86, batch=100, train loss = 1.5810245988092126, train acc = 0.49000000953674316, time = 0.008059978485107422\n",
      "Training at step=86, batch=200, train loss = 1.6996462227629, train acc = 0.4300000071525574, time = 0.008039236068725586\n",
      "Training at step=86, batch=300, train loss = 1.6483194900740463, train acc = 0.4399999976158142, time = 0.008152484893798828\n",
      "Training at step=86, batch=400, train loss = 1.5217103878830267, train acc = 0.47999998927116394, time = 0.008110284805297852\n",
      "Testing at step=86, batch=0, test loss = 1.6420920548149027, test acc = 0.4399999976158142, time = 0.0019202232360839844\n",
      "Testing at step=86, batch=20, test loss = 1.8440905338121327, test acc = 0.36000001430511475, time = 0.0018565654754638672\n",
      "Testing at step=86, batch=40, test loss = 1.9229031555268958, test acc = 0.3499999940395355, time = 0.0017883777618408203\n",
      "Testing at step=86, batch=60, test loss = 1.8208715820524433, test acc = 0.3799999952316284, time = 0.0018482208251953125\n",
      "Testing at step=86, batch=80, test loss = 1.5753651523473728, test acc = 0.41999998688697815, time = 0.0019054412841796875\n",
      "Step 86 finished in 13.049837827682495, Train loss = 1.5842786513154428, Test loss = 1.7588159273144066; Train Acc = 0.4635399969816208, Test Acc = 0.39679999798536303\n",
      "Training at step=87, batch=0, train loss = 1.3899161870641439, train acc = 0.6000000238418579, time = 0.00808572769165039\n",
      "Training at step=87, batch=100, train loss = 1.681174360459908, train acc = 0.4099999964237213, time = 0.008020162582397461\n",
      "Training at step=87, batch=200, train loss = 1.6876678873947397, train acc = 0.46000000834465027, time = 0.00787973403930664\n",
      "Training at step=87, batch=300, train loss = 1.675980747030892, train acc = 0.49000000953674316, time = 0.007880210876464844\n",
      "Training at step=87, batch=400, train loss = 1.462786257959222, train acc = 0.5400000214576721, time = 0.008023977279663086\n",
      "Testing at step=87, batch=0, test loss = 1.8686603469098677, test acc = 0.3700000047683716, time = 0.0019075870513916016\n",
      "Testing at step=87, batch=20, test loss = 1.802577542522769, test acc = 0.38999998569488525, time = 0.0018298625946044922\n",
      "Testing at step=87, batch=40, test loss = 1.7563700468726788, test acc = 0.3700000047683716, time = 0.0017843246459960938\n",
      "Testing at step=87, batch=60, test loss = 1.723609953761088, test acc = 0.36000001430511475, time = 0.0018351078033447266\n",
      "Testing at step=87, batch=80, test loss = 1.737065512411722, test acc = 0.4300000071525574, time = 0.0018661022186279297\n",
      "Step 87 finished in 12.876961946487427, Train loss = 1.5834529461066131, Test loss = 1.7614766656188465; Train Acc = 0.462539997279644, Test Acc = 0.39599999934434893\n",
      "Training at step=88, batch=0, train loss = 1.557176668239533, train acc = 0.4399999976158142, time = 0.008112907409667969\n",
      "Training at step=88, batch=100, train loss = 1.569959206170651, train acc = 0.41999998688697815, time = 0.008159399032592773\n",
      "Training at step=88, batch=200, train loss = 1.8428969164991715, train acc = 0.3400000035762787, time = 0.008088350296020508\n",
      "Training at step=88, batch=300, train loss = 1.5570770451930427, train acc = 0.5199999809265137, time = 0.007976055145263672\n",
      "Training at step=88, batch=400, train loss = 1.7129852290467793, train acc = 0.4399999976158142, time = 0.008114337921142578\n",
      "Testing at step=88, batch=0, test loss = 1.7216129751515166, test acc = 0.38999998569488525, time = 0.0018842220306396484\n",
      "Testing at step=88, batch=20, test loss = 1.8276895335061298, test acc = 0.44999998807907104, time = 0.0018107891082763672\n",
      "Testing at step=88, batch=40, test loss = 1.8690063305961535, test acc = 0.36000001430511475, time = 0.0018801689147949219\n",
      "Testing at step=88, batch=60, test loss = 1.8310911133862293, test acc = 0.3700000047683716, time = 0.0020346641540527344\n",
      "Testing at step=88, batch=80, test loss = 1.8362269431458698, test acc = 0.36000001430511475, time = 0.001821756362915039\n",
      "Step 88 finished in 13.057374238967896, Train loss = 1.5829039193166383, Test loss = 1.7611224887942964; Train Acc = 0.46333999663591385, Test Acc = 0.39589999973773954\n",
      "Training at step=89, batch=0, train loss = 1.4385986919089653, train acc = 0.49000000953674316, time = 0.008087158203125\n",
      "Training at step=89, batch=100, train loss = 1.3891462724286978, train acc = 0.4699999988079071, time = 0.008056879043579102\n",
      "Training at step=89, batch=200, train loss = 1.4774663002865975, train acc = 0.5699999928474426, time = 0.007947444915771484\n",
      "Training at step=89, batch=300, train loss = 1.3935724436544925, train acc = 0.5299999713897705, time = 0.007909059524536133\n",
      "Training at step=89, batch=400, train loss = 1.420406693584382, train acc = 0.5400000214576721, time = 0.008135795593261719\n",
      "Testing at step=89, batch=0, test loss = 1.9369597795565623, test acc = 0.3199999928474426, time = 0.0018148422241210938\n",
      "Testing at step=89, batch=20, test loss = 1.5588586515110345, test acc = 0.5099999904632568, time = 0.0018358230590820312\n",
      "Testing at step=89, batch=40, test loss = 1.8384905166101857, test acc = 0.3100000023841858, time = 0.0018382072448730469\n",
      "Testing at step=89, batch=60, test loss = 1.7657474196538647, test acc = 0.41999998688697815, time = 0.001804351806640625\n",
      "Testing at step=89, batch=80, test loss = 1.6327113468254684, test acc = 0.46000000834465027, time = 0.0017795562744140625\n",
      "Step 89 finished in 13.040119647979736, Train loss = 1.5821828160657614, Test loss = 1.7609750150317336; Train Acc = 0.4639599969983101, Test Acc = 0.3959999978542328\n",
      "Training at step=90, batch=0, train loss = 1.684293174071004, train acc = 0.4300000071525574, time = 0.008168458938598633\n",
      "Training at step=90, batch=100, train loss = 1.5255645564505795, train acc = 0.41999998688697815, time = 0.008008718490600586\n",
      "Training at step=90, batch=200, train loss = 1.663287563580919, train acc = 0.4300000071525574, time = 0.008109331130981445\n",
      "Training at step=90, batch=300, train loss = 1.4753368790557893, train acc = 0.5299999713897705, time = 0.007934093475341797\n",
      "Training at step=90, batch=400, train loss = 1.5857028990850552, train acc = 0.4399999976158142, time = 0.00801396369934082\n",
      "Testing at step=90, batch=0, test loss = 1.8943995400893572, test acc = 0.3400000035762787, time = 0.002005338668823242\n",
      "Testing at step=90, batch=20, test loss = 1.9337951952160244, test acc = 0.3799999952316284, time = 0.0018494129180908203\n",
      "Testing at step=90, batch=40, test loss = 1.708598499842711, test acc = 0.4000000059604645, time = 0.0018396377563476562\n",
      "Testing at step=90, batch=60, test loss = 1.6667891829357284, test acc = 0.4099999964237213, time = 0.0018048286437988281\n",
      "Testing at step=90, batch=80, test loss = 1.6399694793142716, test acc = 0.41999998688697815, time = 0.0018963813781738281\n",
      "Step 90 finished in 13.0447518825531, Train loss = 1.581621118622861, Test loss = 1.7602930989291854; Train Acc = 0.46363999778032305, Test Acc = 0.39799999833106997\n",
      "Training at step=91, batch=0, train loss = 1.5356739979291865, train acc = 0.49000000953674316, time = 0.00824594497680664\n",
      "Training at step=91, batch=100, train loss = 1.655520688558137, train acc = 0.4399999976158142, time = 0.007989645004272461\n",
      "Training at step=91, batch=200, train loss = 1.4360721411721118, train acc = 0.5299999713897705, time = 0.008176803588867188\n",
      "Training at step=91, batch=300, train loss = 1.4349811565096067, train acc = 0.5, time = 0.0080413818359375\n",
      "Training at step=91, batch=400, train loss = 1.5909718806902862, train acc = 0.5, time = 0.007987022399902344\n",
      "Testing at step=91, batch=0, test loss = 1.8368777308017856, test acc = 0.3400000035762787, time = 0.0021314620971679688\n",
      "Testing at step=91, batch=20, test loss = 1.7781229219234929, test acc = 0.4099999964237213, time = 0.001909017562866211\n",
      "Testing at step=91, batch=40, test loss = 1.895903926388488, test acc = 0.27000001072883606, time = 0.0018901824951171875\n",
      "Testing at step=91, batch=60, test loss = 1.5827212520738378, test acc = 0.4399999976158142, time = 0.0019447803497314453\n",
      "Testing at step=91, batch=80, test loss = 1.817748976875618, test acc = 0.41999998688697815, time = 0.0019085407257080078\n",
      "Step 91 finished in 13.074109077453613, Train loss = 1.5810005999098198, Test loss = 1.7643517131173256; Train Acc = 0.4641399964094162, Test Acc = 0.3939999979734421\n",
      "Training at step=92, batch=0, train loss = 1.3800749628910114, train acc = 0.550000011920929, time = 0.008106231689453125\n",
      "Training at step=92, batch=100, train loss = 1.6709491991310566, train acc = 0.4300000071525574, time = 0.008016586303710938\n",
      "Training at step=92, batch=200, train loss = 1.6256020621586649, train acc = 0.47999998927116394, time = 0.0081329345703125\n",
      "Training at step=92, batch=300, train loss = 1.5699957761668926, train acc = 0.5400000214576721, time = 0.008079051971435547\n",
      "Training at step=92, batch=400, train loss = 1.8008550357310944, train acc = 0.3700000047683716, time = 0.008073806762695312\n",
      "Testing at step=92, batch=0, test loss = 1.7379507233505935, test acc = 0.4000000059604645, time = 0.0018639564514160156\n",
      "Testing at step=92, batch=20, test loss = 1.7501553659497302, test acc = 0.4099999964237213, time = 0.0017817020416259766\n",
      "Testing at step=92, batch=40, test loss = 1.6393515388454631, test acc = 0.4699999988079071, time = 0.0018086433410644531\n",
      "Testing at step=92, batch=60, test loss = 1.8592952694306277, test acc = 0.3100000023841858, time = 0.0018682479858398438\n",
      "Testing at step=92, batch=80, test loss = 1.8236143796927735, test acc = 0.36000001430511475, time = 0.0018677711486816406\n",
      "Step 92 finished in 13.149826526641846, Train loss = 1.5802008739745057, Test loss = 1.7629994123751473; Train Acc = 0.46499999696016314, Test Acc = 0.3956999996304512\n",
      "Training at step=93, batch=0, train loss = 1.6337109422718148, train acc = 0.3799999952316284, time = 0.008100032806396484\n",
      "Training at step=93, batch=100, train loss = 1.4815987608250354, train acc = 0.46000000834465027, time = 0.008071184158325195\n",
      "Training at step=93, batch=200, train loss = 1.5806265680864147, train acc = 0.38999998569488525, time = 0.007981300354003906\n",
      "Training at step=93, batch=300, train loss = 1.5689754447601394, train acc = 0.44999998807907104, time = 0.008170604705810547\n",
      "Training at step=93, batch=400, train loss = 1.5889062208458393, train acc = 0.49000000953674316, time = 0.008298158645629883\n",
      "Testing at step=93, batch=0, test loss = 1.8477004288093428, test acc = 0.3199999928474426, time = 0.0018603801727294922\n",
      "Testing at step=93, batch=20, test loss = 1.802088334904253, test acc = 0.3199999928474426, time = 0.0018672943115234375\n",
      "Testing at step=93, batch=40, test loss = 1.7392227586229168, test acc = 0.41999998688697815, time = 0.0019266605377197266\n",
      "Testing at step=93, batch=60, test loss = 1.6275516194036155, test acc = 0.4300000071525574, time = 0.0019638538360595703\n",
      "Testing at step=93, batch=80, test loss = 1.9845160607072745, test acc = 0.3799999952316284, time = 0.001870870590209961\n",
      "Step 93 finished in 13.17240571975708, Train loss = 1.5796007352200805, Test loss = 1.7615674208106697; Train Acc = 0.4650199975371361, Test Acc = 0.39609999835491183\n",
      "Training at step=94, batch=0, train loss = 1.6310601005120933, train acc = 0.4300000071525574, time = 0.008178234100341797\n",
      "Training at step=94, batch=100, train loss = 1.6208354647408505, train acc = 0.5, time = 0.008168220520019531\n",
      "Training at step=94, batch=200, train loss = 1.722766851737047, train acc = 0.4099999964237213, time = 0.008076667785644531\n",
      "Training at step=94, batch=300, train loss = 1.5286638975500575, train acc = 0.49000000953674316, time = 0.008085489273071289\n",
      "Training at step=94, batch=400, train loss = 1.5560252243103156, train acc = 0.4099999964237213, time = 0.008084535598754883\n",
      "Testing at step=94, batch=0, test loss = 1.591595522931505, test acc = 0.47999998927116394, time = 0.0019500255584716797\n",
      "Testing at step=94, batch=20, test loss = 1.852022899656124, test acc = 0.3199999928474426, time = 0.0019605159759521484\n",
      "Testing at step=94, batch=40, test loss = 1.663223012180272, test acc = 0.4300000071525574, time = 0.0018649101257324219\n",
      "Testing at step=94, batch=60, test loss = 1.6004645550089265, test acc = 0.49000000953674316, time = 0.0018532276153564453\n",
      "Testing at step=94, batch=80, test loss = 1.6448197155367907, test acc = 0.4300000071525574, time = 0.0019023418426513672\n",
      "Step 94 finished in 13.100077629089355, Train loss = 1.5789032199346316, Test loss = 1.763437453408226; Train Acc = 0.464619996547699, Test Acc = 0.39609999835491183\n",
      "Training at step=95, batch=0, train loss = 1.6486702127080113, train acc = 0.41999998688697815, time = 0.008145570755004883\n",
      "Training at step=95, batch=100, train loss = 1.6043588873583212, train acc = 0.41999998688697815, time = 0.008166313171386719\n",
      "Training at step=95, batch=200, train loss = 1.4823614518457813, train acc = 0.5099999904632568, time = 0.008269309997558594\n",
      "Training at step=95, batch=300, train loss = 1.6256418390871317, train acc = 0.49000000953674316, time = 0.008049488067626953\n",
      "Training at step=95, batch=400, train loss = 1.6597133989313584, train acc = 0.44999998807907104, time = 0.008009672164916992\n",
      "Testing at step=95, batch=0, test loss = 1.6408780371097194, test acc = 0.46000000834465027, time = 0.0019083023071289062\n",
      "Testing at step=95, batch=20, test loss = 1.775082260261911, test acc = 0.4300000071525574, time = 0.0018916130065917969\n",
      "Testing at step=95, batch=40, test loss = 1.7987849039327863, test acc = 0.38999998569488525, time = 0.001848459243774414\n",
      "Testing at step=95, batch=60, test loss = 1.7809757021514836, test acc = 0.3700000047683716, time = 0.0017659664154052734\n",
      "Testing at step=95, batch=80, test loss = 1.8081403460691627, test acc = 0.3400000035762787, time = 0.0019390583038330078\n",
      "Step 95 finished in 13.013656854629517, Train loss = 1.5784424880844274, Test loss = 1.7658958395787565; Train Acc = 0.46547999715805055, Test Acc = 0.39390000015497206\n",
      "Training at step=96, batch=0, train loss = 1.7662768659678107, train acc = 0.38999998569488525, time = 0.008110761642456055\n",
      "Training at step=96, batch=100, train loss = 1.4838361205905386, train acc = 0.5199999809265137, time = 0.008068561553955078\n",
      "Training at step=96, batch=200, train loss = 1.651774327637173, train acc = 0.4000000059604645, time = 0.008042573928833008\n",
      "Training at step=96, batch=300, train loss = 1.6351535062358993, train acc = 0.44999998807907104, time = 0.00803065299987793\n",
      "Training at step=96, batch=400, train loss = 1.4767803457199622, train acc = 0.5299999713897705, time = 0.008028984069824219\n",
      "Testing at step=96, batch=0, test loss = 1.8199975697524495, test acc = 0.4000000059604645, time = 0.0019123554229736328\n",
      "Testing at step=96, batch=20, test loss = 1.765144485606473, test acc = 0.3400000035762787, time = 0.00188446044921875\n",
      "Testing at step=96, batch=40, test loss = 1.9186954018029205, test acc = 0.28999999165534973, time = 0.0017445087432861328\n",
      "Testing at step=96, batch=60, test loss = 1.7347555388785036, test acc = 0.3499999940395355, time = 0.0017385482788085938\n",
      "Testing at step=96, batch=80, test loss = 1.705771984741898, test acc = 0.5, time = 0.001886129379272461\n",
      "Step 96 finished in 12.994135618209839, Train loss = 1.5781689334841746, Test loss = 1.7650582902105107; Train Acc = 0.46501999658346177, Test Acc = 0.3956999990344048\n",
      "Training at step=97, batch=0, train loss = 1.5879810906078031, train acc = 0.46000000834465027, time = 0.00803518295288086\n",
      "Training at step=97, batch=100, train loss = 1.7461682310685704, train acc = 0.4099999964237213, time = 0.008122920989990234\n",
      "Training at step=97, batch=200, train loss = 1.4778584312699423, train acc = 0.5299999713897705, time = 0.008050680160522461\n",
      "Training at step=97, batch=300, train loss = 1.5985476103978333, train acc = 0.5099999904632568, time = 0.008082866668701172\n",
      "Training at step=97, batch=400, train loss = 1.576676520710719, train acc = 0.5099999904632568, time = 0.007987499237060547\n",
      "Testing at step=97, batch=0, test loss = 1.847906602140772, test acc = 0.33000001311302185, time = 0.0018846988677978516\n",
      "Testing at step=97, batch=20, test loss = 1.8572683989233978, test acc = 0.36000001430511475, time = 0.0018780231475830078\n",
      "Testing at step=97, batch=40, test loss = 1.6834528311170192, test acc = 0.38999998569488525, time = 0.0018050670623779297\n",
      "Testing at step=97, batch=60, test loss = 1.7172506340386886, test acc = 0.4300000071525574, time = 0.0018358230590820312\n",
      "Testing at step=97, batch=80, test loss = 1.7102144960666144, test acc = 0.3799999952316284, time = 0.001806020736694336\n",
      "Step 97 finished in 13.01587724685669, Train loss = 1.5773778606157065, Test loss = 1.7651957888054943; Train Acc = 0.46373999714851377, Test Acc = 0.3935999998450279\n",
      "Training at step=98, batch=0, train loss = 1.5811848821377603, train acc = 0.550000011920929, time = 0.00803232192993164\n",
      "Training at step=98, batch=100, train loss = 1.6601810747299743, train acc = 0.44999998807907104, time = 0.008071422576904297\n",
      "Training at step=98, batch=200, train loss = 1.7389127900070196, train acc = 0.36000001430511475, time = 0.007911920547485352\n",
      "Training at step=98, batch=300, train loss = 1.675191774812512, train acc = 0.44999998807907104, time = 0.008065462112426758\n",
      "Training at step=98, batch=400, train loss = 1.7032328239196146, train acc = 0.4300000071525574, time = 0.008172750473022461\n",
      "Testing at step=98, batch=0, test loss = 1.6522917887244921, test acc = 0.4099999964237213, time = 0.001861572265625\n",
      "Testing at step=98, batch=20, test loss = 1.9155583808595997, test acc = 0.3499999940395355, time = 0.0017704963684082031\n",
      "Testing at step=98, batch=40, test loss = 1.8240129924414927, test acc = 0.4000000059604645, time = 0.0017938613891601562\n",
      "Testing at step=98, batch=60, test loss = 1.6614660662459753, test acc = 0.3700000047683716, time = 0.0017879009246826172\n",
      "Testing at step=98, batch=80, test loss = 1.9180890797535535, test acc = 0.3400000035762787, time = 0.0018727779388427734\n",
      "Step 98 finished in 13.079045057296753, Train loss = 1.5766822877936606, Test loss = 1.766184914319649; Train Acc = 0.4647399969100952, Test Acc = 0.3969999998807907\n",
      "Training at step=99, batch=0, train loss = 1.6666436333905181, train acc = 0.47999998927116394, time = 0.008097648620605469\n",
      "Training at step=99, batch=100, train loss = 1.696185928034765, train acc = 0.38999998569488525, time = 0.008028745651245117\n",
      "Training at step=99, batch=200, train loss = 1.4939661143692962, train acc = 0.46000000834465027, time = 0.008087873458862305\n",
      "Training at step=99, batch=300, train loss = 1.6255966737608412, train acc = 0.4000000059604645, time = 0.00820159912109375\n",
      "Training at step=99, batch=400, train loss = 1.5862095281483604, train acc = 0.5199999809265137, time = 0.007921457290649414\n",
      "Testing at step=99, batch=0, test loss = 1.6406012614045955, test acc = 0.4000000059604645, time = 0.0019469261169433594\n",
      "Testing at step=99, batch=20, test loss = 1.6048121432202058, test acc = 0.4399999976158142, time = 0.0019140243530273438\n",
      "Testing at step=99, batch=40, test loss = 1.956316759011159, test acc = 0.30000001192092896, time = 0.0017957687377929688\n",
      "Testing at step=99, batch=60, test loss = 1.8045334210566297, test acc = 0.3400000035762787, time = 0.0018541812896728516\n",
      "Testing at step=99, batch=80, test loss = 1.8836033616396737, test acc = 0.33000001311302185, time = 0.0019490718841552734\n",
      "Step 99 finished in 13.14969253540039, Train loss = 1.576707877995486, Test loss = 1.7665078780953027; Train Acc = 0.46437999737262725, Test Acc = 0.3953999996185303\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T14:44:30.062458Z",
     "start_time": "2024-04-08T14:44:29.485197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cifar10_simplenet2.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ],
   "id": "c478e8fbddc2cf97",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T14:44:30.070196Z",
     "start_time": "2024-04-08T14:44:30.063821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"cifar10_simplenet2.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ],
   "id": "ffa732623e4e7678",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T14:44:30.073662Z",
     "start_time": "2024-04-08T14:44:30.071453Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f8fd8658c38bede3",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
