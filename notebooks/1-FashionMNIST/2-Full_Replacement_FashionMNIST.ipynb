{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:22:52.948872Z",
     "iopub.status.busy": "2024-04-03T08:22:52.948409Z",
     "iopub.status.idle": "2024-04-03T08:24:31.832368Z",
     "shell.execute_reply": "2024-04-03T08:24:31.831751Z",
     "shell.execute_reply.started": "2024-04-03T08:22:52.948838Z"
    },
    "id": "5ebnFsErY0EM",
    "outputId": "e8932b66-715c-467f-912a-8b449c518214",
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2024-04-07T22:57:39.540265Z",
     "start_time": "2024-04-07T22:55:55.007175Z"
    }
   },
   "source": [
    "# Install packages\n",
    "!pip install torch torchvision torchaudio --upgrade\n",
    "!pip install pennylane cotengra quimb torchmetrics --upgrade\n",
    "#!pip install -U \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.1+cu116)\r\n",
      "Collecting torch\r\n",
      "  Downloading torch-2.2.2-cp39-cp39-manylinux1_x86_64.whl (755.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m755.5/755.5 MB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\r\n",
      "Collecting torchvision\r\n",
      "  Downloading torchvision-0.17.2-cp39-cp39-manylinux1_x86_64.whl (6.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.9/6.9 MB\u001B[0m \u001B[31m48.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.12.1+cu116)\r\n",
      "Collecting torchaudio\r\n",
      "  Downloading torchaudio-2.2.2-cp39-cp39-manylinux1_x86_64.whl (3.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m101.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting typing-extensions>=4.8.0\r\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\r\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.6/410.6 MB\u001B[0m \u001B[31m5.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-nvtx-cu12==12.1.105\r\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/99.1 kB\u001B[0m \u001B[31m36.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m63.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m823.6/823.6 kB\u001B[0m \u001B[31m91.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m93.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\r\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m731.7/731.7 MB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting triton==2.2.0\r\n",
      "  Downloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m167.9/167.9 MB\u001B[0m \u001B[31m12.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-curand-cu12==10.3.2.106\r\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m34.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\r\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.0/196.0 MB\u001B[0m \u001B[31m12.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cufft-cu12==11.0.2.54\r\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.6/121.6 MB\u001B[0m \u001B[31m18.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\r\n",
      "Collecting nvidia-nccl-cu12==2.19.3\r\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.0/166.0 MB\u001B[0m \u001B[31m14.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting sympy\r\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.7/5.7 MB\u001B[0m \u001B[31m104.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\r\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 MB\u001B[0m \u001B[31m16.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\r\n",
      "Collecting nvidia-nvjitlink-cu12\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m85.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\r\n",
      "Collecting mpmath>=0.19\r\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.2/536.2 kB\u001B[0m \u001B[31m69.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: mpmath, typing-extensions, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.4.0\r\n",
      "    Uninstalling typing_extensions-4.4.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.4.0\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.12.1+cu116\r\n",
      "    Uninstalling torch-1.12.1+cu116:\r\n",
      "      Successfully uninstalled torch-1.12.1+cu116\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.13.1+cu116\r\n",
      "    Uninstalling torchvision-0.13.1+cu116:\r\n",
      "      Successfully uninstalled torchvision-0.13.1+cu116\r\n",
      "  Attempting uninstall: torchaudio\r\n",
      "    Found existing installation: torchaudio 0.12.1+cu116\r\n",
      "    Uninstalling torchaudio-0.12.1+cu116:\r\n",
      "      Successfully uninstalled torchaudio-0.12.1+cu116\r\n",
      "Successfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sympy-1.12 torch-2.2.2 torchaudio-2.2.2 torchvision-0.17.2 triton-2.2.0 typing-extensions-4.11.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting pennylane\r\n",
      "  Downloading PennyLane-0.35.1-py3-none-any.whl (1.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m45.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting cotengra\r\n",
      "  Downloading cotengra-0.5.6-py3-none-any.whl (148 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m148.0/148.0 kB\u001B[0m \u001B[31m13.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting quimb\r\n",
      "  Downloading quimb-1.7.3-py3-none-any.whl (500 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m500.7/500.7 kB\u001B[0m \u001B[31m27.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting torchmetrics\r\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m841.5/841.5 kB\u001B[0m \u001B[31m42.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting rustworkx\r\n",
      "  Downloading rustworkx-0.14.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m50.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting pennylane-lightning>=0.35\r\n",
      "  Downloading PennyLane_Lightning-0.35.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.5/18.5 MB\u001B[0m \u001B[31m43.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (3.0)\r\n",
      "Collecting toml\r\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\r\n",
      "Collecting appdirs\r\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Collecting autograd\r\n",
      "  Downloading autograd-1.6.2-py3-none-any.whl (49 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.3/49.3 kB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pennylane) (4.11.0)\r\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\r\n",
      "Collecting semantic-version>=2.7\r\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.23.4)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\r\n",
      "Collecting autoray>=0.6.1\r\n",
      "  Downloading autoray-0.6.9-py3-none-any.whl (49 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.8/49.8 kB\u001B[0m \u001B[31m6.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting numba>=0.39\r\n",
      "  Downloading numba-0.59.1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.7/3.7 MB\u001B[0m \u001B[31m70.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\r\n",
      "Collecting cytoolz>=0.8.0\r\n",
      "  Downloading cytoolz-0.12.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m98.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\r\n",
      "Collecting lightning-utilities>=0.8.0\r\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.2.2)\r\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\r\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0\r\n",
      "  Downloading llvmlite-0.42.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.8/43.8 MB\u001B[0m \u001B[31m37.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2023.1.0)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (3.9.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.127)\r\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\r\n",
      "Installing collected packages: appdirs, toml, semantic-version, rustworkx, llvmlite, lightning-utilities, cytoolz, autoray, autograd, numba, cotengra, quimb, torchmetrics, pennylane-lightning, pennylane\r\n",
      "Successfully installed appdirs-1.4.4 autograd-1.6.2 autoray-0.6.9 cotengra-0.5.6 cytoolz-0.12.3 lightning-utilities-0.11.2 llvmlite-0.42.0 numba-0.59.1 pennylane-0.35.1 pennylane-lightning-0.35.1 quimb-1.7.3 rustworkx-0.14.2 semantic-version-2.10.0 toml-0.10.2 torchmetrics-1.3.2\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:25:45.093069Z",
     "iopub.status.busy": "2024-04-03T08:25:45.092790Z",
     "iopub.status.idle": "2024-04-03T08:26:54.096113Z",
     "shell.execute_reply": "2024-04-03T08:26:54.095258Z",
     "shell.execute_reply.started": "2024-04-03T08:25:45.093044Z"
    },
    "id": "VN2wD-U7bGse",
    "outputId": "efc73948-0832-402c-eafc-b0e6adce4a54",
    "ExecuteTime": {
     "end_time": "2024-04-07T22:57:42.637370Z",
     "start_time": "2024-04-07T22:57:39.541501Z"
    }
   },
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "\n",
    "\n",
    "#import pennylane as qml\n",
    "#import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "#prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "#torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WmQPQuLbSFw"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:54.097750Z",
     "iopub.status.busy": "2024-04-03T08:26:54.097431Z",
     "iopub.status.idle": "2024-04-03T08:26:54.405968Z",
     "shell.execute_reply": "2024-04-03T08:26:54.405273Z",
     "shell.execute_reply.started": "2024-04-03T08:26:54.097729Z"
    },
    "id": "rUhQUP2dbTja",
    "outputId": "0d3641db-a2ff-4689-8b7e-32e0e8b313d4",
    "ExecuteTime": {
     "end_time": "2024-04-07T22:57:47.881892Z",
     "start_time": "2024-04-07T22:57:42.639004Z"
    }
   },
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:01<00:00, 13601554.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 328467.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:00<00:00, 6263631.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 37881187.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "torch.Size([32, 1, 32, 32])\n",
      "torch.Size([32])\n",
      "tensor([4, 1, 7, 3, 5, 8, 2, 6, 4, 6, 6, 3, 6, 9, 9, 7, 8, 7, 7, 7, 2, 7, 3, 8,\n",
      "        8, 8, 2, 2, 8, 2, 1, 5])\n",
      "tensor([-1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j,  0.7176+0.j,  0.5686+0.j,  0.8588+0.j,  0.2235+0.j,  0.0902+0.j,\n",
      "         0.9059+0.j,  0.5529+0.j,  0.6000+0.j,  0.6078+0.j,  0.6392+0.j,  0.6314+0.j,\n",
      "         0.6157+0.j,  0.8039+0.j,  0.4039+0.j, -0.0039+0.j,  0.9922+0.j,  0.6314+0.j,\n",
      "         0.7490+0.j, -0.6235+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQI8WWY6byQq"
   },
   "source": [
    "# Some Utilities"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:54.407720Z",
     "iopub.status.busy": "2024-04-03T08:26:54.407518Z",
     "iopub.status.idle": "2024-04-03T08:26:56.071027Z",
     "shell.execute_reply": "2024-04-03T08:26:56.069162Z",
     "shell.execute_reply.started": "2024-04-03T08:26:54.407700Z"
    },
    "id": "7B4DTncWbwQl",
    "outputId": "38975b7c-66c4-49f5-e5fd-158e55028cdd",
    "ExecuteTime": {
     "end_time": "2024-04-07T22:57:49.579677Z",
     "start_time": "2024-04-07T22:57:47.885004Z"
    }
   },
   "source": [
    "ket = {\n",
    "    '0':torch.tensor([1.,0.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '1':torch.tensor([0.,1.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '+':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) + torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device)),\n",
    "    '-':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) - torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device))\n",
    "}\n",
    "\n",
    "pauli = {\n",
    "    'I':torch.tensor([[1.,0.],[0.,1.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'X':torch.tensor([[0.,1.],[1.,0.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Y':torch.tensor([[0., -1.j],[1.j, 0]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Z':torch.tensor([[1.,0.],[0.,-1.]], dtype = COMPLEX_DTYPE, device=device)\n",
    "}\n",
    "\n",
    "def tensor_product(*args):\n",
    "  input_list = [a for a in args]\n",
    "  return functools.reduce(torch.kron, input_list)\n",
    "\n",
    "def multi_qubit_identity(n_qubits:int):\n",
    "  assert n_qubits>0\n",
    "  if n_qubits == 1:\n",
    "    return pauli['I']\n",
    "  else:\n",
    "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
    "\n",
    "pauli_words_su4 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
    "      #print(key1, key2)\n",
    "      #print(pauli[key1])\n",
    "      #print(pauli[key2])\n",
    "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
    "      #print(pauli_words_su4[key1+key2])\n",
    "\n",
    "pauli_words_su8 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      if not key1+key2+key3 == 'III':\n",
    "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
    "\n",
    "pauli_words_su16 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        if not key1+key2+key3+key4 == 'IIII':\n",
    "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
    "              pauli[key1],\n",
    "              pauli[key2],\n",
    "              pauli[key3],\n",
    "              pauli[key4]\n",
    "          )\n",
    "\n",
    "pauli_words_su32 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        for key5 in pauli.keys():\n",
    "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
    "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
    "                pauli[key1],\n",
    "                pauli[key2],\n",
    "                pauli[key3],\n",
    "                pauli[key4],\n",
    "                pauli[key5]\n",
    "            )\n",
    "\n",
    "def su32_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = list(pauli_words_su32.values())\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, torch.stack(dict_values))\n",
    "  #print(generator.shape)\n",
    "  return torch.matrix_exp(1j*generator)\n",
    "\n",
    "def su4_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = torch.stack(list(pauli_words_su4.values()))\n",
    "  #print(dict_values.shape)\n",
    "  generator = torch.einsum(\"i, ijk -> jk\", params, dict_values)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "def pauli_dict_func(key):\n",
    "    return pauli[key]\n",
    "\n",
    "def pauli_dict_func_multiple_keys(keys):\n",
    "    return list(map(pauli_dict_func, keys))\n",
    "\n",
    "def pauli_string_tensor_prod(pauli_string:str):\n",
    "    paulis_char = list(pauli_string)\n",
    "    paulis_mat = pauli_dict_func_multiple_keys(paulis_char)\n",
    "    return tensor_product(*paulis_mat)\n",
    "\n",
    "def generate_nqubit_pauli_strings(n_qubits:int):\n",
    "    assert n_qubits>0\n",
    "    pauli_labels = ['I', 'X', 'Y', 'Z']\n",
    "    pauli_strings = []\n",
    "    for labels in itertools.product(pauli_labels, repeat=n_qubits):\n",
    "        pauli_str = \"\".join(labels)\n",
    "        if pauli_str != 'I'*n_qubits:\n",
    "            pauli_strings.append(pauli_str)\n",
    "    return pauli_strings\n",
    "\n",
    "def generate_pauli_tensor_list(pauli_strings:list):\n",
    "    return list(map(pauli_string_tensor_prod, pauli_strings))\n",
    "\n",
    "\n",
    "print(\n",
    "    generate_pauli_tensor_list(generate_nqubit_pauli_strings(2))\n",
    ")\n",
    "\n",
    "\n",
    "#test_params = torch.randn(4**2-1,  device=device).type(COMPLEX_DTYPE)\n",
    "#print(test_params.shape)\n",
    "#test_op = su4_op(test_params)\n",
    "#print(test_op)\n",
    "#print(qml.is_unitary(qml.QubitUnitary(test_op.cpu().numpy(), wires=[0,1]))) # will be false if not torch.cdouble\n",
    "\n",
    "#rx = torch.linalg.matrix_exp(1j*torch.pi*pauli[\"X\"]/2)\n",
    "#print(qml.is_unitary(qml.QubitUnitary(rx.cpu().numpy(), wires=[0])))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-0.j, 0.+0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j,  0.+0.j, -1.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-0.j, 0.+0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j,  0.+0.j, -1.+0.j],\n",
      "        [ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.-0.j, 0.-1.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  0.-0.j,  0.-0.j, -1.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j,  1.-0.j,  0.-0.j],\n",
      "        [ 0.+0.j,  1.-0.j,  0.+0.j,  0.-0.j],\n",
      "        [-1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, -0.+0.j, 0.-0.j, 0.+1.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, -0.-1.j, 0.+0.j, -0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -0.+0.j, -1.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -0.+0.j, -1.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-0.j, -0.+0.j, 0.+1.j],\n",
      "        [0.+0.j, 0.+0.j, -0.-1.j, -0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j, -0.+0.j,  1.-0.j]], device='cuda:0')]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.073077Z",
     "iopub.status.busy": "2024-04-03T08:26:56.072615Z",
     "iopub.status.idle": "2024-04-03T08:26:56.079092Z",
     "shell.execute_reply": "2024-04-03T08:26:56.078390Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.073053Z"
    },
    "id": "Xs0c2F1eBnGc",
    "ExecuteTime": {
     "end_time": "2024-04-07T22:57:49.587181Z",
     "start_time": "2024-04-07T22:57:49.581477Z"
    }
   },
   "source": [
    "def measure_sv(\n",
    "    state,\n",
    "    observable\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Measure a statevector with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not\n",
    "  \"\"\"\n",
    "  expectation_value = torch.conj(state)@observable@state\n",
    "  return torch.real(expectation_value)\n",
    "\n",
    "def measure_dm(\n",
    "    rho,\n",
    "    observable\n",
    "):\n",
    "  \"\"\"\n",
    "  Measure a density matrix with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not.\n",
    "  \"\"\"\n",
    "  product = torch.matmul(rho, observable)\n",
    "\n",
    "  # Calculate the trace, which is the sum of diagonal elements\n",
    "  trace = torch.trace(product)\n",
    "\n",
    "  # The expectation value should be real for physical observables\n",
    "  return torch.real(trace)\n",
    "\n",
    "vmap_measure_sv = torch.vmap(measure_sv, in_dims=(None, 0), out_dims=0)\n",
    "vmap_measure_dm = torch.vmap(measure_dm, in_dims=(None, 0), out_dims=0)\n",
    "\n",
    "# assuming the input patch observables (hermitianized) has shape (batchsize,n_patches, c, h, w)\n",
    "# assuming the input set statevectors has shape (c, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_sv_batched_ob = torch.vmap(measure_sv, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "# assuming the input set desnity matrices has shape (batchsize, n_patches, c, 2**n, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_dm_batched_ob = torch.vmap(measure_dm, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "def bitstring_to_state(bitstring:str):\n",
    "  \"\"\"\n",
    "  Convert a bit string, like '0101001' or '+-+-101'\n",
    "  to a statevector. Each character in the bitstring must be among\n",
    "  0, 1, + and -\n",
    "  \"\"\"\n",
    "  assert len(bitstring)>0\n",
    "  for c in bitstring:\n",
    "    assert c in ['0', '1', '+', '-']\n",
    "  single_qubit_states = [ket[c] for c in bitstring]\n",
    "  return tensor_product(*single_qubit_states)\n",
    "\n",
    "\n",
    "#test_patch = torch.randn(size=[5, 2, 3, 4,4], dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_herm_patch = (torch.einsum(\"...jk->...kj\", test_patch)+test_patch)/2\n",
    "#print(test_herm_patch)\n",
    "#print(\"all patches shape\", test_herm_patch.shape)\n",
    "#test_sv = torch.stack([bitstring_to_state('++')]*3, axis = 0)\n",
    "#print(test_sv)\n",
    "#print(\"all sv shape\", test_sv.shape)\n",
    "#res = vmap_measure_channel_sv_batched_ob(test_sv, test_herm_patch)\n",
    "#print(res)\n",
    "#print(\"res shape\",res.shape)\n",
    "\n",
    "#for batchid in range(test_patch.shape[0]):\n",
    "#  batchitem = test_patch[batchid]\n",
    "#  for patch_idx in range(batchitem.shape[0]):\n",
    "#    patch = batchitem[patch_idx]\n",
    "#    for ch_idx in range(patch.shape[0]):\n",
    "#      print(f\"batchid={batchid}; patchid = {patch_idx}; chid = {ch_idx}\")\n",
    "#      ch = patch[ch_idx]\n",
    "#      #print(ch.shape)\n",
    "#      sv_ch = test_sv[ch_idx]\n",
    "#      #print(sv_ch.shape)\n",
    "#      print(measure_sv(sv_ch, ch))\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFZqT6bpElaL"
   },
   "source": [
    "# Flipped Quanvolution Kernel\n",
    "\n",
    "First, let's define the function to extract patches for a single kernel."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.080443Z",
     "iopub.status.busy": "2024-04-03T08:26:56.079871Z",
     "iopub.status.idle": "2024-04-03T08:26:56.085801Z",
     "shell.execute_reply": "2024-04-03T08:26:56.085133Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.080420Z"
    },
    "id": "He4HdMRHC7T6",
    "ExecuteTime": {
     "end_time": "2024-04-07T22:57:49.593758Z",
     "start_time": "2024-04-07T22:57:49.589058Z"
    }
   },
   "source": [
    "def extract_patches(image, patch_size, stride, padding=None):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image with multiple input channels and optional custom padding.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (in_channels, height, width).\n",
    "        patch_size (int): Size of the square patches to extract.\n",
    "        stride (int): Stride between patches.\n",
    "        padding (tuple): Padding value(s) for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n",
    "    pad_l, pad_r, pad_t, pad_b = padding if padding is not None else (0,0,0,0)\n",
    "\n",
    "    if padding is not None:\n",
    "        image = torch.nn.functional.pad(image, (pad_l, pad_r, pad_t, pad_b), mode='constant')\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    height, width = image.shape[-2],  image.shape[-1]\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = image[..., i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    patches = torch.stack(patches, dim=-4)\n",
    "    return patches # has shape (batchsize, n_patches, channel, h, w)\n",
    "\n",
    "\n",
    "#single_img = torch.stack([torch.arange(32*32*1,dtype = torch.double, device=device).reshape((32,32)).type(torch.cdouble)]*3)\n",
    "\n",
    "#test_img = torch.stack([single_img]*2)\n",
    "#print(test_img[0][...,:4,:4])\n",
    "#patches = extract_patches(test_img, patch_size=4, stride=2,padding=(0,0,0,0))\n",
    "#print(patches.shape)\n",
    "#print(patches[0].shape)\n",
    "#print(patches[0][0])\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRqflooEqKN"
   },
   "source": [
    "## A Three-by-Three Quanv Kernel Function\n",
    "\n",
    "For 3 by 3 patches, we only need a 2-qubit parameterised state, generated by the SU4 gate applied to the $|00\\rangle$ state. The first thing to do is to pad the 3 by 3 patches to 4 by 4. However, for input with size\n",
    "\n",
    "$$\n",
    "D_1\\times H_1 \\times W_1\n",
    "$$\n",
    "\n",
    "where $D_1$ is the number of input channels, we will need $D_1$ different two-qubit states, generated by $D_1$ different parameterised circuits. These circuits will produce $D_1$ expectation values. Suppose each circuit is parameterised by $\\theta_i, i\\in \\{1,2,\\cdots, D_1\\}$, the parameterised (pure) state can be written as\n",
    "\n",
    "$$\n",
    "|\\psi (\\theta_i)⟩ = SU4(\\theta_i)|00⟩\n",
    "$$\n",
    "\n",
    "For a channel $M_i$ of patch $M$ with shape $4\\times 4$, the expectation value is calculated by\n",
    "\n",
    "$$\n",
    "⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$\n",
    "\n",
    "Then, a single pixel value of the output channel corresponding this single kernel is\n",
    "\n",
    "$$\n",
    "\\frac{1}{D_1}\\sum_{i=1}^{D_1}⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.086697Z",
     "iopub.status.busy": "2024-04-03T08:26:56.086517Z",
     "iopub.status.idle": "2024-04-03T08:26:56.091796Z",
     "shell.execute_reply": "2024-04-03T08:26:56.091178Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.086679Z"
    },
    "id": "Yzn4KEt5ErG7",
    "ExecuteTime": {
     "end_time": "2024-04-07T22:57:49.599499Z",
     "start_time": "2024-04-07T22:57:49.595480Z"
    }
   },
   "source": [
    "def generate_2q_param_state(theta):\n",
    "  state = bitstring_to_state('00')\n",
    "  state = torch.matmul(su4_op(theta), state)\n",
    "  return state\n",
    "\n",
    "vmap_generate_2q_param_state = torch.vmap(generate_2q_param_state, in_dims=0, out_dims = 0)\n",
    "\n",
    "def single_kernel_op_over_batched_patches(thetas, patch):\n",
    "  # patch has shape (c_in, h, w)\n",
    "  # thetas has shape (c_in, 4^2-1) for SU4 gates\n",
    "  n_theta = thetas.shape[-2]\n",
    "  n_channel = patch.shape[-3]\n",
    "  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n",
    "  states = vmap_generate_2q_param_state(thetas)\n",
    "  #print(\"States shape\", states.shape)\n",
    "  patch = torch.nn.functional.pad(patch, (0, 1, 0, 1), mode='constant')\n",
    "  patch_t = torch.einsum(\"...jk->...kj\", patch)\n",
    "  herm_patch = (patch_t+patch)/2 # has dim (batchsize, num_patches, c, h, w)\n",
    "  #print(\"Herm patch shape\", herm_patch.shape)\n",
    "  channel_out = vmap_measure_channel_sv_batched_ob(states, herm_patch) # has dim (batchsize,n_patches, c)\n",
    "  return torch.sum(channel_out, axis = -1)/n_theta # has dim (batchszie, n_patches)\n",
    "\n",
    "\n",
    "#test_params = torch.randn((1,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_img = dummy_x.to(device)#.type(torch.cdouble)\n",
    "#print(test_img.shape)\n",
    "#test_patches = extract_patches(test_img, patch_size=3, stride=1,padding=(1,1,1,1))\n",
    "#print(test_patches.shape)\n",
    "#print(test_params.shape)\n",
    "#print(vmap_generate_2q_param_state(test_params))\n",
    "#test_out = single_kernel_op_over_batched_patches(test_params, test_patches)\n",
    "#print(test_out.shape)\n",
    "#h = (32-3+1*2)//1 +1\n",
    "#h**2\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4BXEKd8I67_"
   },
   "source": [
    "## Multiple Output Channels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.092668Z",
     "iopub.status.busy": "2024-04-03T08:26:56.092483Z",
     "iopub.status.idle": "2024-04-03T08:26:56.095952Z",
     "shell.execute_reply": "2024-04-03T08:26:56.095319Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.092650Z"
    },
    "id": "72vkHV_BI80l",
    "ExecuteTime": {
     "end_time": "2024-04-07T22:57:49.603714Z",
     "start_time": "2024-04-07T22:57:49.601160Z"
    }
   },
   "source": [
    "# For multiple channel output\n",
    "# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n",
    "vmap_vmap_single_kernel_op_through_extracted_patches = torch.vmap(single_kernel_op_over_batched_patches, in_dims=(0, None), out_dims=-2) # output has dim (batchsize, c_out, n_patches)\n",
    "\n",
    "#c_in = 1\n",
    "#c_out = 2\n",
    "\n",
    "#test_params2 = torch.randn((c_out, c_in,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_out2 = vmap_vmap_single_kernel_op_through_extracted_patches(test_params2, test_patches)\n",
    "#print(test_out2.shape)\n",
    "#test_out_2_features = test_out2.reshape((-1,c_out, 32, 32))\n",
    "#print(test_out_2_features.shape)\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuZ6G5-sKK_J"
   },
   "source": [
    "## Pytorch Module for Three-by-Three Flipped Quanv"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.096785Z",
     "iopub.status.busy": "2024-04-03T08:26:56.096604Z",
     "iopub.status.idle": "2024-04-03T08:26:56.103091Z",
     "shell.execute_reply": "2024-04-03T08:26:56.102542Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.096768Z"
    },
    "id": "Gww_XdJ5KPJt",
    "ExecuteTime": {
     "end_time": "2024-04-07T22:57:49.611026Z",
     "start_time": "2024-04-07T22:57:49.605255Z"
    }
   },
   "source": [
    "class FlippedQuanv3x3(torch.nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, padding):\n",
    "    super(FlippedQuanv3x3, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.pad_l, self.pad_r, self.pad_t, self.pad_b = padding if padding is not None else (0,0,0,0)\n",
    "    self.pad = (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n",
    "    self.weight= torch.nn.Parameter(torch.randn((out_channels, in_channels, 4**2-1)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn((out_channels, 1)).type(COMPLEX_DTYPE))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batchsize ,c_in, h, w)\n",
    "    # weight has shape (c_out, c_in, 15)\n",
    "    # bias has shape (c_out, 1)\n",
    "    x = x.type(COMPLEX_DTYPE)\n",
    "    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n",
    "    h_out = (h_in-3+(self.pad_t+self.pad_b))//self.stride +1\n",
    "    w_out = (w_in-3+(self.pad_l+self.pad_r))//self.stride +1\n",
    "\n",
    "    #print(self.weight.shape)\n",
    "    #print(patches.shape)\n",
    "\n",
    "    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n",
    "\n",
    "\n",
    "    out = out + self.bias\n",
    "    return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_channels={self.in_channels}, out_channels={self.out_channels}, stride={self.stride}, padding={self.padding}'\n",
    "\n",
    "#test_module = FlippedQuanv3x3(in_channels=1, out_channels=2, stride=1, padding=(1,1,1,1)).to(device)\n",
    "#print(dummy_x.shape)\n",
    "#test_out = test_module(dummy_x.to(device))\n",
    "#print(test_out.shape)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jY9sQZ7Ynla"
   },
   "source": [
    "# Linear Layer\n",
    "\n",
    "For input dimension $D$, the quantum linear layer is a $n = \\lceil \\log_4(D+1)⌉$-qubit quantum circuit. Both the data encoding and parameterised circuit is achieved via the $SU(2^n)$ unitary."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.103883Z",
     "iopub.status.busy": "2024-04-03T08:26:56.103708Z",
     "iopub.status.idle": "2024-04-03T08:26:57.562457Z",
     "shell.execute_reply": "2024-04-03T08:26:57.561682Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.103865Z"
    },
    "id": "AXxNIObFYnPW",
    "ExecuteTime": {
     "end_time": "2024-04-07T22:57:51.229977Z",
     "start_time": "2024-04-07T22:57:49.612800Z"
    }
   },
   "source": [
    "def data_encode_unitary(padded_data, t):\n",
    "  original_dim = padded_data.shape[-1]\n",
    "  new_dim = torch.sqrt(torch.tensor(original_dim)).type(torch.int)\n",
    "  data = torch.reshape(padded_data, (new_dim, new_dim))\n",
    "  generator = (data + torch.einsum(\"...jk->...kj\", data))/2\n",
    "  return torch.linalg.matrix_exp(1.0j*generator*t)\n",
    "\n",
    "def su_n(params, pauli_string_tensor_list):\n",
    "  # params has dim 4**n-1\n",
    "  paulis = torch.stack(pauli_string_tensor_list)\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, paulis)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "def linear_layer_func(padded_data, params, pauli_string_tensor_list, observables, n_qubits):\n",
    "  n_rep = params.shape[0]\n",
    "  state = bitstring_to_state(\"+\"*n_qubits)\n",
    "  for i in range(n_rep):\n",
    "    data_unitary = data_encode_unitary(padded_data, 1/n_rep)\n",
    "    #print(data_unitary.shape)\n",
    "    #print(state.shape)\n",
    "    state = torch.matmul(\n",
    "      data_unitary,\n",
    "      state\n",
    "    )\n",
    "    sun_gate = su_n(params[i], pauli_string_tensor_list)\n",
    "    #print(sun_gate.shape)\n",
    "    state = torch.matmul(\n",
    "      sun_gate,\n",
    "      state\n",
    "    )\n",
    "  return vmap_measure_sv(state, observables)\n",
    "\n",
    "test_params = torch.randn((3, 4**2-1),device=device).type(COMPLEX_DTYPE)\n",
    "test_data = torch.randn(4**2, device=device).type(COMPLEX_DTYPE)\n",
    "test_obs = torch.stack([torch.outer(bitstring_to_state('00'), bitstring_to_state('00')),\n",
    "                        torch.outer(bitstring_to_state('01'), bitstring_to_state('01')),\n",
    "                        torch.outer(bitstring_to_state('10'), bitstring_to_state('10')),\n",
    "                         torch.outer(bitstring_to_state('11'), bitstring_to_state('11'))])\n",
    "test_pauli_string_tensor_list = generate_pauli_tensor_list(generate_nqubit_pauli_strings(2))\n",
    "\n",
    "\n",
    "test_out = linear_layer_func(test_data, test_params, test_pauli_string_tensor_list, test_obs, 2)\n",
    "print(test_out)\n",
    "print(torch.sum(test_out))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0847, 0.6649, 0.0675, 0.1829], device='cuda:0')\n",
      "tensor(1.0000, device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:57.564844Z",
     "iopub.status.busy": "2024-04-03T08:26:57.564417Z",
     "iopub.status.idle": "2024-04-03T08:26:57.576557Z",
     "shell.execute_reply": "2024-04-03T08:26:57.575906Z",
     "shell.execute_reply.started": "2024-04-03T08:26:57.564823Z"
    },
    "id": "2F4_SBgIYnMv",
    "ExecuteTime": {
     "end_time": "2024-04-07T22:57:51.243825Z",
     "start_time": "2024-04-07T22:57:51.231334Z"
    }
   },
   "source": [
    "vmap_batch_linear_layer_func = torch.vmap(linear_layer_func, in_dims=(0, None, None, None, None), out_dims=0)\n",
    "\n",
    "test_data = torch.randn((3, 4**2), device=device).type(COMPLEX_DTYPE)\n",
    "test_out = vmap_batch_linear_layer_func(test_data, test_params, test_pauli_string_tensor_list, test_obs, 2)\n",
    "print(test_out)\n",
    "print(torch.sum(test_out))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3414, 0.4328, 0.0295, 0.1964],\n",
      "        [0.4175, 0.1313, 0.2182, 0.2330],\n",
      "        [0.1889, 0.5187, 0.1613, 0.1311]], device='cuda:0')\n",
      "tensor(3.0000, device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryROGtnC_3po"
   },
   "source": [
    "## PyTorch Module for the Quantum Version of Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:57.577462Z",
     "iopub.status.busy": "2024-04-03T08:26:57.577280Z",
     "iopub.status.idle": "2024-04-03T08:26:57.605899Z",
     "shell.execute_reply": "2024-04-03T08:26:57.605187Z",
     "shell.execute_reply.started": "2024-04-03T08:26:57.577443Z"
    },
    "id": "RlTC952w_8VW",
    "ExecuteTime": {
     "end_time": "2024-04-07T22:57:51.275499Z",
     "start_time": "2024-04-07T22:57:51.245352Z"
    }
   },
   "source": [
    "class DataReUploadingLinear(torch.nn.Module):\n",
    "  def __init__(self, in_dim, out_dim, n_qubits, n_reps):\n",
    "    super(DataReUploadingLinear, self).__init__()\n",
    "    assert 2**n_qubits >= out_dim\n",
    "    assert 4**n_qubits >= in_dim\n",
    "    self.in_dim = in_dim\n",
    "    self.out_dim = out_dim\n",
    "    self.n_qubits = n_qubits\n",
    "    self.n_reps = n_reps\n",
    "    self.pauli_string_tensor_list = generate_pauli_tensor_list(generate_nqubit_pauli_strings(n_qubits))\n",
    "    self.param_dim = 4**n_qubits-1\n",
    "    self.params = torch.nn.Parameter(torch.randn((self.n_reps,self.param_dim)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn(out_dim).type(REAL_DTYPE))\n",
    "    self.observables = self.generate_observables()\n",
    "    self.pad_size = 4**n_qubits-self.in_dim\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has size (batchsize, in_dim)\n",
    "    # pad x\n",
    "    x = torch.nn.functional.pad(x, (0, self.pad_size)).type(COMPLEX_DTYPE)\n",
    "    out = vmap_batch_linear_layer_func(x, self.params, self.pauli_string_tensor_list, self.observables, self.n_qubits)\n",
    "    out = out.type(REAL_DTYPE) + self.bias\n",
    "    return out\n",
    "\n",
    "  def generate_observables(self):\n",
    "    observables = []\n",
    "    for i in range(self.out_dim):\n",
    "      temp_bitstring = '{0:b}'.format(i).zfill(self.n_qubits)\n",
    "      ob = torch.outer(bitstring_to_state(temp_bitstring), bitstring_to_state(temp_bitstring))\n",
    "      observables.append(ob)\n",
    "    return torch.stack(observables)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_dim={self.in_dim}, out_dim={self.out_dim}, n_qubits={self.n_qubits}, n_reps={self.n_reps}'\n",
    "\n",
    "test_linear_module = DataReUploadingLinear(in_dim=45, out_dim=6, n_qubits=3, n_reps = 10).to(device)\n",
    "test_data = torch.randn((16, 45), device=device).type(COMPLEX_DTYPE)\n",
    "print(test_data.shape)\n",
    "test_out = test_linear_module(test_data.to(device))\n",
    "print(test_out.shape)\n",
    "print(test_linear_module)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 45])\n",
      "torch.Size([16, 6])\n",
      "DataReUploadingLinear(in_dim=45, out_dim=6, n_qubits=3, n_reps=10)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yumZTjDVu63"
   },
   "source": [
    "# Replacing Classical Layers with Quantum Layers\n",
    "\n",
    "Let's replace `Conv2d` with `FlippedQuanv3x3`, and `Linear` with `QuantLinear`,"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:57.606924Z",
     "iopub.status.busy": "2024-04-03T08:26:57.606727Z",
     "iopub.status.idle": "2024-04-03T08:26:59.000203Z",
     "shell.execute_reply": "2024-04-03T08:26:58.999449Z",
     "shell.execute_reply.started": "2024-04-03T08:26:57.606905Z"
    },
    "id": "W4RC5ou-VzbE",
    "outputId": "9eb0b35d-ed18-431b-8a53-ba5216d3fd06",
    "ExecuteTime": {
     "end_time": "2024-04-07T22:57:52.806893Z",
     "start_time": "2024-04-07T22:57:51.278268Z"
    }
   },
   "source": [
    "class HybridNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HybridNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None), # 32->30\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None), # 30->28\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        DataReUploadingLinear(16*28*28, 10, 7, 10)\n",
    "        #torch.nn.Linear(32*14*14, 10).type(REAL_DTYPE)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "\n",
    "net = HybridNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 32, 32])\n",
      "HybridNet(\n",
      "  (layers): Sequential(\n",
      "    (0): FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None)\n",
      "    (1): FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): DataReUploadingLinear(in_dim=12544, out_dim=10, n_qubits=7, n_reps=10)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43/1953195815.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n",
      "/tmp/ipykernel_43/300606786.py:30: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:301.)\n",
      "  return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:59.001280Z",
     "iopub.status.busy": "2024-04-03T08:26:59.001080Z",
     "iopub.status.idle": "2024-04-03T17:06:02.379965Z",
     "shell.execute_reply": "2024-04-03T17:06:02.379200Z",
     "shell.execute_reply.started": "2024-04-03T08:26:59.001260Z"
    },
    "id": "x4ZY59q1WS4x",
    "outputId": "6532aea1-47a3-49f3-fe36-b774aee1905f",
    "ExecuteTime": {
     "end_time": "2024-04-08T07:37:46.460164Z",
     "start_time": "2024-04-07T22:57:52.808161Z"
    }
   },
   "source": [
    "import torchmetrics\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.1\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 300, Number of test batches = 50\n",
      "Print every train batch = 30, Print every test batch = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43/1953195815.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at step=0, batch=0, train loss = 2.6324901580810547, train acc = 0.11999999731779099, time = 1.1243293285369873\n",
      "Training at step=0, batch=30, train loss = 2.633659601211548, train acc = 0.09000000357627869, time = 0.938570499420166\n",
      "Training at step=0, batch=60, train loss = 2.636003017425537, train acc = 0.10999999940395355, time = 0.9394218921661377\n",
      "Training at step=0, batch=90, train loss = 2.7073450088500977, train acc = 0.07999999821186066, time = 0.9440512657165527\n",
      "Training at step=0, batch=120, train loss = 2.644988775253296, train acc = 0.07999999821186066, time = 0.9394698143005371\n",
      "Training at step=0, batch=150, train loss = 2.6260995864868164, train acc = 0.09000000357627869, time = 0.941119909286499\n",
      "Training at step=0, batch=180, train loss = 2.5308444499969482, train acc = 0.14499999582767487, time = 0.9467628002166748\n",
      "Training at step=0, batch=210, train loss = 2.5431880950927734, train acc = 0.08500000089406967, time = 0.941239595413208\n",
      "Training at step=0, batch=240, train loss = 2.4410648345947266, train acc = 0.0949999988079071, time = 0.932135820388794\n",
      "Training at step=0, batch=270, train loss = 2.2734453678131104, train acc = 0.20000000298023224, time = 0.9360489845275879\n",
      "Testing at step=0, batch=0, test loss = 1.9773999452590942, test acc = 0.29499998688697815, time = 0.3397197723388672\n",
      "Testing at step=0, batch=5, test loss = 1.8858060836791992, test acc = 0.35499998927116394, time = 0.3386728763580322\n",
      "Testing at step=0, batch=10, test loss = 1.845513105392456, test acc = 0.4099999964237213, time = 0.3378291130065918\n",
      "Testing at step=0, batch=15, test loss = 1.8958139419555664, test acc = 0.33500000834465027, time = 0.3422253131866455\n",
      "Testing at step=0, batch=20, test loss = 1.992842197418213, test acc = 0.3199999928474426, time = 0.3410162925720215\n",
      "Testing at step=0, batch=25, test loss = 1.8771522045135498, test acc = 0.3700000047683716, time = 0.3614351749420166\n",
      "Testing at step=0, batch=30, test loss = 1.8399581909179688, test acc = 0.4050000011920929, time = 0.34236860275268555\n",
      "Testing at step=0, batch=35, test loss = 1.9402482509613037, test acc = 0.29499998688697815, time = 0.33948636054992676\n",
      "Testing at step=0, batch=40, test loss = 1.7956640720367432, test acc = 0.38999998569488525, time = 0.341541051864624\n",
      "Testing at step=0, batch=45, test loss = 1.8283978700637817, test acc = 0.4000000059604645, time = 0.34043121337890625\n",
      "Step 0 finished in 310.7989990711212, Train loss = 2.521625447273254, Test loss = 1.8800033116340638; Train Acc = 0.11729999976853529, Test Acc = 0.36109999895095823\n",
      "Training at step=1, batch=0, train loss = 1.898888349533081, train acc = 0.36000001430511475, time = 0.9402499198913574\n",
      "Training at step=1, batch=30, train loss = 1.2082985639572144, train acc = 0.6000000238418579, time = 0.9522786140441895\n",
      "Training at step=1, batch=60, train loss = 0.9217548966407776, train acc = 0.675000011920929, time = 0.9377796649932861\n",
      "Training at step=1, batch=90, train loss = 0.796904981136322, train acc = 0.7099999785423279, time = 0.9355626106262207\n",
      "Training at step=1, batch=120, train loss = 0.9339263439178467, train acc = 0.675000011920929, time = 0.9445509910583496\n",
      "Training at step=1, batch=150, train loss = 0.6475290060043335, train acc = 0.7450000047683716, time = 0.9472036361694336\n",
      "Training at step=1, batch=180, train loss = 0.7375340461730957, train acc = 0.7350000143051147, time = 0.9318087100982666\n",
      "Training at step=1, batch=210, train loss = 0.6775838732719421, train acc = 0.7599999904632568, time = 0.9374649524688721\n",
      "Training at step=1, batch=240, train loss = 0.5683263540267944, train acc = 0.7799999713897705, time = 0.9594976902008057\n",
      "Training at step=1, batch=270, train loss = 0.6484802961349487, train acc = 0.75, time = 0.9433813095092773\n",
      "Testing at step=1, batch=0, test loss = 0.6801043152809143, test acc = 0.7549999952316284, time = 0.3370335102081299\n",
      "Testing at step=1, batch=5, test loss = 0.5662970542907715, test acc = 0.7850000262260437, time = 0.3424675464630127\n",
      "Testing at step=1, batch=10, test loss = 0.661701500415802, test acc = 0.7749999761581421, time = 0.33631348609924316\n",
      "Testing at step=1, batch=15, test loss = 0.6583585143089294, test acc = 0.7599999904632568, time = 0.3417525291442871\n",
      "Testing at step=1, batch=20, test loss = 0.5506714582443237, test acc = 0.7900000214576721, time = 0.33647704124450684\n",
      "Testing at step=1, batch=25, test loss = 0.5271157026290894, test acc = 0.8199999928474426, time = 0.3520486354827881\n",
      "Testing at step=1, batch=30, test loss = 0.608951210975647, test acc = 0.7549999952316284, time = 0.34180259704589844\n",
      "Testing at step=1, batch=35, test loss = 0.6015451550483704, test acc = 0.800000011920929, time = 0.3462216854095459\n",
      "Testing at step=1, batch=40, test loss = 0.6270647644996643, test acc = 0.7799999713897705, time = 0.33608245849609375\n",
      "Testing at step=1, batch=45, test loss = 0.6899468302726746, test acc = 0.75, time = 0.33631443977355957\n",
      "Step 1 finished in 310.17412662506104, Train loss = 0.8088569776217143, Test loss = 0.6175058603286743; Train Acc = 0.713400000333786, Test Acc = 0.7769999992847443\n",
      "Training at step=2, batch=0, train loss = 0.5864378213882446, train acc = 0.7749999761581421, time = 0.9355800151824951\n",
      "Training at step=2, batch=30, train loss = 0.6797444820404053, train acc = 0.7649999856948853, time = 0.9353947639465332\n",
      "Training at step=2, batch=60, train loss = 0.7019408345222473, train acc = 0.75, time = 0.9586074352264404\n",
      "Training at step=2, batch=90, train loss = 0.6285226345062256, train acc = 0.7850000262260437, time = 0.9330344200134277\n",
      "Training at step=2, batch=120, train loss = 0.6104055643081665, train acc = 0.7599999904632568, time = 0.9452176094055176\n",
      "Training at step=2, batch=150, train loss = 0.4700329601764679, train acc = 0.8450000286102295, time = 0.9384148120880127\n",
      "Training at step=2, batch=180, train loss = 0.6058640480041504, train acc = 0.7950000166893005, time = 0.9345436096191406\n",
      "Training at step=2, batch=210, train loss = 0.7153646349906921, train acc = 0.7749999761581421, time = 0.9385433197021484\n",
      "Training at step=2, batch=240, train loss = 0.6634766459465027, train acc = 0.7799999713897705, time = 0.9361681938171387\n",
      "Training at step=2, batch=270, train loss = 0.46847155690193176, train acc = 0.8500000238418579, time = 0.9330706596374512\n",
      "Testing at step=2, batch=0, test loss = 0.49200230836868286, test acc = 0.8149999976158142, time = 0.3372180461883545\n",
      "Testing at step=2, batch=5, test loss = 0.4819304645061493, test acc = 0.8199999928474426, time = 0.3393242359161377\n",
      "Testing at step=2, batch=10, test loss = 0.6328757405281067, test acc = 0.7850000262260437, time = 0.3457827568054199\n",
      "Testing at step=2, batch=15, test loss = 0.523841142654419, test acc = 0.7900000214576721, time = 0.3495762348175049\n",
      "Testing at step=2, batch=20, test loss = 0.47848397493362427, test acc = 0.8100000023841858, time = 0.34213805198669434\n",
      "Testing at step=2, batch=25, test loss = 0.538916289806366, test acc = 0.7950000166893005, time = 0.342193603515625\n",
      "Testing at step=2, batch=30, test loss = 0.5316864252090454, test acc = 0.8100000023841858, time = 0.3405110836029053\n",
      "Testing at step=2, batch=35, test loss = 0.5990573763847351, test acc = 0.800000011920929, time = 0.3449680805206299\n",
      "Testing at step=2, batch=40, test loss = 0.6923640370368958, test acc = 0.7400000095367432, time = 0.34884095191955566\n",
      "Testing at step=2, batch=45, test loss = 0.5766544938087463, test acc = 0.7699999809265137, time = 0.3394932746887207\n",
      "Step 2 finished in 309.44537377357483, Train loss = 0.5489970763524373, Test loss = 0.6041200667619705; Train Acc = 0.8002000008026758, Test Acc = 0.7780000042915344\n",
      "Training at step=3, batch=0, train loss = 0.6248357892036438, train acc = 0.7450000047683716, time = 0.9422879219055176\n",
      "Training at step=3, batch=30, train loss = 0.5774136185646057, train acc = 0.7900000214576721, time = 0.9362847805023193\n",
      "Training at step=3, batch=60, train loss = 0.5021598935127258, train acc = 0.8299999833106995, time = 0.9589707851409912\n",
      "Training at step=3, batch=90, train loss = 0.5038403868675232, train acc = 0.8199999928474426, time = 0.9633426666259766\n",
      "Training at step=3, batch=120, train loss = 0.4877021312713623, train acc = 0.8199999928474426, time = 0.9607889652252197\n",
      "Training at step=3, batch=150, train loss = 0.507901132106781, train acc = 0.8050000071525574, time = 0.936039924621582\n",
      "Training at step=3, batch=180, train loss = 0.4510159194469452, train acc = 0.8450000286102295, time = 0.9331991672515869\n",
      "Training at step=3, batch=210, train loss = 0.5315421223640442, train acc = 0.7950000166893005, time = 0.934943675994873\n",
      "Training at step=3, batch=240, train loss = 0.5781252980232239, train acc = 0.7850000262260437, time = 0.9375550746917725\n",
      "Training at step=3, batch=270, train loss = 0.42382732033729553, train acc = 0.8600000143051147, time = 0.9627413749694824\n",
      "Testing at step=3, batch=0, test loss = 0.5318158864974976, test acc = 0.8349999785423279, time = 0.33924341201782227\n",
      "Testing at step=3, batch=5, test loss = 0.5113268494606018, test acc = 0.824999988079071, time = 0.33764028549194336\n",
      "Testing at step=3, batch=10, test loss = 0.5872256755828857, test acc = 0.7799999713897705, time = 0.3436908721923828\n",
      "Testing at step=3, batch=15, test loss = 0.4935346841812134, test acc = 0.8199999928474426, time = 0.3407018184661865\n",
      "Testing at step=3, batch=20, test loss = 0.5136545896530151, test acc = 0.8349999785423279, time = 0.3418731689453125\n",
      "Testing at step=3, batch=25, test loss = 0.5938969254493713, test acc = 0.800000011920929, time = 0.34058666229248047\n",
      "Testing at step=3, batch=30, test loss = 0.5366051197052002, test acc = 0.7900000214576721, time = 0.3344995975494385\n",
      "Testing at step=3, batch=35, test loss = 0.49045249819755554, test acc = 0.824999988079071, time = 0.33954739570617676\n",
      "Testing at step=3, batch=40, test loss = 0.6978700160980225, test acc = 0.7549999952316284, time = 0.33969712257385254\n",
      "Testing at step=3, batch=45, test loss = 0.5322703719139099, test acc = 0.7950000166893005, time = 0.33826136589050293\n",
      "Step 3 finished in 311.9320788383484, Train loss = 0.5044872511426608, Test loss = 0.5491838383674622; Train Acc = 0.8163333348433177, Test Acc = 0.7966999995708466\n",
      "Training at step=4, batch=0, train loss = 0.537350594997406, train acc = 0.800000011920929, time = 0.9359102249145508\n",
      "Training at step=4, batch=30, train loss = 0.5840953588485718, train acc = 0.7699999809265137, time = 0.9358038902282715\n",
      "Training at step=4, batch=60, train loss = 0.4427729845046997, train acc = 0.8550000190734863, time = 0.936798095703125\n",
      "Training at step=4, batch=90, train loss = 0.4227495491504669, train acc = 0.8700000047683716, time = 0.9335837364196777\n",
      "Training at step=4, batch=120, train loss = 0.5012999773025513, train acc = 0.8100000023841858, time = 0.939227819442749\n",
      "Training at step=4, batch=150, train loss = 0.46143731474876404, train acc = 0.8100000023841858, time = 0.9428355693817139\n",
      "Training at step=4, batch=180, train loss = 0.4891752302646637, train acc = 0.8399999737739563, time = 0.9401021003723145\n",
      "Training at step=4, batch=210, train loss = 0.473970502614975, train acc = 0.8550000190734863, time = 1.0490303039550781\n",
      "Training at step=4, batch=240, train loss = 0.487853467464447, train acc = 0.7950000166893005, time = 0.9372551441192627\n",
      "Training at step=4, batch=270, train loss = 0.5814155340194702, train acc = 0.800000011920929, time = 0.9412040710449219\n",
      "Testing at step=4, batch=0, test loss = 0.5991391539573669, test acc = 0.7749999761581421, time = 0.34126901626586914\n",
      "Testing at step=4, batch=5, test loss = 0.4607207775115967, test acc = 0.8600000143051147, time = 0.33751487731933594\n",
      "Testing at step=4, batch=10, test loss = 0.62893146276474, test acc = 0.7799999713897705, time = 0.3385884761810303\n",
      "Testing at step=4, batch=15, test loss = 0.5907430052757263, test acc = 0.7900000214576721, time = 0.3386094570159912\n",
      "Testing at step=4, batch=20, test loss = 0.50588059425354, test acc = 0.8199999928474426, time = 0.3397195339202881\n",
      "Testing at step=4, batch=25, test loss = 0.4083746671676636, test acc = 0.8199999928474426, time = 0.3381941318511963\n",
      "Testing at step=4, batch=30, test loss = 0.5479294061660767, test acc = 0.8100000023841858, time = 0.3428683280944824\n",
      "Testing at step=4, batch=35, test loss = 0.41830945014953613, test acc = 0.8700000047683716, time = 0.3394434452056885\n",
      "Testing at step=4, batch=40, test loss = 0.46126601099967957, test acc = 0.8349999785423279, time = 0.34072279930114746\n",
      "Testing at step=4, batch=45, test loss = 0.6161686778068542, test acc = 0.7850000262260437, time = 0.3390474319458008\n",
      "Step 4 finished in 309.8711631298065, Train loss = 0.47289028922716775, Test loss = 0.5343732243776321; Train Acc = 0.8286999988555909, Test Acc = 0.8136999976634979\n",
      "Training at step=5, batch=0, train loss = 0.5634426474571228, train acc = 0.824999988079071, time = 0.9397540092468262\n",
      "Training at step=5, batch=30, train loss = 0.45028725266456604, train acc = 0.8399999737739563, time = 0.9338197708129883\n",
      "Training at step=5, batch=60, train loss = 0.494974285364151, train acc = 0.7950000166893005, time = 0.938976526260376\n",
      "Training at step=5, batch=90, train loss = 0.3900342583656311, train acc = 0.8399999737739563, time = 0.9424612522125244\n",
      "Training at step=5, batch=120, train loss = 0.5259581804275513, train acc = 0.8149999976158142, time = 0.9357266426086426\n",
      "Training at step=5, batch=150, train loss = 0.46463683247566223, train acc = 0.8149999976158142, time = 0.9350235462188721\n",
      "Training at step=5, batch=180, train loss = 0.479614794254303, train acc = 0.7900000214576721, time = 0.9401164054870605\n",
      "Training at step=5, batch=210, train loss = 0.4682787358760834, train acc = 0.8149999976158142, time = 0.9399359226226807\n",
      "Training at step=5, batch=240, train loss = 0.4904974400997162, train acc = 0.8349999785423279, time = 0.939408540725708\n",
      "Training at step=5, batch=270, train loss = 0.32988259196281433, train acc = 0.875, time = 0.9358923435211182\n",
      "Testing at step=5, batch=0, test loss = 0.5944109559059143, test acc = 0.7950000166893005, time = 0.3382563591003418\n",
      "Testing at step=5, batch=5, test loss = 0.4190615713596344, test acc = 0.8450000286102295, time = 0.3376157283782959\n",
      "Testing at step=5, batch=10, test loss = 0.3346959054470062, test acc = 0.8700000047683716, time = 0.35880446434020996\n",
      "Testing at step=5, batch=15, test loss = 0.4731779396533966, test acc = 0.8299999833106995, time = 0.36818838119506836\n",
      "Testing at step=5, batch=20, test loss = 0.5978025794029236, test acc = 0.8149999976158142, time = 0.3626880645751953\n",
      "Testing at step=5, batch=25, test loss = 0.4956555962562561, test acc = 0.8199999928474426, time = 0.35680198669433594\n",
      "Testing at step=5, batch=30, test loss = 0.5570885539054871, test acc = 0.7649999856948853, time = 0.3612790107727051\n",
      "Testing at step=5, batch=35, test loss = 0.37147387862205505, test acc = 0.8949999809265137, time = 0.3634617328643799\n",
      "Testing at step=5, batch=40, test loss = 0.5354249477386475, test acc = 0.7850000262260437, time = 0.3650553226470947\n",
      "Testing at step=5, batch=45, test loss = 0.49258095026016235, test acc = 0.8299999833106995, time = 0.3608591556549072\n",
      "Step 5 finished in 311.051944732666, Train loss = 0.44758581707874934, Test loss = 0.4689279896020889; Train Acc = 0.8405166681607564, Test Acc = 0.8318999969959259\n",
      "Training at step=6, batch=0, train loss = 0.37356966733932495, train acc = 0.8600000143051147, time = 0.9572372436523438\n",
      "Training at step=6, batch=30, train loss = 0.34941989183425903, train acc = 0.8500000238418579, time = 0.9365651607513428\n",
      "Training at step=6, batch=60, train loss = 0.40017956495285034, train acc = 0.8600000143051147, time = 0.9363822937011719\n",
      "Training at step=6, batch=90, train loss = 0.45768678188323975, train acc = 0.8349999785423279, time = 0.953596830368042\n",
      "Training at step=6, batch=120, train loss = 0.3658718764781952, train acc = 0.8550000190734863, time = 0.9348533153533936\n",
      "Training at step=6, batch=150, train loss = 0.4045048952102661, train acc = 0.8700000047683716, time = 0.9388566017150879\n",
      "Training at step=6, batch=180, train loss = 0.4924551248550415, train acc = 0.800000011920929, time = 0.9363763332366943\n",
      "Training at step=6, batch=210, train loss = 0.2607940435409546, train acc = 0.9150000214576721, time = 0.9377384185791016\n",
      "Training at step=6, batch=240, train loss = 0.39243409037590027, train acc = 0.8600000143051147, time = 0.9647488594055176\n",
      "Training at step=6, batch=270, train loss = 0.5131052136421204, train acc = 0.7950000166893005, time = 0.9352681636810303\n",
      "Testing at step=6, batch=0, test loss = 0.5143383741378784, test acc = 0.8199999928474426, time = 0.3371560573577881\n",
      "Testing at step=6, batch=5, test loss = 0.5223510265350342, test acc = 0.800000011920929, time = 0.3423585891723633\n",
      "Testing at step=6, batch=10, test loss = 0.36105915904045105, test acc = 0.8550000190734863, time = 0.3373868465423584\n",
      "Testing at step=6, batch=15, test loss = 0.5372439622879028, test acc = 0.7900000214576721, time = 0.34087085723876953\n",
      "Testing at step=6, batch=20, test loss = 0.525912880897522, test acc = 0.8349999785423279, time = 0.34035682678222656\n",
      "Testing at step=6, batch=25, test loss = 0.48021987080574036, test acc = 0.8399999737739563, time = 0.3486611843109131\n",
      "Testing at step=6, batch=30, test loss = 0.5836953520774841, test acc = 0.800000011920929, time = 0.34113407135009766\n",
      "Testing at step=6, batch=35, test loss = 0.47460633516311646, test acc = 0.8450000286102295, time = 0.3357219696044922\n",
      "Testing at step=6, batch=40, test loss = 0.37046799063682556, test acc = 0.8650000095367432, time = 0.3366382122039795\n",
      "Testing at step=6, batch=45, test loss = 0.45934611558914185, test acc = 0.8399999737739563, time = 0.3407249450683594\n",
      "Step 6 finished in 310.08094334602356, Train loss = 0.43452675292889276, Test loss = 0.4878337091207504; Train Acc = 0.8436166689793269, Test Acc = 0.8220999991893768\n",
      "Training at step=7, batch=0, train loss = 0.43872925639152527, train acc = 0.8600000143051147, time = 0.9386041164398193\n",
      "Training at step=7, batch=30, train loss = 0.4571903944015503, train acc = 0.8700000047683716, time = 0.937244176864624\n",
      "Training at step=7, batch=60, train loss = 0.4185717701911926, train acc = 0.8399999737739563, time = 0.9394309520721436\n",
      "Training at step=7, batch=90, train loss = 0.3915994167327881, train acc = 0.8650000095367432, time = 0.9411122798919678\n",
      "Training at step=7, batch=120, train loss = 0.5228777527809143, train acc = 0.8149999976158142, time = 0.9353461265563965\n",
      "Training at step=7, batch=150, train loss = 0.4424373507499695, train acc = 0.824999988079071, time = 0.959575891494751\n",
      "Training at step=7, batch=180, train loss = 0.49390366673469543, train acc = 0.7950000166893005, time = 0.9407961368560791\n",
      "Training at step=7, batch=210, train loss = 0.3813292384147644, train acc = 0.8899999856948853, time = 0.9417505264282227\n",
      "Training at step=7, batch=240, train loss = 0.41554591059684753, train acc = 0.8600000143051147, time = 0.9360640048980713\n",
      "Training at step=7, batch=270, train loss = 0.3830977976322174, train acc = 0.8650000095367432, time = 0.9317827224731445\n",
      "Testing at step=7, batch=0, test loss = 0.35113784670829773, test acc = 0.8650000095367432, time = 0.34030604362487793\n",
      "Testing at step=7, batch=5, test loss = 0.4645135998725891, test acc = 0.8450000286102295, time = 0.34292054176330566\n",
      "Testing at step=7, batch=10, test loss = 0.4762398600578308, test acc = 0.8299999833106995, time = 0.3432962894439697\n",
      "Testing at step=7, batch=15, test loss = 0.5595568418502808, test acc = 0.7850000262260437, time = 0.3425922393798828\n",
      "Testing at step=7, batch=20, test loss = 0.45999327301979065, test acc = 0.8349999785423279, time = 0.3422267436981201\n",
      "Testing at step=7, batch=25, test loss = 0.47451838850975037, test acc = 0.8399999737739563, time = 0.34027981758117676\n",
      "Testing at step=7, batch=30, test loss = 0.4559875428676605, test acc = 0.8349999785423279, time = 0.34165525436401367\n",
      "Testing at step=7, batch=35, test loss = 0.37984612584114075, test acc = 0.8700000047683716, time = 0.34179019927978516\n",
      "Testing at step=7, batch=40, test loss = 0.42223989963531494, test acc = 0.8600000143051147, time = 0.34096860885620117\n",
      "Testing at step=7, batch=45, test loss = 0.3690585792064667, test acc = 0.8799999952316284, time = 0.34369897842407227\n",
      "Step 7 finished in 310.28024888038635, Train loss = 0.42833946327368416, Test loss = 0.4446519029140472; Train Acc = 0.8453000017007192, Test Acc = 0.8397000014781952\n",
      "Training at step=8, batch=0, train loss = 0.4249233603477478, train acc = 0.8600000143051147, time = 0.9363386631011963\n",
      "Training at step=8, batch=30, train loss = 0.4530143737792969, train acc = 0.8299999833106995, time = 0.9449419975280762\n",
      "Training at step=8, batch=60, train loss = 0.32870379090309143, train acc = 0.8949999809265137, time = 0.9460849761962891\n",
      "Training at step=8, batch=90, train loss = 0.4064846932888031, train acc = 0.8650000095367432, time = 0.9452142715454102\n",
      "Training at step=8, batch=120, train loss = 0.29246610403060913, train acc = 0.875, time = 0.9405412673950195\n",
      "Training at step=8, batch=150, train loss = 0.3815300762653351, train acc = 0.8700000047683716, time = 0.9317922592163086\n",
      "Training at step=8, batch=180, train loss = 0.5524840354919434, train acc = 0.8050000071525574, time = 1.0083093643188477\n",
      "Training at step=8, batch=210, train loss = 0.3636670410633087, train acc = 0.875, time = 0.933788537979126\n",
      "Training at step=8, batch=240, train loss = 0.389840692281723, train acc = 0.8700000047683716, time = 0.935333251953125\n",
      "Training at step=8, batch=270, train loss = 0.4042898416519165, train acc = 0.8799999952316284, time = 0.935826301574707\n",
      "Testing at step=8, batch=0, test loss = 0.5173552632331848, test acc = 0.824999988079071, time = 0.3405568599700928\n",
      "Testing at step=8, batch=5, test loss = 0.5024318099021912, test acc = 0.8199999928474426, time = 0.3413870334625244\n",
      "Testing at step=8, batch=10, test loss = 0.4614609479904175, test acc = 0.8100000023841858, time = 0.33704161643981934\n",
      "Testing at step=8, batch=15, test loss = 0.4094748795032501, test acc = 0.8650000095367432, time = 0.34659862518310547\n",
      "Testing at step=8, batch=20, test loss = 0.5489149689674377, test acc = 0.800000011920929, time = 0.3403456211090088\n",
      "Testing at step=8, batch=25, test loss = 0.46099215745925903, test acc = 0.8399999737739563, time = 0.34229302406311035\n",
      "Testing at step=8, batch=30, test loss = 0.474595308303833, test acc = 0.824999988079071, time = 0.3557260036468506\n",
      "Testing at step=8, batch=35, test loss = 0.4435648024082184, test acc = 0.8399999737739563, time = 0.3383018970489502\n",
      "Testing at step=8, batch=40, test loss = 0.43762749433517456, test acc = 0.8450000286102295, time = 0.3401334285736084\n",
      "Testing at step=8, batch=45, test loss = 0.4259156882762909, test acc = 0.8500000238418579, time = 0.35284972190856934\n",
      "Step 8 finished in 310.0308301448822, Train loss = 0.41352867225805917, Test loss = 0.48942054867744444; Train Acc = 0.8508500005801519, Test Acc = 0.82299999833107\n",
      "Training at step=9, batch=0, train loss = 0.4952884316444397, train acc = 0.8399999737739563, time = 0.9376823902130127\n",
      "Training at step=9, batch=30, train loss = 0.5070008635520935, train acc = 0.8600000143051147, time = 0.937382698059082\n",
      "Training at step=9, batch=60, train loss = 0.3657212555408478, train acc = 0.8650000095367432, time = 0.9436624050140381\n",
      "Training at step=9, batch=90, train loss = 0.4029735326766968, train acc = 0.8550000190734863, time = 0.937230110168457\n",
      "Training at step=9, batch=120, train loss = 0.41769734025001526, train acc = 0.8550000190734863, time = 0.940082311630249\n",
      "Training at step=9, batch=150, train loss = 0.29930007457733154, train acc = 0.8949999809265137, time = 0.9400677680969238\n",
      "Training at step=9, batch=180, train loss = 0.3298719823360443, train acc = 0.8849999904632568, time = 0.9424145221710205\n",
      "Training at step=9, batch=210, train loss = 0.4888996481895447, train acc = 0.8500000238418579, time = 0.9367625713348389\n",
      "Training at step=9, batch=240, train loss = 0.43133050203323364, train acc = 0.8399999737739563, time = 0.9399917125701904\n",
      "Training at step=9, batch=270, train loss = 0.31760653853416443, train acc = 0.8949999809265137, time = 0.9411709308624268\n",
      "Testing at step=9, batch=0, test loss = 0.4796988368034363, test acc = 0.8600000143051147, time = 0.3399839401245117\n",
      "Testing at step=9, batch=5, test loss = 0.474253386259079, test acc = 0.8100000023841858, time = 0.33767104148864746\n",
      "Testing at step=9, batch=10, test loss = 0.3978610932826996, test acc = 0.8700000047683716, time = 0.33582377433776855\n",
      "Testing at step=9, batch=15, test loss = 0.6025837063789368, test acc = 0.800000011920929, time = 0.3343997001647949\n",
      "Testing at step=9, batch=20, test loss = 0.3947415053844452, test acc = 0.8600000143051147, time = 0.3375420570373535\n",
      "Testing at step=9, batch=25, test loss = 0.33778437972068787, test acc = 0.8650000095367432, time = 0.33370351791381836\n",
      "Testing at step=9, batch=30, test loss = 0.39220723509788513, test acc = 0.8500000238418579, time = 0.3368086814880371\n",
      "Testing at step=9, batch=35, test loss = 0.410313218832016, test acc = 0.8450000286102295, time = 0.33962416648864746\n",
      "Testing at step=9, batch=40, test loss = 0.37668707966804504, test acc = 0.8600000143051147, time = 0.3411240577697754\n",
      "Testing at step=9, batch=45, test loss = 0.4219478368759155, test acc = 0.875, time = 0.3399477005004883\n",
      "Step 9 finished in 309.5019724369049, Train loss = 0.40131776124238966, Test loss = 0.4411370670795441; Train Acc = 0.8551333326101304, Test Acc = 0.8437000024318695\n",
      "Training at step=10, batch=0, train loss = 0.45095235109329224, train acc = 0.8500000238418579, time = 0.9351475238800049\n",
      "Training at step=10, batch=30, train loss = 0.41259679198265076, train acc = 0.8650000095367432, time = 0.9302978515625\n",
      "Training at step=10, batch=60, train loss = 0.4196808934211731, train acc = 0.8700000047683716, time = 0.9361701011657715\n",
      "Training at step=10, batch=90, train loss = 0.3468223810195923, train acc = 0.8650000095367432, time = 0.9498786926269531\n",
      "Training at step=10, batch=120, train loss = 0.3524075448513031, train acc = 0.8799999952316284, time = 0.9594311714172363\n",
      "Training at step=10, batch=150, train loss = 0.4636645019054413, train acc = 0.8450000286102295, time = 0.9445376396179199\n",
      "Training at step=10, batch=180, train loss = 0.3905143439769745, train acc = 0.8450000286102295, time = 0.9325613975524902\n",
      "Training at step=10, batch=210, train loss = 0.41978657245635986, train acc = 0.8600000143051147, time = 0.9365603923797607\n",
      "Training at step=10, batch=240, train loss = 0.3707844018936157, train acc = 0.875, time = 0.944939374923706\n",
      "Training at step=10, batch=270, train loss = 0.38323062658309937, train acc = 0.8349999785423279, time = 0.9382798671722412\n",
      "Testing at step=10, batch=0, test loss = 0.48581257462501526, test acc = 0.8550000190734863, time = 0.34006237983703613\n",
      "Testing at step=10, batch=5, test loss = 0.4404861330986023, test acc = 0.824999988079071, time = 0.3367481231689453\n",
      "Testing at step=10, batch=10, test loss = 0.5259210467338562, test acc = 0.8050000071525574, time = 0.3372342586517334\n",
      "Testing at step=10, batch=15, test loss = 0.36390089988708496, test acc = 0.8600000143051147, time = 0.336073637008667\n",
      "Testing at step=10, batch=20, test loss = 0.4249964952468872, test acc = 0.8299999833106995, time = 0.3348515033721924\n",
      "Testing at step=10, batch=25, test loss = 0.515057384967804, test acc = 0.8299999833106995, time = 0.338115930557251\n",
      "Testing at step=10, batch=30, test loss = 0.4724746346473694, test acc = 0.8399999737739563, time = 0.33754539489746094\n",
      "Testing at step=10, batch=35, test loss = 0.43267735838890076, test acc = 0.8600000143051147, time = 0.34095215797424316\n",
      "Testing at step=10, batch=40, test loss = 0.38448187708854675, test acc = 0.8700000047683716, time = 0.336397647857666\n",
      "Testing at step=10, batch=45, test loss = 0.5076762437820435, test acc = 0.8149999976158142, time = 0.33522534370422363\n",
      "Step 10 finished in 308.6220200061798, Train loss = 0.3932539436717828, Test loss = 0.4569488435983658; Train Acc = 0.857683334549268, Test Acc = 0.8383999979496002\n",
      "Training at step=11, batch=0, train loss = 0.525031566619873, train acc = 0.8199999928474426, time = 0.9454810619354248\n",
      "Training at step=11, batch=30, train loss = 0.3971358835697174, train acc = 0.8999999761581421, time = 0.9529218673706055\n",
      "Training at step=11, batch=60, train loss = 0.4147067368030548, train acc = 0.8650000095367432, time = 0.935929536819458\n",
      "Training at step=11, batch=90, train loss = 0.3208402693271637, train acc = 0.8849999904632568, time = 0.9316151142120361\n",
      "Training at step=11, batch=120, train loss = 0.5622550249099731, train acc = 0.8349999785423279, time = 0.9376986026763916\n",
      "Training at step=11, batch=150, train loss = 0.4356343448162079, train acc = 0.8600000143051147, time = 0.9372744560241699\n",
      "Training at step=11, batch=180, train loss = 0.36208420991897583, train acc = 0.875, time = 0.9341363906860352\n",
      "Training at step=11, batch=210, train loss = 0.41110023856163025, train acc = 0.8550000190734863, time = 0.935246467590332\n",
      "Training at step=11, batch=240, train loss = 0.3243024945259094, train acc = 0.8799999952316284, time = 0.9404618740081787\n",
      "Training at step=11, batch=270, train loss = 0.4463879466056824, train acc = 0.8149999976158142, time = 0.9394278526306152\n",
      "Testing at step=11, batch=0, test loss = 0.4812721610069275, test acc = 0.8050000071525574, time = 0.3372776508331299\n",
      "Testing at step=11, batch=5, test loss = 0.4584938883781433, test acc = 0.8500000238418579, time = 0.3362753391265869\n",
      "Testing at step=11, batch=10, test loss = 0.35942474007606506, test acc = 0.8700000047683716, time = 0.3367574214935303\n",
      "Testing at step=11, batch=15, test loss = 0.4477642774581909, test acc = 0.8550000190734863, time = 0.3391239643096924\n",
      "Testing at step=11, batch=20, test loss = 0.4347440004348755, test acc = 0.8500000238418579, time = 0.3369922637939453\n",
      "Testing at step=11, batch=25, test loss = 0.3302968144416809, test acc = 0.8799999952316284, time = 0.3433082103729248\n",
      "Testing at step=11, batch=30, test loss = 0.3788459897041321, test acc = 0.8650000095367432, time = 0.3659079074859619\n",
      "Testing at step=11, batch=35, test loss = 0.3893497586250305, test acc = 0.8550000190734863, time = 0.3611745834350586\n",
      "Testing at step=11, batch=40, test loss = 0.4256807267665863, test acc = 0.8500000238418579, time = 0.3614926338195801\n",
      "Testing at step=11, batch=45, test loss = 0.44804930686950684, test acc = 0.8399999737739563, time = 0.3394629955291748\n",
      "Step 11 finished in 309.5100622177124, Train loss = 0.39280681158105535, Test loss = 0.4135518062114716; Train Acc = 0.8586999992529551, Test Acc = 0.8522000026702881\n",
      "Training at step=12, batch=0, train loss = 0.337538480758667, train acc = 0.875, time = 0.9429440498352051\n",
      "Training at step=12, batch=30, train loss = 0.3600521385669708, train acc = 0.8600000143051147, time = 0.9723141193389893\n",
      "Training at step=12, batch=60, train loss = 0.29475265741348267, train acc = 0.9100000262260437, time = 0.9494428634643555\n",
      "Training at step=12, batch=90, train loss = 0.32244303822517395, train acc = 0.9100000262260437, time = 0.9459981918334961\n",
      "Training at step=12, batch=120, train loss = 0.35810086131095886, train acc = 0.8700000047683716, time = 0.9419445991516113\n",
      "Training at step=12, batch=150, train loss = 0.3868931531906128, train acc = 0.8450000286102295, time = 0.9388301372528076\n",
      "Training at step=12, batch=180, train loss = 0.32106277346611023, train acc = 0.8700000047683716, time = 0.9426124095916748\n",
      "Training at step=12, batch=210, train loss = 0.4148126244544983, train acc = 0.8550000190734863, time = 0.9386696815490723\n",
      "Training at step=12, batch=240, train loss = 0.33965519070625305, train acc = 0.8849999904632568, time = 0.9374265670776367\n",
      "Training at step=12, batch=270, train loss = 0.3875829577445984, train acc = 0.8600000143051147, time = 0.937657356262207\n",
      "Testing at step=12, batch=0, test loss = 0.36510032415390015, test acc = 0.8450000286102295, time = 0.3398592472076416\n",
      "Testing at step=12, batch=5, test loss = 0.40988609194755554, test acc = 0.8650000095367432, time = 0.3379495143890381\n",
      "Testing at step=12, batch=10, test loss = 0.5064655542373657, test acc = 0.8199999928474426, time = 0.33879923820495605\n",
      "Testing at step=12, batch=15, test loss = 0.5262196660041809, test acc = 0.8050000071525574, time = 0.3397252559661865\n",
      "Testing at step=12, batch=20, test loss = 0.35518133640289307, test acc = 0.875, time = 0.34266138076782227\n",
      "Testing at step=12, batch=25, test loss = 0.2818392515182495, test acc = 0.8999999761581421, time = 0.33909082412719727\n",
      "Testing at step=12, batch=30, test loss = 0.36446520686149597, test acc = 0.8600000143051147, time = 0.337329626083374\n",
      "Testing at step=12, batch=35, test loss = 0.3795776665210724, test acc = 0.8700000047683716, time = 0.33971190452575684\n",
      "Testing at step=12, batch=40, test loss = 0.3968967795372009, test acc = 0.8550000190734863, time = 0.3432447910308838\n",
      "Testing at step=12, batch=45, test loss = 0.4613778591156006, test acc = 0.8450000286102295, time = 0.34362006187438965\n",
      "Step 12 finished in 310.39442253112793, Train loss = 0.38246973921855293, Test loss = 0.41300652384757996; Train Acc = 0.8622833345333735, Test Acc = 0.8525\n",
      "Training at step=13, batch=0, train loss = 0.3001781702041626, train acc = 0.8949999809265137, time = 0.9369935989379883\n",
      "Training at step=13, batch=30, train loss = 0.43142199516296387, train acc = 0.8450000286102295, time = 0.935250997543335\n",
      "Training at step=13, batch=60, train loss = 0.26357975602149963, train acc = 0.8999999761581421, time = 0.9418411254882812\n",
      "Training at step=13, batch=90, train loss = 0.4062100946903229, train acc = 0.8399999737739563, time = 0.9617576599121094\n",
      "Training at step=13, batch=120, train loss = 0.5051947236061096, train acc = 0.8149999976158142, time = 0.9383077621459961\n",
      "Training at step=13, batch=150, train loss = 0.4012298583984375, train acc = 0.8849999904632568, time = 0.9326331615447998\n",
      "Training at step=13, batch=180, train loss = 0.3526185154914856, train acc = 0.8949999809265137, time = 0.9335415363311768\n",
      "Training at step=13, batch=210, train loss = 0.3605826497077942, train acc = 0.8450000286102295, time = 0.9439215660095215\n",
      "Training at step=13, batch=240, train loss = 0.31937557458877563, train acc = 0.9100000262260437, time = 0.9461302757263184\n",
      "Training at step=13, batch=270, train loss = 0.3532772362232208, train acc = 0.8899999856948853, time = 0.9377350807189941\n",
      "Testing at step=13, batch=0, test loss = 0.3892909288406372, test acc = 0.8600000143051147, time = 0.34612536430358887\n",
      "Testing at step=13, batch=5, test loss = 0.38830938935279846, test acc = 0.8650000095367432, time = 0.34220457077026367\n",
      "Testing at step=13, batch=10, test loss = 0.4586546719074249, test acc = 0.8149999976158142, time = 0.3426847457885742\n",
      "Testing at step=13, batch=15, test loss = 0.3458520770072937, test acc = 0.8849999904632568, time = 0.3513927459716797\n",
      "Testing at step=13, batch=20, test loss = 0.5099212527275085, test acc = 0.8100000023841858, time = 0.33869004249572754\n",
      "Testing at step=13, batch=25, test loss = 0.5203941464424133, test acc = 0.8100000023841858, time = 0.3395097255706787\n",
      "Testing at step=13, batch=30, test loss = 0.41854867339134216, test acc = 0.8299999833106995, time = 0.34398746490478516\n",
      "Testing at step=13, batch=35, test loss = 0.4951549172401428, test acc = 0.800000011920929, time = 0.339921236038208\n",
      "Testing at step=13, batch=40, test loss = 0.41878142952919006, test acc = 0.8650000095367432, time = 0.34989023208618164\n",
      "Testing at step=13, batch=45, test loss = 0.36494094133377075, test acc = 0.8849999904632568, time = 0.3379638195037842\n",
      "Step 13 finished in 310.33257484436035, Train loss = 0.38089189315835636, Test loss = 0.433855043053627; Train Acc = 0.8618833351135254, Test Acc = 0.8425000035762786\n",
      "Training at step=14, batch=0, train loss = 0.40473559498786926, train acc = 0.8399999737739563, time = 0.9400472640991211\n",
      "Training at step=14, batch=30, train loss = 0.3499322831630707, train acc = 0.875, time = 0.9358541965484619\n",
      "Training at step=14, batch=60, train loss = 0.32309916615486145, train acc = 0.8799999952316284, time = 0.9394004344940186\n",
      "Training at step=14, batch=90, train loss = 0.4126160442829132, train acc = 0.8700000047683716, time = 0.9363713264465332\n",
      "Training at step=14, batch=120, train loss = 0.4787771701812744, train acc = 0.8349999785423279, time = 0.9426441192626953\n",
      "Training at step=14, batch=150, train loss = 0.24032177031040192, train acc = 0.9200000166893005, time = 0.942955493927002\n",
      "Training at step=14, batch=180, train loss = 0.30070361495018005, train acc = 0.8899999856948853, time = 0.9358258247375488\n",
      "Training at step=14, batch=210, train loss = 0.42707979679107666, train acc = 0.8500000238418579, time = 0.9360523223876953\n",
      "Training at step=14, batch=240, train loss = 0.4054366648197174, train acc = 0.8600000143051147, time = 0.9361996650695801\n",
      "Training at step=14, batch=270, train loss = 0.38189223408699036, train acc = 0.8650000095367432, time = 0.9442849159240723\n",
      "Testing at step=14, batch=0, test loss = 0.5674417614936829, test acc = 0.8149999976158142, time = 0.343064546585083\n",
      "Testing at step=14, batch=5, test loss = 0.3972436487674713, test acc = 0.8600000143051147, time = 0.34003710746765137\n",
      "Testing at step=14, batch=10, test loss = 0.3523145020008087, test acc = 0.875, time = 0.3401498794555664\n",
      "Testing at step=14, batch=15, test loss = 0.30316510796546936, test acc = 0.8949999809265137, time = 0.33960938453674316\n",
      "Testing at step=14, batch=20, test loss = 0.4342309534549713, test acc = 0.8349999785423279, time = 0.33771395683288574\n",
      "Testing at step=14, batch=25, test loss = 0.39351847767829895, test acc = 0.8700000047683716, time = 0.3426203727722168\n",
      "Testing at step=14, batch=30, test loss = 0.35933026671409607, test acc = 0.8450000286102295, time = 0.34084534645080566\n",
      "Testing at step=14, batch=35, test loss = 0.5122123956680298, test acc = 0.8199999928474426, time = 0.36217188835144043\n",
      "Testing at step=14, batch=40, test loss = 0.30525171756744385, test acc = 0.8999999761581421, time = 0.33843374252319336\n",
      "Testing at step=14, batch=45, test loss = 0.4570622742176056, test acc = 0.8399999737739563, time = 0.33716678619384766\n",
      "Step 14 finished in 309.78665685653687, Train loss = 0.3734522819519043, Test loss = 0.4083119767904282; Train Acc = 0.8655666673183441, Test Acc = 0.8525\n",
      "Training at step=15, batch=0, train loss = 0.40618082880973816, train acc = 0.8349999785423279, time = 0.9413487911224365\n",
      "Training at step=15, batch=30, train loss = 0.35696619749069214, train acc = 0.8600000143051147, time = 0.9324562549591064\n",
      "Training at step=15, batch=60, train loss = 0.3490943908691406, train acc = 0.8799999952316284, time = 0.9347636699676514\n",
      "Training at step=15, batch=90, train loss = 0.32736802101135254, train acc = 0.8949999809265137, time = 0.9400193691253662\n",
      "Training at step=15, batch=120, train loss = 0.25939249992370605, train acc = 0.8999999761581421, time = 0.9942560195922852\n",
      "Training at step=15, batch=150, train loss = 0.2776011526584625, train acc = 0.8999999761581421, time = 0.9358105659484863\n",
      "Training at step=15, batch=180, train loss = 0.2850044369697571, train acc = 0.8899999856948853, time = 0.9416816234588623\n",
      "Training at step=15, batch=210, train loss = 0.34678661823272705, train acc = 0.8799999952316284, time = 0.9377720355987549\n",
      "Training at step=15, batch=240, train loss = 0.29823100566864014, train acc = 0.8899999856948853, time = 0.9457540512084961\n",
      "Training at step=15, batch=270, train loss = 0.32614368200302124, train acc = 0.875, time = 0.935183048248291\n",
      "Testing at step=15, batch=0, test loss = 0.42389586567878723, test acc = 0.8299999833106995, time = 0.33904504776000977\n",
      "Testing at step=15, batch=5, test loss = 0.43748462200164795, test acc = 0.8399999737739563, time = 0.3377346992492676\n",
      "Testing at step=15, batch=10, test loss = 0.33764827251434326, test acc = 0.8899999856948853, time = 0.3551216125488281\n",
      "Testing at step=15, batch=15, test loss = 0.35448959469795227, test acc = 0.8650000095367432, time = 0.34394049644470215\n",
      "Testing at step=15, batch=20, test loss = 0.39615997672080994, test acc = 0.8500000238418579, time = 0.3378145694732666\n",
      "Testing at step=15, batch=25, test loss = 0.3775264620780945, test acc = 0.8399999737739563, time = 0.35260558128356934\n",
      "Testing at step=15, batch=30, test loss = 0.3625946044921875, test acc = 0.8600000143051147, time = 0.36215806007385254\n",
      "Testing at step=15, batch=35, test loss = 0.43445420265197754, test acc = 0.8349999785423279, time = 0.36492347717285156\n",
      "Testing at step=15, batch=40, test loss = 0.47021228075027466, test acc = 0.8650000095367432, time = 0.3648691177368164\n",
      "Testing at step=15, batch=45, test loss = 0.44742080569267273, test acc = 0.8500000238418579, time = 0.3635375499725342\n",
      "Step 15 finished in 310.13094830513, Train loss = 0.3667571790516376, Test loss = 0.4068866539001465; Train Acc = 0.8670166691144308, Test Acc = 0.8516000032424926\n",
      "Training at step=16, batch=0, train loss = 0.3048543632030487, train acc = 0.8899999856948853, time = 0.9669959545135498\n",
      "Training at step=16, batch=30, train loss = 0.4366176724433899, train acc = 0.8450000286102295, time = 0.9337878227233887\n",
      "Training at step=16, batch=60, train loss = 0.3168167173862457, train acc = 0.8999999761581421, time = 0.9316320419311523\n",
      "Training at step=16, batch=90, train loss = 0.31729352474212646, train acc = 0.8949999809265137, time = 0.9354524612426758\n",
      "Training at step=16, batch=120, train loss = 0.39943554997444153, train acc = 0.8600000143051147, time = 0.9393258094787598\n",
      "Training at step=16, batch=150, train loss = 0.3403439223766327, train acc = 0.8700000047683716, time = 0.9558577537536621\n",
      "Training at step=16, batch=180, train loss = 0.3922545611858368, train acc = 0.8550000190734863, time = 0.9374935626983643\n",
      "Training at step=16, batch=210, train loss = 0.3206944167613983, train acc = 0.8650000095367432, time = 0.9600930213928223\n",
      "Training at step=16, batch=240, train loss = 0.3154275417327881, train acc = 0.9100000262260437, time = 0.9369680881500244\n",
      "Training at step=16, batch=270, train loss = 0.3400317430496216, train acc = 0.8949999809265137, time = 0.9492356777191162\n",
      "Testing at step=16, batch=0, test loss = 0.3978854715824127, test acc = 0.8450000286102295, time = 0.3402230739593506\n",
      "Testing at step=16, batch=5, test loss = 0.3614353835582733, test acc = 0.8849999904632568, time = 0.33960485458374023\n",
      "Testing at step=16, batch=10, test loss = 0.32666832208633423, test acc = 0.8799999952316284, time = 0.3364579677581787\n",
      "Testing at step=16, batch=15, test loss = 0.40988409519195557, test acc = 0.824999988079071, time = 0.33994030952453613\n",
      "Testing at step=16, batch=20, test loss = 0.339569091796875, test acc = 0.875, time = 0.3409430980682373\n",
      "Testing at step=16, batch=25, test loss = 0.494652658700943, test acc = 0.8199999928474426, time = 0.3415040969848633\n",
      "Testing at step=16, batch=30, test loss = 0.45453357696533203, test acc = 0.875, time = 0.3394052982330322\n",
      "Testing at step=16, batch=35, test loss = 0.3518582582473755, test acc = 0.8600000143051147, time = 0.3418762683868408\n",
      "Testing at step=16, batch=40, test loss = 0.39710888266563416, test acc = 0.8700000047683716, time = 0.3437354564666748\n",
      "Testing at step=16, batch=45, test loss = 0.3311656713485718, test acc = 0.8799999952316284, time = 0.3364381790161133\n",
      "Step 16 finished in 309.9698042869568, Train loss = 0.36514034966627756, Test loss = 0.40650291085243223; Train Acc = 0.8680000013113022, Test Acc = 0.8561999988555908\n",
      "Training at step=17, batch=0, train loss = 0.3182198107242584, train acc = 0.8700000047683716, time = 0.9349410533905029\n",
      "Training at step=17, batch=30, train loss = 0.42191392183303833, train acc = 0.8299999833106995, time = 0.9412474632263184\n",
      "Training at step=17, batch=60, train loss = 0.4404664933681488, train acc = 0.8299999833106995, time = 0.9432694911956787\n",
      "Training at step=17, batch=90, train loss = 0.41747403144836426, train acc = 0.8550000190734863, time = 0.9354250431060791\n",
      "Training at step=17, batch=120, train loss = 0.32433784008026123, train acc = 0.8799999952316284, time = 0.9403679370880127\n",
      "Training at step=17, batch=150, train loss = 0.4046687185764313, train acc = 0.8550000190734863, time = 0.940535306930542\n",
      "Training at step=17, batch=180, train loss = 0.3394964635372162, train acc = 0.8799999952316284, time = 0.9384245872497559\n",
      "Training at step=17, batch=210, train loss = 0.45903080701828003, train acc = 0.8399999737739563, time = 0.9359927177429199\n",
      "Training at step=17, batch=240, train loss = 0.372714638710022, train acc = 0.8500000238418579, time = 0.9383692741394043\n",
      "Training at step=17, batch=270, train loss = 0.31073203682899475, train acc = 0.9150000214576721, time = 0.9362537860870361\n",
      "Testing at step=17, batch=0, test loss = 0.39042022824287415, test acc = 0.8899999856948853, time = 0.33690476417541504\n",
      "Testing at step=17, batch=5, test loss = 0.30724087357521057, test acc = 0.8999999761581421, time = 0.3375692367553711\n",
      "Testing at step=17, batch=10, test loss = 0.37183433771133423, test acc = 0.8650000095367432, time = 0.33785057067871094\n",
      "Testing at step=17, batch=15, test loss = 0.38559722900390625, test acc = 0.8849999904632568, time = 0.33680105209350586\n",
      "Testing at step=17, batch=20, test loss = 0.3656834661960602, test acc = 0.8600000143051147, time = 0.33794116973876953\n",
      "Testing at step=17, batch=25, test loss = 0.4206210970878601, test acc = 0.8550000190734863, time = 0.3351426124572754\n",
      "Testing at step=17, batch=30, test loss = 0.35315096378326416, test acc = 0.8450000286102295, time = 0.3439474105834961\n",
      "Testing at step=17, batch=35, test loss = 0.40279537439346313, test acc = 0.8600000143051147, time = 0.37620115280151367\n",
      "Testing at step=17, batch=40, test loss = 0.3767206072807312, test acc = 0.8600000143051147, time = 0.33950161933898926\n",
      "Testing at step=17, batch=45, test loss = 0.4750741720199585, test acc = 0.8600000143051147, time = 0.3385167121887207\n",
      "Step 17 finished in 309.32621335983276, Train loss = 0.3590805585682392, Test loss = 0.3949188524484634; Train Acc = 0.86953333457311, Test Acc = 0.8602000010013581\n",
      "Training at step=18, batch=0, train loss = 0.2361120581626892, train acc = 0.9350000023841858, time = 0.9322292804718018\n",
      "Training at step=18, batch=30, train loss = 0.24000778794288635, train acc = 0.9150000214576721, time = 0.9313468933105469\n",
      "Training at step=18, batch=60, train loss = 0.4378964900970459, train acc = 0.8399999737739563, time = 0.9336879253387451\n",
      "Training at step=18, batch=90, train loss = 0.3612034320831299, train acc = 0.875, time = 0.9404559135437012\n",
      "Training at step=18, batch=120, train loss = 0.33944445848464966, train acc = 0.8799999952316284, time = 0.9405472278594971\n",
      "Training at step=18, batch=150, train loss = 0.3554559350013733, train acc = 0.8700000047683716, time = 0.9586811065673828\n",
      "Training at step=18, batch=180, train loss = 0.2964347004890442, train acc = 0.9049999713897705, time = 0.942516565322876\n",
      "Training at step=18, batch=210, train loss = 0.3325524628162384, train acc = 0.875, time = 0.939910888671875\n",
      "Training at step=18, batch=240, train loss = 0.30512744188308716, train acc = 0.8849999904632568, time = 0.9351251125335693\n",
      "Training at step=18, batch=270, train loss = 0.3408666253089905, train acc = 0.8849999904632568, time = 0.9606170654296875\n",
      "Testing at step=18, batch=0, test loss = 0.36030441522598267, test acc = 0.8899999856948853, time = 0.34359121322631836\n",
      "Testing at step=18, batch=5, test loss = 0.4534749686717987, test acc = 0.8450000286102295, time = 0.3395059108734131\n",
      "Testing at step=18, batch=10, test loss = 0.38561561703681946, test acc = 0.8550000190734863, time = 0.3477976322174072\n",
      "Testing at step=18, batch=15, test loss = 0.4121750295162201, test acc = 0.8500000238418579, time = 0.33875346183776855\n",
      "Testing at step=18, batch=20, test loss = 0.4004043638706207, test acc = 0.8550000190734863, time = 0.34279346466064453\n",
      "Testing at step=18, batch=25, test loss = 0.2832488715648651, test acc = 0.8949999809265137, time = 0.3445432186126709\n",
      "Testing at step=18, batch=30, test loss = 0.3898022174835205, test acc = 0.9049999713897705, time = 0.3397655487060547\n",
      "Testing at step=18, batch=35, test loss = 0.5377346277236938, test acc = 0.8100000023841858, time = 0.34505701065063477\n",
      "Testing at step=18, batch=40, test loss = 0.41076821088790894, test acc = 0.8500000238418579, time = 0.34444713592529297\n",
      "Testing at step=18, batch=45, test loss = 0.4137268364429474, test acc = 0.824999988079071, time = 0.34243249893188477\n",
      "Step 18 finished in 310.5963923931122, Train loss = 0.35408128996690114, Test loss = 0.4255604833364487; Train Acc = 0.8701666661103566, Test Acc = 0.8494999980926514\n",
      "Training at step=19, batch=0, train loss = 0.37579646706581116, train acc = 0.8899999856948853, time = 0.9418435096740723\n",
      "Training at step=19, batch=30, train loss = 0.39239969849586487, train acc = 0.8500000238418579, time = 0.9334049224853516\n",
      "Training at step=19, batch=60, train loss = 0.31964007019996643, train acc = 0.8700000047683716, time = 0.935988187789917\n",
      "Training at step=19, batch=90, train loss = 0.32427772879600525, train acc = 0.8949999809265137, time = 0.9321086406707764\n",
      "Training at step=19, batch=120, train loss = 0.3612300157546997, train acc = 0.8650000095367432, time = 0.9503135681152344\n",
      "Training at step=19, batch=150, train loss = 0.5017253756523132, train acc = 0.8149999976158142, time = 0.9347226619720459\n",
      "Training at step=19, batch=180, train loss = 0.3195510506629944, train acc = 0.9049999713897705, time = 0.9810845851898193\n",
      "Training at step=19, batch=210, train loss = 0.35905590653419495, train acc = 0.8650000095367432, time = 0.9321520328521729\n",
      "Training at step=19, batch=240, train loss = 0.3320121765136719, train acc = 0.8799999952316284, time = 0.9311552047729492\n",
      "Training at step=19, batch=270, train loss = 0.37666159868240356, train acc = 0.875, time = 0.9358644485473633\n",
      "Testing at step=19, batch=0, test loss = 0.31355151534080505, test acc = 0.8949999809265137, time = 0.3376476764678955\n",
      "Testing at step=19, batch=5, test loss = 0.39097461104393005, test acc = 0.8349999785423279, time = 0.36278390884399414\n",
      "Testing at step=19, batch=10, test loss = 0.40797221660614014, test acc = 0.8799999952316284, time = 0.34121227264404297\n",
      "Testing at step=19, batch=15, test loss = 0.4269944727420807, test acc = 0.8650000095367432, time = 0.33760547637939453\n",
      "Testing at step=19, batch=20, test loss = 0.46768704056739807, test acc = 0.8450000286102295, time = 0.35861754417419434\n",
      "Testing at step=19, batch=25, test loss = 0.3790868818759918, test acc = 0.8700000047683716, time = 0.3372795581817627\n",
      "Testing at step=19, batch=30, test loss = 0.3274361491203308, test acc = 0.9100000262260437, time = 0.3388237953186035\n",
      "Testing at step=19, batch=35, test loss = 0.3714199364185333, test acc = 0.8650000095367432, time = 0.3411064147949219\n",
      "Testing at step=19, batch=40, test loss = 0.38686850666999817, test acc = 0.8899999856948853, time = 0.3389742374420166\n",
      "Testing at step=19, batch=45, test loss = 0.34809234738349915, test acc = 0.8849999904632568, time = 0.33821558952331543\n",
      "Step 19 finished in 309.6927001476288, Train loss = 0.35147034709652264, Test loss = 0.38751808166503904; Train Acc = 0.8711333330472311, Test Acc = 0.8648000025749206\n",
      "Training at step=20, batch=0, train loss = 0.26842406392097473, train acc = 0.9049999713897705, time = 0.9400773048400879\n",
      "Training at step=20, batch=30, train loss = 0.3675498962402344, train acc = 0.8949999809265137, time = 0.9396045207977295\n",
      "Training at step=20, batch=60, train loss = 0.35500195622444153, train acc = 0.8700000047683716, time = 0.9342527389526367\n",
      "Training at step=20, batch=90, train loss = 0.3859440088272095, train acc = 0.8600000143051147, time = 0.9439589977264404\n",
      "Training at step=20, batch=120, train loss = 0.37115591764450073, train acc = 0.8600000143051147, time = 0.9379763603210449\n",
      "Training at step=20, batch=150, train loss = 0.332017183303833, train acc = 0.8700000047683716, time = 0.9435105323791504\n",
      "Training at step=20, batch=180, train loss = 0.3421367406845093, train acc = 0.8799999952316284, time = 0.940359354019165\n",
      "Training at step=20, batch=210, train loss = 0.39138656854629517, train acc = 0.8550000190734863, time = 0.9371845722198486\n",
      "Training at step=20, batch=240, train loss = 0.3309796452522278, train acc = 0.8899999856948853, time = 0.9604580402374268\n",
      "Training at step=20, batch=270, train loss = 0.37307488918304443, train acc = 0.824999988079071, time = 0.9607079029083252\n",
      "Testing at step=20, batch=0, test loss = 0.4126761257648468, test acc = 0.8399999737739563, time = 0.33898496627807617\n",
      "Testing at step=20, batch=5, test loss = 0.4431349039077759, test acc = 0.8299999833106995, time = 0.3370058536529541\n",
      "Testing at step=20, batch=10, test loss = 0.5143716931343079, test acc = 0.824999988079071, time = 0.33817529678344727\n",
      "Testing at step=20, batch=15, test loss = 0.3511470556259155, test acc = 0.8650000095367432, time = 0.3437821865081787\n",
      "Testing at step=20, batch=20, test loss = 0.4260382056236267, test acc = 0.8349999785423279, time = 0.3384425640106201\n",
      "Testing at step=20, batch=25, test loss = 0.4031327962875366, test acc = 0.8799999952316284, time = 0.3421030044555664\n",
      "Testing at step=20, batch=30, test loss = 0.454507052898407, test acc = 0.8399999737739563, time = 0.34238553047180176\n",
      "Testing at step=20, batch=35, test loss = 0.3539758622646332, test acc = 0.8700000047683716, time = 0.33744072914123535\n",
      "Testing at step=20, batch=40, test loss = 0.40807217359542847, test acc = 0.8399999737739563, time = 0.3423924446105957\n",
      "Testing at step=20, batch=45, test loss = 0.4069058299064636, test acc = 0.8550000190734863, time = 0.337169885635376\n",
      "Step 20 finished in 311.3695504665375, Train loss = 0.34950936287641526, Test loss = 0.4005784124135971; Train Acc = 0.873700000445048, Test Acc = 0.8561000001430511\n",
      "Training at step=21, batch=0, train loss = 0.3712248206138611, train acc = 0.8799999952316284, time = 0.9356458187103271\n",
      "Training at step=21, batch=30, train loss = 0.30573052167892456, train acc = 0.8999999761581421, time = 0.9385101795196533\n",
      "Training at step=21, batch=60, train loss = 0.29822033643722534, train acc = 0.8799999952316284, time = 0.9592607021331787\n",
      "Training at step=21, batch=90, train loss = 0.32714805006980896, train acc = 0.8899999856948853, time = 0.9337871074676514\n",
      "Training at step=21, batch=120, train loss = 0.33184361457824707, train acc = 0.8299999833106995, time = 0.9356577396392822\n",
      "Training at step=21, batch=150, train loss = 0.35325995087623596, train acc = 0.8899999856948853, time = 0.9375133514404297\n",
      "Training at step=21, batch=180, train loss = 0.32588160037994385, train acc = 0.8799999952316284, time = 0.9395883083343506\n",
      "Training at step=21, batch=210, train loss = 0.32900363206863403, train acc = 0.875, time = 0.9391665458679199\n",
      "Training at step=21, batch=240, train loss = 0.36115187406539917, train acc = 0.8349999785423279, time = 0.9365389347076416\n",
      "Training at step=21, batch=270, train loss = 0.3198467493057251, train acc = 0.875, time = 0.9365670680999756\n",
      "Testing at step=21, batch=0, test loss = 0.3987639546394348, test acc = 0.8700000047683716, time = 0.3510279655456543\n",
      "Testing at step=21, batch=5, test loss = 0.42907580733299255, test acc = 0.8349999785423279, time = 0.33976292610168457\n",
      "Testing at step=21, batch=10, test loss = 0.6768657565116882, test acc = 0.7699999809265137, time = 0.33808207511901855\n",
      "Testing at step=21, batch=15, test loss = 0.3941872715950012, test acc = 0.8700000047683716, time = 0.33682966232299805\n",
      "Testing at step=21, batch=20, test loss = 0.3677792251110077, test acc = 0.875, time = 0.3364276885986328\n",
      "Testing at step=21, batch=25, test loss = 0.42772507667541504, test acc = 0.8849999904632568, time = 0.33768296241760254\n",
      "Testing at step=21, batch=30, test loss = 0.48078662157058716, test acc = 0.8299999833106995, time = 0.3385624885559082\n",
      "Testing at step=21, batch=35, test loss = 0.5528408288955688, test acc = 0.824999988079071, time = 0.33918094635009766\n",
      "Testing at step=21, batch=40, test loss = 0.5093271136283875, test acc = 0.8450000286102295, time = 0.33800792694091797\n",
      "Testing at step=21, batch=45, test loss = 0.3323540985584259, test acc = 0.8799999952316284, time = 0.3388397693634033\n",
      "Step 21 finished in 309.72075843811035, Train loss = 0.34353576133648556, Test loss = 0.39401940882205966; Train Acc = 0.8749333345890045, Test Acc = 0.8630000042915345\n",
      "Training at step=22, batch=0, train loss = 0.3365565538406372, train acc = 0.8899999856948853, time = 0.9390666484832764\n",
      "Training at step=22, batch=30, train loss = 0.38260743021965027, train acc = 0.8600000143051147, time = 0.9414951801300049\n",
      "Training at step=22, batch=60, train loss = 0.30924922227859497, train acc = 0.8849999904632568, time = 0.9375340938568115\n",
      "Training at step=22, batch=90, train loss = 0.3205525577068329, train acc = 0.9049999713897705, time = 0.9759731292724609\n",
      "Training at step=22, batch=120, train loss = 0.4372921884059906, train acc = 0.8450000286102295, time = 0.9392921924591064\n",
      "Training at step=22, batch=150, train loss = 0.3576521575450897, train acc = 0.8799999952316284, time = 0.9718317985534668\n",
      "Training at step=22, batch=180, train loss = 0.2845776081085205, train acc = 0.8949999809265137, time = 0.9399683475494385\n",
      "Training at step=22, batch=210, train loss = 0.33247825503349304, train acc = 0.8650000095367432, time = 0.9360742568969727\n",
      "Training at step=22, batch=240, train loss = 0.4015904366970062, train acc = 0.8700000047683716, time = 0.9356639385223389\n",
      "Training at step=22, batch=270, train loss = 0.3839915096759796, train acc = 0.8550000190734863, time = 0.9703221321105957\n",
      "Testing at step=22, batch=0, test loss = 0.33710426092147827, test acc = 0.8849999904632568, time = 0.3354146480560303\n",
      "Testing at step=22, batch=5, test loss = 0.32039138674736023, test acc = 0.8949999809265137, time = 0.3350250720977783\n",
      "Testing at step=22, batch=10, test loss = 0.4280087649822235, test acc = 0.8600000143051147, time = 0.3396310806274414\n",
      "Testing at step=22, batch=15, test loss = 0.39415037631988525, test acc = 0.8600000143051147, time = 0.34530138969421387\n",
      "Testing at step=22, batch=20, test loss = 0.3882911801338196, test acc = 0.8700000047683716, time = 0.3373582363128662\n",
      "Testing at step=22, batch=25, test loss = 0.40179499983787537, test acc = 0.8550000190734863, time = 0.34085536003112793\n",
      "Testing at step=22, batch=30, test loss = 0.36749228835105896, test acc = 0.8500000238418579, time = 0.3420829772949219\n",
      "Testing at step=22, batch=35, test loss = 0.4371611773967743, test acc = 0.875, time = 0.34496617317199707\n",
      "Testing at step=22, batch=40, test loss = 0.3775408864021301, test acc = 0.8650000095367432, time = 0.3380904197692871\n",
      "Testing at step=22, batch=45, test loss = 0.5202162265777588, test acc = 0.8650000095367432, time = 0.3359506130218506\n",
      "Step 22 finished in 310.0458974838257, Train loss = 0.3402934557199478, Test loss = 0.40067907989025114; Train Acc = 0.8764500008026759, Test Acc = 0.8603000044822693\n",
      "Training at step=23, batch=0, train loss = 0.3230516314506531, train acc = 0.8949999809265137, time = 0.9335646629333496\n",
      "Training at step=23, batch=30, train loss = 0.3317643105983734, train acc = 0.875, time = 0.9811177253723145\n",
      "Training at step=23, batch=60, train loss = 0.38362109661102295, train acc = 0.8700000047683716, time = 0.9361121654510498\n",
      "Training at step=23, batch=90, train loss = 0.3297795057296753, train acc = 0.875, time = 0.940460205078125\n",
      "Training at step=23, batch=120, train loss = 0.35657694935798645, train acc = 0.8550000190734863, time = 0.9385809898376465\n",
      "Training at step=23, batch=150, train loss = 0.2945021688938141, train acc = 0.8999999761581421, time = 0.9501798152923584\n",
      "Training at step=23, batch=180, train loss = 0.3789258599281311, train acc = 0.8600000143051147, time = 0.9376873970031738\n",
      "Training at step=23, batch=210, train loss = 0.4224868416786194, train acc = 0.8600000143051147, time = 0.9380245208740234\n",
      "Training at step=23, batch=240, train loss = 0.265501469373703, train acc = 0.8899999856948853, time = 0.940582275390625\n",
      "Training at step=23, batch=270, train loss = 0.3385305404663086, train acc = 0.8849999904632568, time = 0.9461178779602051\n",
      "Testing at step=23, batch=0, test loss = 0.46769818663597107, test acc = 0.8199999928474426, time = 0.3621695041656494\n",
      "Testing at step=23, batch=5, test loss = 0.3951907753944397, test acc = 0.8700000047683716, time = 0.36148691177368164\n",
      "Testing at step=23, batch=10, test loss = 0.37524595856666565, test acc = 0.8650000095367432, time = 0.3646883964538574\n",
      "Testing at step=23, batch=15, test loss = 0.35081908106803894, test acc = 0.875, time = 0.35910558700561523\n",
      "Testing at step=23, batch=20, test loss = 0.29158109426498413, test acc = 0.8849999904632568, time = 0.35144639015197754\n",
      "Testing at step=23, batch=25, test loss = 0.4547092914581299, test acc = 0.8399999737739563, time = 0.34667015075683594\n",
      "Testing at step=23, batch=30, test loss = 0.3173954486846924, test acc = 0.8899999856948853, time = 0.36107516288757324\n",
      "Testing at step=23, batch=35, test loss = 0.36241790652275085, test acc = 0.8799999952316284, time = 0.36114931106567383\n",
      "Testing at step=23, batch=40, test loss = 0.4405292868614197, test acc = 0.8600000143051147, time = 0.36174631118774414\n",
      "Testing at step=23, batch=45, test loss = 0.4183427691459656, test acc = 0.8600000143051147, time = 0.4610424041748047\n",
      "Step 23 finished in 312.0307967662811, Train loss = 0.33487417389949165, Test loss = 0.3820480042695999; Train Acc = 0.8779166648785274, Test Acc = 0.8639999997615814\n",
      "Training at step=24, batch=0, train loss = 0.3079165518283844, train acc = 0.8899999856948853, time = 0.9364814758300781\n",
      "Training at step=24, batch=30, train loss = 0.30396145582199097, train acc = 0.8949999809265137, time = 0.9666013717651367\n",
      "Training at step=24, batch=60, train loss = 0.38095623254776, train acc = 0.8650000095367432, time = 0.9374761581420898\n",
      "Training at step=24, batch=90, train loss = 0.3556387424468994, train acc = 0.875, time = 0.9404723644256592\n",
      "Training at step=24, batch=120, train loss = 0.3485751748085022, train acc = 0.8899999856948853, time = 0.935887336730957\n",
      "Training at step=24, batch=150, train loss = 0.4757615923881531, train acc = 0.8050000071525574, time = 0.9426839351654053\n",
      "Training at step=24, batch=180, train loss = 0.410740464925766, train acc = 0.8700000047683716, time = 0.9454941749572754\n",
      "Training at step=24, batch=210, train loss = 0.32491549849510193, train acc = 0.8899999856948853, time = 0.9482343196868896\n",
      "Training at step=24, batch=240, train loss = 0.299797922372818, train acc = 0.8949999809265137, time = 0.9414858818054199\n",
      "Training at step=24, batch=270, train loss = 0.27272936701774597, train acc = 0.8999999761581421, time = 0.944500207901001\n",
      "Testing at step=24, batch=0, test loss = 0.26693886518478394, test acc = 0.925000011920929, time = 0.3406503200531006\n",
      "Testing at step=24, batch=5, test loss = 0.4842621684074402, test acc = 0.8450000286102295, time = 0.34895777702331543\n",
      "Testing at step=24, batch=10, test loss = 0.3428255319595337, test acc = 0.8550000190734863, time = 0.3402531147003174\n",
      "Testing at step=24, batch=15, test loss = 0.3818356394767761, test acc = 0.8799999952316284, time = 0.34079885482788086\n",
      "Testing at step=24, batch=20, test loss = 0.3820129334926605, test acc = 0.8600000143051147, time = 0.34286928176879883\n",
      "Testing at step=24, batch=25, test loss = 0.4108230471611023, test acc = 0.8349999785423279, time = 0.3418290615081787\n",
      "Testing at step=24, batch=30, test loss = 0.29950302839279175, test acc = 0.8999999761581421, time = 0.33677148818969727\n",
      "Testing at step=24, batch=35, test loss = 0.3848239481449127, test acc = 0.8600000143051147, time = 0.33965539932250977\n",
      "Testing at step=24, batch=40, test loss = 0.40627938508987427, test acc = 0.8700000047683716, time = 0.342639684677124\n",
      "Testing at step=24, batch=45, test loss = 0.4172070324420929, test acc = 0.8849999904632568, time = 0.3408029079437256\n",
      "Step 24 finished in 309.9614691734314, Train loss = 0.3349219997227192, Test loss = 0.3911008697748184; Train Acc = 0.878333331545194, Test Acc = 0.8612999987602233\n",
      "Training at step=25, batch=0, train loss = 0.41455134749412537, train acc = 0.8500000238418579, time = 0.9452848434448242\n",
      "Training at step=25, batch=30, train loss = 0.31017979979515076, train acc = 0.8799999952316284, time = 0.9426517486572266\n",
      "Training at step=25, batch=60, train loss = 0.34303417801856995, train acc = 0.8999999761581421, time = 0.9416425228118896\n",
      "Training at step=25, batch=90, train loss = 0.26568445563316345, train acc = 0.9200000166893005, time = 0.9400732517242432\n",
      "Training at step=25, batch=120, train loss = 0.3613605201244354, train acc = 0.8899999856948853, time = 0.9409027099609375\n",
      "Training at step=25, batch=150, train loss = 0.3569219708442688, train acc = 0.8899999856948853, time = 0.9565248489379883\n",
      "Training at step=25, batch=180, train loss = 0.31702739000320435, train acc = 0.875, time = 0.9362232685089111\n",
      "Training at step=25, batch=210, train loss = 0.30122604966163635, train acc = 0.8799999952316284, time = 0.9353287220001221\n",
      "Training at step=25, batch=240, train loss = 0.2733510732650757, train acc = 0.9150000214576721, time = 0.9359352588653564\n",
      "Training at step=25, batch=270, train loss = 0.3513355255126953, train acc = 0.8949999809265137, time = 0.9383671283721924\n",
      "Testing at step=25, batch=0, test loss = 0.3638419806957245, test acc = 0.8700000047683716, time = 0.33992481231689453\n",
      "Testing at step=25, batch=5, test loss = 0.33100631833076477, test acc = 0.8799999952316284, time = 0.34340453147888184\n",
      "Testing at step=25, batch=10, test loss = 0.4845079779624939, test acc = 0.824999988079071, time = 0.3414194583892822\n",
      "Testing at step=25, batch=15, test loss = 0.4303240180015564, test acc = 0.8500000238418579, time = 0.34109973907470703\n",
      "Testing at step=25, batch=20, test loss = 0.4011145830154419, test acc = 0.8550000190734863, time = 0.3375682830810547\n",
      "Testing at step=25, batch=25, test loss = 0.3486735224723816, test acc = 0.8849999904632568, time = 0.3381621837615967\n",
      "Testing at step=25, batch=30, test loss = 0.41120946407318115, test acc = 0.8399999737739563, time = 0.3496575355529785\n",
      "Testing at step=25, batch=35, test loss = 0.46277862787246704, test acc = 0.8550000190734863, time = 0.3401377201080322\n",
      "Testing at step=25, batch=40, test loss = 0.4640878438949585, test acc = 0.8450000286102295, time = 0.3465452194213867\n",
      "Testing at step=25, batch=45, test loss = 0.4413616955280304, test acc = 0.8299999833106995, time = 0.3402988910675049\n",
      "Step 25 finished in 310.2230408191681, Train loss = 0.33541601806879046, Test loss = 0.3816421601176262; Train Acc = 0.8770499976476034, Test Acc = 0.8620000076293945\n",
      "Training at step=26, batch=0, train loss = 0.32053428888320923, train acc = 0.8849999904632568, time = 0.9581019878387451\n",
      "Training at step=26, batch=30, train loss = 0.2572000026702881, train acc = 0.8999999761581421, time = 0.9414751529693604\n",
      "Training at step=26, batch=60, train loss = 0.39801785349845886, train acc = 0.8600000143051147, time = 0.9662549495697021\n",
      "Training at step=26, batch=90, train loss = 0.3359866440296173, train acc = 0.8849999904632568, time = 0.9432544708251953\n",
      "Training at step=26, batch=120, train loss = 0.35027173161506653, train acc = 0.8849999904632568, time = 0.9632508754730225\n",
      "Training at step=26, batch=150, train loss = 0.3288027048110962, train acc = 0.8899999856948853, time = 0.9338085651397705\n",
      "Training at step=26, batch=180, train loss = 0.348371684551239, train acc = 0.8849999904632568, time = 0.9636437892913818\n",
      "Training at step=26, batch=210, train loss = 0.4247373938560486, train acc = 0.8500000238418579, time = 0.9372005462646484\n",
      "Training at step=26, batch=240, train loss = 0.27379316091537476, train acc = 0.8949999809265137, time = 0.9474213123321533\n",
      "Training at step=26, batch=270, train loss = 0.3881351053714752, train acc = 0.8450000286102295, time = 0.9438571929931641\n",
      "Testing at step=26, batch=0, test loss = 0.3816942572593689, test acc = 0.8550000190734863, time = 0.34230470657348633\n",
      "Testing at step=26, batch=5, test loss = 0.4406157433986664, test acc = 0.8349999785423279, time = 0.3367180824279785\n",
      "Testing at step=26, batch=10, test loss = 0.4195803701877594, test acc = 0.8700000047683716, time = 0.34465742111206055\n",
      "Testing at step=26, batch=15, test loss = 0.36325398087501526, test acc = 0.875, time = 0.3462212085723877\n",
      "Testing at step=26, batch=20, test loss = 0.33097365498542786, test acc = 0.8949999809265137, time = 0.33867597579956055\n",
      "Testing at step=26, batch=25, test loss = 0.3904646933078766, test acc = 0.8650000095367432, time = 0.38719940185546875\n",
      "Testing at step=26, batch=30, test loss = 0.36715686321258545, test acc = 0.8500000238418579, time = 0.3503878116607666\n",
      "Testing at step=26, batch=35, test loss = 0.34107688069343567, test acc = 0.875, time = 0.33789896965026855\n",
      "Testing at step=26, batch=40, test loss = 0.3498641550540924, test acc = 0.8799999952316284, time = 0.3353290557861328\n",
      "Testing at step=26, batch=45, test loss = 0.4290826916694641, test acc = 0.8450000286102295, time = 0.33713388442993164\n",
      "Step 26 finished in 311.1021206378937, Train loss = 0.3260214290519555, Test loss = 0.37806212455034255; Train Acc = 0.8816166647275289, Test Acc = 0.8643000006675721\n",
      "Training at step=27, batch=0, train loss = 0.2844810485839844, train acc = 0.8650000095367432, time = 0.9362218379974365\n",
      "Training at step=27, batch=30, train loss = 0.34417733550071716, train acc = 0.8650000095367432, time = 0.938737154006958\n",
      "Training at step=27, batch=60, train loss = 0.2806280851364136, train acc = 0.9100000262260437, time = 0.940906286239624\n",
      "Training at step=27, batch=90, train loss = 0.37411174178123474, train acc = 0.875, time = 0.9352118968963623\n",
      "Training at step=27, batch=120, train loss = 0.3224709630012512, train acc = 0.8999999761581421, time = 0.9375481605529785\n",
      "Training at step=27, batch=150, train loss = 0.36714649200439453, train acc = 0.8600000143051147, time = 0.9388325214385986\n",
      "Training at step=27, batch=180, train loss = 0.3834788501262665, train acc = 0.8450000286102295, time = 0.9402046203613281\n",
      "Training at step=27, batch=210, train loss = 0.298705518245697, train acc = 0.9100000262260437, time = 0.9375839233398438\n",
      "Training at step=27, batch=240, train loss = 0.32436442375183105, train acc = 0.8550000190734863, time = 0.9357254505157471\n",
      "Training at step=27, batch=270, train loss = 0.29517656564712524, train acc = 0.9049999713897705, time = 0.9454753398895264\n",
      "Testing at step=27, batch=0, test loss = 0.2961156964302063, test acc = 0.8849999904632568, time = 0.3546435832977295\n",
      "Testing at step=27, batch=5, test loss = 0.4373939633369446, test acc = 0.8399999737739563, time = 0.33765387535095215\n",
      "Testing at step=27, batch=10, test loss = 0.3493458032608032, test acc = 0.9049999713897705, time = 0.3364846706390381\n",
      "Testing at step=27, batch=15, test loss = 0.3812178373336792, test acc = 0.8650000095367432, time = 0.3373575210571289\n",
      "Testing at step=27, batch=20, test loss = 0.4085770547389984, test acc = 0.8600000143051147, time = 0.33866190910339355\n",
      "Testing at step=27, batch=25, test loss = 0.4861217141151428, test acc = 0.8349999785423279, time = 0.33788251876831055\n",
      "Testing at step=27, batch=30, test loss = 0.4922974407672882, test acc = 0.8199999928474426, time = 0.3354334831237793\n",
      "Testing at step=27, batch=35, test loss = 0.40214282274246216, test acc = 0.8550000190734863, time = 0.3363215923309326\n",
      "Testing at step=27, batch=40, test loss = 0.4639689326286316, test acc = 0.8349999785423279, time = 0.34081602096557617\n",
      "Testing at step=27, batch=45, test loss = 0.40283969044685364, test acc = 0.8199999928474426, time = 0.3398094177246094\n",
      "Step 27 finished in 309.1057753562927, Train loss = 0.3257359160979589, Test loss = 0.3973506274819374; Train Acc = 0.8820500006278356, Test Acc = 0.8579000008106231\n",
      "Training at step=28, batch=0, train loss = 0.33893775939941406, train acc = 0.9049999713897705, time = 0.9439365863800049\n",
      "Training at step=28, batch=30, train loss = 0.37612012028694153, train acc = 0.8100000023841858, time = 0.9544346332550049\n",
      "Training at step=28, batch=60, train loss = 0.28752854466438293, train acc = 0.8899999856948853, time = 0.9360122680664062\n",
      "Training at step=28, batch=90, train loss = 0.264200359582901, train acc = 0.9150000214576721, time = 0.9396097660064697\n",
      "Training at step=28, batch=120, train loss = 0.2778715491294861, train acc = 0.9350000023841858, time = 0.9408931732177734\n",
      "Training at step=28, batch=150, train loss = 0.32127419114112854, train acc = 0.8650000095367432, time = 0.9377481937408447\n",
      "Training at step=28, batch=180, train loss = 0.37354323267936707, train acc = 0.8799999952316284, time = 0.9389007091522217\n",
      "Training at step=28, batch=210, train loss = 0.29471123218536377, train acc = 0.8999999761581421, time = 0.959012508392334\n",
      "Training at step=28, batch=240, train loss = 0.26701661944389343, train acc = 0.8999999761581421, time = 0.9609766006469727\n",
      "Training at step=28, batch=270, train loss = 0.37122753262519836, train acc = 0.8700000047683716, time = 0.9367034435272217\n",
      "Testing at step=28, batch=0, test loss = 0.47911158204078674, test acc = 0.8450000286102295, time = 0.36226773262023926\n",
      "Testing at step=28, batch=5, test loss = 0.5348727703094482, test acc = 0.824999988079071, time = 0.3611578941345215\n",
      "Testing at step=28, batch=10, test loss = 0.4215056896209717, test acc = 0.8500000238418579, time = 0.36087584495544434\n",
      "Testing at step=28, batch=15, test loss = 0.4257785677909851, test acc = 0.8600000143051147, time = 0.36065196990966797\n",
      "Testing at step=28, batch=20, test loss = 0.2754993140697479, test acc = 0.8949999809265137, time = 0.3608973026275635\n",
      "Testing at step=28, batch=25, test loss = 0.3322926461696625, test acc = 0.8899999856948853, time = 0.3595559597015381\n",
      "Testing at step=28, batch=30, test loss = 0.4102400839328766, test acc = 0.8899999856948853, time = 0.3624563217163086\n",
      "Testing at step=28, batch=35, test loss = 0.4278443157672882, test acc = 0.8550000190734863, time = 0.3393237590789795\n",
      "Testing at step=28, batch=40, test loss = 0.29398563504219055, test acc = 0.8899999856948853, time = 0.3426673412322998\n",
      "Testing at step=28, batch=45, test loss = 0.45296233892440796, test acc = 0.8500000238418579, time = 0.3457224369049072\n",
      "Step 28 finished in 312.15673780441284, Train loss = 0.3236011382440726, Test loss = 0.3877979892492294; Train Acc = 0.881883331934611, Test Acc = 0.865\n",
      "Training at step=29, batch=0, train loss = 0.28403493762016296, train acc = 0.8849999904632568, time = 0.9604864120483398\n",
      "Training at step=29, batch=30, train loss = 0.3784109950065613, train acc = 0.8550000190734863, time = 0.9630670547485352\n",
      "Training at step=29, batch=60, train loss = 0.23973417282104492, train acc = 0.9049999713897705, time = 0.947873592376709\n",
      "Training at step=29, batch=90, train loss = 0.24078264832496643, train acc = 0.9399999976158142, time = 0.939739465713501\n",
      "Training at step=29, batch=120, train loss = 0.36537644267082214, train acc = 0.8399999737739563, time = 0.9350471496582031\n",
      "Training at step=29, batch=150, train loss = 0.3731892704963684, train acc = 0.8700000047683716, time = 0.9496080875396729\n",
      "Training at step=29, batch=180, train loss = 0.26880133152008057, train acc = 0.9100000262260437, time = 0.9360222816467285\n",
      "Training at step=29, batch=210, train loss = 0.26296189427375793, train acc = 0.8999999761581421, time = 0.9423410892486572\n",
      "Training at step=29, batch=240, train loss = 0.2324354499578476, train acc = 0.9300000071525574, time = 0.9424409866333008\n",
      "Training at step=29, batch=270, train loss = 0.29539117217063904, train acc = 0.8999999761581421, time = 0.9346597194671631\n",
      "Testing at step=29, batch=0, test loss = 0.48965224623680115, test acc = 0.8450000286102295, time = 0.3402237892150879\n",
      "Testing at step=29, batch=5, test loss = 0.3376133441925049, test acc = 0.8899999856948853, time = 0.3406708240509033\n",
      "Testing at step=29, batch=10, test loss = 0.43630871176719666, test acc = 0.8299999833106995, time = 0.3395955562591553\n",
      "Testing at step=29, batch=15, test loss = 0.5014948844909668, test acc = 0.8149999976158142, time = 0.3389313220977783\n",
      "Testing at step=29, batch=20, test loss = 0.4305171072483063, test acc = 0.8349999785423279, time = 0.3374049663543701\n",
      "Testing at step=29, batch=25, test loss = 0.40560203790664673, test acc = 0.8550000190734863, time = 0.3407156467437744\n",
      "Testing at step=29, batch=30, test loss = 0.3921773433685303, test acc = 0.8650000095367432, time = 0.33974123001098633\n",
      "Testing at step=29, batch=35, test loss = 0.2747024595737457, test acc = 0.9049999713897705, time = 0.3390192985534668\n",
      "Testing at step=29, batch=40, test loss = 0.4075486660003662, test acc = 0.8899999856948853, time = 0.336245059967041\n",
      "Testing at step=29, batch=45, test loss = 0.28911304473876953, test acc = 0.8650000095367432, time = 0.34053826332092285\n",
      "Step 29 finished in 310.15996766090393, Train loss = 0.3210695952177048, Test loss = 0.4005775275826454; Train Acc = 0.8823999985059102, Test Acc = 0.856800000667572\n",
      "Training at step=30, batch=0, train loss = 0.34132900834083557, train acc = 0.8849999904632568, time = 0.9434309005737305\n",
      "Training at step=30, batch=30, train loss = 0.30794277787208557, train acc = 0.8799999952316284, time = 0.966580867767334\n",
      "Training at step=30, batch=60, train loss = 0.19772066175937653, train acc = 0.9300000071525574, time = 0.9325416088104248\n",
      "Training at step=30, batch=90, train loss = 0.291847825050354, train acc = 0.9049999713897705, time = 0.9362456798553467\n",
      "Training at step=30, batch=120, train loss = 0.26201871037483215, train acc = 0.8999999761581421, time = 0.935537576675415\n",
      "Training at step=30, batch=150, train loss = 0.1861787587404251, train acc = 0.9300000071525574, time = 0.9332213401794434\n",
      "Training at step=30, batch=180, train loss = 0.30585530400276184, train acc = 0.8849999904632568, time = 0.9382801055908203\n",
      "Training at step=30, batch=210, train loss = 0.2803976535797119, train acc = 0.8949999809265137, time = 0.94234299659729\n",
      "Training at step=30, batch=240, train loss = 0.35979610681533813, train acc = 0.8799999952316284, time = 0.9453554153442383\n",
      "Training at step=30, batch=270, train loss = 0.33614933490753174, train acc = 0.875, time = 0.9407379627227783\n",
      "Testing at step=30, batch=0, test loss = 0.3802735209465027, test acc = 0.8700000047683716, time = 0.3409152030944824\n",
      "Testing at step=30, batch=5, test loss = 0.38117849826812744, test acc = 0.8650000095367432, time = 0.3377392292022705\n",
      "Testing at step=30, batch=10, test loss = 0.5312320590019226, test acc = 0.8149999976158142, time = 0.33850860595703125\n",
      "Testing at step=30, batch=15, test loss = 0.39531493186950684, test acc = 0.875, time = 0.3913543224334717\n",
      "Testing at step=30, batch=20, test loss = 0.3696098327636719, test acc = 0.8700000047683716, time = 0.3373277187347412\n",
      "Testing at step=30, batch=25, test loss = 0.39482712745666504, test acc = 0.8700000047683716, time = 0.3396296501159668\n",
      "Testing at step=30, batch=30, test loss = 0.34031009674072266, test acc = 0.8550000190734863, time = 0.33795857429504395\n",
      "Testing at step=30, batch=35, test loss = 0.41495242714881897, test acc = 0.8500000238418579, time = 0.34638118743896484\n",
      "Testing at step=30, batch=40, test loss = 0.2516460120677948, test acc = 0.925000011920929, time = 0.3408162593841553\n",
      "Testing at step=30, batch=45, test loss = 0.3880510628223419, test acc = 0.8600000143051147, time = 0.36540889739990234\n",
      "Step 30 finished in 309.5218069553375, Train loss = 0.31532163297136623, Test loss = 0.38614450007677076; Train Acc = 0.8861499977111816, Test Acc = 0.8637000024318695\n",
      "Training at step=31, batch=0, train loss = 0.360330730676651, train acc = 0.8899999856948853, time = 0.974128246307373\n",
      "Training at step=31, batch=30, train loss = 0.297358900308609, train acc = 0.8849999904632568, time = 0.9368863105773926\n",
      "Training at step=31, batch=60, train loss = 0.37947794795036316, train acc = 0.8600000143051147, time = 0.9596209526062012\n",
      "Training at step=31, batch=90, train loss = 0.2969462275505066, train acc = 0.9049999713897705, time = 0.9396922588348389\n",
      "Training at step=31, batch=120, train loss = 0.279900461435318, train acc = 0.8899999856948853, time = 0.9719142913818359\n",
      "Training at step=31, batch=150, train loss = 0.32392293214797974, train acc = 0.9150000214576721, time = 0.9398932456970215\n",
      "Training at step=31, batch=180, train loss = 0.36786437034606934, train acc = 0.8600000143051147, time = 0.945425271987915\n",
      "Training at step=31, batch=210, train loss = 0.38523152470588684, train acc = 0.8650000095367432, time = 0.9349379539489746\n",
      "Training at step=31, batch=240, train loss = 0.29824939370155334, train acc = 0.8849999904632568, time = 0.934511661529541\n",
      "Training at step=31, batch=270, train loss = 0.29302167892456055, train acc = 0.8849999904632568, time = 0.9430646896362305\n",
      "Testing at step=31, batch=0, test loss = 0.4264635443687439, test acc = 0.8450000286102295, time = 0.3388557434082031\n",
      "Testing at step=31, batch=5, test loss = 0.3424362540245056, test acc = 0.8899999856948853, time = 0.34291625022888184\n",
      "Testing at step=31, batch=10, test loss = 0.4124119281768799, test acc = 0.8399999737739563, time = 0.3371562957763672\n",
      "Testing at step=31, batch=15, test loss = 0.4028085768222809, test acc = 0.875, time = 0.3398001194000244\n",
      "Testing at step=31, batch=20, test loss = 0.3762933611869812, test acc = 0.8500000238418579, time = 0.33745312690734863\n",
      "Testing at step=31, batch=25, test loss = 0.2727760076522827, test acc = 0.8949999809265137, time = 0.3423609733581543\n",
      "Testing at step=31, batch=30, test loss = 0.44374722242355347, test acc = 0.8299999833106995, time = 0.34118032455444336\n",
      "Testing at step=31, batch=35, test loss = 0.3030868470668793, test acc = 0.8700000047683716, time = 0.34076738357543945\n",
      "Testing at step=31, batch=40, test loss = 0.29089629650115967, test acc = 0.9150000214576721, time = 0.33953261375427246\n",
      "Testing at step=31, batch=45, test loss = 0.42348578572273254, test acc = 0.8199999928474426, time = 0.33777499198913574\n",
      "Step 31 finished in 310.9946608543396, Train loss = 0.31667129213611284, Test loss = 0.3699572294950485; Train Acc = 0.8834833323955535, Test Acc = 0.8675999999046325\n",
      "Training at step=32, batch=0, train loss = 0.29881882667541504, train acc = 0.8899999856948853, time = 0.9369630813598633\n",
      "Training at step=32, batch=30, train loss = 0.29234999418258667, train acc = 0.8999999761581421, time = 0.9418156147003174\n",
      "Training at step=32, batch=60, train loss = 0.327057421207428, train acc = 0.8949999809265137, time = 0.937415361404419\n",
      "Training at step=32, batch=90, train loss = 0.3903537690639496, train acc = 0.8700000047683716, time = 0.9346661567687988\n",
      "Training at step=32, batch=120, train loss = 0.26201894879341125, train acc = 0.8899999856948853, time = 0.9318630695343018\n",
      "Training at step=32, batch=150, train loss = 0.3527950942516327, train acc = 0.8650000095367432, time = 0.9646751880645752\n",
      "Training at step=32, batch=180, train loss = 0.3534306287765503, train acc = 0.8700000047683716, time = 0.9327349662780762\n",
      "Training at step=32, batch=210, train loss = 0.23823308944702148, train acc = 0.9100000262260437, time = 1.1203687191009521\n",
      "Training at step=32, batch=240, train loss = 0.4079383611679077, train acc = 0.8799999952316284, time = 0.9409410953521729\n",
      "Training at step=32, batch=270, train loss = 0.2747197449207306, train acc = 0.8949999809265137, time = 1.7951314449310303\n",
      "Testing at step=32, batch=0, test loss = 0.3609439432621002, test acc = 0.8799999952316284, time = 0.8130984306335449\n",
      "Testing at step=32, batch=5, test loss = 0.33529049158096313, test acc = 0.875, time = 1.2091574668884277\n",
      "Testing at step=32, batch=10, test loss = 0.3768438994884491, test acc = 0.8650000095367432, time = 0.7949240207672119\n",
      "Testing at step=32, batch=15, test loss = 0.5674644708633423, test acc = 0.8149999976158142, time = 0.8059473037719727\n",
      "Testing at step=32, batch=20, test loss = 0.3547675609588623, test acc = 0.8849999904632568, time = 0.8259317874908447\n",
      "Testing at step=32, batch=25, test loss = 0.44476157426834106, test acc = 0.8349999785423279, time = 0.9777026176452637\n",
      "Testing at step=32, batch=30, test loss = 0.355257123708725, test acc = 0.8650000095367432, time = 1.0271024703979492\n",
      "Testing at step=32, batch=35, test loss = 0.38169965147972107, test acc = 0.8500000238418579, time = 0.3420705795288086\n",
      "Testing at step=32, batch=40, test loss = 0.38128966093063354, test acc = 0.8700000047683716, time = 0.35753488540649414\n",
      "Testing at step=32, batch=45, test loss = 0.4258773922920227, test acc = 0.824999988079071, time = 0.3473048210144043\n",
      "Step 32 finished in 374.52610969543457, Train loss = 0.31592180743813514, Test loss = 0.3927775079011917; Train Acc = 0.8837999989589055, Test Acc = 0.8601999962329865\n",
      "Training at step=33, batch=0, train loss = 0.30673322081565857, train acc = 0.8999999761581421, time = 0.9408352375030518\n",
      "Training at step=33, batch=30, train loss = 0.23494727909564972, train acc = 0.9300000071525574, time = 0.9387774467468262\n",
      "Training at step=33, batch=60, train loss = 0.2559904158115387, train acc = 0.9049999713897705, time = 0.9586770534515381\n",
      "Training at step=33, batch=90, train loss = 0.2800772488117218, train acc = 0.8849999904632568, time = 0.9354078769683838\n",
      "Training at step=33, batch=120, train loss = 0.30769863724708557, train acc = 0.875, time = 0.9362401962280273\n",
      "Training at step=33, batch=150, train loss = 0.35649800300598145, train acc = 0.8700000047683716, time = 0.9367942810058594\n",
      "Training at step=33, batch=180, train loss = 0.28999418020248413, train acc = 0.9049999713897705, time = 0.9583742618560791\n",
      "Training at step=33, batch=210, train loss = 0.2511158883571625, train acc = 0.9049999713897705, time = 0.9371721744537354\n",
      "Training at step=33, batch=240, train loss = 0.3110660910606384, train acc = 0.8999999761581421, time = 0.941953182220459\n",
      "Training at step=33, batch=270, train loss = 0.30646729469299316, train acc = 0.8849999904632568, time = 0.9347233772277832\n",
      "Testing at step=33, batch=0, test loss = 0.4327053129673004, test acc = 0.8349999785423279, time = 0.3353404998779297\n",
      "Testing at step=33, batch=5, test loss = 0.3663560450077057, test acc = 0.8650000095367432, time = 0.3407778739929199\n",
      "Testing at step=33, batch=10, test loss = 0.3792443573474884, test acc = 0.8600000143051147, time = 0.34027886390686035\n",
      "Testing at step=33, batch=15, test loss = 0.27462923526763916, test acc = 0.9049999713897705, time = 0.33980417251586914\n",
      "Testing at step=33, batch=20, test loss = 0.30740079283714294, test acc = 0.9049999713897705, time = 0.33938050270080566\n",
      "Testing at step=33, batch=25, test loss = 0.313798725605011, test acc = 0.8700000047683716, time = 0.3428812026977539\n",
      "Testing at step=33, batch=30, test loss = 0.33765357732772827, test acc = 0.8700000047683716, time = 0.33915042877197266\n",
      "Testing at step=33, batch=35, test loss = 0.3262313902378082, test acc = 0.8799999952316284, time = 0.3405930995941162\n",
      "Testing at step=33, batch=40, test loss = 0.37837010622024536, test acc = 0.8600000143051147, time = 0.33983302116394043\n",
      "Testing at step=33, batch=45, test loss = 0.45871371030807495, test acc = 0.8700000047683716, time = 0.34247875213623047\n",
      "Step 33 finished in 310.2590730190277, Train loss = 0.311849530339241, Test loss = 0.3566139751672745; Train Acc = 0.886433331767718, Test Acc = 0.8732999968528747\n",
      "Training at step=34, batch=0, train loss = 0.21869175136089325, train acc = 0.9049999713897705, time = 0.9454331398010254\n",
      "Training at step=34, batch=30, train loss = 0.38095447421073914, train acc = 0.8700000047683716, time = 0.9392604827880859\n",
      "Training at step=34, batch=60, train loss = 0.25662386417388916, train acc = 0.8799999952316284, time = 0.9379279613494873\n",
      "Training at step=34, batch=90, train loss = 0.24096691608428955, train acc = 0.9150000214576721, time = 0.9451344013214111\n",
      "Training at step=34, batch=120, train loss = 0.26907795667648315, train acc = 0.8949999809265137, time = 0.9728765487670898\n",
      "Training at step=34, batch=150, train loss = 0.27753517031669617, train acc = 0.8849999904632568, time = 0.9581401348114014\n",
      "Training at step=34, batch=180, train loss = 0.22159670293331146, train acc = 0.9150000214576721, time = 0.9530019760131836\n",
      "Training at step=34, batch=210, train loss = 0.27950820326805115, train acc = 0.9049999713897705, time = 0.938157320022583\n",
      "Training at step=34, batch=240, train loss = 0.35310012102127075, train acc = 0.8799999952316284, time = 0.948101282119751\n",
      "Training at step=34, batch=270, train loss = 0.22387905418872833, train acc = 0.9100000262260437, time = 0.9434921741485596\n",
      "Testing at step=34, batch=0, test loss = 0.3356321454048157, test acc = 0.8999999761581421, time = 0.3399825096130371\n",
      "Testing at step=34, batch=5, test loss = 0.3663432002067566, test acc = 0.8650000095367432, time = 0.3444528579711914\n",
      "Testing at step=34, batch=10, test loss = 0.3276776969432831, test acc = 0.8799999952316284, time = 0.33949851989746094\n",
      "Testing at step=34, batch=15, test loss = 0.4442157745361328, test acc = 0.824999988079071, time = 0.3384668827056885\n",
      "Testing at step=34, batch=20, test loss = 0.3988156020641327, test acc = 0.8399999737739563, time = 0.3427860736846924\n",
      "Testing at step=34, batch=25, test loss = 0.45178279280662537, test acc = 0.8450000286102295, time = 0.3462505340576172\n",
      "Testing at step=34, batch=30, test loss = 0.38099387288093567, test acc = 0.8799999952316284, time = 0.34427356719970703\n",
      "Testing at step=34, batch=35, test loss = 0.35134357213974, test acc = 0.8949999809265137, time = 0.44988417625427246\n",
      "Testing at step=34, batch=40, test loss = 0.3292822241783142, test acc = 0.875, time = 0.35387706756591797\n",
      "Testing at step=34, batch=45, test loss = 0.29423561692237854, test acc = 0.9049999713897705, time = 0.34417009353637695\n",
      "Step 34 finished in 310.97665572166443, Train loss = 0.3127827531099319, Test loss = 0.36490608036518096; Train Acc = 0.8852833292881648, Test Acc = 0.8722999978065491\n",
      "Training at step=35, batch=0, train loss = 0.2766411006450653, train acc = 0.8999999761581421, time = 0.9543583393096924\n",
      "Training at step=35, batch=30, train loss = 0.3144073188304901, train acc = 0.8700000047683716, time = 0.9434981346130371\n",
      "Training at step=35, batch=60, train loss = 0.2203000783920288, train acc = 0.9200000166893005, time = 0.9368407726287842\n",
      "Training at step=35, batch=90, train loss = 0.3228594660758972, train acc = 0.8700000047683716, time = 0.9419863224029541\n",
      "Training at step=35, batch=120, train loss = 0.25252169370651245, train acc = 0.9049999713897705, time = 0.9359753131866455\n",
      "Training at step=35, batch=150, train loss = 0.2742525637149811, train acc = 0.8849999904632568, time = 0.9415369033813477\n",
      "Training at step=35, batch=180, train loss = 0.38866832852363586, train acc = 0.8600000143051147, time = 0.9421184062957764\n",
      "Training at step=35, batch=210, train loss = 0.3082498610019684, train acc = 0.875, time = 0.96482253074646\n",
      "Training at step=35, batch=240, train loss = 0.3195172846317291, train acc = 0.8949999809265137, time = 0.9473388195037842\n",
      "Training at step=35, batch=270, train loss = 0.36985138058662415, train acc = 0.8550000190734863, time = 0.944378137588501\n",
      "Testing at step=35, batch=0, test loss = 0.3382086455821991, test acc = 0.875, time = 0.33887577056884766\n",
      "Testing at step=35, batch=5, test loss = 0.3417700231075287, test acc = 0.875, time = 0.3390645980834961\n",
      "Testing at step=35, batch=10, test loss = 0.34074482321739197, test acc = 0.875, time = 0.3392202854156494\n",
      "Testing at step=35, batch=15, test loss = 0.3286522626876831, test acc = 0.8799999952316284, time = 0.33742284774780273\n",
      "Testing at step=35, batch=20, test loss = 0.33088603615760803, test acc = 0.8899999856948853, time = 0.3402845859527588\n",
      "Testing at step=35, batch=25, test loss = 0.41981762647628784, test acc = 0.8399999737739563, time = 0.36250948905944824\n",
      "Testing at step=35, batch=30, test loss = 0.3211215138435364, test acc = 0.8899999856948853, time = 0.3431589603424072\n",
      "Testing at step=35, batch=35, test loss = 0.3919571042060852, test acc = 0.8450000286102295, time = 0.34189820289611816\n",
      "Testing at step=35, batch=40, test loss = 0.341318815946579, test acc = 0.8899999856948853, time = 0.35030412673950195\n",
      "Testing at step=35, batch=45, test loss = 0.368958055973053, test acc = 0.8450000286102295, time = 0.34120821952819824\n",
      "Step 35 finished in 311.86696600914, Train loss = 0.30525479798515637, Test loss = 0.36142565310001373; Train Acc = 0.8875666644175847, Test Acc = 0.8725999987125397\n",
      "Training at step=36, batch=0, train loss = 0.25290369987487793, train acc = 0.8949999809265137, time = 0.9391980171203613\n",
      "Training at step=36, batch=30, train loss = 0.3270191550254822, train acc = 0.8500000238418579, time = 0.9395012855529785\n",
      "Training at step=36, batch=60, train loss = 0.23045490682125092, train acc = 0.9200000166893005, time = 0.9404494762420654\n",
      "Training at step=36, batch=90, train loss = 0.28482258319854736, train acc = 0.9100000262260437, time = 0.9535200595855713\n",
      "Training at step=36, batch=120, train loss = 0.28049710392951965, train acc = 0.875, time = 0.9399120807647705\n",
      "Training at step=36, batch=150, train loss = 0.30770087242126465, train acc = 0.8949999809265137, time = 0.9391465187072754\n",
      "Training at step=36, batch=180, train loss = 0.34190791845321655, train acc = 0.8899999856948853, time = 0.9373867511749268\n",
      "Training at step=36, batch=210, train loss = 0.29317721724510193, train acc = 0.9100000262260437, time = 0.9370298385620117\n",
      "Training at step=36, batch=240, train loss = 0.25891709327697754, train acc = 0.9150000214576721, time = 0.9443666934967041\n",
      "Training at step=36, batch=270, train loss = 0.3978307247161865, train acc = 0.8349999785423279, time = 0.9453332424163818\n",
      "Testing at step=36, batch=0, test loss = 0.25866198539733887, test acc = 0.8949999809265137, time = 0.3498852252960205\n",
      "Testing at step=36, batch=5, test loss = 0.41566070914268494, test acc = 0.8399999737739563, time = 0.3374035358428955\n",
      "Testing at step=36, batch=10, test loss = 0.2806544601917267, test acc = 0.8949999809265137, time = 0.3387117385864258\n",
      "Testing at step=36, batch=15, test loss = 0.47981083393096924, test acc = 0.8550000190734863, time = 0.33983755111694336\n",
      "Testing at step=36, batch=20, test loss = 0.37895214557647705, test acc = 0.8600000143051147, time = 0.338031530380249\n",
      "Testing at step=36, batch=25, test loss = 0.41031280159950256, test acc = 0.8500000238418579, time = 0.33874011039733887\n",
      "Testing at step=36, batch=30, test loss = 0.3334248661994934, test acc = 0.8849999904632568, time = 0.335695743560791\n",
      "Testing at step=36, batch=35, test loss = 0.3652189373970032, test acc = 0.8700000047683716, time = 0.4033360481262207\n",
      "Testing at step=36, batch=40, test loss = 0.3185363709926605, test acc = 0.8949999809265137, time = 0.337252140045166\n",
      "Testing at step=36, batch=45, test loss = 0.25928622484207153, test acc = 0.8999999761581421, time = 0.3474445343017578\n",
      "Step 36 finished in 310.7283720970154, Train loss = 0.3050857449074586, Test loss = 0.37114024788141253; Train Acc = 0.8881999981403351, Test Acc = 0.8648000013828278\n",
      "Training at step=37, batch=0, train loss = 0.3113834261894226, train acc = 0.8899999856948853, time = 0.9429724216461182\n",
      "Training at step=37, batch=30, train loss = 0.2795807123184204, train acc = 0.9150000214576721, time = 0.9635634422302246\n",
      "Training at step=37, batch=60, train loss = 0.27025869488716125, train acc = 0.9100000262260437, time = 0.9403648376464844\n",
      "Training at step=37, batch=90, train loss = 0.2582213282585144, train acc = 0.8899999856948853, time = 0.934981107711792\n",
      "Training at step=37, batch=120, train loss = 0.31839263439178467, train acc = 0.8700000047683716, time = 0.9465720653533936\n",
      "Training at step=37, batch=150, train loss = 0.2843729257583618, train acc = 0.8849999904632568, time = 0.9425475597381592\n",
      "Training at step=37, batch=180, train loss = 0.23366336524486542, train acc = 0.9350000023841858, time = 0.9508790969848633\n",
      "Training at step=37, batch=210, train loss = 0.33932340145111084, train acc = 0.9100000262260437, time = 0.9413607120513916\n",
      "Training at step=37, batch=240, train loss = 0.30836355686187744, train acc = 0.8650000095367432, time = 0.9373950958251953\n",
      "Training at step=37, batch=270, train loss = 0.24930325150489807, train acc = 0.925000011920929, time = 0.938223123550415\n",
      "Testing at step=37, batch=0, test loss = 0.2686344385147095, test acc = 0.8899999856948853, time = 0.341766357421875\n",
      "Testing at step=37, batch=5, test loss = 0.3605844974517822, test acc = 0.875, time = 0.33754515647888184\n",
      "Testing at step=37, batch=10, test loss = 0.3121105432510376, test acc = 0.8849999904632568, time = 0.3384859561920166\n",
      "Testing at step=37, batch=15, test loss = 0.4123944342136383, test acc = 0.8500000238418579, time = 0.339876651763916\n",
      "Testing at step=37, batch=20, test loss = 0.35100382566452026, test acc = 0.8700000047683716, time = 0.34259724617004395\n",
      "Testing at step=37, batch=25, test loss = 0.4181238114833832, test acc = 0.8799999952316284, time = 0.34385013580322266\n",
      "Testing at step=37, batch=30, test loss = 0.3386877775192261, test acc = 0.8700000047683716, time = 0.3416178226470947\n",
      "Testing at step=37, batch=35, test loss = 0.3783649206161499, test acc = 0.8650000095367432, time = 0.34426259994506836\n",
      "Testing at step=37, batch=40, test loss = 0.29647380113601685, test acc = 0.8949999809265137, time = 0.33870935440063477\n",
      "Testing at step=37, batch=45, test loss = 0.39949092268943787, test acc = 0.8600000143051147, time = 0.3383352756500244\n",
      "Step 37 finished in 311.7121181488037, Train loss = 0.30130065366625786, Test loss = 0.3573355108499527; Train Acc = 0.8893666634956996, Test Acc = 0.8726999986171723\n",
      "Training at step=38, batch=0, train loss = 0.4331377446651459, train acc = 0.8500000238418579, time = 0.9414358139038086\n",
      "Training at step=38, batch=30, train loss = 0.30821889638900757, train acc = 0.8899999856948853, time = 0.956885576248169\n",
      "Training at step=38, batch=60, train loss = 0.2639341354370117, train acc = 0.8849999904632568, time = 0.9387035369873047\n",
      "Training at step=38, batch=90, train loss = 0.2929198145866394, train acc = 0.8999999761581421, time = 0.9431915283203125\n",
      "Training at step=38, batch=120, train loss = 0.34932079911231995, train acc = 0.8650000095367432, time = 0.9632871150970459\n",
      "Training at step=38, batch=150, train loss = 0.31265029311180115, train acc = 0.8799999952316284, time = 0.9347057342529297\n",
      "Training at step=38, batch=180, train loss = 0.22775417566299438, train acc = 0.9150000214576721, time = 0.944697380065918\n",
      "Training at step=38, batch=210, train loss = 0.34491512179374695, train acc = 0.8799999952316284, time = 0.9531583786010742\n",
      "Training at step=38, batch=240, train loss = 0.22010096907615662, train acc = 0.9150000214576721, time = 0.9459729194641113\n",
      "Training at step=38, batch=270, train loss = 0.3405572175979614, train acc = 0.8500000238418579, time = 0.9396915435791016\n",
      "Testing at step=38, batch=0, test loss = 0.507203996181488, test acc = 0.8399999737739563, time = 0.3382108211517334\n",
      "Testing at step=38, batch=5, test loss = 0.3067835569381714, test acc = 0.9200000166893005, time = 0.33829832077026367\n",
      "Testing at step=38, batch=10, test loss = 0.22932344675064087, test acc = 0.9049999713897705, time = 0.34352946281433105\n",
      "Testing at step=38, batch=15, test loss = 0.35967132449150085, test acc = 0.875, time = 0.3399238586425781\n",
      "Testing at step=38, batch=20, test loss = 0.2801276743412018, test acc = 0.8949999809265137, time = 0.33806872367858887\n",
      "Testing at step=38, batch=25, test loss = 0.30936864018440247, test acc = 0.9150000214576721, time = 0.3364574909210205\n",
      "Testing at step=38, batch=30, test loss = 0.35463643074035645, test acc = 0.9150000214576721, time = 0.34144067764282227\n",
      "Testing at step=38, batch=35, test loss = 0.35626286268234253, test acc = 0.8700000047683716, time = 0.340686559677124\n",
      "Testing at step=38, batch=40, test loss = 0.3772864043712616, test acc = 0.8700000047683716, time = 0.3485839366912842\n",
      "Testing at step=38, batch=45, test loss = 0.38081812858581543, test acc = 0.875, time = 0.34598684310913086\n",
      "Step 38 finished in 311.4450352191925, Train loss = 0.29955636863907176, Test loss = 0.36000752210617065; Train Acc = 0.8911999986569087, Test Acc = 0.8736000001430512\n",
      "Training at step=39, batch=0, train loss = 0.3106929659843445, train acc = 0.8849999904632568, time = 0.938378095626831\n",
      "Training at step=39, batch=30, train loss = 0.3574240803718567, train acc = 0.8849999904632568, time = 0.9445316791534424\n",
      "Training at step=39, batch=60, train loss = 0.30998697876930237, train acc = 0.8899999856948853, time = 0.9625382423400879\n",
      "Training at step=39, batch=90, train loss = 0.2935671806335449, train acc = 0.8899999856948853, time = 0.9378852844238281\n",
      "Training at step=39, batch=120, train loss = 0.2397657036781311, train acc = 0.8949999809265137, time = 0.941840648651123\n",
      "Training at step=39, batch=150, train loss = 0.27690762281417847, train acc = 0.8949999809265137, time = 0.959244966506958\n",
      "Training at step=39, batch=180, train loss = 0.22934618592262268, train acc = 0.9049999713897705, time = 0.9398379325866699\n",
      "Training at step=39, batch=210, train loss = 0.31102055311203003, train acc = 0.875, time = 0.9374978542327881\n",
      "Training at step=39, batch=240, train loss = 0.34819865226745605, train acc = 0.8600000143051147, time = 0.9381465911865234\n",
      "Training at step=39, batch=270, train loss = 0.32520735263824463, train acc = 0.8849999904632568, time = 0.9340403079986572\n",
      "Testing at step=39, batch=0, test loss = 0.5183383822441101, test acc = 0.8700000047683716, time = 0.35060834884643555\n",
      "Testing at step=39, batch=5, test loss = 0.37242671847343445, test acc = 0.8450000286102295, time = 0.34471869468688965\n",
      "Testing at step=39, batch=10, test loss = 0.2669679820537567, test acc = 0.8949999809265137, time = 0.34101057052612305\n",
      "Testing at step=39, batch=15, test loss = 0.2931324541568756, test acc = 0.8949999809265137, time = 0.3403046131134033\n",
      "Testing at step=39, batch=20, test loss = 0.355141818523407, test acc = 0.8500000238418579, time = 0.34015846252441406\n",
      "Testing at step=39, batch=25, test loss = 0.3413696587085724, test acc = 0.8650000095367432, time = 0.3405032157897949\n",
      "Testing at step=39, batch=30, test loss = 0.34693199396133423, test acc = 0.8899999856948853, time = 0.343092679977417\n",
      "Testing at step=39, batch=35, test loss = 0.38043278455734253, test acc = 0.8600000143051147, time = 0.3379380702972412\n",
      "Testing at step=39, batch=40, test loss = 0.445965051651001, test acc = 0.8600000143051147, time = 0.3390965461730957\n",
      "Testing at step=39, batch=45, test loss = 0.3923182785511017, test acc = 0.8550000190734863, time = 0.3408224582672119\n",
      "Step 39 finished in 310.8943977355957, Train loss = 0.30202253073453905, Test loss = 0.37170723468065264; Train Acc = 0.8885333295663198, Test Acc = 0.8670000052452087\n",
      "Training at step=40, batch=0, train loss = 0.2767707109451294, train acc = 0.8849999904632568, time = 0.9380753040313721\n",
      "Training at step=40, batch=30, train loss = 0.2756887674331665, train acc = 0.8949999809265137, time = 0.9427869319915771\n",
      "Training at step=40, batch=60, train loss = 0.2956761419773102, train acc = 0.8799999952316284, time = 0.948533296585083\n",
      "Training at step=40, batch=90, train loss = 0.27476704120635986, train acc = 0.9100000262260437, time = 0.9498827457427979\n",
      "Training at step=40, batch=120, train loss = 0.30133679509162903, train acc = 0.8799999952316284, time = 0.945227861404419\n",
      "Training at step=40, batch=150, train loss = 0.3421154022216797, train acc = 0.8600000143051147, time = 0.9370508193969727\n",
      "Training at step=40, batch=180, train loss = 0.3671043813228607, train acc = 0.8550000190734863, time = 0.9403669834136963\n",
      "Training at step=40, batch=210, train loss = 0.25137099623680115, train acc = 0.9150000214576721, time = 0.9646363258361816\n",
      "Training at step=40, batch=240, train loss = 0.35210052132606506, train acc = 0.8799999952316284, time = 0.9458651542663574\n",
      "Training at step=40, batch=270, train loss = 0.28857219219207764, train acc = 0.9150000214576721, time = 0.9369442462921143\n",
      "Testing at step=40, batch=0, test loss = 0.2801307737827301, test acc = 0.8949999809265137, time = 0.34535646438598633\n",
      "Testing at step=40, batch=5, test loss = 0.32365357875823975, test acc = 0.8700000047683716, time = 0.3401665687561035\n",
      "Testing at step=40, batch=10, test loss = 0.3225746154785156, test acc = 0.8899999856948853, time = 0.3406238555908203\n",
      "Testing at step=40, batch=15, test loss = 0.3633614778518677, test acc = 0.8500000238418579, time = 0.3456106185913086\n",
      "Testing at step=40, batch=20, test loss = 0.34921514987945557, test acc = 0.8949999809265137, time = 0.3430027961730957\n",
      "Testing at step=40, batch=25, test loss = 0.4209567606449127, test acc = 0.8650000095367432, time = 0.34343552589416504\n",
      "Testing at step=40, batch=30, test loss = 0.3314341604709625, test acc = 0.8949999809265137, time = 0.34672975540161133\n",
      "Testing at step=40, batch=35, test loss = 0.310507595539093, test acc = 0.8799999952316284, time = 0.44312405586242676\n",
      "Testing at step=40, batch=40, test loss = 0.3926711976528168, test acc = 0.8550000190734863, time = 0.33783864974975586\n",
      "Testing at step=40, batch=45, test loss = 0.2975722551345825, test acc = 0.8849999904632568, time = 0.3391551971435547\n",
      "Step 40 finished in 311.13006830215454, Train loss = 0.2988900942603747, Test loss = 0.3671241569519043; Train Acc = 0.890083331267039, Test Acc = 0.8677999973297119\n",
      "Training at step=41, batch=0, train loss = 0.3325713872909546, train acc = 0.8849999904632568, time = 0.9417994022369385\n",
      "Training at step=41, batch=30, train loss = 0.31858447194099426, train acc = 0.8799999952316284, time = 0.9454565048217773\n",
      "Training at step=41, batch=60, train loss = 0.3099816143512726, train acc = 0.8700000047683716, time = 0.9468522071838379\n",
      "Training at step=41, batch=90, train loss = 0.27502182126045227, train acc = 0.9049999713897705, time = 0.9409546852111816\n",
      "Training at step=41, batch=120, train loss = 0.2513256072998047, train acc = 0.8949999809265137, time = 0.9406051635742188\n",
      "Training at step=41, batch=150, train loss = 0.2923496961593628, train acc = 0.9049999713897705, time = 0.9346115589141846\n",
      "Training at step=41, batch=180, train loss = 0.3881644010543823, train acc = 0.8550000190734863, time = 0.9451169967651367\n",
      "Training at step=41, batch=210, train loss = 0.2136523425579071, train acc = 0.925000011920929, time = 0.9538326263427734\n",
      "Training at step=41, batch=240, train loss = 0.3124271333217621, train acc = 0.8849999904632568, time = 0.9652392864227295\n",
      "Training at step=41, batch=270, train loss = 0.20889243483543396, train acc = 0.9300000071525574, time = 0.9621198177337646\n",
      "Testing at step=41, batch=0, test loss = 0.3474139869213104, test acc = 0.8849999904632568, time = 0.33961009979248047\n",
      "Testing at step=41, batch=5, test loss = 0.3191338777542114, test acc = 0.8799999952316284, time = 0.34821224212646484\n",
      "Testing at step=41, batch=10, test loss = 0.35090935230255127, test acc = 0.8399999737739563, time = 0.34430813789367676\n",
      "Testing at step=41, batch=15, test loss = 0.27664077281951904, test acc = 0.8799999952316284, time = 0.3388230800628662\n",
      "Testing at step=41, batch=20, test loss = 0.40391549468040466, test acc = 0.8500000238418579, time = 0.3370850086212158\n",
      "Testing at step=41, batch=25, test loss = 0.27583032846450806, test acc = 0.9100000262260437, time = 0.33870601654052734\n",
      "Testing at step=41, batch=30, test loss = 0.37939557433128357, test acc = 0.8650000095367432, time = 0.34954071044921875\n",
      "Testing at step=41, batch=35, test loss = 0.5036107897758484, test acc = 0.8500000238418579, time = 0.37856221199035645\n",
      "Testing at step=41, batch=40, test loss = 0.3106862008571625, test acc = 0.8849999904632568, time = 0.34305310249328613\n",
      "Testing at step=41, batch=45, test loss = 0.40411388874053955, test acc = 0.8799999952316284, time = 0.33741068840026855\n",
      "Step 41 finished in 310.75033617019653, Train loss = 0.2976649769147237, Test loss = 0.3647757524251938; Train Acc = 0.8909999976555506, Test Acc = 0.8685000002384186\n",
      "Training at step=42, batch=0, train loss = 0.24818967282772064, train acc = 0.9049999713897705, time = 0.9454712867736816\n",
      "Training at step=42, batch=30, train loss = 0.3277578055858612, train acc = 0.875, time = 0.9467599391937256\n",
      "Training at step=42, batch=60, train loss = 0.2899368107318878, train acc = 0.8949999809265137, time = 0.9371581077575684\n",
      "Training at step=42, batch=90, train loss = 0.3092944622039795, train acc = 0.8899999856948853, time = 0.9414925575256348\n",
      "Training at step=42, batch=120, train loss = 0.3175511658191681, train acc = 0.8949999809265137, time = 0.9413018226623535\n",
      "Training at step=42, batch=150, train loss = 0.38716575503349304, train acc = 0.875, time = 0.9386518001556396\n",
      "Training at step=42, batch=180, train loss = 0.22307850420475006, train acc = 0.9100000262260437, time = 0.9439651966094971\n",
      "Training at step=42, batch=210, train loss = 0.290809690952301, train acc = 0.8949999809265137, time = 0.9427735805511475\n",
      "Training at step=42, batch=240, train loss = 0.20734888315200806, train acc = 0.925000011920929, time = 0.947380781173706\n",
      "Training at step=42, batch=270, train loss = 0.34485071897506714, train acc = 0.875, time = 0.9469842910766602\n",
      "Testing at step=42, batch=0, test loss = 0.3052595257759094, test acc = 0.8999999761581421, time = 0.34354352951049805\n",
      "Testing at step=42, batch=5, test loss = 0.30724602937698364, test acc = 0.9200000166893005, time = 0.3414726257324219\n",
      "Testing at step=42, batch=10, test loss = 0.36711302399635315, test acc = 0.8799999952316284, time = 0.340299129486084\n",
      "Testing at step=42, batch=15, test loss = 0.3193965554237366, test acc = 0.8700000047683716, time = 0.3389008045196533\n",
      "Testing at step=42, batch=20, test loss = 0.3444876968860626, test acc = 0.8550000190734863, time = 0.3399219512939453\n",
      "Testing at step=42, batch=25, test loss = 0.3393535912036896, test acc = 0.8899999856948853, time = 0.3524606227874756\n",
      "Testing at step=42, batch=30, test loss = 0.3563116490840912, test acc = 0.8799999952316284, time = 0.3360929489135742\n",
      "Testing at step=42, batch=35, test loss = 0.33648043870925903, test acc = 0.8949999809265137, time = 0.3369333744049072\n",
      "Testing at step=42, batch=40, test loss = 0.3528822064399719, test acc = 0.8399999737739563, time = 0.33939456939697266\n",
      "Testing at step=42, batch=45, test loss = 0.3918724060058594, test acc = 0.875, time = 0.3408315181732178\n",
      "Step 42 finished in 310.7830526828766, Train loss = 0.29506385599573454, Test loss = 0.35602866142988204; Train Acc = 0.8917666639884313, Test Acc = 0.8732000017166137\n",
      "Training at step=43, batch=0, train loss = 0.29491519927978516, train acc = 0.8799999952316284, time = 0.9451656341552734\n",
      "Training at step=43, batch=30, train loss = 0.2682414650917053, train acc = 0.9100000262260437, time = 0.9481146335601807\n",
      "Training at step=43, batch=60, train loss = 0.2843756079673767, train acc = 0.9049999713897705, time = 0.9352145195007324\n",
      "Training at step=43, batch=90, train loss = 0.26326146721839905, train acc = 0.8999999761581421, time = 0.9460873603820801\n",
      "Training at step=43, batch=120, train loss = 0.35327446460723877, train acc = 0.8799999952316284, time = 0.9397215843200684\n",
      "Training at step=43, batch=150, train loss = 0.33834466338157654, train acc = 0.8500000238418579, time = 0.9620213508605957\n",
      "Training at step=43, batch=180, train loss = 0.2690460681915283, train acc = 0.9100000262260437, time = 0.9601254463195801\n",
      "Training at step=43, batch=210, train loss = 0.22266969084739685, train acc = 0.925000011920929, time = 0.9419155120849609\n",
      "Training at step=43, batch=240, train loss = 0.35499241948127747, train acc = 0.875, time = 0.9409034252166748\n",
      "Training at step=43, batch=270, train loss = 0.248447984457016, train acc = 0.9150000214576721, time = 0.9393494129180908\n",
      "Testing at step=43, batch=0, test loss = 0.3241567313671112, test acc = 0.8450000286102295, time = 0.3478834629058838\n",
      "Testing at step=43, batch=5, test loss = 0.21015943586826324, test acc = 0.9049999713897705, time = 0.3422098159790039\n",
      "Testing at step=43, batch=10, test loss = 0.3956127166748047, test acc = 0.8700000047683716, time = 0.3391101360321045\n",
      "Testing at step=43, batch=15, test loss = 0.46477916836738586, test acc = 0.8500000238418579, time = 0.3387033939361572\n",
      "Testing at step=43, batch=20, test loss = 0.3516994118690491, test acc = 0.8700000047683716, time = 0.3423950672149658\n",
      "Testing at step=43, batch=25, test loss = 0.4295486509799957, test acc = 0.8500000238418579, time = 0.3407716751098633\n",
      "Testing at step=43, batch=30, test loss = 0.3715405762195587, test acc = 0.8849999904632568, time = 0.3421502113342285\n",
      "Testing at step=43, batch=35, test loss = 0.3095923662185669, test acc = 0.8949999809265137, time = 0.34500598907470703\n",
      "Testing at step=43, batch=40, test loss = 0.3765645921230316, test acc = 0.8799999952316284, time = 0.34991931915283203\n",
      "Testing at step=43, batch=45, test loss = 0.3839420676231384, test acc = 0.8799999952316284, time = 0.34337806701660156\n",
      "Step 43 finished in 311.21030497550964, Train loss = 0.29295140773057937, Test loss = 0.3561394992470741; Train Acc = 0.8929999977350235, Test Acc = 0.8732999992370606\n",
      "Training at step=44, batch=0, train loss = 0.29498612880706787, train acc = 0.8849999904632568, time = 0.9368083477020264\n",
      "Training at step=44, batch=30, train loss = 0.24926874041557312, train acc = 0.9150000214576721, time = 0.9569535255432129\n",
      "Training at step=44, batch=60, train loss = 0.3434542417526245, train acc = 0.8949999809265137, time = 0.9378752708435059\n",
      "Training at step=44, batch=90, train loss = 0.2542285621166229, train acc = 0.9150000214576721, time = 0.9444615840911865\n",
      "Training at step=44, batch=120, train loss = 0.3274889886379242, train acc = 0.9049999713897705, time = 0.9385275840759277\n",
      "Training at step=44, batch=150, train loss = 0.24173003435134888, train acc = 0.9100000262260437, time = 0.9386711120605469\n",
      "Training at step=44, batch=180, train loss = 0.2750012278556824, train acc = 0.875, time = 0.9713702201843262\n",
      "Training at step=44, batch=210, train loss = 0.21754197776317596, train acc = 0.9049999713897705, time = 0.9382612705230713\n",
      "Training at step=44, batch=240, train loss = 0.29019153118133545, train acc = 0.8799999952316284, time = 0.9359846115112305\n",
      "Training at step=44, batch=270, train loss = 0.3987928628921509, train acc = 0.8949999809265137, time = 0.9441893100738525\n",
      "Testing at step=44, batch=0, test loss = 0.3960120379924774, test acc = 0.8450000286102295, time = 0.33699607849121094\n",
      "Testing at step=44, batch=5, test loss = 0.3098398447036743, test acc = 0.8650000095367432, time = 0.33988118171691895\n",
      "Testing at step=44, batch=10, test loss = 0.35819435119628906, test acc = 0.8799999952316284, time = 0.3400900363922119\n",
      "Testing at step=44, batch=15, test loss = 0.4422972500324249, test acc = 0.8399999737739563, time = 0.3376307487487793\n",
      "Testing at step=44, batch=20, test loss = 0.40696942806243896, test acc = 0.8650000095367432, time = 0.35010194778442383\n",
      "Testing at step=44, batch=25, test loss = 0.33828744292259216, test acc = 0.8700000047683716, time = 0.3387949466705322\n",
      "Testing at step=44, batch=30, test loss = 0.40629351139068604, test acc = 0.8650000095367432, time = 0.3468968868255615\n",
      "Testing at step=44, batch=35, test loss = 0.324726939201355, test acc = 0.8899999856948853, time = 0.343686580657959\n",
      "Testing at step=44, batch=40, test loss = 0.49091339111328125, test acc = 0.8550000190734863, time = 0.4134526252746582\n",
      "Testing at step=44, batch=45, test loss = 0.4505569040775299, test acc = 0.8650000095367432, time = 0.3381779193878174\n",
      "Step 44 finished in 310.501366853714, Train loss = 0.29251703634858134, Test loss = 0.36211892336606977; Train Acc = 0.8921666649977366, Test Acc = 0.8755999994277954\n",
      "Training at step=45, batch=0, train loss = 0.33065181970596313, train acc = 0.8849999904632568, time = 0.9403715133666992\n",
      "Training at step=45, batch=30, train loss = 0.2386055737733841, train acc = 0.9100000262260437, time = 1.0235836505889893\n",
      "Training at step=45, batch=60, train loss = 0.28976136445999146, train acc = 0.8849999904632568, time = 0.9456117153167725\n",
      "Training at step=45, batch=90, train loss = 0.3751167058944702, train acc = 0.8450000286102295, time = 0.9372410774230957\n",
      "Training at step=45, batch=120, train loss = 0.27911055088043213, train acc = 0.8899999856948853, time = 0.9380769729614258\n",
      "Training at step=45, batch=150, train loss = 0.43226858973503113, train acc = 0.8100000023841858, time = 0.9390859603881836\n",
      "Training at step=45, batch=180, train loss = 0.21217207610607147, train acc = 0.9200000166893005, time = 0.9660155773162842\n",
      "Training at step=45, batch=210, train loss = 0.2640961706638336, train acc = 0.9049999713897705, time = 0.9429914951324463\n",
      "Training at step=45, batch=240, train loss = 0.2892715334892273, train acc = 0.9049999713897705, time = 0.936859130859375\n",
      "Training at step=45, batch=270, train loss = 0.28391745686531067, train acc = 0.8600000143051147, time = 0.9407074451446533\n",
      "Testing at step=45, batch=0, test loss = 0.35412827134132385, test acc = 0.875, time = 0.340226411819458\n",
      "Testing at step=45, batch=5, test loss = 0.3538205027580261, test acc = 0.8849999904632568, time = 0.33873486518859863\n",
      "Testing at step=45, batch=10, test loss = 0.30233675241470337, test acc = 0.875, time = 0.33750224113464355\n",
      "Testing at step=45, batch=15, test loss = 0.2790790796279907, test acc = 0.8949999809265137, time = 0.34383702278137207\n",
      "Testing at step=45, batch=20, test loss = 0.38660645484924316, test acc = 0.8550000190734863, time = 0.34832119941711426\n",
      "Testing at step=45, batch=25, test loss = 0.3817136883735657, test acc = 0.8799999952316284, time = 0.3391296863555908\n",
      "Testing at step=45, batch=30, test loss = 0.2554146945476532, test acc = 0.8899999856948853, time = 0.33709168434143066\n",
      "Testing at step=45, batch=35, test loss = 0.3826921880245209, test acc = 0.8899999856948853, time = 0.33949899673461914\n",
      "Testing at step=45, batch=40, test loss = 0.39578965306282043, test acc = 0.8500000238418579, time = 0.344224214553833\n",
      "Testing at step=45, batch=45, test loss = 0.3874408006668091, test acc = 0.8799999952316284, time = 0.3448643684387207\n",
      "Step 45 finished in 310.6357274055481, Train loss = 0.29012783989310265, Test loss = 0.3616571393609047; Train Acc = 0.89293332974116, Test Acc = 0.8714000058174133\n",
      "Training at step=46, batch=0, train loss = 0.3548939526081085, train acc = 0.8700000047683716, time = 0.9452295303344727\n",
      "Training at step=46, batch=30, train loss = 0.1874454915523529, train acc = 0.9350000023841858, time = 0.9582059383392334\n",
      "Training at step=46, batch=60, train loss = 0.30369681119918823, train acc = 0.9150000214576721, time = 0.9341433048248291\n",
      "Training at step=46, batch=90, train loss = 0.3531688451766968, train acc = 0.8700000047683716, time = 0.9344770908355713\n",
      "Training at step=46, batch=120, train loss = 0.28698620200157166, train acc = 0.8899999856948853, time = 0.9520385265350342\n",
      "Training at step=46, batch=150, train loss = 0.12583991885185242, train acc = 0.9549999833106995, time = 0.9523770809173584\n",
      "Training at step=46, batch=180, train loss = 0.3576301634311676, train acc = 0.8849999904632568, time = 0.945347785949707\n",
      "Training at step=46, batch=210, train loss = 0.29410994052886963, train acc = 0.8849999904632568, time = 0.9391686916351318\n",
      "Training at step=46, batch=240, train loss = 0.4054041802883148, train acc = 0.8500000238418579, time = 0.9461076259613037\n",
      "Training at step=46, batch=270, train loss = 0.33050835132598877, train acc = 0.8550000190734863, time = 0.9353570938110352\n",
      "Testing at step=46, batch=0, test loss = 0.29362010955810547, test acc = 0.8799999952316284, time = 0.33777379989624023\n",
      "Testing at step=46, batch=5, test loss = 0.33983132243156433, test acc = 0.8899999856948853, time = 0.3422815799713135\n",
      "Testing at step=46, batch=10, test loss = 0.37447044253349304, test acc = 0.8500000238418579, time = 0.3365657329559326\n",
      "Testing at step=46, batch=15, test loss = 0.43247130513191223, test acc = 0.8199999928474426, time = 0.34079766273498535\n",
      "Testing at step=46, batch=20, test loss = 0.3131379187107086, test acc = 0.8949999809265137, time = 0.34023237228393555\n",
      "Testing at step=46, batch=25, test loss = 0.42442965507507324, test acc = 0.8600000143051147, time = 0.34005117416381836\n",
      "Testing at step=46, batch=30, test loss = 0.38457560539245605, test acc = 0.8500000238418579, time = 0.3415699005126953\n",
      "Testing at step=46, batch=35, test loss = 0.3443899154663086, test acc = 0.8949999809265137, time = 0.34142422676086426\n",
      "Testing at step=46, batch=40, test loss = 0.3790059983730316, test acc = 0.875, time = 0.3420405387878418\n",
      "Testing at step=46, batch=45, test loss = 0.3945915102958679, test acc = 0.8500000238418579, time = 0.3410956859588623\n",
      "Step 46 finished in 311.21417355537415, Train loss = 0.28819092084964115, Test loss = 0.36504311859607697; Train Acc = 0.8946166654427846, Test Acc = 0.8682000005245208\n",
      "Training at step=47, batch=0, train loss = 0.3885321021080017, train acc = 0.8849999904632568, time = 0.9481058120727539\n",
      "Training at step=47, batch=30, train loss = 0.24300003051757812, train acc = 0.9049999713897705, time = 1.0227677822113037\n",
      "Training at step=47, batch=60, train loss = 0.2497214525938034, train acc = 0.9049999713897705, time = 0.9467997550964355\n",
      "Training at step=47, batch=90, train loss = 0.39456623792648315, train acc = 0.8399999737739563, time = 0.9408235549926758\n",
      "Training at step=47, batch=120, train loss = 0.31635674834251404, train acc = 0.8799999952316284, time = 0.9364638328552246\n",
      "Training at step=47, batch=150, train loss = 0.32667091488838196, train acc = 0.8799999952316284, time = 0.9382798671722412\n",
      "Training at step=47, batch=180, train loss = 0.27758878469467163, train acc = 0.8949999809265137, time = 0.9399011135101318\n",
      "Training at step=47, batch=210, train loss = 0.2729080617427826, train acc = 0.9049999713897705, time = 0.961418628692627\n",
      "Training at step=47, batch=240, train loss = 0.38995102047920227, train acc = 0.8700000047683716, time = 0.9514596462249756\n",
      "Training at step=47, batch=270, train loss = 0.24231737852096558, train acc = 0.9200000166893005, time = 0.9405632019042969\n",
      "Testing at step=47, batch=0, test loss = 0.5324980020523071, test acc = 0.8199999928474426, time = 0.34065794944763184\n",
      "Testing at step=47, batch=5, test loss = 0.3873445391654968, test acc = 0.8700000047683716, time = 0.3367934226989746\n",
      "Testing at step=47, batch=10, test loss = 0.24821913242340088, test acc = 0.9200000166893005, time = 0.3442821502685547\n",
      "Testing at step=47, batch=15, test loss = 0.41911768913269043, test acc = 0.8700000047683716, time = 0.3510878086090088\n",
      "Testing at step=47, batch=20, test loss = 0.41556259989738464, test acc = 0.8799999952316284, time = 0.3420746326446533\n",
      "Testing at step=47, batch=25, test loss = 0.3642198443412781, test acc = 0.8999999761581421, time = 0.33873414993286133\n",
      "Testing at step=47, batch=30, test loss = 0.409236878156662, test acc = 0.8500000238418579, time = 0.33808326721191406\n",
      "Testing at step=47, batch=35, test loss = 0.35055553913116455, test acc = 0.8849999904632568, time = 0.35971570014953613\n",
      "Testing at step=47, batch=40, test loss = 0.34124821424484253, test acc = 0.8399999737739563, time = 0.34303712844848633\n",
      "Testing at step=47, batch=45, test loss = 0.31973111629486084, test acc = 0.8500000238418579, time = 0.3375282287597656\n",
      "Step 47 finished in 311.5589394569397, Train loss = 0.2880259437362353, Test loss = 0.3531069931387901; Train Acc = 0.8945333309968313, Test Acc = 0.8764000010490417\n",
      "Training at step=48, batch=0, train loss = 0.27310729026794434, train acc = 0.8949999809265137, time = 0.9460957050323486\n",
      "Training at step=48, batch=30, train loss = 0.2655174136161804, train acc = 0.925000011920929, time = 0.9401390552520752\n",
      "Training at step=48, batch=60, train loss = 0.2846698462963104, train acc = 0.8849999904632568, time = 0.9436116218566895\n",
      "Training at step=48, batch=90, train loss = 0.2783394753932953, train acc = 0.875, time = 0.9387416839599609\n",
      "Training at step=48, batch=120, train loss = 0.26962167024612427, train acc = 0.9100000262260437, time = 0.9420809745788574\n",
      "Training at step=48, batch=150, train loss = 0.3380950093269348, train acc = 0.8799999952316284, time = 0.9402880668640137\n",
      "Training at step=48, batch=180, train loss = 0.21447964012622833, train acc = 0.9100000262260437, time = 0.9617199897766113\n",
      "Training at step=48, batch=210, train loss = 0.26032376289367676, train acc = 0.9150000214576721, time = 0.9440228939056396\n",
      "Training at step=48, batch=240, train loss = 0.3430696427822113, train acc = 0.875, time = 0.9617140293121338\n",
      "Training at step=48, batch=270, train loss = 0.3138749301433563, train acc = 0.8849999904632568, time = 0.939033031463623\n",
      "Testing at step=48, batch=0, test loss = 0.3642365634441376, test acc = 0.8799999952316284, time = 0.33901238441467285\n",
      "Testing at step=48, batch=5, test loss = 0.28886380791664124, test acc = 0.8849999904632568, time = 0.3424670696258545\n",
      "Testing at step=48, batch=10, test loss = 0.3048902750015259, test acc = 0.875, time = 0.3423941135406494\n",
      "Testing at step=48, batch=15, test loss = 0.33087262511253357, test acc = 0.9150000214576721, time = 0.34926509857177734\n",
      "Testing at step=48, batch=20, test loss = 0.3138929307460785, test acc = 0.8650000095367432, time = 0.36951446533203125\n",
      "Testing at step=48, batch=25, test loss = 0.4516730010509491, test acc = 0.875, time = 0.34104275703430176\n",
      "Testing at step=48, batch=30, test loss = 0.4299105703830719, test acc = 0.8450000286102295, time = 0.3399391174316406\n",
      "Testing at step=48, batch=35, test loss = 0.3596753776073456, test acc = 0.8600000143051147, time = 0.34050536155700684\n",
      "Testing at step=48, batch=40, test loss = 0.4013153910636902, test acc = 0.8450000286102295, time = 0.3427543640136719\n",
      "Testing at step=48, batch=45, test loss = 0.3921844959259033, test acc = 0.8650000095367432, time = 0.3397505283355713\n",
      "Step 48 finished in 311.39813470840454, Train loss = 0.2863515750567118, Test loss = 0.3556913688778877; Train Acc = 0.8950666630268097, Test Acc = 0.8767000007629394\n",
      "Training at step=49, batch=0, train loss = 0.30982741713523865, train acc = 0.8949999809265137, time = 0.9397752285003662\n",
      "Training at step=49, batch=30, train loss = 0.29800719022750854, train acc = 0.9100000262260437, time = 0.9554793834686279\n",
      "Training at step=49, batch=60, train loss = 0.3090834319591522, train acc = 0.8999999761581421, time = 0.9420273303985596\n",
      "Training at step=49, batch=90, train loss = 0.313091903924942, train acc = 0.8849999904632568, time = 0.9426577091217041\n",
      "Training at step=49, batch=120, train loss = 0.2876511514186859, train acc = 0.8949999809265137, time = 0.9517960548400879\n",
      "Training at step=49, batch=150, train loss = 0.21919286251068115, train acc = 0.9100000262260437, time = 0.9406073093414307\n",
      "Training at step=49, batch=180, train loss = 0.24446655809879303, train acc = 0.9049999713897705, time = 0.9396576881408691\n",
      "Training at step=49, batch=210, train loss = 0.25287970900535583, train acc = 0.8899999856948853, time = 0.940563440322876\n",
      "Training at step=49, batch=240, train loss = 0.23980456590652466, train acc = 0.8999999761581421, time = 0.9400179386138916\n",
      "Training at step=49, batch=270, train loss = 0.27853715419769287, train acc = 0.9100000262260437, time = 0.9343893527984619\n",
      "Testing at step=49, batch=0, test loss = 0.37504300475120544, test acc = 0.8700000047683716, time = 0.34943509101867676\n",
      "Testing at step=49, batch=5, test loss = 0.31106406450271606, test acc = 0.875, time = 0.34061622619628906\n",
      "Testing at step=49, batch=10, test loss = 0.3160178065299988, test acc = 0.8849999904632568, time = 0.34221506118774414\n",
      "Testing at step=49, batch=15, test loss = 0.38649046421051025, test acc = 0.8650000095367432, time = 0.33912205696105957\n",
      "Testing at step=49, batch=20, test loss = 0.3352939486503601, test acc = 0.8600000143051147, time = 0.34881091117858887\n",
      "Testing at step=49, batch=25, test loss = 0.26220282912254333, test acc = 0.8949999809265137, time = 0.33771324157714844\n",
      "Testing at step=49, batch=30, test loss = 0.31078240275382996, test acc = 0.8949999809265137, time = 0.3411262035369873\n",
      "Testing at step=49, batch=35, test loss = 0.401620477437973, test acc = 0.8700000047683716, time = 0.3425896167755127\n",
      "Testing at step=49, batch=40, test loss = 0.25910013914108276, test acc = 0.8949999809265137, time = 0.35702061653137207\n",
      "Testing at step=49, batch=45, test loss = 0.2838379740715027, test acc = 0.9100000262260437, time = 0.36672425270080566\n",
      "Step 49 finished in 311.87825655937195, Train loss = 0.28667092020312945, Test loss = 0.35067598164081576; Train Acc = 0.8951166631778081, Test Acc = 0.8761999976634979\n",
      "Training at step=50, batch=0, train loss = 0.31915003061294556, train acc = 0.8949999809265137, time = 0.9443814754486084\n",
      "Training at step=50, batch=30, train loss = 0.27087661623954773, train acc = 0.9200000166893005, time = 0.9380242824554443\n",
      "Training at step=50, batch=60, train loss = 0.3632168173789978, train acc = 0.8550000190734863, time = 0.9538178443908691\n",
      "Training at step=50, batch=90, train loss = 0.2237529158592224, train acc = 0.9049999713897705, time = 0.9400341510772705\n",
      "Training at step=50, batch=120, train loss = 0.42917731404304504, train acc = 0.824999988079071, time = 0.9408597946166992\n",
      "Training at step=50, batch=150, train loss = 0.2832511365413666, train acc = 0.9100000262260437, time = 0.9451959133148193\n",
      "Training at step=50, batch=180, train loss = 0.25583764910697937, train acc = 0.9049999713897705, time = 0.9680256843566895\n",
      "Training at step=50, batch=210, train loss = 0.25293073058128357, train acc = 0.8999999761581421, time = 0.943068265914917\n",
      "Training at step=50, batch=240, train loss = 0.2785031199455261, train acc = 0.9100000262260437, time = 0.9454550743103027\n",
      "Training at step=50, batch=270, train loss = 0.34690606594085693, train acc = 0.8650000095367432, time = 0.941288948059082\n",
      "Testing at step=50, batch=0, test loss = 0.3387433886528015, test acc = 0.8799999952316284, time = 0.3403737545013428\n",
      "Testing at step=50, batch=5, test loss = 0.37328431010246277, test acc = 0.875, time = 0.3391895294189453\n",
      "Testing at step=50, batch=10, test loss = 0.33557403087615967, test acc = 0.9049999713897705, time = 0.3374052047729492\n",
      "Testing at step=50, batch=15, test loss = 0.35894349217414856, test acc = 0.8700000047683716, time = 0.3413398265838623\n",
      "Testing at step=50, batch=20, test loss = 0.3123817443847656, test acc = 0.8949999809265137, time = 0.344066858291626\n",
      "Testing at step=50, batch=25, test loss = 0.380756139755249, test acc = 0.8799999952316284, time = 0.34125614166259766\n",
      "Testing at step=50, batch=30, test loss = 0.4437696039676666, test acc = 0.8650000095367432, time = 0.346240758895874\n",
      "Testing at step=50, batch=35, test loss = 0.3160040080547333, test acc = 0.8650000095367432, time = 0.34066271781921387\n",
      "Testing at step=50, batch=40, test loss = 0.29888251423835754, test acc = 0.8899999856948853, time = 0.34111857414245605\n",
      "Testing at step=50, batch=45, test loss = 0.36109885573387146, test acc = 0.8550000190734863, time = 0.3456912040710449\n",
      "Step 50 finished in 310.3910856246948, Train loss = 0.28454462269941966, Test loss = 0.34505467504262927; Train Acc = 0.8950333291292191, Test Acc = 0.8780999994277954\n",
      "Training at step=51, batch=0, train loss = 0.32519295811653137, train acc = 0.8799999952316284, time = 0.9405248165130615\n",
      "Training at step=51, batch=30, train loss = 0.39212340116500854, train acc = 0.8500000238418579, time = 1.002417802810669\n",
      "Training at step=51, batch=60, train loss = 0.2071746289730072, train acc = 0.9150000214576721, time = 0.9381623268127441\n",
      "Training at step=51, batch=90, train loss = 0.23155853152275085, train acc = 0.9049999713897705, time = 0.9430179595947266\n",
      "Training at step=51, batch=120, train loss = 0.20826385915279388, train acc = 0.9150000214576721, time = 0.9401075839996338\n",
      "Training at step=51, batch=150, train loss = 0.3516998291015625, train acc = 0.8799999952316284, time = 0.9373724460601807\n",
      "Training at step=51, batch=180, train loss = 0.2947404086589813, train acc = 0.875, time = 0.9368813037872314\n",
      "Training at step=51, batch=210, train loss = 0.22418886423110962, train acc = 0.9200000166893005, time = 0.9402294158935547\n",
      "Training at step=51, batch=240, train loss = 0.17656473815441132, train acc = 0.949999988079071, time = 0.9387035369873047\n",
      "Training at step=51, batch=270, train loss = 0.34174808859825134, train acc = 0.875, time = 0.9597806930541992\n",
      "Testing at step=51, batch=0, test loss = 0.25851500034332275, test acc = 0.9200000166893005, time = 0.420363187789917\n",
      "Testing at step=51, batch=5, test loss = 0.3673733174800873, test acc = 0.875, time = 0.34026408195495605\n",
      "Testing at step=51, batch=10, test loss = 0.4231759309768677, test acc = 0.8299999833106995, time = 0.3420581817626953\n",
      "Testing at step=51, batch=15, test loss = 0.2353559285402298, test acc = 0.9200000166893005, time = 0.340132474899292\n",
      "Testing at step=51, batch=20, test loss = 0.3142983913421631, test acc = 0.8849999904632568, time = 0.34423017501831055\n",
      "Testing at step=51, batch=25, test loss = 0.3830752968788147, test acc = 0.8700000047683716, time = 0.33959150314331055\n",
      "Testing at step=51, batch=30, test loss = 0.3743796944618225, test acc = 0.8799999952316284, time = 0.34070467948913574\n",
      "Testing at step=51, batch=35, test loss = 0.36471474170684814, test acc = 0.8450000286102295, time = 0.3403441905975342\n",
      "Testing at step=51, batch=40, test loss = 0.4236675500869751, test acc = 0.8450000286102295, time = 0.3425893783569336\n",
      "Testing at step=51, batch=45, test loss = 0.47118040919303894, test acc = 0.8500000238418579, time = 0.3412652015686035\n",
      "Step 51 finished in 310.71716713905334, Train loss = 0.2800903641184171, Test loss = 0.3600807556509972; Train Acc = 0.8965666657686233, Test Acc = 0.8756000006198883\n",
      "Training at step=52, batch=0, train loss = 0.2047695368528366, train acc = 0.9300000071525574, time = 0.9416103363037109\n",
      "Training at step=52, batch=30, train loss = 0.3621664047241211, train acc = 0.8550000190734863, time = 0.9416844844818115\n",
      "Training at step=52, batch=60, train loss = 0.2012425810098648, train acc = 0.925000011920929, time = 0.940131664276123\n",
      "Training at step=52, batch=90, train loss = 0.232835590839386, train acc = 0.8949999809265137, time = 0.9392426013946533\n",
      "Training at step=52, batch=120, train loss = 0.3194892704486847, train acc = 0.9049999713897705, time = 0.9326913356781006\n",
      "Training at step=52, batch=150, train loss = 0.3391578793525696, train acc = 0.8550000190734863, time = 0.9400966167449951\n",
      "Training at step=52, batch=180, train loss = 0.29235464334487915, train acc = 0.8999999761581421, time = 0.9402682781219482\n",
      "Training at step=52, batch=210, train loss = 0.2518349587917328, train acc = 0.8999999761581421, time = 0.948065996170044\n",
      "Training at step=52, batch=240, train loss = 0.29923829436302185, train acc = 0.8899999856948853, time = 0.9384644031524658\n",
      "Training at step=52, batch=270, train loss = 0.3256644904613495, train acc = 0.8849999904632568, time = 0.9502906799316406\n",
      "Testing at step=52, batch=0, test loss = 0.359000563621521, test acc = 0.8700000047683716, time = 0.3421626091003418\n",
      "Testing at step=52, batch=5, test loss = 0.38184162974357605, test acc = 0.875, time = 0.34121155738830566\n",
      "Testing at step=52, batch=10, test loss = 0.33130955696105957, test acc = 0.8450000286102295, time = 0.34240174293518066\n",
      "Testing at step=52, batch=15, test loss = 0.3615744709968567, test acc = 0.8799999952316284, time = 0.33733272552490234\n",
      "Testing at step=52, batch=20, test loss = 0.3700588643550873, test acc = 0.8700000047683716, time = 0.3394770622253418\n",
      "Testing at step=52, batch=25, test loss = 0.31753861904144287, test acc = 0.8949999809265137, time = 0.3380444049835205\n",
      "Testing at step=52, batch=30, test loss = 0.27589237689971924, test acc = 0.8949999809265137, time = 0.34343838691711426\n",
      "Testing at step=52, batch=35, test loss = 0.28764334321022034, test acc = 0.8899999856948853, time = 0.3441925048828125\n",
      "Testing at step=52, batch=40, test loss = 0.41916465759277344, test acc = 0.8450000286102295, time = 0.33974385261535645\n",
      "Testing at step=52, batch=45, test loss = 0.34852561354637146, test acc = 0.8799999952316284, time = 0.3430910110473633\n",
      "Step 52 finished in 311.08964228630066, Train loss = 0.28129039898514746, Test loss = 0.3578565636277199; Train Acc = 0.8958999969561895, Test Acc = 0.8741000008583069\n",
      "Training at step=53, batch=0, train loss = 0.2653507888317108, train acc = 0.8899999856948853, time = 0.9421126842498779\n",
      "Training at step=53, batch=30, train loss = 0.328354150056839, train acc = 0.8849999904632568, time = 0.9415445327758789\n",
      "Training at step=53, batch=60, train loss = 0.24447961151599884, train acc = 0.8949999809265137, time = 0.9415743350982666\n",
      "Training at step=53, batch=90, train loss = 0.22633551061153412, train acc = 0.9100000262260437, time = 0.9390268325805664\n",
      "Training at step=53, batch=120, train loss = 0.25245270133018494, train acc = 0.9100000262260437, time = 0.9381682872772217\n",
      "Training at step=53, batch=150, train loss = 0.26021987199783325, train acc = 0.8949999809265137, time = 0.9494562149047852\n",
      "Training at step=53, batch=180, train loss = 0.24515481293201447, train acc = 0.9100000262260437, time = 0.937781572341919\n",
      "Training at step=53, batch=210, train loss = 0.262759804725647, train acc = 0.8899999856948853, time = 0.959956169128418\n",
      "Training at step=53, batch=240, train loss = 0.294155091047287, train acc = 0.8849999904632568, time = 0.9451544284820557\n",
      "Training at step=53, batch=270, train loss = 0.40907448530197144, train acc = 0.8550000190734863, time = 0.958768367767334\n",
      "Testing at step=53, batch=0, test loss = 0.41117390990257263, test acc = 0.8550000190734863, time = 0.34338879585266113\n",
      "Testing at step=53, batch=5, test loss = 0.2847362756729126, test acc = 0.8799999952316284, time = 0.33998703956604004\n",
      "Testing at step=53, batch=10, test loss = 0.31522834300994873, test acc = 0.8999999761581421, time = 0.3446311950683594\n",
      "Testing at step=53, batch=15, test loss = 0.3212971091270447, test acc = 0.8849999904632568, time = 0.3469700813293457\n",
      "Testing at step=53, batch=20, test loss = 0.3721161186695099, test acc = 0.8500000238418579, time = 0.3389699459075928\n",
      "Testing at step=53, batch=25, test loss = 0.38318324089050293, test acc = 0.8899999856948853, time = 0.34490537643432617\n",
      "Testing at step=53, batch=30, test loss = 0.32707110047340393, test acc = 0.8799999952316284, time = 0.3472144603729248\n",
      "Testing at step=53, batch=35, test loss = 0.5057884454727173, test acc = 0.8700000047683716, time = 0.34626054763793945\n",
      "Testing at step=53, batch=40, test loss = 0.2830989360809326, test acc = 0.9049999713897705, time = 0.3431127071380615\n",
      "Testing at step=53, batch=45, test loss = 0.2749650180339813, test acc = 0.9049999713897705, time = 0.34100818634033203\n",
      "Step 53 finished in 311.1164405345917, Train loss = 0.27833713601032894, Test loss = 0.3472431117296219; Train Acc = 0.8981166646877925, Test Acc = 0.8794999980926513\n",
      "Training at step=54, batch=0, train loss = 0.2785388231277466, train acc = 0.8999999761581421, time = 0.937706708908081\n",
      "Training at step=54, batch=30, train loss = 0.2776740789413452, train acc = 0.8849999904632568, time = 0.9424853324890137\n",
      "Training at step=54, batch=60, train loss = 0.25291258096694946, train acc = 0.8999999761581421, time = 0.9441444873809814\n",
      "Training at step=54, batch=90, train loss = 0.3079492449760437, train acc = 0.8799999952316284, time = 0.9375922679901123\n",
      "Training at step=54, batch=120, train loss = 0.29880332946777344, train acc = 0.8849999904632568, time = 0.9439647197723389\n",
      "Training at step=54, batch=150, train loss = 0.2467639148235321, train acc = 0.9200000166893005, time = 0.9440789222717285\n",
      "Training at step=54, batch=180, train loss = 0.23250748217105865, train acc = 0.9350000023841858, time = 0.9377617835998535\n",
      "Training at step=54, batch=210, train loss = 0.2663065195083618, train acc = 0.8899999856948853, time = 0.9353909492492676\n",
      "Training at step=54, batch=240, train loss = 0.2984263300895691, train acc = 0.8899999856948853, time = 0.9354450702667236\n",
      "Training at step=54, batch=270, train loss = 0.2548476755619049, train acc = 0.9350000023841858, time = 0.9400529861450195\n",
      "Testing at step=54, batch=0, test loss = 0.29236069321632385, test acc = 0.8999999761581421, time = 0.35285091400146484\n",
      "Testing at step=54, batch=5, test loss = 0.3886989653110504, test acc = 0.8700000047683716, time = 0.3474748134613037\n",
      "Testing at step=54, batch=10, test loss = 0.38694241642951965, test acc = 0.8450000286102295, time = 0.34496521949768066\n",
      "Testing at step=54, batch=15, test loss = 0.4479432702064514, test acc = 0.8799999952316284, time = 0.34195876121520996\n",
      "Testing at step=54, batch=20, test loss = 0.28629618883132935, test acc = 0.8949999809265137, time = 0.33976316452026367\n",
      "Testing at step=54, batch=25, test loss = 0.3208337724208832, test acc = 0.8849999904632568, time = 0.3398861885070801\n",
      "Testing at step=54, batch=30, test loss = 0.3479255437850952, test acc = 0.8700000047683716, time = 0.3599879741668701\n",
      "Testing at step=54, batch=35, test loss = 0.3567999303340912, test acc = 0.8600000143051147, time = 0.3447153568267822\n",
      "Testing at step=54, batch=40, test loss = 0.31606370210647583, test acc = 0.8799999952316284, time = 0.348236083984375\n",
      "Testing at step=54, batch=45, test loss = 0.27718326449394226, test acc = 0.9049999713897705, time = 0.3451399803161621\n",
      "Step 54 finished in 311.11649203300476, Train loss = 0.2768667444835107, Test loss = 0.3543854355812073; Train Acc = 0.8987666648626328, Test Acc = 0.8758000004291534\n",
      "Training at step=55, batch=0, train loss = 0.2874680459499359, train acc = 0.8799999952316284, time = 0.9432382583618164\n",
      "Training at step=55, batch=30, train loss = 0.25830012559890747, train acc = 0.9049999713897705, time = 0.9379072189331055\n",
      "Training at step=55, batch=60, train loss = 0.24331778287887573, train acc = 0.9200000166893005, time = 0.9325425624847412\n",
      "Training at step=55, batch=90, train loss = 0.23934286832809448, train acc = 0.9399999976158142, time = 0.9439661502838135\n",
      "Training at step=55, batch=120, train loss = 0.24559080600738525, train acc = 0.9049999713897705, time = 0.9510703086853027\n",
      "Training at step=55, batch=150, train loss = 0.29398584365844727, train acc = 0.8799999952316284, time = 0.9429895877838135\n",
      "Training at step=55, batch=180, train loss = 0.2227974385023117, train acc = 0.9100000262260437, time = 0.9367432594299316\n",
      "Training at step=55, batch=210, train loss = 0.21791456639766693, train acc = 0.9449999928474426, time = 0.9375989437103271\n",
      "Training at step=55, batch=240, train loss = 0.30943557620048523, train acc = 0.8700000047683716, time = 0.9579222202301025\n",
      "Training at step=55, batch=270, train loss = 0.219610333442688, train acc = 0.9049999713897705, time = 0.936091423034668\n",
      "Testing at step=55, batch=0, test loss = 0.2724955976009369, test acc = 0.8949999809265137, time = 0.3403134346008301\n",
      "Testing at step=55, batch=5, test loss = 0.3855741620063782, test acc = 0.875, time = 0.3391420841217041\n",
      "Testing at step=55, batch=10, test loss = 0.3332017660140991, test acc = 0.875, time = 0.34418559074401855\n",
      "Testing at step=55, batch=15, test loss = 0.45012715458869934, test acc = 0.8550000190734863, time = 0.33817195892333984\n",
      "Testing at step=55, batch=20, test loss = 0.20789800584316254, test acc = 0.9200000166893005, time = 0.34628725051879883\n",
      "Testing at step=55, batch=25, test loss = 0.31453627347946167, test acc = 0.875, time = 0.3364224433898926\n",
      "Testing at step=55, batch=30, test loss = 0.43343573808670044, test acc = 0.8600000143051147, time = 0.3372683525085449\n",
      "Testing at step=55, batch=35, test loss = 0.3839677572250366, test acc = 0.8450000286102295, time = 0.34179091453552246\n",
      "Testing at step=55, batch=40, test loss = 0.34362873435020447, test acc = 0.8700000047683716, time = 0.3451371192932129\n",
      "Testing at step=55, batch=45, test loss = 0.5298288464546204, test acc = 0.8399999737739563, time = 0.33863282203674316\n",
      "Step 55 finished in 311.3161108493805, Train loss = 0.27453413501381874, Test loss = 0.38604826241731643; Train Acc = 0.8985499978065491, Test Acc = 0.8658000016212464\n",
      "Training at step=56, batch=0, train loss = 0.3469017446041107, train acc = 0.8650000095367432, time = 0.9402563571929932\n",
      "Training at step=56, batch=30, train loss = 0.2830393612384796, train acc = 0.8949999809265137, time = 0.9344780445098877\n",
      "Training at step=56, batch=60, train loss = 0.3610191345214844, train acc = 0.8700000047683716, time = 0.938767671585083\n",
      "Training at step=56, batch=90, train loss = 0.3141797184944153, train acc = 0.8949999809265137, time = 0.9441425800323486\n",
      "Training at step=56, batch=120, train loss = 0.33650362491607666, train acc = 0.8949999809265137, time = 0.9473743438720703\n",
      "Training at step=56, batch=150, train loss = 0.2754417955875397, train acc = 0.8799999952316284, time = 0.9373955726623535\n",
      "Training at step=56, batch=180, train loss = 0.32007521390914917, train acc = 0.8949999809265137, time = 0.9426000118255615\n",
      "Training at step=56, batch=210, train loss = 0.20116853713989258, train acc = 0.925000011920929, time = 0.9395759105682373\n",
      "Training at step=56, batch=240, train loss = 0.21335917711257935, train acc = 0.9150000214576721, time = 0.9419336318969727\n",
      "Training at step=56, batch=270, train loss = 0.2486734390258789, train acc = 0.8999999761581421, time = 0.9368033409118652\n",
      "Testing at step=56, batch=0, test loss = 0.32855814695358276, test acc = 0.8799999952316284, time = 0.3539292812347412\n",
      "Testing at step=56, batch=5, test loss = 0.297892689704895, test acc = 0.9100000262260437, time = 0.34638500213623047\n",
      "Testing at step=56, batch=10, test loss = 0.28016430139541626, test acc = 0.8999999761581421, time = 0.3606548309326172\n",
      "Testing at step=56, batch=15, test loss = 0.2924797236919403, test acc = 0.9049999713897705, time = 0.3396303653717041\n",
      "Testing at step=56, batch=20, test loss = 0.36368945240974426, test acc = 0.8899999856948853, time = 0.3403472900390625\n",
      "Testing at step=56, batch=25, test loss = 0.3506510257720947, test acc = 0.8899999856948853, time = 0.34387993812561035\n",
      "Testing at step=56, batch=30, test loss = 0.29338163137435913, test acc = 0.8899999856948853, time = 0.3393898010253906\n",
      "Testing at step=56, batch=35, test loss = 0.3296060562133789, test acc = 0.8849999904632568, time = 0.3664968013763428\n",
      "Testing at step=56, batch=40, test loss = 0.3352324366569519, test acc = 0.8849999904632568, time = 0.34224605560302734\n",
      "Testing at step=56, batch=45, test loss = 0.3373596966266632, test acc = 0.8849999904632568, time = 0.4086606502532959\n",
      "Step 56 finished in 311.37970089912415, Train loss = 0.27634786928693456, Test loss = 0.34887776225805284; Train Acc = 0.8971666644016901, Test Acc = 0.8786999988555908\n",
      "Training at step=57, batch=0, train loss = 0.3592601418495178, train acc = 0.8849999904632568, time = 0.943533182144165\n",
      "Training at step=57, batch=30, train loss = 0.35998424887657166, train acc = 0.8550000190734863, time = 0.9376983642578125\n",
      "Training at step=57, batch=60, train loss = 0.30381080508232117, train acc = 0.9049999713897705, time = 0.9440984725952148\n",
      "Training at step=57, batch=90, train loss = 0.3007519841194153, train acc = 0.9150000214576721, time = 0.9384372234344482\n",
      "Training at step=57, batch=120, train loss = 0.3424985408782959, train acc = 0.8849999904632568, time = 0.9461314678192139\n",
      "Training at step=57, batch=150, train loss = 0.28625547885894775, train acc = 0.9100000262260437, time = 0.9457540512084961\n",
      "Training at step=57, batch=180, train loss = 0.2095165103673935, train acc = 0.9150000214576721, time = 0.9374010562896729\n",
      "Training at step=57, batch=210, train loss = 0.22524602711200714, train acc = 0.9200000166893005, time = 0.9559826850891113\n",
      "Training at step=57, batch=240, train loss = 0.2628500759601593, train acc = 0.9150000214576721, time = 0.938054084777832\n",
      "Training at step=57, batch=270, train loss = 0.24490320682525635, train acc = 0.8999999761581421, time = 0.9390537738800049\n",
      "Testing at step=57, batch=0, test loss = 0.37673428654670715, test acc = 0.8799999952316284, time = 0.3626980781555176\n",
      "Testing at step=57, batch=5, test loss = 0.3800700902938843, test acc = 0.9049999713897705, time = 0.3379185199737549\n",
      "Testing at step=57, batch=10, test loss = 0.38054797053337097, test acc = 0.8650000095367432, time = 0.36234378814697266\n",
      "Testing at step=57, batch=15, test loss = 0.34112417697906494, test acc = 0.8700000047683716, time = 0.36464524269104004\n",
      "Testing at step=57, batch=20, test loss = 0.23175619542598724, test acc = 0.9150000214576721, time = 0.3610081672668457\n",
      "Testing at step=57, batch=25, test loss = 0.24765312671661377, test acc = 0.8999999761581421, time = 0.3608517646789551\n",
      "Testing at step=57, batch=30, test loss = 0.2786865532398224, test acc = 0.9049999713897705, time = 0.3603198528289795\n",
      "Testing at step=57, batch=35, test loss = 0.4259239137172699, test acc = 0.8349999785423279, time = 0.3451573848724365\n",
      "Testing at step=57, batch=40, test loss = 0.3043217957019806, test acc = 0.8949999809265137, time = 0.34558916091918945\n",
      "Testing at step=57, batch=45, test loss = 0.33624276518821716, test acc = 0.9100000262260437, time = 0.3530571460723877\n",
      "Step 57 finished in 311.8409366607666, Train loss = 0.27401812598109243, Test loss = 0.34816202878952024; Train Acc = 0.9001833313703537, Test Acc = 0.8776999986171723\n",
      "Training at step=58, batch=0, train loss = 0.2934871017932892, train acc = 0.8849999904632568, time = 0.9381263256072998\n",
      "Training at step=58, batch=30, train loss = 0.3156481385231018, train acc = 0.8899999856948853, time = 0.939659833908081\n",
      "Training at step=58, batch=60, train loss = 0.20945009589195251, train acc = 0.9150000214576721, time = 0.9364075660705566\n",
      "Training at step=58, batch=90, train loss = 0.18718889355659485, train acc = 0.9350000023841858, time = 0.9373738765716553\n",
      "Training at step=58, batch=120, train loss = 0.32454192638397217, train acc = 0.8399999737739563, time = 0.9395973682403564\n",
      "Training at step=58, batch=150, train loss = 0.2906375229358673, train acc = 0.8999999761581421, time = 0.9413065910339355\n",
      "Training at step=58, batch=180, train loss = 0.2923436164855957, train acc = 0.8899999856948853, time = 1.6622724533081055\n",
      "Training at step=58, batch=210, train loss = 0.32399100065231323, train acc = 0.8500000238418579, time = 0.9465060234069824\n",
      "Training at step=58, batch=240, train loss = 0.287993460893631, train acc = 0.8849999904632568, time = 1.6628530025482178\n",
      "Training at step=58, batch=270, train loss = 0.39482638239860535, train acc = 0.8500000238418579, time = 1.7942116260528564\n",
      "Testing at step=58, batch=0, test loss = 0.23857381939888, test acc = 0.9100000262260437, time = 0.3444857597351074\n",
      "Testing at step=58, batch=5, test loss = 0.33890673518180847, test acc = 0.875, time = 0.3480260372161865\n",
      "Testing at step=58, batch=10, test loss = 0.42754077911376953, test acc = 0.875, time = 0.3456888198852539\n",
      "Testing at step=58, batch=15, test loss = 0.3787844479084015, test acc = 0.8650000095367432, time = 0.34225916862487793\n",
      "Testing at step=58, batch=20, test loss = 0.43768611550331116, test acc = 0.8550000190734863, time = 0.34656596183776855\n",
      "Testing at step=58, batch=25, test loss = 0.27448731660842896, test acc = 0.8949999809265137, time = 0.34534358978271484\n",
      "Testing at step=58, batch=30, test loss = 0.3048815429210663, test acc = 0.8999999761581421, time = 0.34224629402160645\n",
      "Testing at step=58, batch=35, test loss = 0.30088794231414795, test acc = 0.8949999809265137, time = 0.3421773910522461\n",
      "Testing at step=58, batch=40, test loss = 0.4040610194206238, test acc = 0.8849999904632568, time = 0.341092586517334\n",
      "Testing at step=58, batch=45, test loss = 0.3899804353713989, test acc = 0.8550000190734863, time = 0.34496140480041504\n",
      "Step 58 finished in 369.491797208786, Train loss = 0.2715507491926352, Test loss = 0.35664448857307435; Train Acc = 0.9000333325068156, Test Acc = 0.8749000024795532\n",
      "Training at step=59, batch=0, train loss = 0.36241447925567627, train acc = 0.8899999856948853, time = 0.9375314712524414\n",
      "Training at step=59, batch=30, train loss = 0.32262688875198364, train acc = 0.8999999761581421, time = 0.9401307106018066\n",
      "Training at step=59, batch=60, train loss = 0.22437608242034912, train acc = 0.9150000214576721, time = 0.9396572113037109\n",
      "Training at step=59, batch=90, train loss = 0.27531689405441284, train acc = 0.8899999856948853, time = 0.9620702266693115\n",
      "Training at step=59, batch=120, train loss = 0.2101910412311554, train acc = 0.925000011920929, time = 0.9738528728485107\n",
      "Training at step=59, batch=150, train loss = 0.2732171416282654, train acc = 0.8999999761581421, time = 0.9706614017486572\n",
      "Training at step=59, batch=180, train loss = 0.22846665978431702, train acc = 0.9049999713897705, time = 0.9502151012420654\n",
      "Training at step=59, batch=210, train loss = 0.2942524254322052, train acc = 0.8949999809265137, time = 0.9637796878814697\n",
      "Training at step=59, batch=240, train loss = 0.39375028014183044, train acc = 0.8650000095367432, time = 0.9507229328155518\n",
      "Training at step=59, batch=270, train loss = 0.25965622067451477, train acc = 0.9300000071525574, time = 0.9456524848937988\n",
      "Testing at step=59, batch=0, test loss = 0.33668050169944763, test acc = 0.8899999856948853, time = 0.3670163154602051\n",
      "Testing at step=59, batch=5, test loss = 0.25919532775878906, test acc = 0.9100000262260437, time = 0.35031795501708984\n",
      "Testing at step=59, batch=10, test loss = 0.35940349102020264, test acc = 0.8999999761581421, time = 0.35123419761657715\n",
      "Testing at step=59, batch=15, test loss = 0.33678969740867615, test acc = 0.8799999952316284, time = 0.3587164878845215\n",
      "Testing at step=59, batch=20, test loss = 0.3165854513645172, test acc = 0.8999999761581421, time = 0.34113192558288574\n",
      "Testing at step=59, batch=25, test loss = 0.364844411611557, test acc = 0.8650000095367432, time = 0.3389151096343994\n",
      "Testing at step=59, batch=30, test loss = 0.4393242299556732, test acc = 0.8450000286102295, time = 0.34317898750305176\n",
      "Testing at step=59, batch=35, test loss = 0.3528733551502228, test acc = 0.8650000095367432, time = 0.43338871002197266\n",
      "Testing at step=59, batch=40, test loss = 0.41108667850494385, test acc = 0.875, time = 0.3450474739074707\n",
      "Testing at step=59, batch=45, test loss = 0.46931400895118713, test acc = 0.8399999737739563, time = 0.33985233306884766\n",
      "Step 59 finished in 313.5945339202881, Train loss = 0.2706326673924923, Test loss = 0.3613764786720276; Train Acc = 0.9007166641950607, Test Acc = 0.8751999962329865\n",
      "Training at step=60, batch=0, train loss = 0.2347245067358017, train acc = 0.9200000166893005, time = 0.9380359649658203\n",
      "Training at step=60, batch=30, train loss = 0.2561691999435425, train acc = 0.8949999809265137, time = 0.9438884258270264\n",
      "Training at step=60, batch=60, train loss = 0.24610988795757294, train acc = 0.9100000262260437, time = 1.030574083328247\n",
      "Training at step=60, batch=90, train loss = 0.3337370753288269, train acc = 0.8650000095367432, time = 0.9414803981781006\n",
      "Training at step=60, batch=120, train loss = 0.20673872530460358, train acc = 0.9300000071525574, time = 0.9398400783538818\n",
      "Training at step=60, batch=150, train loss = 0.23016856610774994, train acc = 0.9150000214576721, time = 0.9602060317993164\n",
      "Training at step=60, batch=180, train loss = 0.2969246506690979, train acc = 0.8799999952316284, time = 0.9517028331756592\n",
      "Training at step=60, batch=210, train loss = 0.22978201508522034, train acc = 0.9049999713897705, time = 0.9551980495452881\n",
      "Training at step=60, batch=240, train loss = 0.2593919038772583, train acc = 0.8899999856948853, time = 0.9391272068023682\n",
      "Training at step=60, batch=270, train loss = 0.2836376428604126, train acc = 0.8849999904632568, time = 0.9558022022247314\n",
      "Testing at step=60, batch=0, test loss = 0.3104808032512665, test acc = 0.9049999713897705, time = 0.3444492816925049\n",
      "Testing at step=60, batch=5, test loss = 0.3519081175327301, test acc = 0.8550000190734863, time = 0.34085965156555176\n",
      "Testing at step=60, batch=10, test loss = 0.35190021991729736, test acc = 0.8600000143051147, time = 0.34241223335266113\n",
      "Testing at step=60, batch=15, test loss = 0.23718996345996857, test acc = 0.9350000023841858, time = 0.3405170440673828\n",
      "Testing at step=60, batch=20, test loss = 0.3678084909915924, test acc = 0.8799999952316284, time = 0.3405928611755371\n",
      "Testing at step=60, batch=25, test loss = 0.43085986375808716, test acc = 0.8700000047683716, time = 0.3401601314544678\n",
      "Testing at step=60, batch=30, test loss = 0.31664252281188965, test acc = 0.875, time = 0.343930721282959\n",
      "Testing at step=60, batch=35, test loss = 0.3190051019191742, test acc = 0.8999999761581421, time = 0.3376758098602295\n",
      "Testing at step=60, batch=40, test loss = 0.2913523316383362, test acc = 0.8899999856948853, time = 0.33988189697265625\n",
      "Testing at step=60, batch=45, test loss = 0.32187098264694214, test acc = 0.8600000143051147, time = 0.341141939163208\n",
      "Step 60 finished in 313.07682728767395, Train loss = 0.2708596543967724, Test loss = 0.3547947835922241; Train Acc = 0.9006666664282481, Test Acc = 0.8757000017166138\n",
      "Training at step=61, batch=0, train loss = 0.3354993462562561, train acc = 0.8799999952316284, time = 0.9442987442016602\n",
      "Training at step=61, batch=30, train loss = 0.19563879072666168, train acc = 0.8949999809265137, time = 0.9377241134643555\n",
      "Training at step=61, batch=60, train loss = 0.2708390951156616, train acc = 0.9049999713897705, time = 0.9429481029510498\n",
      "Training at step=61, batch=90, train loss = 0.23214712738990784, train acc = 0.9049999713897705, time = 0.9541637897491455\n",
      "Training at step=61, batch=120, train loss = 0.28270137310028076, train acc = 0.8899999856948853, time = 0.9406721591949463\n",
      "Training at step=61, batch=150, train loss = 0.34798189997673035, train acc = 0.8849999904632568, time = 0.9418618679046631\n",
      "Training at step=61, batch=180, train loss = 0.2624465227127075, train acc = 0.8949999809265137, time = 0.9418418407440186\n",
      "Training at step=61, batch=210, train loss = 0.19801856577396393, train acc = 0.925000011920929, time = 1.025902271270752\n",
      "Training at step=61, batch=240, train loss = 0.2869943380355835, train acc = 0.8899999856948853, time = 0.9388792514801025\n",
      "Training at step=61, batch=270, train loss = 0.27534493803977966, train acc = 0.8949999809265137, time = 0.9386224746704102\n",
      "Testing at step=61, batch=0, test loss = 0.4286632537841797, test acc = 0.8349999785423279, time = 0.3483419418334961\n",
      "Testing at step=61, batch=5, test loss = 0.3850604295730591, test acc = 0.8450000286102295, time = 0.34276413917541504\n",
      "Testing at step=61, batch=10, test loss = 0.40252041816711426, test acc = 0.8700000047683716, time = 0.3400845527648926\n",
      "Testing at step=61, batch=15, test loss = 0.31130990386009216, test acc = 0.8799999952316284, time = 0.34284400939941406\n",
      "Testing at step=61, batch=20, test loss = 0.3278411030769348, test acc = 0.8700000047683716, time = 0.34705185890197754\n",
      "Testing at step=61, batch=25, test loss = 0.426018625497818, test acc = 0.8700000047683716, time = 0.3461191654205322\n",
      "Testing at step=61, batch=30, test loss = 0.2839778661727905, test acc = 0.8999999761581421, time = 0.3378920555114746\n",
      "Testing at step=61, batch=35, test loss = 0.31688278913497925, test acc = 0.8899999856948853, time = 0.3399195671081543\n",
      "Testing at step=61, batch=40, test loss = 0.27489134669303894, test acc = 0.8949999809265137, time = 0.34128475189208984\n",
      "Testing at step=61, batch=45, test loss = 0.29457223415374756, test acc = 0.8849999904632568, time = 0.344257116317749\n",
      "Step 61 finished in 311.6199028491974, Train loss = 0.26967269301414487, Test loss = 0.35231791287660597; Train Acc = 0.9005666651328404, Test Acc = 0.8763000011444092\n",
      "Training at step=62, batch=0, train loss = 0.2707807421684265, train acc = 0.9049999713897705, time = 0.9418110847473145\n",
      "Training at step=62, batch=30, train loss = 0.33021971583366394, train acc = 0.8799999952316284, time = 0.9408879280090332\n",
      "Training at step=62, batch=60, train loss = 0.2579137086868286, train acc = 0.8999999761581421, time = 0.9444403648376465\n",
      "Training at step=62, batch=90, train loss = 0.27096474170684814, train acc = 0.8849999904632568, time = 0.9450552463531494\n",
      "Training at step=62, batch=120, train loss = 0.30261126160621643, train acc = 0.8650000095367432, time = 0.9410943984985352\n",
      "Training at step=62, batch=150, train loss = 0.3111337423324585, train acc = 0.9150000214576721, time = 0.9408493041992188\n",
      "Training at step=62, batch=180, train loss = 0.3242165744304657, train acc = 0.8999999761581421, time = 0.975358247756958\n",
      "Training at step=62, batch=210, train loss = 0.1983824223279953, train acc = 0.925000011920929, time = 0.9391543865203857\n",
      "Training at step=62, batch=240, train loss = 0.23825706541538239, train acc = 0.9150000214576721, time = 0.9600052833557129\n",
      "Training at step=62, batch=270, train loss = 0.36985546350479126, train acc = 0.8600000143051147, time = 0.9398717880249023\n",
      "Testing at step=62, batch=0, test loss = 0.24055662751197815, test acc = 0.9150000214576721, time = 0.3576512336730957\n",
      "Testing at step=62, batch=5, test loss = 0.4388171136379242, test acc = 0.8399999737739563, time = 0.33884525299072266\n",
      "Testing at step=62, batch=10, test loss = 0.2911033630371094, test acc = 0.8999999761581421, time = 0.34140753746032715\n",
      "Testing at step=62, batch=15, test loss = 0.31526488065719604, test acc = 0.8799999952316284, time = 0.3400566577911377\n",
      "Testing at step=62, batch=20, test loss = 0.3299822211265564, test acc = 0.8899999856948853, time = 0.3404386043548584\n",
      "Testing at step=62, batch=25, test loss = 0.40295085310935974, test acc = 0.8600000143051147, time = 0.3391683101654053\n",
      "Testing at step=62, batch=30, test loss = 0.28626418113708496, test acc = 0.8949999809265137, time = 0.3377680778503418\n",
      "Testing at step=62, batch=35, test loss = 0.384476900100708, test acc = 0.9049999713897705, time = 0.33740806579589844\n",
      "Testing at step=62, batch=40, test loss = 0.48497775197029114, test acc = 0.8349999785423279, time = 0.34009575843811035\n",
      "Testing at step=62, batch=45, test loss = 0.2802273631095886, test acc = 0.9049999713897705, time = 0.3452603816986084\n",
      "Step 62 finished in 310.82704877853394, Train loss = 0.27132803430159885, Test loss = 0.34349482029676437; Train Acc = 0.9000999989112218, Test Acc = 0.8800999975204468\n",
      "Training at step=63, batch=0, train loss = 0.24626664817333221, train acc = 0.8949999809265137, time = 0.9468486309051514\n",
      "Training at step=63, batch=30, train loss = 0.16824640333652496, train acc = 0.949999988079071, time = 0.9416139125823975\n",
      "Training at step=63, batch=60, train loss = 0.2640734314918518, train acc = 0.9100000262260437, time = 0.9320862293243408\n",
      "Training at step=63, batch=90, train loss = 0.23988871276378632, train acc = 0.9200000166893005, time = 0.9590513706207275\n",
      "Training at step=63, batch=120, train loss = 0.27330437302589417, train acc = 0.8849999904632568, time = 0.9391906261444092\n",
      "Training at step=63, batch=150, train loss = 0.35056450963020325, train acc = 0.8799999952316284, time = 0.9398300647735596\n",
      "Training at step=63, batch=180, train loss = 0.31491386890411377, train acc = 0.875, time = 0.9618639945983887\n",
      "Training at step=63, batch=210, train loss = 0.2039458155632019, train acc = 0.9150000214576721, time = 0.9335227012634277\n",
      "Training at step=63, batch=240, train loss = 0.2619123160839081, train acc = 0.9150000214576721, time = 0.935474157333374\n",
      "Training at step=63, batch=270, train loss = 0.23974266648292542, train acc = 0.9049999713897705, time = 0.9432327747344971\n",
      "Testing at step=63, batch=0, test loss = 0.4355721175670624, test acc = 0.8550000190734863, time = 0.3388357162475586\n",
      "Testing at step=63, batch=5, test loss = 0.33354026079177856, test acc = 0.8849999904632568, time = 0.3396792411804199\n",
      "Testing at step=63, batch=10, test loss = 0.46300143003463745, test acc = 0.8450000286102295, time = 0.3399679660797119\n",
      "Testing at step=63, batch=15, test loss = 0.44204795360565186, test acc = 0.8600000143051147, time = 0.33618664741516113\n",
      "Testing at step=63, batch=20, test loss = 0.3628039062023163, test acc = 0.875, time = 0.34122490882873535\n",
      "Testing at step=63, batch=25, test loss = 0.35532334446907043, test acc = 0.8949999809265137, time = 0.3388512134552002\n",
      "Testing at step=63, batch=30, test loss = 0.390121728181839, test acc = 0.8600000143051147, time = 0.35465335845947266\n",
      "Testing at step=63, batch=35, test loss = 0.39029738306999207, test acc = 0.8600000143051147, time = 0.3476736545562744\n",
      "Testing at step=63, batch=40, test loss = 0.3049090504646301, test acc = 0.9150000214576721, time = 0.33710145950317383\n",
      "Testing at step=63, batch=45, test loss = 0.3198806345462799, test acc = 0.8600000143051147, time = 0.33663058280944824\n",
      "Step 63 finished in 310.9868998527527, Train loss = 0.26737662265698114, Test loss = 0.3567217919230461; Train Acc = 0.9007166647911071, Test Acc = 0.8774000000953674\n",
      "Training at step=64, batch=0, train loss = 0.29204362630844116, train acc = 0.8949999809265137, time = 0.9353015422821045\n",
      "Training at step=64, batch=30, train loss = 0.2486213743686676, train acc = 0.8999999761581421, time = 0.9358882904052734\n",
      "Training at step=64, batch=60, train loss = 0.16043484210968018, train acc = 0.9399999976158142, time = 0.9693446159362793\n",
      "Training at step=64, batch=90, train loss = 0.22840125858783722, train acc = 0.8949999809265137, time = 0.9398970603942871\n",
      "Training at step=64, batch=120, train loss = 0.2125694900751114, train acc = 0.9399999976158142, time = 0.9402141571044922\n",
      "Training at step=64, batch=150, train loss = 0.3439464867115021, train acc = 0.875, time = 0.9634904861450195\n",
      "Training at step=64, batch=180, train loss = 0.3415992856025696, train acc = 0.875, time = 0.9354815483093262\n",
      "Training at step=64, batch=210, train loss = 0.2329493761062622, train acc = 0.9150000214576721, time = 0.9409759044647217\n",
      "Training at step=64, batch=240, train loss = 0.2565157413482666, train acc = 0.9049999713897705, time = 0.9339768886566162\n",
      "Training at step=64, batch=270, train loss = 0.20854195952415466, train acc = 0.9350000023841858, time = 0.9411027431488037\n",
      "Testing at step=64, batch=0, test loss = 0.2633226215839386, test acc = 0.8949999809265137, time = 0.33922719955444336\n",
      "Testing at step=64, batch=5, test loss = 0.29442402720451355, test acc = 0.8799999952316284, time = 0.3407306671142578\n",
      "Testing at step=64, batch=10, test loss = 0.3651348352432251, test acc = 0.8700000047683716, time = 0.3381166458129883\n",
      "Testing at step=64, batch=15, test loss = 0.3059115707874298, test acc = 0.8949999809265137, time = 0.3370943069458008\n",
      "Testing at step=64, batch=20, test loss = 0.34130218625068665, test acc = 0.8550000190734863, time = 0.33783745765686035\n",
      "Testing at step=64, batch=25, test loss = 0.3669187128543854, test acc = 0.8299999833106995, time = 0.336315393447876\n",
      "Testing at step=64, batch=30, test loss = 0.3912166953086853, test acc = 0.8799999952316284, time = 0.3393404483795166\n",
      "Testing at step=64, batch=35, test loss = 0.4808608293533325, test acc = 0.875, time = 0.36373186111450195\n",
      "Testing at step=64, batch=40, test loss = 0.3865297734737396, test acc = 0.875, time = 0.33896899223327637\n",
      "Testing at step=64, batch=45, test loss = 0.2937577962875366, test acc = 0.8999999761581421, time = 0.3414332866668701\n",
      "Step 64 finished in 310.2066562175751, Train loss = 0.26645362878839174, Test loss = 0.3580314368009567; Train Acc = 0.9013499983151754, Test Acc = 0.8712999999523163\n",
      "Training at step=65, batch=0, train loss = 0.2911626994609833, train acc = 0.8999999761581421, time = 0.9431216716766357\n",
      "Training at step=65, batch=30, train loss = 0.24233871698379517, train acc = 0.9100000262260437, time = 0.934539794921875\n",
      "Training at step=65, batch=60, train loss = 0.3046669661998749, train acc = 0.8799999952316284, time = 0.9396605491638184\n",
      "Training at step=65, batch=90, train loss = 0.26678773760795593, train acc = 0.8949999809265137, time = 0.939185380935669\n",
      "Training at step=65, batch=120, train loss = 0.262827068567276, train acc = 0.8849999904632568, time = 0.9363107681274414\n",
      "Training at step=65, batch=150, train loss = 0.2510988116264343, train acc = 0.9049999713897705, time = 0.9369966983795166\n",
      "Training at step=65, batch=180, train loss = 0.2128325253725052, train acc = 0.9150000214576721, time = 0.9523208141326904\n",
      "Training at step=65, batch=210, train loss = 0.18796144425868988, train acc = 0.9300000071525574, time = 0.9430868625640869\n",
      "Training at step=65, batch=240, train loss = 0.3471654951572418, train acc = 0.8799999952316284, time = 0.9323327541351318\n",
      "Training at step=65, batch=270, train loss = 0.3393631875514984, train acc = 0.8849999904632568, time = 0.9332573413848877\n",
      "Testing at step=65, batch=0, test loss = 0.3291673958301544, test acc = 0.8600000143051147, time = 0.3382594585418701\n",
      "Testing at step=65, batch=5, test loss = 0.32602038979530334, test acc = 0.8650000095367432, time = 0.3363678455352783\n",
      "Testing at step=65, batch=10, test loss = 0.23380230367183685, test acc = 0.9049999713897705, time = 0.3449587821960449\n",
      "Testing at step=65, batch=15, test loss = 0.4291134178638458, test acc = 0.8349999785423279, time = 0.3394932746887207\n",
      "Testing at step=65, batch=20, test loss = 0.34566208720207214, test acc = 0.8700000047683716, time = 0.33978915214538574\n",
      "Testing at step=65, batch=25, test loss = 0.3364677131175995, test acc = 0.8799999952316284, time = 0.33896851539611816\n",
      "Testing at step=65, batch=30, test loss = 0.3352782130241394, test acc = 0.8849999904632568, time = 0.3419780731201172\n",
      "Testing at step=65, batch=35, test loss = 0.2671312689781189, test acc = 0.9049999713897705, time = 0.34033703804016113\n",
      "Testing at step=65, batch=40, test loss = 0.31940239667892456, test acc = 0.875, time = 0.34398984909057617\n",
      "Testing at step=65, batch=45, test loss = 0.4513944685459137, test acc = 0.8550000190734863, time = 0.33721041679382324\n",
      "Step 65 finished in 310.6702699661255, Train loss = 0.26518931696812315, Test loss = 0.35821649461984634; Train Acc = 0.9019833316405614, Test Acc = 0.8719999992847443\n",
      "Training at step=66, batch=0, train loss = 0.29952624440193176, train acc = 0.875, time = 0.9410698413848877\n",
      "Training at step=66, batch=30, train loss = 0.2889895737171173, train acc = 0.875, time = 0.9402930736541748\n",
      "Training at step=66, batch=60, train loss = 0.2225457727909088, train acc = 0.9150000214576721, time = 0.9394009113311768\n",
      "Training at step=66, batch=90, train loss = 0.23829467594623566, train acc = 0.9049999713897705, time = 0.9366216659545898\n",
      "Training at step=66, batch=120, train loss = 0.25478479266166687, train acc = 0.8899999856948853, time = 0.93636155128479\n",
      "Training at step=66, batch=150, train loss = 0.1906309276819229, train acc = 0.9200000166893005, time = 0.9405443668365479\n",
      "Training at step=66, batch=180, train loss = 0.2838335931301117, train acc = 0.875, time = 0.938929557800293\n",
      "Training at step=66, batch=210, train loss = 0.28715670108795166, train acc = 0.8849999904632568, time = 0.9561202526092529\n",
      "Training at step=66, batch=240, train loss = 0.2562512159347534, train acc = 0.9100000262260437, time = 0.9405815601348877\n",
      "Training at step=66, batch=270, train loss = 0.24592065811157227, train acc = 0.8999999761581421, time = 0.9331166744232178\n",
      "Testing at step=66, batch=0, test loss = 0.3264530301094055, test acc = 0.8600000143051147, time = 0.340350866317749\n",
      "Testing at step=66, batch=5, test loss = 0.39460089802742004, test acc = 0.8650000095367432, time = 0.33805274963378906\n",
      "Testing at step=66, batch=10, test loss = 0.31488391757011414, test acc = 0.9049999713897705, time = 0.3405182361602783\n",
      "Testing at step=66, batch=15, test loss = 0.3441077470779419, test acc = 0.8849999904632568, time = 0.3402726650238037\n",
      "Testing at step=66, batch=20, test loss = 0.4678718149662018, test acc = 0.8650000095367432, time = 0.34002089500427246\n",
      "Testing at step=66, batch=25, test loss = 0.445890873670578, test acc = 0.8700000047683716, time = 0.3409159183502197\n",
      "Testing at step=66, batch=30, test loss = 0.3113287091255188, test acc = 0.8849999904632568, time = 0.3439640998840332\n",
      "Testing at step=66, batch=35, test loss = 0.269746333360672, test acc = 0.8949999809265137, time = 0.34442758560180664\n",
      "Testing at step=66, batch=40, test loss = 0.4209868609905243, test acc = 0.8600000143051147, time = 0.3761579990386963\n",
      "Testing at step=66, batch=45, test loss = 0.36161744594573975, test acc = 0.8650000095367432, time = 0.34824347496032715\n",
      "Step 66 finished in 310.01418447494507, Train loss = 0.26190585280458134, Test loss = 0.3600101882219315; Train Acc = 0.9038999990622203, Test Acc = 0.8756000006198883\n",
      "Training at step=67, batch=0, train loss = 0.3452186584472656, train acc = 0.8650000095367432, time = 0.9612371921539307\n",
      "Training at step=67, batch=30, train loss = 0.22408132255077362, train acc = 0.925000011920929, time = 0.9356784820556641\n",
      "Training at step=67, batch=60, train loss = 0.23538623750209808, train acc = 0.9049999713897705, time = 0.9428055286407471\n",
      "Training at step=67, batch=90, train loss = 0.24929502606391907, train acc = 0.8999999761581421, time = 0.9401588439941406\n",
      "Training at step=67, batch=120, train loss = 0.19198770821094513, train acc = 0.925000011920929, time = 0.9329769611358643\n",
      "Training at step=67, batch=150, train loss = 0.23908674716949463, train acc = 0.9049999713897705, time = 0.9433033466339111\n",
      "Training at step=67, batch=180, train loss = 0.23573391139507294, train acc = 0.9100000262260437, time = 0.9407026767730713\n",
      "Training at step=67, batch=210, train loss = 0.27442002296447754, train acc = 0.9200000166893005, time = 0.9360551834106445\n",
      "Training at step=67, batch=240, train loss = 0.25557172298431396, train acc = 0.925000011920929, time = 0.9349782466888428\n",
      "Training at step=67, batch=270, train loss = 0.21138860285282135, train acc = 0.9150000214576721, time = 0.9367368221282959\n",
      "Testing at step=67, batch=0, test loss = 0.35223740339279175, test acc = 0.8650000095367432, time = 0.3402974605560303\n",
      "Testing at step=67, batch=5, test loss = 0.43954041600227356, test acc = 0.875, time = 0.3404526710510254\n",
      "Testing at step=67, batch=10, test loss = 0.3014957010746002, test acc = 0.9150000214576721, time = 0.33464837074279785\n",
      "Testing at step=67, batch=15, test loss = 0.4201376736164093, test acc = 0.8500000238418579, time = 0.34218621253967285\n",
      "Testing at step=67, batch=20, test loss = 0.38271602988243103, test acc = 0.8550000190734863, time = 0.3411831855773926\n",
      "Testing at step=67, batch=25, test loss = 0.400309294462204, test acc = 0.8600000143051147, time = 0.3441123962402344\n",
      "Testing at step=67, batch=30, test loss = 0.31884053349494934, test acc = 0.8849999904632568, time = 0.3498516082763672\n",
      "Testing at step=67, batch=35, test loss = 0.3654239773750305, test acc = 0.8399999737739563, time = 0.3613896369934082\n",
      "Testing at step=67, batch=40, test loss = 0.2980354428291321, test acc = 0.8999999761581421, time = 0.3624727725982666\n",
      "Testing at step=67, batch=45, test loss = 0.2930552661418915, test acc = 0.8949999809265137, time = 0.3398146629333496\n",
      "Step 67 finished in 310.232390165329, Train loss = 0.26028294652700423, Test loss = 0.35057260721921923; Train Acc = 0.9031333337227504, Test Acc = 0.8774999988079071\n",
      "Training at step=68, batch=0, train loss = 0.24000579118728638, train acc = 0.9150000214576721, time = 0.9377591609954834\n",
      "Training at step=68, batch=30, train loss = 0.26895174384117126, train acc = 0.8999999761581421, time = 0.93393874168396\n",
      "Training at step=68, batch=60, train loss = 0.24220043420791626, train acc = 0.8999999761581421, time = 0.9469714164733887\n",
      "Training at step=68, batch=90, train loss = 0.26218682527542114, train acc = 0.8899999856948853, time = 0.9413793087005615\n",
      "Training at step=68, batch=120, train loss = 0.2802654802799225, train acc = 0.9049999713897705, time = 0.9375720024108887\n",
      "Training at step=68, batch=150, train loss = 0.22476564347743988, train acc = 0.9399999976158142, time = 0.9403963088989258\n",
      "Training at step=68, batch=180, train loss = 0.35589146614074707, train acc = 0.8799999952316284, time = 0.9394288063049316\n",
      "Training at step=68, batch=210, train loss = 0.23244673013687134, train acc = 0.9100000262260437, time = 0.9670596122741699\n",
      "Training at step=68, batch=240, train loss = 0.2074890285730362, train acc = 0.9300000071525574, time = 0.9403519630432129\n",
      "Training at step=68, batch=270, train loss = 0.28365135192871094, train acc = 0.9100000262260437, time = 0.9440858364105225\n",
      "Testing at step=68, batch=0, test loss = 0.29243940114974976, test acc = 0.9200000166893005, time = 0.3652160167694092\n",
      "Testing at step=68, batch=5, test loss = 0.2746613025665283, test acc = 0.8999999761581421, time = 0.3689901828765869\n",
      "Testing at step=68, batch=10, test loss = 0.3931960165500641, test acc = 0.8799999952316284, time = 0.3679828643798828\n",
      "Testing at step=68, batch=15, test loss = 0.3411675691604614, test acc = 0.8999999761581421, time = 0.34604859352111816\n",
      "Testing at step=68, batch=20, test loss = 0.5142585635185242, test acc = 0.8299999833106995, time = 0.337723970413208\n",
      "Testing at step=68, batch=25, test loss = 0.2557719647884369, test acc = 0.9049999713897705, time = 0.3404085636138916\n",
      "Testing at step=68, batch=30, test loss = 0.5195981860160828, test acc = 0.8199999928474426, time = 0.34003329277038574\n",
      "Testing at step=68, batch=35, test loss = 0.3446040749549866, test acc = 0.8849999904632568, time = 0.3452606201171875\n",
      "Testing at step=68, batch=40, test loss = 0.36877402663230896, test acc = 0.8650000095367432, time = 0.3440122604370117\n",
      "Testing at step=68, batch=45, test loss = 0.3512840270996094, test acc = 0.875, time = 0.3367455005645752\n",
      "Step 68 finished in 310.60288739204407, Train loss = 0.26060725197196005, Test loss = 0.3449887147545814; Train Acc = 0.9035166652997335, Test Acc = 0.8787999975681305\n",
      "Training at step=69, batch=0, train loss = 0.1937638521194458, train acc = 0.9300000071525574, time = 0.9365236759185791\n",
      "Training at step=69, batch=30, train loss = 0.22652454674243927, train acc = 0.9150000214576721, time = 0.938368558883667\n",
      "Training at step=69, batch=60, train loss = 0.2712562680244446, train acc = 0.8899999856948853, time = 0.9386932849884033\n",
      "Training at step=69, batch=90, train loss = 0.18773750960826874, train acc = 0.9300000071525574, time = 0.9406766891479492\n",
      "Training at step=69, batch=120, train loss = 0.30805355310440063, train acc = 0.8700000047683716, time = 0.9375574588775635\n",
      "Training at step=69, batch=150, train loss = 0.3145354092121124, train acc = 0.8600000143051147, time = 0.9802916049957275\n",
      "Training at step=69, batch=180, train loss = 0.24450740218162537, train acc = 0.8949999809265137, time = 0.9317443370819092\n",
      "Training at step=69, batch=210, train loss = 0.2616659104824066, train acc = 0.925000011920929, time = 0.9440515041351318\n",
      "Training at step=69, batch=240, train loss = 0.22498983144760132, train acc = 0.925000011920929, time = 0.9387247562408447\n",
      "Training at step=69, batch=270, train loss = 0.2702936828136444, train acc = 0.8899999856948853, time = 0.9654150009155273\n",
      "Testing at step=69, batch=0, test loss = 0.37053367495536804, test acc = 0.8700000047683716, time = 0.3582746982574463\n",
      "Testing at step=69, batch=5, test loss = 0.47858989238739014, test acc = 0.8399999737739563, time = 0.34583592414855957\n",
      "Testing at step=69, batch=10, test loss = 0.41344842314720154, test acc = 0.8650000095367432, time = 0.3389313220977783\n",
      "Testing at step=69, batch=15, test loss = 0.32572242617607117, test acc = 0.8949999809265137, time = 0.3430044651031494\n",
      "Testing at step=69, batch=20, test loss = 0.34898844361305237, test acc = 0.9049999713897705, time = 0.34759068489074707\n",
      "Testing at step=69, batch=25, test loss = 0.5308353900909424, test acc = 0.8299999833106995, time = 0.3417978286743164\n",
      "Testing at step=69, batch=30, test loss = 0.2491275817155838, test acc = 0.8899999856948853, time = 0.3439457416534424\n",
      "Testing at step=69, batch=35, test loss = 0.279157429933548, test acc = 0.9049999713897705, time = 0.3404097557067871\n",
      "Testing at step=69, batch=40, test loss = 0.3692864179611206, test acc = 0.8600000143051147, time = 0.34247660636901855\n",
      "Testing at step=69, batch=45, test loss = 0.5451791286468506, test acc = 0.8199999928474426, time = 0.33996152877807617\n",
      "Step 69 finished in 310.51740741729736, Train loss = 0.2617894312242667, Test loss = 0.36178658574819567; Train Acc = 0.903433334628741, Test Acc = 0.8751999950408935\n",
      "Training at step=70, batch=0, train loss = 0.30310314893722534, train acc = 0.8999999761581421, time = 0.9450325965881348\n",
      "Training at step=70, batch=30, train loss = 0.33746373653411865, train acc = 0.875, time = 0.9450323581695557\n",
      "Training at step=70, batch=60, train loss = 0.26101091504096985, train acc = 0.8799999952316284, time = 0.9463627338409424\n",
      "Training at step=70, batch=90, train loss = 0.27041110396385193, train acc = 0.9100000262260437, time = 0.9438982009887695\n",
      "Training at step=70, batch=120, train loss = 0.22164201736450195, train acc = 0.9300000071525574, time = 0.9565973281860352\n",
      "Training at step=70, batch=150, train loss = 0.2661084532737732, train acc = 0.9049999713897705, time = 0.9566757678985596\n",
      "Training at step=70, batch=180, train loss = 0.25843334197998047, train acc = 0.8999999761581421, time = 0.9351568222045898\n",
      "Training at step=70, batch=210, train loss = 0.29361066222190857, train acc = 0.8899999856948853, time = 0.935727596282959\n",
      "Training at step=70, batch=240, train loss = 0.29089054465293884, train acc = 0.9049999713897705, time = 0.9597020149230957\n",
      "Training at step=70, batch=270, train loss = 0.27842751145362854, train acc = 0.8849999904632568, time = 0.9347383975982666\n",
      "Testing at step=70, batch=0, test loss = 0.3717760145664215, test acc = 0.875, time = 0.33751869201660156\n",
      "Testing at step=70, batch=5, test loss = 0.3831941485404968, test acc = 0.8550000190734863, time = 0.3664093017578125\n",
      "Testing at step=70, batch=10, test loss = 0.44921383261680603, test acc = 0.8700000047683716, time = 0.34459376335144043\n",
      "Testing at step=70, batch=15, test loss = 0.3328937888145447, test acc = 0.875, time = 0.3394441604614258\n",
      "Testing at step=70, batch=20, test loss = 0.49208858609199524, test acc = 0.8550000190734863, time = 0.3440883159637451\n",
      "Testing at step=70, batch=25, test loss = 0.42676910758018494, test acc = 0.8650000095367432, time = 0.3395650386810303\n",
      "Testing at step=70, batch=30, test loss = 0.3445814549922943, test acc = 0.8899999856948853, time = 0.354569673538208\n",
      "Testing at step=70, batch=35, test loss = 0.31003981828689575, test acc = 0.8849999904632568, time = 0.3397059440612793\n",
      "Testing at step=70, batch=40, test loss = 0.378622442483902, test acc = 0.8849999904632568, time = 0.34116387367248535\n",
      "Testing at step=70, batch=45, test loss = 0.33818337321281433, test acc = 0.8849999904632568, time = 0.34468913078308105\n",
      "Step 70 finished in 311.65056920051575, Train loss = 0.25947033137083053, Test loss = 0.351611530482769; Train Acc = 0.9050166654586792, Test Acc = 0.8802999973297119\n",
      "Training at step=71, batch=0, train loss = 0.26553091406822205, train acc = 0.8899999856948853, time = 0.9495089054107666\n",
      "Training at step=71, batch=30, train loss = 0.20416881144046783, train acc = 0.9049999713897705, time = 0.9332442283630371\n",
      "Training at step=71, batch=60, train loss = 0.24206075072288513, train acc = 0.9150000214576721, time = 0.9374587535858154\n",
      "Training at step=71, batch=90, train loss = 0.21971628069877625, train acc = 0.9100000262260437, time = 0.9427931308746338\n",
      "Training at step=71, batch=120, train loss = 0.23933525383472443, train acc = 0.9049999713897705, time = 0.9406170845031738\n",
      "Training at step=71, batch=150, train loss = 0.267619788646698, train acc = 0.9049999713897705, time = 0.9756901264190674\n",
      "Training at step=71, batch=180, train loss = 0.17787308990955353, train acc = 0.9399999976158142, time = 0.9458518028259277\n",
      "Training at step=71, batch=210, train loss = 0.1460665464401245, train acc = 0.9350000023841858, time = 0.9363253116607666\n",
      "Training at step=71, batch=240, train loss = 0.2627521753311157, train acc = 0.8849999904632568, time = 0.9663050174713135\n",
      "Training at step=71, batch=270, train loss = 0.26160070300102234, train acc = 0.8849999904632568, time = 0.9423644542694092\n",
      "Testing at step=71, batch=0, test loss = 0.3886040449142456, test acc = 0.8700000047683716, time = 0.33784937858581543\n",
      "Testing at step=71, batch=5, test loss = 0.31436505913734436, test acc = 0.8849999904632568, time = 0.34154605865478516\n",
      "Testing at step=71, batch=10, test loss = 0.31259337067604065, test acc = 0.8949999809265137, time = 0.33895277976989746\n",
      "Testing at step=71, batch=15, test loss = 0.33906757831573486, test acc = 0.8799999952316284, time = 0.3371620178222656\n",
      "Testing at step=71, batch=20, test loss = 0.369086354970932, test acc = 0.8550000190734863, time = 0.3420088291168213\n",
      "Testing at step=71, batch=25, test loss = 0.2571870982646942, test acc = 0.9150000214576721, time = 0.3447272777557373\n",
      "Testing at step=71, batch=30, test loss = 0.33191463351249695, test acc = 0.8899999856948853, time = 0.3382375240325928\n",
      "Testing at step=71, batch=35, test loss = 0.2927018105983734, test acc = 0.875, time = 0.33757638931274414\n",
      "Testing at step=71, batch=40, test loss = 0.31110066175460815, test acc = 0.8799999952316284, time = 0.336714506149292\n",
      "Testing at step=71, batch=45, test loss = 0.3412666618824005, test acc = 0.8999999761581421, time = 0.33722400665283203\n",
      "Step 71 finished in 310.2117598056793, Train loss = 0.2551029238353173, Test loss = 0.34621949136257174; Train Acc = 0.9061333318551381, Test Acc = 0.8796999967098236\n",
      "Training at step=72, batch=0, train loss = 0.18282857537269592, train acc = 0.949999988079071, time = 0.9551973342895508\n",
      "Training at step=72, batch=30, train loss = 0.3387378752231598, train acc = 0.875, time = 0.9415440559387207\n",
      "Training at step=72, batch=60, train loss = 0.24650664627552032, train acc = 0.8999999761581421, time = 0.9482841491699219\n",
      "Training at step=72, batch=90, train loss = 0.22778618335723877, train acc = 0.8849999904632568, time = 0.94291090965271\n",
      "Training at step=72, batch=120, train loss = 0.16024862229824066, train acc = 0.9599999785423279, time = 0.9363596439361572\n",
      "Training at step=72, batch=150, train loss = 0.2294521927833557, train acc = 0.9300000071525574, time = 0.9421992301940918\n",
      "Training at step=72, batch=180, train loss = 0.2400606870651245, train acc = 0.9150000214576721, time = 0.9426088333129883\n",
      "Training at step=72, batch=210, train loss = 0.20143380761146545, train acc = 0.9350000023841858, time = 0.9446539878845215\n",
      "Training at step=72, batch=240, train loss = 0.1792183667421341, train acc = 0.949999988079071, time = 0.9455647468566895\n",
      "Training at step=72, batch=270, train loss = 0.19018328189849854, train acc = 0.9350000023841858, time = 0.936718225479126\n",
      "Testing at step=72, batch=0, test loss = 0.39462172985076904, test acc = 0.8999999761581421, time = 0.34015345573425293\n",
      "Testing at step=72, batch=5, test loss = 0.34467777609825134, test acc = 0.8849999904632568, time = 0.3415560722351074\n",
      "Testing at step=72, batch=10, test loss = 0.3374679684638977, test acc = 0.8700000047683716, time = 0.33706235885620117\n",
      "Testing at step=72, batch=15, test loss = 0.3212924599647522, test acc = 0.8700000047683716, time = 0.3380465507507324\n",
      "Testing at step=72, batch=20, test loss = 0.28377366065979004, test acc = 0.8999999761581421, time = 0.3457825183868408\n",
      "Testing at step=72, batch=25, test loss = 0.337062269449234, test acc = 0.8999999761581421, time = 0.3648090362548828\n",
      "Testing at step=72, batch=30, test loss = 0.3343551754951477, test acc = 0.875, time = 0.3609461784362793\n",
      "Testing at step=72, batch=35, test loss = 0.36069047451019287, test acc = 0.8550000190734863, time = 0.3607032299041748\n",
      "Testing at step=72, batch=40, test loss = 0.33288052678108215, test acc = 0.8700000047683716, time = 0.3829026222229004\n",
      "Testing at step=72, batch=45, test loss = 0.34946390986442566, test acc = 0.8650000095367432, time = 0.36057162284851074\n",
      "Step 72 finished in 310.9560286998749, Train loss = 0.25479623839259147, Test loss = 0.3609954378008842; Train Acc = 0.9058833322922388, Test Acc = 0.8740999984741211\n",
      "Training at step=73, batch=0, train loss = 0.26265814900398254, train acc = 0.9049999713897705, time = 0.9625704288482666\n",
      "Training at step=73, batch=30, train loss = 0.1924283355474472, train acc = 0.925000011920929, time = 0.9447722434997559\n",
      "Training at step=73, batch=60, train loss = 0.26541611552238464, train acc = 0.8899999856948853, time = 0.9346680641174316\n",
      "Training at step=73, batch=90, train loss = 0.30211329460144043, train acc = 0.8949999809265137, time = 0.9621598720550537\n",
      "Training at step=73, batch=120, train loss = 0.26775145530700684, train acc = 0.8849999904632568, time = 0.9364736080169678\n",
      "Training at step=73, batch=150, train loss = 0.2808194160461426, train acc = 0.8999999761581421, time = 0.9357078075408936\n",
      "Training at step=73, batch=180, train loss = 0.20958587527275085, train acc = 0.9150000214576721, time = 0.9374229907989502\n",
      "Training at step=73, batch=210, train loss = 0.23738184571266174, train acc = 0.9100000262260437, time = 0.945777177810669\n",
      "Training at step=73, batch=240, train loss = 0.35385075211524963, train acc = 0.8899999856948853, time = 0.9655191898345947\n",
      "Training at step=73, batch=270, train loss = 0.2315337359905243, train acc = 0.9200000166893005, time = 0.9327750205993652\n",
      "Testing at step=73, batch=0, test loss = 0.28945192694664, test acc = 0.8999999761581421, time = 0.3401000499725342\n",
      "Testing at step=73, batch=5, test loss = 0.23149321973323822, test acc = 0.8999999761581421, time = 0.3400306701660156\n",
      "Testing at step=73, batch=10, test loss = 0.2756692171096802, test acc = 0.925000011920929, time = 0.34029078483581543\n",
      "Testing at step=73, batch=15, test loss = 0.34237948060035706, test acc = 0.8849999904632568, time = 0.33916711807250977\n",
      "Testing at step=73, batch=20, test loss = 0.3637920320034027, test acc = 0.8550000190734863, time = 0.3373715877532959\n",
      "Testing at step=73, batch=25, test loss = 0.3779556155204773, test acc = 0.875, time = 0.3373599052429199\n",
      "Testing at step=73, batch=30, test loss = 0.33341968059539795, test acc = 0.8949999809265137, time = 0.3470640182495117\n",
      "Testing at step=73, batch=35, test loss = 0.2018168568611145, test acc = 0.9200000166893005, time = 0.34020161628723145\n",
      "Testing at step=73, batch=40, test loss = 0.4408102035522461, test acc = 0.875, time = 0.3449432849884033\n",
      "Testing at step=73, batch=45, test loss = 0.31488919258117676, test acc = 0.8949999809265137, time = 0.3371737003326416\n",
      "Step 73 finished in 310.0167450904846, Train loss = 0.2574270152052244, Test loss = 0.3490659454464912; Train Acc = 0.9033833328882853, Test Acc = 0.8795999979972839\n",
      "Training at step=74, batch=0, train loss = 0.33002594113349915, train acc = 0.8949999809265137, time = 0.9355888366699219\n",
      "Training at step=74, batch=30, train loss = 0.27946293354034424, train acc = 0.8999999761581421, time = 0.938563346862793\n",
      "Training at step=74, batch=60, train loss = 0.2943486273288727, train acc = 0.875, time = 0.9377727508544922\n",
      "Training at step=74, batch=90, train loss = 0.20789934694766998, train acc = 0.9449999928474426, time = 0.9428365230560303\n",
      "Training at step=74, batch=120, train loss = 0.29981765151023865, train acc = 0.875, time = 0.9465301036834717\n",
      "Training at step=74, batch=150, train loss = 0.17838677763938904, train acc = 0.925000011920929, time = 0.9337930679321289\n",
      "Training at step=74, batch=180, train loss = 0.21755893528461456, train acc = 0.9200000166893005, time = 0.9349522590637207\n",
      "Training at step=74, batch=210, train loss = 0.2540583908557892, train acc = 0.8949999809265137, time = 0.9378347396850586\n",
      "Training at step=74, batch=240, train loss = 0.2879626154899597, train acc = 0.9100000262260437, time = 0.9463891983032227\n",
      "Training at step=74, batch=270, train loss = 0.24845951795578003, train acc = 0.9200000166893005, time = 0.9394454956054688\n",
      "Testing at step=74, batch=0, test loss = 0.29704150557518005, test acc = 0.8849999904632568, time = 0.33665895462036133\n",
      "Testing at step=74, batch=5, test loss = 0.41340380907058716, test acc = 0.8600000143051147, time = 0.3385009765625\n",
      "Testing at step=74, batch=10, test loss = 0.3513490557670593, test acc = 0.8949999809265137, time = 0.33875608444213867\n",
      "Testing at step=74, batch=15, test loss = 0.46589264273643494, test acc = 0.824999988079071, time = 0.3403666019439697\n",
      "Testing at step=74, batch=20, test loss = 0.40023764967918396, test acc = 0.8500000238418579, time = 0.3495028018951416\n",
      "Testing at step=74, batch=25, test loss = 0.3291749656200409, test acc = 0.9049999713897705, time = 0.337172269821167\n",
      "Testing at step=74, batch=30, test loss = 0.34755760431289673, test acc = 0.8999999761581421, time = 0.3360300064086914\n",
      "Testing at step=74, batch=35, test loss = 0.35295069217681885, test acc = 0.8700000047683716, time = 0.3406107425689697\n",
      "Testing at step=74, batch=40, test loss = 0.39093920588493347, test acc = 0.8550000190734863, time = 0.33728551864624023\n",
      "Testing at step=74, batch=45, test loss = 0.43726956844329834, test acc = 0.8550000190734863, time = 0.3549673557281494\n",
      "Step 74 finished in 309.25556349754333, Train loss = 0.2533765410383542, Test loss = 0.36209674924612045; Train Acc = 0.9064833301305771, Test Acc = 0.8760999989509582\n",
      "Training at step=75, batch=0, train loss = 0.30113112926483154, train acc = 0.8799999952316284, time = 0.9345204830169678\n",
      "Training at step=75, batch=30, train loss = 0.23470638692378998, train acc = 0.9100000262260437, time = 0.9355239868164062\n",
      "Training at step=75, batch=60, train loss = 0.19617822766304016, train acc = 0.9350000023841858, time = 0.9384880065917969\n",
      "Training at step=75, batch=90, train loss = 0.21614427864551544, train acc = 0.9100000262260437, time = 0.935640811920166\n",
      "Training at step=75, batch=120, train loss = 0.3288331925868988, train acc = 0.8799999952316284, time = 0.9312710762023926\n",
      "Training at step=75, batch=150, train loss = 0.2435695230960846, train acc = 0.925000011920929, time = 0.9385583400726318\n",
      "Training at step=75, batch=180, train loss = 0.2949429750442505, train acc = 0.8849999904632568, time = 1.0016653537750244\n",
      "Training at step=75, batch=210, train loss = 0.31327155232429504, train acc = 0.8849999904632568, time = 0.93540358543396\n",
      "Training at step=75, batch=240, train loss = 0.3588188588619232, train acc = 0.8949999809265137, time = 0.9444422721862793\n",
      "Training at step=75, batch=270, train loss = 0.3319547176361084, train acc = 0.8899999856948853, time = 0.9316911697387695\n",
      "Testing at step=75, batch=0, test loss = 0.3876454830169678, test acc = 0.8550000190734863, time = 0.34263181686401367\n",
      "Testing at step=75, batch=5, test loss = 0.3361091613769531, test acc = 0.9049999713897705, time = 0.3442850112915039\n",
      "Testing at step=75, batch=10, test loss = 0.38120952248573303, test acc = 0.8600000143051147, time = 0.34162402153015137\n",
      "Testing at step=75, batch=15, test loss = 0.27888309955596924, test acc = 0.8899999856948853, time = 0.33766913414001465\n",
      "Testing at step=75, batch=20, test loss = 0.42493751645088196, test acc = 0.8500000238418579, time = 0.34205150604248047\n",
      "Testing at step=75, batch=25, test loss = 0.3245149850845337, test acc = 0.8849999904632568, time = 0.34017348289489746\n",
      "Testing at step=75, batch=30, test loss = 0.29958638548851013, test acc = 0.8849999904632568, time = 0.3367433547973633\n",
      "Testing at step=75, batch=35, test loss = 0.30143389105796814, test acc = 0.875, time = 0.33577728271484375\n",
      "Testing at step=75, batch=40, test loss = 0.3732817471027374, test acc = 0.8600000143051147, time = 0.3367781639099121\n",
      "Testing at step=75, batch=45, test loss = 0.31578439474105835, test acc = 0.8799999952316284, time = 0.3411526679992676\n",
      "Step 75 finished in 309.35267663002014, Train loss = 0.25552820359667144, Test loss = 0.3543082639575005; Train Acc = 0.9051999992132187, Test Acc = 0.8768999981880188\n",
      "Training at step=76, batch=0, train loss = 0.21842752397060394, train acc = 0.9100000262260437, time = 0.9473140239715576\n",
      "Training at step=76, batch=30, train loss = 0.18968087434768677, train acc = 0.9200000166893005, time = 0.9368011951446533\n",
      "Training at step=76, batch=60, train loss = 0.22822101414203644, train acc = 0.9200000166893005, time = 0.9369916915893555\n",
      "Training at step=76, batch=90, train loss = 0.19719882309436798, train acc = 0.9350000023841858, time = 0.9363174438476562\n",
      "Training at step=76, batch=120, train loss = 0.2727763056755066, train acc = 0.8999999761581421, time = 0.9387965202331543\n",
      "Training at step=76, batch=150, train loss = 0.2602294087409973, train acc = 0.8999999761581421, time = 0.9562106132507324\n",
      "Training at step=76, batch=180, train loss = 0.25359928607940674, train acc = 0.8999999761581421, time = 0.9566388130187988\n",
      "Training at step=76, batch=210, train loss = 0.21234014630317688, train acc = 0.925000011920929, time = 0.9347658157348633\n",
      "Training at step=76, batch=240, train loss = 0.3526807427406311, train acc = 0.8650000095367432, time = 0.9393744468688965\n",
      "Training at step=76, batch=270, train loss = 0.2052120566368103, train acc = 0.9100000262260437, time = 0.9445278644561768\n",
      "Testing at step=76, batch=0, test loss = 0.4615200459957123, test acc = 0.8600000143051147, time = 0.34487032890319824\n",
      "Testing at step=76, batch=5, test loss = 0.29227781295776367, test acc = 0.9049999713897705, time = 0.34426426887512207\n",
      "Testing at step=76, batch=10, test loss = 0.4731382727622986, test acc = 0.8299999833106995, time = 0.3479933738708496\n",
      "Testing at step=76, batch=15, test loss = 0.34135276079177856, test acc = 0.8899999856948853, time = 0.34868693351745605\n",
      "Testing at step=76, batch=20, test loss = 0.33967429399490356, test acc = 0.8650000095367432, time = 0.3406562805175781\n",
      "Testing at step=76, batch=25, test loss = 0.3516358435153961, test acc = 0.8700000047683716, time = 0.3434934616088867\n",
      "Testing at step=76, batch=30, test loss = 0.24167652428150177, test acc = 0.8999999761581421, time = 0.3439445495605469\n",
      "Testing at step=76, batch=35, test loss = 0.3543461561203003, test acc = 0.8899999856948853, time = 0.33954501152038574\n",
      "Testing at step=76, batch=40, test loss = 0.295246422290802, test acc = 0.9049999713897705, time = 0.34325432777404785\n",
      "Testing at step=76, batch=45, test loss = 0.46900084614753723, test acc = 0.8500000238418579, time = 0.3379702568054199\n",
      "Step 76 finished in 310.30561900138855, Train loss = 0.25226957152287166, Test loss = 0.3652080434560776; Train Acc = 0.9065666651725769, Test Acc = 0.8748999977111817\n",
      "Training at step=77, batch=0, train loss = 0.22770079970359802, train acc = 0.925000011920929, time = 0.9406187534332275\n",
      "Training at step=77, batch=30, train loss = 0.2925375699996948, train acc = 0.8899999856948853, time = 0.9653880596160889\n",
      "Training at step=77, batch=60, train loss = 0.1667155772447586, train acc = 0.9300000071525574, time = 0.9429552555084229\n",
      "Training at step=77, batch=90, train loss = 0.22718818485736847, train acc = 0.925000011920929, time = 0.9419000148773193\n",
      "Training at step=77, batch=120, train loss = 0.3078485131263733, train acc = 0.8799999952316284, time = 0.9553935527801514\n",
      "Training at step=77, batch=150, train loss = 0.2401394098997116, train acc = 0.8949999809265137, time = 0.9413704872131348\n",
      "Training at step=77, batch=180, train loss = 0.30879807472229004, train acc = 0.8899999856948853, time = 0.9415183067321777\n",
      "Training at step=77, batch=210, train loss = 0.27287328243255615, train acc = 0.9100000262260437, time = 0.9398961067199707\n",
      "Training at step=77, batch=240, train loss = 0.27266165614128113, train acc = 0.8899999856948853, time = 0.9350428581237793\n",
      "Training at step=77, batch=270, train loss = 0.32813510298728943, train acc = 0.875, time = 0.9418208599090576\n",
      "Testing at step=77, batch=0, test loss = 0.3205874562263489, test acc = 0.8600000143051147, time = 0.3372650146484375\n",
      "Testing at step=77, batch=5, test loss = 0.3242834508419037, test acc = 0.8899999856948853, time = 0.3436598777770996\n",
      "Testing at step=77, batch=10, test loss = 0.4159854054450989, test acc = 0.8700000047683716, time = 0.3381080627441406\n",
      "Testing at step=77, batch=15, test loss = 0.44889310002326965, test acc = 0.8650000095367432, time = 0.3388638496398926\n",
      "Testing at step=77, batch=20, test loss = 0.34779202938079834, test acc = 0.8849999904632568, time = 0.3378777503967285\n",
      "Testing at step=77, batch=25, test loss = 0.2829045057296753, test acc = 0.9100000262260437, time = 0.3424384593963623\n",
      "Testing at step=77, batch=30, test loss = 0.3138287663459778, test acc = 0.9049999713897705, time = 0.33768296241760254\n",
      "Testing at step=77, batch=35, test loss = 0.2526347041130066, test acc = 0.9049999713897705, time = 0.3393692970275879\n",
      "Testing at step=77, batch=40, test loss = 0.32566696405410767, test acc = 0.8949999809265137, time = 0.34170103073120117\n",
      "Testing at step=77, batch=45, test loss = 0.3716019093990326, test acc = 0.8849999904632568, time = 0.3486199378967285\n",
      "Step 77 finished in 310.70946431159973, Train loss = 0.25052619566520057, Test loss = 0.3483204621076584; Train Acc = 0.9074333322048187, Test Acc = 0.8768999993801116\n",
      "Training at step=78, batch=0, train loss = 0.18311548233032227, train acc = 0.9449999928474426, time = 0.9597933292388916\n",
      "Training at step=78, batch=30, train loss = 0.25164663791656494, train acc = 0.9150000214576721, time = 0.9357790946960449\n",
      "Training at step=78, batch=60, train loss = 0.24889902770519257, train acc = 0.9100000262260437, time = 0.9383547306060791\n",
      "Training at step=78, batch=90, train loss = 0.24651235342025757, train acc = 0.9049999713897705, time = 0.9356043338775635\n",
      "Training at step=78, batch=120, train loss = 0.2514054775238037, train acc = 0.8999999761581421, time = 0.9307534694671631\n",
      "Training at step=78, batch=150, train loss = 0.2851509749889374, train acc = 0.8949999809265137, time = 0.9388148784637451\n",
      "Training at step=78, batch=180, train loss = 0.1983157992362976, train acc = 0.9399999976158142, time = 0.9336051940917969\n",
      "Training at step=78, batch=210, train loss = 0.26767125725746155, train acc = 0.8999999761581421, time = 0.9367096424102783\n",
      "Training at step=78, batch=240, train loss = 0.2899722158908844, train acc = 0.9100000262260437, time = 0.9378273487091064\n",
      "Training at step=78, batch=270, train loss = 0.27630025148391724, train acc = 0.925000011920929, time = 0.9471099376678467\n",
      "Testing at step=78, batch=0, test loss = 0.32933616638183594, test acc = 0.8799999952316284, time = 0.3412497043609619\n",
      "Testing at step=78, batch=5, test loss = 0.3273125886917114, test acc = 0.8849999904632568, time = 0.3641221523284912\n",
      "Testing at step=78, batch=10, test loss = 0.3428242802619934, test acc = 0.8899999856948853, time = 0.3629729747772217\n",
      "Testing at step=78, batch=15, test loss = 0.2713264226913452, test acc = 0.9100000262260437, time = 0.3411726951599121\n",
      "Testing at step=78, batch=20, test loss = 0.32261136174201965, test acc = 0.8799999952316284, time = 0.34163355827331543\n",
      "Testing at step=78, batch=25, test loss = 0.43014276027679443, test acc = 0.8500000238418579, time = 0.37980198860168457\n",
      "Testing at step=78, batch=30, test loss = 0.3974718451499939, test acc = 0.8650000095367432, time = 0.3381190299987793\n",
      "Testing at step=78, batch=35, test loss = 0.3069472014904022, test acc = 0.8799999952316284, time = 0.3377852439880371\n",
      "Testing at step=78, batch=40, test loss = 0.3831300735473633, test acc = 0.8949999809265137, time = 0.3440861701965332\n",
      "Testing at step=78, batch=45, test loss = 0.43637165427207947, test acc = 0.8450000286102295, time = 0.34275054931640625\n",
      "Step 78 finished in 310.57594060897827, Train loss = 0.2507234446704388, Test loss = 0.35024738222360613; Train Acc = 0.9073666667938233, Test Acc = 0.8778000020980835\n",
      "Training at step=79, batch=0, train loss = 0.17963670194149017, train acc = 0.9150000214576721, time = 0.9389233589172363\n",
      "Training at step=79, batch=30, train loss = 0.21667176485061646, train acc = 0.8949999809265137, time = 0.937974214553833\n",
      "Training at step=79, batch=60, train loss = 0.27280527353286743, train acc = 0.8949999809265137, time = 0.937326192855835\n",
      "Training at step=79, batch=90, train loss = 0.17647594213485718, train acc = 0.9350000023841858, time = 0.967151403427124\n",
      "Training at step=79, batch=120, train loss = 0.23888713121414185, train acc = 0.9100000262260437, time = 0.9348182678222656\n",
      "Training at step=79, batch=150, train loss = 0.2082173377275467, train acc = 0.9049999713897705, time = 0.9399833679199219\n",
      "Training at step=79, batch=180, train loss = 0.28929436206817627, train acc = 0.8899999856948853, time = 0.9435324668884277\n",
      "Training at step=79, batch=210, train loss = 0.3036613464355469, train acc = 0.8849999904632568, time = 0.9382603168487549\n",
      "Training at step=79, batch=240, train loss = 0.25274142622947693, train acc = 0.8999999761581421, time = 0.9347493648529053\n",
      "Training at step=79, batch=270, train loss = 0.35727423429489136, train acc = 0.8899999856948853, time = 0.9669415950775146\n",
      "Testing at step=79, batch=0, test loss = 0.3229534924030304, test acc = 0.875, time = 0.33905911445617676\n",
      "Testing at step=79, batch=5, test loss = 0.33267146348953247, test acc = 0.8849999904632568, time = 0.3390014171600342\n",
      "Testing at step=79, batch=10, test loss = 0.3792002499103546, test acc = 0.8899999856948853, time = 0.34305357933044434\n",
      "Testing at step=79, batch=15, test loss = 0.342195063829422, test acc = 0.8899999856948853, time = 0.3393433094024658\n",
      "Testing at step=79, batch=20, test loss = 0.3636974096298218, test acc = 0.8650000095367432, time = 0.3390023708343506\n",
      "Testing at step=79, batch=25, test loss = 0.4603421092033386, test acc = 0.8299999833106995, time = 0.33973217010498047\n",
      "Testing at step=79, batch=30, test loss = 0.4223652780056, test acc = 0.8399999737739563, time = 0.3466482162475586\n",
      "Testing at step=79, batch=35, test loss = 0.34865638613700867, test acc = 0.8700000047683716, time = 0.3371391296386719\n",
      "Testing at step=79, batch=40, test loss = 0.3635294735431671, test acc = 0.8849999904632568, time = 0.34415602684020996\n",
      "Testing at step=79, batch=45, test loss = 0.31770801544189453, test acc = 0.8650000095367432, time = 0.3423774242401123\n",
      "Step 79 finished in 310.94526648521423, Train loss = 0.24988698961834113, Test loss = 0.35070867359638214; Train Acc = 0.9073333334922791, Test Acc = 0.8787000012397767\n",
      "Training at step=80, batch=0, train loss = 0.29841306805610657, train acc = 0.8799999952316284, time = 0.9416444301605225\n",
      "Training at step=80, batch=30, train loss = 0.18495997786521912, train acc = 0.9449999928474426, time = 0.9451220035552979\n",
      "Training at step=80, batch=60, train loss = 0.23943102359771729, train acc = 0.9150000214576721, time = 0.9404869079589844\n",
      "Training at step=80, batch=90, train loss = 0.23170314729213715, train acc = 0.8949999809265137, time = 0.9660365581512451\n",
      "Training at step=80, batch=120, train loss = 0.19422543048858643, train acc = 0.9150000214576721, time = 0.9354836940765381\n",
      "Training at step=80, batch=150, train loss = 0.25587236881256104, train acc = 0.8949999809265137, time = 0.9378964900970459\n",
      "Training at step=80, batch=180, train loss = 0.17687390744686127, train acc = 0.9200000166893005, time = 0.9378468990325928\n",
      "Training at step=80, batch=210, train loss = 0.3760639429092407, train acc = 0.8899999856948853, time = 0.9370913505554199\n",
      "Training at step=80, batch=240, train loss = 0.34779879450798035, train acc = 0.8550000190734863, time = 0.9450974464416504\n",
      "Training at step=80, batch=270, train loss = 0.28261029720306396, train acc = 0.8949999809265137, time = 0.9359183311462402\n",
      "Testing at step=80, batch=0, test loss = 0.3506949245929718, test acc = 0.8650000095367432, time = 0.3406956195831299\n",
      "Testing at step=80, batch=5, test loss = 0.2552453279495239, test acc = 0.9150000214576721, time = 0.33885931968688965\n",
      "Testing at step=80, batch=10, test loss = 0.4542754292488098, test acc = 0.8299999833106995, time = 0.33935117721557617\n",
      "Testing at step=80, batch=15, test loss = 0.39599528908729553, test acc = 0.8999999761581421, time = 0.3391454219818115\n",
      "Testing at step=80, batch=20, test loss = 0.30961111187934875, test acc = 0.8899999856948853, time = 0.3414268493652344\n",
      "Testing at step=80, batch=25, test loss = 0.4725196957588196, test acc = 0.8650000095367432, time = 0.36048340797424316\n",
      "Testing at step=80, batch=30, test loss = 0.33683452010154724, test acc = 0.8849999904632568, time = 0.41716885566711426\n",
      "Testing at step=80, batch=35, test loss = 0.29092276096343994, test acc = 0.9150000214576721, time = 0.33548927307128906\n",
      "Testing at step=80, batch=40, test loss = 0.3346634805202484, test acc = 0.875, time = 0.33805298805236816\n",
      "Testing at step=80, batch=45, test loss = 0.3000149130821228, test acc = 0.8999999761581421, time = 0.3408081531524658\n",
      "Step 80 finished in 310.1440415382385, Train loss = 0.2502220905323823, Test loss = 0.3666371750831604; Train Acc = 0.9072833339373271, Test Acc = 0.8748999977111817\n",
      "Training at step=81, batch=0, train loss = 0.22846662998199463, train acc = 0.9100000262260437, time = 0.9515078067779541\n",
      "Training at step=81, batch=30, train loss = 0.22742228209972382, train acc = 0.9150000214576721, time = 0.9402599334716797\n",
      "Training at step=81, batch=60, train loss = 0.23759090900421143, train acc = 0.9200000166893005, time = 0.9447200298309326\n",
      "Training at step=81, batch=90, train loss = 0.20588958263397217, train acc = 0.8949999809265137, time = 0.9748432636260986\n",
      "Training at step=81, batch=120, train loss = 0.3472427725791931, train acc = 0.8550000190734863, time = 0.9651358127593994\n",
      "Training at step=81, batch=150, train loss = 0.20954178273677826, train acc = 0.9449999928474426, time = 0.9424288272857666\n",
      "Training at step=81, batch=180, train loss = 0.30722567439079285, train acc = 0.8799999952316284, time = 0.945458173751831\n",
      "Training at step=81, batch=210, train loss = 0.2885093092918396, train acc = 0.8949999809265137, time = 0.9354081153869629\n",
      "Training at step=81, batch=240, train loss = 0.3044711947441101, train acc = 0.8949999809265137, time = 0.9390449523925781\n",
      "Training at step=81, batch=270, train loss = 0.30024290084838867, train acc = 0.8949999809265137, time = 0.9343194961547852\n",
      "Testing at step=81, batch=0, test loss = 0.28793996572494507, test acc = 0.9150000214576721, time = 0.3368384838104248\n",
      "Testing at step=81, batch=5, test loss = 0.38036099076271057, test acc = 0.8550000190734863, time = 0.34148645401000977\n",
      "Testing at step=81, batch=10, test loss = 0.3505571484565735, test acc = 0.9150000214576721, time = 0.3399968147277832\n",
      "Testing at step=81, batch=15, test loss = 0.4435044229030609, test acc = 0.8500000238418579, time = 0.358475923538208\n",
      "Testing at step=81, batch=20, test loss = 0.3451980650424957, test acc = 0.8999999761581421, time = 0.36522746086120605\n",
      "Testing at step=81, batch=25, test loss = 0.2798628509044647, test acc = 0.8949999809265137, time = 0.33927226066589355\n",
      "Testing at step=81, batch=30, test loss = 0.25577589869499207, test acc = 0.8899999856948853, time = 0.34138941764831543\n",
      "Testing at step=81, batch=35, test loss = 0.24300144612789154, test acc = 0.9049999713897705, time = 0.3417544364929199\n",
      "Testing at step=81, batch=40, test loss = 0.34175464510917664, test acc = 0.8600000143051147, time = 0.3474297523498535\n",
      "Testing at step=81, batch=45, test loss = 0.41452479362487793, test acc = 0.8500000238418579, time = 0.3433394432067871\n",
      "Step 81 finished in 311.0483069419861, Train loss = 0.24803344508012135, Test loss = 0.34898538887500763; Train Acc = 0.9071500007311503, Test Acc = 0.8832000005245209\n",
      "Training at step=82, batch=0, train loss = 0.2742427885532379, train acc = 0.925000011920929, time = 0.9432122707366943\n",
      "Training at step=82, batch=30, train loss = 0.23377186059951782, train acc = 0.8999999761581421, time = 0.9355878829956055\n",
      "Training at step=82, batch=60, train loss = 0.28060856461524963, train acc = 0.8650000095367432, time = 0.9384679794311523\n",
      "Training at step=82, batch=90, train loss = 0.19535726308822632, train acc = 0.9200000166893005, time = 0.9337267875671387\n",
      "Training at step=82, batch=120, train loss = 0.2292855978012085, train acc = 0.9150000214576721, time = 0.9364628791809082\n",
      "Training at step=82, batch=150, train loss = 0.23248344659805298, train acc = 0.9200000166893005, time = 0.9618377685546875\n",
      "Training at step=82, batch=180, train loss = 0.2388090342283249, train acc = 0.9049999713897705, time = 0.9364254474639893\n",
      "Training at step=82, batch=210, train loss = 0.20969977974891663, train acc = 0.925000011920929, time = 1.0931026935577393\n",
      "Training at step=82, batch=240, train loss = 0.22640018165111542, train acc = 0.925000011920929, time = 0.9488065242767334\n",
      "Training at step=82, batch=270, train loss = 0.19447502493858337, train acc = 0.9300000071525574, time = 0.9363558292388916\n",
      "Testing at step=82, batch=0, test loss = 0.3281891345977783, test acc = 0.875, time = 0.3397338390350342\n",
      "Testing at step=82, batch=5, test loss = 0.3001512289047241, test acc = 0.8700000047683716, time = 0.340010404586792\n",
      "Testing at step=82, batch=10, test loss = 0.4422336518764496, test acc = 0.8550000190734863, time = 0.3419034481048584\n",
      "Testing at step=82, batch=15, test loss = 0.36660727858543396, test acc = 0.8600000143051147, time = 0.34305286407470703\n",
      "Testing at step=82, batch=20, test loss = 0.3537672460079193, test acc = 0.875, time = 0.34323596954345703\n",
      "Testing at step=82, batch=25, test loss = 0.320966899394989, test acc = 0.8999999761581421, time = 0.34162259101867676\n",
      "Testing at step=82, batch=30, test loss = 0.3005307912826538, test acc = 0.8650000095367432, time = 0.3431241512298584\n",
      "Testing at step=82, batch=35, test loss = 0.32568037509918213, test acc = 0.8849999904632568, time = 0.33507704734802246\n",
      "Testing at step=82, batch=40, test loss = 0.43599167466163635, test acc = 0.8600000143051147, time = 0.34145689010620117\n",
      "Testing at step=82, batch=45, test loss = 0.47067123651504517, test acc = 0.824999988079071, time = 0.34245920181274414\n",
      "Step 82 finished in 312.3679633140564, Train loss = 0.24585561588406563, Test loss = 0.3571888503432274; Train Acc = 0.9082333326339722, Test Acc = 0.8741999995708466\n",
      "Training at step=83, batch=0, train loss = 0.2220321148633957, train acc = 0.925000011920929, time = 0.939293622970581\n",
      "Training at step=83, batch=30, train loss = 0.2201000154018402, train acc = 0.9150000214576721, time = 0.9331724643707275\n",
      "Training at step=83, batch=60, train loss = 0.25130248069763184, train acc = 0.9100000262260437, time = 0.9464542865753174\n",
      "Training at step=83, batch=90, train loss = 0.1800127625465393, train acc = 0.9350000023841858, time = 1.0944221019744873\n",
      "Training at step=83, batch=120, train loss = 0.16290563344955444, train acc = 0.9350000023841858, time = 0.9371662139892578\n",
      "Training at step=83, batch=150, train loss = 0.21453985571861267, train acc = 0.9350000023841858, time = 0.9333829879760742\n",
      "Training at step=83, batch=180, train loss = 0.2675515115261078, train acc = 0.8949999809265137, time = 0.9522824287414551\n",
      "Training at step=83, batch=210, train loss = 0.22838488221168518, train acc = 0.9200000166893005, time = 0.9332981109619141\n",
      "Training at step=83, batch=240, train loss = 0.20744414627552032, train acc = 0.9200000166893005, time = 0.9592854976654053\n",
      "Training at step=83, batch=270, train loss = 0.25875455141067505, train acc = 0.8999999761581421, time = 0.9607584476470947\n",
      "Testing at step=83, batch=0, test loss = 0.43201950192451477, test acc = 0.8399999737739563, time = 0.3382568359375\n",
      "Testing at step=83, batch=5, test loss = 0.31152158975601196, test acc = 0.8849999904632568, time = 0.3392364978790283\n",
      "Testing at step=83, batch=10, test loss = 0.33172711730003357, test acc = 0.8650000095367432, time = 0.3390171527862549\n",
      "Testing at step=83, batch=15, test loss = 0.3600350320339203, test acc = 0.8500000238418579, time = 0.33597254753112793\n",
      "Testing at step=83, batch=20, test loss = 0.33268192410469055, test acc = 0.875, time = 0.347689151763916\n",
      "Testing at step=83, batch=25, test loss = 0.41082626581192017, test acc = 0.8700000047683716, time = 0.33911848068237305\n",
      "Testing at step=83, batch=30, test loss = 0.3813728094100952, test acc = 0.875, time = 0.34310317039489746\n",
      "Testing at step=83, batch=35, test loss = 0.24728095531463623, test acc = 0.925000011920929, time = 0.33887243270874023\n",
      "Testing at step=83, batch=40, test loss = 0.3528806269168854, test acc = 0.8849999904632568, time = 0.34032487869262695\n",
      "Testing at step=83, batch=45, test loss = 0.4040033221244812, test acc = 0.8450000286102295, time = 0.33919596672058105\n",
      "Step 83 finished in 314.017254114151, Train loss = 0.24577059745788574, Test loss = 0.3615789583325386; Train Acc = 0.9090000009536743, Test Acc = 0.8749999988079071\n",
      "Training at step=84, batch=0, train loss = 0.2207534909248352, train acc = 0.9200000166893005, time = 0.9632701873779297\n",
      "Training at step=84, batch=30, train loss = 0.19246019423007965, train acc = 0.9399999976158142, time = 0.9478559494018555\n",
      "Training at step=84, batch=60, train loss = 0.22977428138256073, train acc = 0.9100000262260437, time = 0.9393742084503174\n",
      "Training at step=84, batch=90, train loss = 0.2724328637123108, train acc = 0.8999999761581421, time = 0.9440279006958008\n",
      "Training at step=84, batch=120, train loss = 0.22234433889389038, train acc = 0.925000011920929, time = 0.9482202529907227\n",
      "Training at step=84, batch=150, train loss = 0.2698439359664917, train acc = 0.9049999713897705, time = 0.9327864646911621\n",
      "Training at step=84, batch=180, train loss = 0.22439983487129211, train acc = 0.9200000166893005, time = 0.9395463466644287\n",
      "Training at step=84, batch=210, train loss = 0.19641482830047607, train acc = 0.925000011920929, time = 0.9389984607696533\n",
      "Training at step=84, batch=240, train loss = 0.3436235189437866, train acc = 0.8799999952316284, time = 0.9384074211120605\n",
      "Training at step=84, batch=270, train loss = 0.2523968517780304, train acc = 0.9100000262260437, time = 0.9441335201263428\n",
      "Testing at step=84, batch=0, test loss = 0.39916637539863586, test acc = 0.8550000190734863, time = 0.3420090675354004\n",
      "Testing at step=84, batch=5, test loss = 0.3898099958896637, test acc = 0.8700000047683716, time = 0.3404817581176758\n",
      "Testing at step=84, batch=10, test loss = 0.3188193738460541, test acc = 0.875, time = 0.3374927043914795\n",
      "Testing at step=84, batch=15, test loss = 0.3782377243041992, test acc = 0.8500000238418579, time = 0.3421449661254883\n",
      "Testing at step=84, batch=20, test loss = 0.38408592343330383, test acc = 0.8849999904632568, time = 0.34094762802124023\n",
      "Testing at step=84, batch=25, test loss = 0.4278492033481598, test acc = 0.8550000190734863, time = 0.3383803367614746\n",
      "Testing at step=84, batch=30, test loss = 0.40319985151290894, test acc = 0.8650000095367432, time = 0.33989763259887695\n",
      "Testing at step=84, batch=35, test loss = 0.47362220287323, test acc = 0.8299999833106995, time = 0.33824729919433594\n",
      "Testing at step=84, batch=40, test loss = 0.36614787578582764, test acc = 0.8949999809265137, time = 0.3392312526702881\n",
      "Testing at step=84, batch=45, test loss = 0.3522188663482666, test acc = 0.9049999713897705, time = 0.340038537979126\n",
      "Step 84 finished in 310.8945474624634, Train loss = 0.24464779218037924, Test loss = 0.3718508929014206; Train Acc = 0.909066666563352, Test Acc = 0.876399998664856\n",
      "Training at step=85, batch=0, train loss = 0.23790611326694489, train acc = 0.9300000071525574, time = 0.937471866607666\n",
      "Training at step=85, batch=30, train loss = 0.3413645625114441, train acc = 0.8600000143051147, time = 0.9447901248931885\n",
      "Training at step=85, batch=60, train loss = 0.19113671779632568, train acc = 0.9300000071525574, time = 0.9392178058624268\n",
      "Training at step=85, batch=90, train loss = 0.2165423184633255, train acc = 0.9399999976158142, time = 0.962146520614624\n",
      "Training at step=85, batch=120, train loss = 0.1955384463071823, train acc = 0.9449999928474426, time = 0.9389030933380127\n",
      "Training at step=85, batch=150, train loss = 0.2709609568119049, train acc = 0.8999999761581421, time = 0.9372024536132812\n",
      "Training at step=85, batch=180, train loss = 0.29004359245300293, train acc = 0.8799999952316284, time = 0.942267656326294\n",
      "Training at step=85, batch=210, train loss = 0.1590748280286789, train acc = 0.9399999976158142, time = 0.9366602897644043\n",
      "Training at step=85, batch=240, train loss = 0.2581191658973694, train acc = 0.8849999904632568, time = 0.9360795021057129\n",
      "Training at step=85, batch=270, train loss = 0.2255088835954666, train acc = 0.9200000166893005, time = 0.942380428314209\n",
      "Testing at step=85, batch=0, test loss = 0.3831036388874054, test acc = 0.8799999952316284, time = 0.3632233142852783\n",
      "Testing at step=85, batch=5, test loss = 0.3698369860649109, test acc = 0.8999999761581421, time = 0.3390655517578125\n",
      "Testing at step=85, batch=10, test loss = 0.2888094484806061, test acc = 0.9100000262260437, time = 0.3377103805541992\n",
      "Testing at step=85, batch=15, test loss = 0.3245694637298584, test acc = 0.8949999809265137, time = 0.3364756107330322\n",
      "Testing at step=85, batch=20, test loss = 0.4211503267288208, test acc = 0.875, time = 0.3387579917907715\n",
      "Testing at step=85, batch=25, test loss = 0.38636040687561035, test acc = 0.875, time = 0.338942289352417\n",
      "Testing at step=85, batch=30, test loss = 0.3757416605949402, test acc = 0.8600000143051147, time = 0.3418858051300049\n",
      "Testing at step=85, batch=35, test loss = 0.31640806794166565, test acc = 0.8550000190734863, time = 0.3455924987792969\n",
      "Testing at step=85, batch=40, test loss = 0.2778801918029785, test acc = 0.8899999856948853, time = 0.339139461517334\n",
      "Testing at step=85, batch=45, test loss = 0.33392414450645447, test acc = 0.875, time = 0.3428950309753418\n",
      "Step 85 finished in 310.36098194122314, Train loss = 0.24500166644652685, Test loss = 0.3485498321056366; Train Acc = 0.9099000014861425, Test Acc = 0.8810999965667725\n",
      "Training at step=86, batch=0, train loss = 0.3481638431549072, train acc = 0.875, time = 0.9414072036743164\n",
      "Training at step=86, batch=30, train loss = 0.242888405919075, train acc = 0.925000011920929, time = 0.9678244590759277\n",
      "Training at step=86, batch=60, train loss = 0.27455681562423706, train acc = 0.8799999952316284, time = 0.9360642433166504\n",
      "Training at step=86, batch=90, train loss = 0.16415733098983765, train acc = 0.9350000023841858, time = 0.93984055519104\n",
      "Training at step=86, batch=120, train loss = 0.3331933915615082, train acc = 0.8899999856948853, time = 0.9373025894165039\n",
      "Training at step=86, batch=150, train loss = 0.2059669941663742, train acc = 0.9150000214576721, time = 0.9337964057922363\n",
      "Training at step=86, batch=180, train loss = 0.28995993733406067, train acc = 0.8949999809265137, time = 0.9388871192932129\n",
      "Training at step=86, batch=210, train loss = 0.23791836202144623, train acc = 0.9150000214576721, time = 1.029280662536621\n",
      "Training at step=86, batch=240, train loss = 0.26988014578819275, train acc = 0.9100000262260437, time = 0.9435040950775146\n",
      "Training at step=86, batch=270, train loss = 0.24764680862426758, train acc = 0.9100000262260437, time = 0.9366600513458252\n",
      "Testing at step=86, batch=0, test loss = 0.41794657707214355, test acc = 0.8550000190734863, time = 0.3429408073425293\n",
      "Testing at step=86, batch=5, test loss = 0.3718856871128082, test acc = 0.8799999952316284, time = 0.347348690032959\n",
      "Testing at step=86, batch=10, test loss = 0.41719910502433777, test acc = 0.8500000238418579, time = 0.33634042739868164\n",
      "Testing at step=86, batch=15, test loss = 0.4160464406013489, test acc = 0.8899999856948853, time = 0.3458127975463867\n",
      "Testing at step=86, batch=20, test loss = 0.29757851362228394, test acc = 0.8949999809265137, time = 0.3383162021636963\n",
      "Testing at step=86, batch=25, test loss = 0.38799992203712463, test acc = 0.8500000238418579, time = 0.3394966125488281\n",
      "Testing at step=86, batch=30, test loss = 0.39130523800849915, test acc = 0.875, time = 0.34537792205810547\n",
      "Testing at step=86, batch=35, test loss = 0.3566357493400574, test acc = 0.875, time = 0.3610658645629883\n",
      "Testing at step=86, batch=40, test loss = 0.3989015221595764, test acc = 0.875, time = 0.33686137199401855\n",
      "Testing at step=86, batch=45, test loss = 0.5448562502861023, test acc = 0.8600000143051147, time = 0.3394179344177246\n",
      "Step 86 finished in 310.2001316547394, Train loss = 0.24684626509745916, Test loss = 0.35600104987621306; Train Acc = 0.9083833336830139, Test Acc = 0.8779000008106231\n",
      "Training at step=87, batch=0, train loss = 0.2986714541912079, train acc = 0.8899999856948853, time = 0.9374308586120605\n",
      "Training at step=87, batch=30, train loss = 0.2672916650772095, train acc = 0.8949999809265137, time = 0.9603409767150879\n",
      "Training at step=87, batch=60, train loss = 0.20079277455806732, train acc = 0.9350000023841858, time = 0.9351489543914795\n",
      "Training at step=87, batch=90, train loss = 0.24338321387767792, train acc = 0.9300000071525574, time = 0.9441227912902832\n",
      "Training at step=87, batch=120, train loss = 0.28529518842697144, train acc = 0.9150000214576721, time = 0.9514837265014648\n",
      "Training at step=87, batch=150, train loss = 0.24397599697113037, train acc = 0.9150000214576721, time = 0.9361705780029297\n",
      "Training at step=87, batch=180, train loss = 0.2694147229194641, train acc = 0.8999999761581421, time = 0.9359171390533447\n",
      "Training at step=87, batch=210, train loss = 0.19540922343730927, train acc = 0.8999999761581421, time = 0.9705295562744141\n",
      "Training at step=87, batch=240, train loss = 0.28253090381622314, train acc = 0.8999999761581421, time = 0.9349021911621094\n",
      "Training at step=87, batch=270, train loss = 0.28811824321746826, train acc = 0.9150000214576721, time = 0.9366371631622314\n",
      "Testing at step=87, batch=0, test loss = 0.33562171459198, test acc = 0.8849999904632568, time = 0.3373291492462158\n",
      "Testing at step=87, batch=5, test loss = 0.3654251992702484, test acc = 0.8700000047683716, time = 0.33882999420166016\n",
      "Testing at step=87, batch=10, test loss = 0.4470534026622772, test acc = 0.8399999737739563, time = 0.3423316478729248\n",
      "Testing at step=87, batch=15, test loss = 0.4043402075767517, test acc = 0.8849999904632568, time = 0.3403053283691406\n",
      "Testing at step=87, batch=20, test loss = 0.3330841064453125, test acc = 0.875, time = 0.33829641342163086\n",
      "Testing at step=87, batch=25, test loss = 0.37267693877220154, test acc = 0.875, time = 0.3388631343841553\n",
      "Testing at step=87, batch=30, test loss = 0.2555968463420868, test acc = 0.8999999761581421, time = 0.33769845962524414\n",
      "Testing at step=87, batch=35, test loss = 0.36893853545188904, test acc = 0.8799999952316284, time = 0.3415255546569824\n",
      "Testing at step=87, batch=40, test loss = 0.28058791160583496, test acc = 0.9200000166893005, time = 0.3400745391845703\n",
      "Testing at step=87, batch=45, test loss = 0.40692055225372314, test acc = 0.8450000286102295, time = 0.34146618843078613\n",
      "Step 87 finished in 310.36782693862915, Train loss = 0.24139316494266191, Test loss = 0.35522410362958906; Train Acc = 0.9099999990065892, Test Acc = 0.8798999977111817\n",
      "Training at step=88, batch=0, train loss = 0.25361984968185425, train acc = 0.9049999713897705, time = 0.943199634552002\n",
      "Training at step=88, batch=30, train loss = 0.317699670791626, train acc = 0.8700000047683716, time = 0.942664384841919\n",
      "Training at step=88, batch=60, train loss = 0.23248359560966492, train acc = 0.9100000262260437, time = 0.935253381729126\n",
      "Training at step=88, batch=90, train loss = 0.10702667385339737, train acc = 0.9649999737739563, time = 0.9357635974884033\n",
      "Training at step=88, batch=120, train loss = 0.22492992877960205, train acc = 0.9049999713897705, time = 0.9406783580780029\n",
      "Training at step=88, batch=150, train loss = 0.25771573185920715, train acc = 0.925000011920929, time = 0.9597342014312744\n",
      "Training at step=88, batch=180, train loss = 0.23585183918476105, train acc = 0.9049999713897705, time = 0.9575445652008057\n",
      "Training at step=88, batch=210, train loss = 0.23717136681079865, train acc = 0.9150000214576721, time = 1.015650749206543\n",
      "Training at step=88, batch=240, train loss = 0.1848999410867691, train acc = 0.9449999928474426, time = 0.9379830360412598\n",
      "Training at step=88, batch=270, train loss = 0.26281169056892395, train acc = 0.8999999761581421, time = 0.9566304683685303\n",
      "Testing at step=88, batch=0, test loss = 0.399748831987381, test acc = 0.8299999833106995, time = 0.3443472385406494\n",
      "Testing at step=88, batch=5, test loss = 0.3401627838611603, test acc = 0.8949999809265137, time = 0.3414425849914551\n",
      "Testing at step=88, batch=10, test loss = 0.3498205244541168, test acc = 0.8600000143051147, time = 0.3424489498138428\n",
      "Testing at step=88, batch=15, test loss = 0.3326510488986969, test acc = 0.875, time = 0.3405582904815674\n",
      "Testing at step=88, batch=20, test loss = 0.3047329783439636, test acc = 0.9100000262260437, time = 0.34387707710266113\n",
      "Testing at step=88, batch=25, test loss = 0.2842194736003876, test acc = 0.8849999904632568, time = 0.34761619567871094\n",
      "Testing at step=88, batch=30, test loss = 0.2945650815963745, test acc = 0.9200000166893005, time = 0.4347662925720215\n",
      "Testing at step=88, batch=35, test loss = 0.40507563948631287, test acc = 0.8600000143051147, time = 0.34377264976501465\n",
      "Testing at step=88, batch=40, test loss = 0.42788830399513245, test acc = 0.8349999785423279, time = 0.34093761444091797\n",
      "Testing at step=88, batch=45, test loss = 0.4586212933063507, test acc = 0.8550000190734863, time = 0.33805227279663086\n",
      "Step 88 finished in 311.15321588516235, Train loss = 0.24269477414588134, Test loss = 0.35781792253255845; Train Acc = 0.9096999990940094, Test Acc = 0.8751999974250794\n",
      "Training at step=89, batch=0, train loss = 0.27891165018081665, train acc = 0.9300000071525574, time = 0.9416747093200684\n",
      "Training at step=89, batch=30, train loss = 0.21539361774921417, train acc = 0.9049999713897705, time = 0.9400675296783447\n",
      "Training at step=89, batch=60, train loss = 0.22092698514461517, train acc = 0.9049999713897705, time = 0.9354479312896729\n",
      "Training at step=89, batch=90, train loss = 0.22047485411167145, train acc = 0.9150000214576721, time = 0.9391822814941406\n",
      "Training at step=89, batch=120, train loss = 0.2087881863117218, train acc = 0.9200000166893005, time = 0.9418492317199707\n",
      "Training at step=89, batch=150, train loss = 0.19522151350975037, train acc = 0.9350000023841858, time = 0.9439065456390381\n",
      "Training at step=89, batch=180, train loss = 0.27580034732818604, train acc = 0.9150000214576721, time = 0.9475605487823486\n",
      "Training at step=89, batch=210, train loss = 0.17658936977386475, train acc = 0.9350000023841858, time = 0.9324214458465576\n",
      "Training at step=89, batch=240, train loss = 0.2291841357946396, train acc = 0.9100000262260437, time = 0.9604818820953369\n",
      "Training at step=89, batch=270, train loss = 0.16342617571353912, train acc = 0.9399999976158142, time = 0.9385037422180176\n",
      "Testing at step=89, batch=0, test loss = 0.3592096269130707, test acc = 0.8949999809265137, time = 0.34221577644348145\n",
      "Testing at step=89, batch=5, test loss = 0.33322086930274963, test acc = 0.8650000095367432, time = 0.3416585922241211\n",
      "Testing at step=89, batch=10, test loss = 0.4407638609409332, test acc = 0.8199999928474426, time = 0.338512659072876\n",
      "Testing at step=89, batch=15, test loss = 0.4175952970981598, test acc = 0.8849999904632568, time = 0.3400743007659912\n",
      "Testing at step=89, batch=20, test loss = 0.44088008999824524, test acc = 0.8500000238418579, time = 0.33669042587280273\n",
      "Testing at step=89, batch=25, test loss = 0.269935667514801, test acc = 0.8799999952316284, time = 0.3381674289703369\n",
      "Testing at step=89, batch=30, test loss = 0.5255000591278076, test acc = 0.8100000023841858, time = 0.341935396194458\n",
      "Testing at step=89, batch=35, test loss = 0.30858445167541504, test acc = 0.8700000047683716, time = 0.33558154106140137\n",
      "Testing at step=89, batch=40, test loss = 0.43485918641090393, test acc = 0.8399999737739563, time = 0.33684825897216797\n",
      "Testing at step=89, batch=45, test loss = 0.38407257199287415, test acc = 0.8399999737739563, time = 0.33637213706970215\n",
      "Step 89 finished in 309.02565932273865, Train loss = 0.24166938910881677, Test loss = 0.36522257298231126; Train Acc = 0.9110666676362356, Test Acc = 0.8736999976634979\n",
      "Training at step=90, batch=0, train loss = 0.285447359085083, train acc = 0.8999999761581421, time = 0.9664697647094727\n",
      "Training at step=90, batch=30, train loss = 0.2879743278026581, train acc = 0.875, time = 0.9325625896453857\n",
      "Training at step=90, batch=60, train loss = 0.30610617995262146, train acc = 0.8949999809265137, time = 0.9409940242767334\n",
      "Training at step=90, batch=90, train loss = 0.2345975637435913, train acc = 0.9200000166893005, time = 0.9303092956542969\n",
      "Training at step=90, batch=120, train loss = 0.19782231748104095, train acc = 0.925000011920929, time = 0.9375138282775879\n",
      "Training at step=90, batch=150, train loss = 0.24450674653053284, train acc = 0.9049999713897705, time = 0.9608204364776611\n",
      "Training at step=90, batch=180, train loss = 0.18471069633960724, train acc = 0.9399999976158142, time = 0.9409580230712891\n",
      "Training at step=90, batch=210, train loss = 0.2590528428554535, train acc = 0.9200000166893005, time = 0.9412510395050049\n",
      "Training at step=90, batch=240, train loss = 0.2360917031764984, train acc = 0.9150000214576721, time = 0.934105634689331\n",
      "Training at step=90, batch=270, train loss = 0.1955433040857315, train acc = 0.9350000023841858, time = 0.9381735324859619\n",
      "Testing at step=90, batch=0, test loss = 0.4959311783313751, test acc = 0.8650000095367432, time = 0.3653552532196045\n",
      "Testing at step=90, batch=5, test loss = 0.34841689467430115, test acc = 0.8500000238418579, time = 0.36033201217651367\n",
      "Testing at step=90, batch=10, test loss = 0.30635866522789, test acc = 0.8899999856948853, time = 0.36054563522338867\n",
      "Testing at step=90, batch=15, test loss = 0.3209812641143799, test acc = 0.8700000047683716, time = 0.3599057197570801\n",
      "Testing at step=90, batch=20, test loss = 0.3150092661380768, test acc = 0.9049999713897705, time = 0.3654043674468994\n",
      "Testing at step=90, batch=25, test loss = 0.24265140295028687, test acc = 0.8999999761581421, time = 0.3626730442047119\n",
      "Testing at step=90, batch=30, test loss = 0.2378271222114563, test acc = 0.9150000214576721, time = 0.3426504135131836\n",
      "Testing at step=90, batch=35, test loss = 0.42774245142936707, test acc = 0.8700000047683716, time = 0.3388848304748535\n",
      "Testing at step=90, batch=40, test loss = 0.43213585019111633, test acc = 0.8550000190734863, time = 0.33983302116394043\n",
      "Testing at step=90, batch=45, test loss = 0.32027870416641235, test acc = 0.8899999856948853, time = 0.3400142192840576\n",
      "Step 90 finished in 311.0813617706299, Train loss = 0.2402179982761542, Test loss = 0.36324226170778273; Train Acc = 0.9107833351691564, Test Acc = 0.8747000002861023\n",
      "Training at step=91, batch=0, train loss = 0.17795638740062714, train acc = 0.9449999928474426, time = 0.9475409984588623\n",
      "Training at step=91, batch=30, train loss = 0.2227039784193039, train acc = 0.925000011920929, time = 0.9444730281829834\n",
      "Training at step=91, batch=60, train loss = 0.2753644287586212, train acc = 0.8949999809265137, time = 0.943812370300293\n",
      "Training at step=91, batch=90, train loss = 0.22717095911502838, train acc = 0.9150000214576721, time = 0.9385251998901367\n",
      "Training at step=91, batch=120, train loss = 0.1989988088607788, train acc = 0.9300000071525574, time = 0.9375615119934082\n",
      "Training at step=91, batch=150, train loss = 0.30671510100364685, train acc = 0.8799999952316284, time = 0.9332261085510254\n",
      "Training at step=91, batch=180, train loss = 0.26902416348457336, train acc = 0.9100000262260437, time = 0.9355340003967285\n",
      "Training at step=91, batch=210, train loss = 0.357059121131897, train acc = 0.8650000095367432, time = 0.9378585815429688\n",
      "Training at step=91, batch=240, train loss = 0.24987675249576569, train acc = 0.9049999713897705, time = 0.9400668144226074\n",
      "Training at step=91, batch=270, train loss = 0.2054705172777176, train acc = 0.9350000023841858, time = 0.9361171722412109\n",
      "Testing at step=91, batch=0, test loss = 0.323525607585907, test acc = 0.8899999856948853, time = 0.34229516983032227\n",
      "Testing at step=91, batch=5, test loss = 0.44783562421798706, test acc = 0.8650000095367432, time = 0.3409252166748047\n",
      "Testing at step=91, batch=10, test loss = 0.29530811309814453, test acc = 0.9100000262260437, time = 0.36154794692993164\n",
      "Testing at step=91, batch=15, test loss = 0.46957194805145264, test acc = 0.8949999809265137, time = 0.359508752822876\n",
      "Testing at step=91, batch=20, test loss = 0.4885711669921875, test acc = 0.8450000286102295, time = 0.33838343620300293\n",
      "Testing at step=91, batch=25, test loss = 0.34559738636016846, test acc = 0.8949999809265137, time = 0.34601306915283203\n",
      "Testing at step=91, batch=30, test loss = 0.389566570520401, test acc = 0.8700000047683716, time = 0.3370249271392822\n",
      "Testing at step=91, batch=35, test loss = 0.3329462707042694, test acc = 0.8999999761581421, time = 0.33925938606262207\n",
      "Testing at step=91, batch=40, test loss = 0.47245919704437256, test acc = 0.8899999856948853, time = 0.3419921398162842\n",
      "Testing at step=91, batch=45, test loss = 0.21654671430587769, test acc = 0.8899999856948853, time = 0.34972357749938965\n",
      "Step 91 finished in 310.33938574790955, Train loss = 0.24073122471570968, Test loss = 0.3522971972823143; Train Acc = 0.9107333346207936, Test Acc = 0.8825999987125397\n",
      "Training at step=92, batch=0, train loss = 0.2660902440547943, train acc = 0.8949999809265137, time = 0.939039945602417\n",
      "Training at step=92, batch=30, train loss = 0.35084277391433716, train acc = 0.8999999761581421, time = 0.9435427188873291\n",
      "Training at step=92, batch=60, train loss = 0.22182446718215942, train acc = 0.9049999713897705, time = 0.9355154037475586\n",
      "Training at step=92, batch=90, train loss = 0.18950939178466797, train acc = 0.9150000214576721, time = 0.9696750640869141\n",
      "Training at step=92, batch=120, train loss = 0.2595823407173157, train acc = 0.9150000214576721, time = 0.9386041164398193\n",
      "Training at step=92, batch=150, train loss = 0.2323351800441742, train acc = 0.8999999761581421, time = 0.9419138431549072\n",
      "Training at step=92, batch=180, train loss = 0.17427560687065125, train acc = 0.9350000023841858, time = 0.9438967704772949\n",
      "Training at step=92, batch=210, train loss = 0.1749298870563507, train acc = 0.925000011920929, time = 0.9357161521911621\n",
      "Training at step=92, batch=240, train loss = 0.1587277054786682, train acc = 0.949999988079071, time = 0.9587218761444092\n",
      "Training at step=92, batch=270, train loss = 0.18612192571163177, train acc = 0.9350000023841858, time = 0.9346904754638672\n",
      "Testing at step=92, batch=0, test loss = 0.4018312692642212, test acc = 0.8600000143051147, time = 0.41913485527038574\n",
      "Testing at step=92, batch=5, test loss = 0.4503623843193054, test acc = 0.8550000190734863, time = 0.3392045497894287\n",
      "Testing at step=92, batch=10, test loss = 0.28119394183158875, test acc = 0.8949999809265137, time = 0.37328553199768066\n",
      "Testing at step=92, batch=15, test loss = 0.3297896087169647, test acc = 0.8999999761581421, time = 0.3415036201477051\n",
      "Testing at step=92, batch=20, test loss = 0.35698533058166504, test acc = 0.8650000095367432, time = 0.3398888111114502\n",
      "Testing at step=92, batch=25, test loss = 0.28547203540802, test acc = 0.8999999761581421, time = 0.33739328384399414\n",
      "Testing at step=92, batch=30, test loss = 0.40015289187431335, test acc = 0.8600000143051147, time = 0.3382413387298584\n",
      "Testing at step=92, batch=35, test loss = 0.4146597981452942, test acc = 0.8450000286102295, time = 0.33996081352233887\n",
      "Testing at step=92, batch=40, test loss = 0.3758390545845032, test acc = 0.8849999904632568, time = 0.362246036529541\n",
      "Testing at step=92, batch=45, test loss = 0.2591478228569031, test acc = 0.9049999713897705, time = 0.3405799865722656\n",
      "Step 92 finished in 310.6661858558655, Train loss = 0.23827380046248436, Test loss = 0.35691183656454084; Train Acc = 0.9114166649182638, Test Acc = 0.8767999970912933\n",
      "Training at step=93, batch=0, train loss = 0.203227698802948, train acc = 0.9150000214576721, time = 0.9344744682312012\n",
      "Training at step=93, batch=30, train loss = 0.22213679552078247, train acc = 0.8999999761581421, time = 1.0262532234191895\n",
      "Training at step=93, batch=60, train loss = 0.22615253925323486, train acc = 0.9200000166893005, time = 0.9378538131713867\n",
      "Training at step=93, batch=90, train loss = 0.29759323596954346, train acc = 0.9150000214576721, time = 0.9372358322143555\n",
      "Training at step=93, batch=120, train loss = 0.23329603672027588, train acc = 0.9300000071525574, time = 0.9356005191802979\n",
      "Training at step=93, batch=150, train loss = 0.27458029985427856, train acc = 0.8999999761581421, time = 0.936119556427002\n",
      "Training at step=93, batch=180, train loss = 0.24198368191719055, train acc = 0.8899999856948853, time = 0.9429745674133301\n",
      "Training at step=93, batch=210, train loss = 0.2255125492811203, train acc = 0.8949999809265137, time = 0.9531700611114502\n",
      "Training at step=93, batch=240, train loss = 0.2549844980239868, train acc = 0.9150000214576721, time = 0.9399874210357666\n",
      "Training at step=93, batch=270, train loss = 0.28348010778427124, train acc = 0.9049999713897705, time = 0.9397096633911133\n",
      "Testing at step=93, batch=0, test loss = 0.48917779326438904, test acc = 0.8299999833106995, time = 0.34036946296691895\n",
      "Testing at step=93, batch=5, test loss = 0.32181987166404724, test acc = 0.8600000143051147, time = 0.33742427825927734\n",
      "Testing at step=93, batch=10, test loss = 0.4045301079750061, test acc = 0.8550000190734863, time = 0.3421757221221924\n",
      "Testing at step=93, batch=15, test loss = 0.518164336681366, test acc = 0.8399999737739563, time = 0.33959197998046875\n",
      "Testing at step=93, batch=20, test loss = 0.4623798429965973, test acc = 0.875, time = 0.33710670471191406\n",
      "Testing at step=93, batch=25, test loss = 0.3421962857246399, test acc = 0.8600000143051147, time = 0.34860920906066895\n",
      "Testing at step=93, batch=30, test loss = 0.6090681552886963, test acc = 0.8399999737739563, time = 0.3367340564727783\n",
      "Testing at step=93, batch=35, test loss = 0.26141464710235596, test acc = 0.9150000214576721, time = 0.3372194766998291\n",
      "Testing at step=93, batch=40, test loss = 0.2476959377527237, test acc = 0.8999999761581421, time = 0.336367130279541\n",
      "Testing at step=93, batch=45, test loss = 0.3579227030277252, test acc = 0.8849999904632568, time = 0.33760809898376465\n",
      "Step 93 finished in 310.16613578796387, Train loss = 0.2390045581261317, Test loss = 0.35268562465906145; Train Acc = 0.9120500018199285, Test Acc = 0.8789999997615814\n",
      "Training at step=94, batch=0, train loss = 0.15560382604599, train acc = 0.949999988079071, time = 0.9377880096435547\n",
      "Training at step=94, batch=30, train loss = 0.16953207552433014, train acc = 0.9399999976158142, time = 0.9424471855163574\n",
      "Training at step=94, batch=60, train loss = 0.17754745483398438, train acc = 0.9350000023841858, time = 0.9370238780975342\n",
      "Training at step=94, batch=90, train loss = 0.20618243515491486, train acc = 0.9300000071525574, time = 0.9418573379516602\n",
      "Training at step=94, batch=120, train loss = 0.25668272376060486, train acc = 0.8999999761581421, time = 1.0024056434631348\n",
      "Training at step=94, batch=150, train loss = 0.22370734810829163, train acc = 0.925000011920929, time = 0.9533579349517822\n",
      "Training at step=94, batch=180, train loss = 0.2111624777317047, train acc = 0.8999999761581421, time = 0.9348220825195312\n",
      "Training at step=94, batch=210, train loss = 0.3032209873199463, train acc = 0.8500000238418579, time = 0.9445157051086426\n",
      "Training at step=94, batch=240, train loss = 0.2308143824338913, train acc = 0.9200000166893005, time = 0.950531005859375\n",
      "Training at step=94, batch=270, train loss = 0.2629837095737457, train acc = 0.8999999761581421, time = 0.9383111000061035\n",
      "Testing at step=94, batch=0, test loss = 0.3093852698802948, test acc = 0.8949999809265137, time = 0.3372201919555664\n",
      "Testing at step=94, batch=5, test loss = 0.29755499958992004, test acc = 0.8949999809265137, time = 0.3355276584625244\n",
      "Testing at step=94, batch=10, test loss = 0.43577539920806885, test acc = 0.8500000238418579, time = 0.33699607849121094\n",
      "Testing at step=94, batch=15, test loss = 0.3135809600353241, test acc = 0.8849999904632568, time = 0.3384580612182617\n",
      "Testing at step=94, batch=20, test loss = 0.3984452188014984, test acc = 0.8450000286102295, time = 0.3506181240081787\n",
      "Testing at step=94, batch=25, test loss = 0.35164394974708557, test acc = 0.8700000047683716, time = 0.33837151527404785\n",
      "Testing at step=94, batch=30, test loss = 0.40274548530578613, test acc = 0.875, time = 0.33919548988342285\n",
      "Testing at step=94, batch=35, test loss = 0.2638920843601227, test acc = 0.8849999904632568, time = 0.33945775032043457\n",
      "Testing at step=94, batch=40, test loss = 0.28719040751457214, test acc = 0.8949999809265137, time = 0.3388798236846924\n",
      "Testing at step=94, batch=45, test loss = 0.26270875334739685, test acc = 0.9100000262260437, time = 0.3383641242980957\n",
      "Step 94 finished in 311.542995929718, Train loss = 0.23691640953222912, Test loss = 0.3505648168921471; Train Acc = 0.9121500015258789, Test Acc = 0.8808999991416931\n",
      "Training at step=95, batch=0, train loss = 0.14497758448123932, train acc = 0.9549999833106995, time = 0.939873456954956\n",
      "Training at step=95, batch=30, train loss = 0.22532856464385986, train acc = 0.8899999856948853, time = 0.9549715518951416\n",
      "Training at step=95, batch=60, train loss = 0.28987425565719604, train acc = 0.9150000214576721, time = 0.9497432708740234\n",
      "Training at step=95, batch=90, train loss = 0.21498937904834747, train acc = 0.9300000071525574, time = 0.9385232925415039\n",
      "Training at step=95, batch=120, train loss = 0.1779676228761673, train acc = 0.9300000071525574, time = 0.937065839767456\n",
      "Training at step=95, batch=150, train loss = 0.18603667616844177, train acc = 0.9449999928474426, time = 0.9489274024963379\n",
      "Training at step=95, batch=180, train loss = 0.21307064592838287, train acc = 0.9350000023841858, time = 0.9596023559570312\n",
      "Training at step=95, batch=210, train loss = 0.18694017827510834, train acc = 0.9200000166893005, time = 0.940859317779541\n",
      "Training at step=95, batch=240, train loss = 0.2483329176902771, train acc = 0.925000011920929, time = 0.9351508617401123\n",
      "Training at step=95, batch=270, train loss = 0.1981310248374939, train acc = 0.9350000023841858, time = 0.9332480430603027\n",
      "Testing at step=95, batch=0, test loss = 0.4236900210380554, test acc = 0.8700000047683716, time = 0.4149479866027832\n",
      "Testing at step=95, batch=5, test loss = 0.21989460289478302, test acc = 0.9049999713897705, time = 0.3382718563079834\n",
      "Testing at step=95, batch=10, test loss = 0.46464332938194275, test acc = 0.875, time = 0.3402833938598633\n",
      "Testing at step=95, batch=15, test loss = 0.3699222803115845, test acc = 0.8550000190734863, time = 0.34215879440307617\n",
      "Testing at step=95, batch=20, test loss = 0.4658265709877014, test acc = 0.8500000238418579, time = 0.33912110328674316\n",
      "Testing at step=95, batch=25, test loss = 0.34132203459739685, test acc = 0.8799999952316284, time = 0.34075474739074707\n",
      "Testing at step=95, batch=30, test loss = 0.2763119041919708, test acc = 0.8999999761581421, time = 0.34340667724609375\n",
      "Testing at step=95, batch=35, test loss = 0.3685642182826996, test acc = 0.8700000047683716, time = 0.34485387802124023\n",
      "Testing at step=95, batch=40, test loss = 0.4105442464351654, test acc = 0.8550000190734863, time = 0.3438591957092285\n",
      "Testing at step=95, batch=45, test loss = 0.35526737570762634, test acc = 0.8500000238418579, time = 0.3442533016204834\n",
      "Step 95 finished in 310.8607771396637, Train loss = 0.23555160894989968, Test loss = 0.3648610219359398; Train Acc = 0.9139166686932246, Test Acc = 0.8792999970912934\n",
      "Training at step=96, batch=0, train loss = 0.20431111752986908, train acc = 0.949999988079071, time = 0.9401350021362305\n",
      "Training at step=96, batch=30, train loss = 0.21602009236812592, train acc = 0.925000011920929, time = 0.9381225109100342\n",
      "Training at step=96, batch=60, train loss = 0.3059298098087311, train acc = 0.9150000214576721, time = 0.9413609504699707\n",
      "Training at step=96, batch=90, train loss = 0.15745753049850464, train acc = 0.9350000023841858, time = 0.9410891532897949\n",
      "Training at step=96, batch=120, train loss = 0.2740265727043152, train acc = 0.8999999761581421, time = 0.939265251159668\n",
      "Training at step=96, batch=150, train loss = 0.23245204985141754, train acc = 0.9200000166893005, time = 0.9379870891571045\n",
      "Training at step=96, batch=180, train loss = 0.26877841353416443, train acc = 0.8700000047683716, time = 0.9382803440093994\n",
      "Training at step=96, batch=210, train loss = 0.23184700310230255, train acc = 0.9150000214576721, time = 0.942624568939209\n",
      "Training at step=96, batch=240, train loss = 0.30109912157058716, train acc = 0.8799999952316284, time = 0.9374608993530273\n",
      "Training at step=96, batch=270, train loss = 0.23569753766059875, train acc = 0.9100000262260437, time = 0.9637987613677979\n",
      "Testing at step=96, batch=0, test loss = 0.35404032468795776, test acc = 0.8899999856948853, time = 0.3422670364379883\n",
      "Testing at step=96, batch=5, test loss = 0.3163052201271057, test acc = 0.8899999856948853, time = 0.3426482677459717\n",
      "Testing at step=96, batch=10, test loss = 0.36830997467041016, test acc = 0.8700000047683716, time = 0.34294986724853516\n",
      "Testing at step=96, batch=15, test loss = 0.4042350649833679, test acc = 0.8700000047683716, time = 0.34117817878723145\n",
      "Testing at step=96, batch=20, test loss = 0.37766820192337036, test acc = 0.8899999856948853, time = 0.34506654739379883\n",
      "Testing at step=96, batch=25, test loss = 0.2677161395549774, test acc = 0.8999999761581421, time = 0.34158921241760254\n",
      "Testing at step=96, batch=30, test loss = 0.2229139506816864, test acc = 0.9150000214576721, time = 0.3427855968475342\n",
      "Testing at step=96, batch=35, test loss = 0.35405048727989197, test acc = 0.8600000143051147, time = 0.34946680068969727\n",
      "Testing at step=96, batch=40, test loss = 0.39524033665657043, test acc = 0.8550000190734863, time = 0.3466684818267822\n",
      "Testing at step=96, batch=45, test loss = 0.37211140990257263, test acc = 0.8650000095367432, time = 0.3395884037017822\n",
      "Step 96 finished in 310.49611377716064, Train loss = 0.23586914052565894, Test loss = 0.3636771947145462; Train Acc = 0.912483334938685, Test Acc = 0.8793999993801117\n",
      "Training at step=97, batch=0, train loss = 0.25170329213142395, train acc = 0.9049999713897705, time = 0.9355614185333252\n",
      "Training at step=97, batch=30, train loss = 0.277018666267395, train acc = 0.9049999713897705, time = 0.9660525321960449\n",
      "Training at step=97, batch=60, train loss = 0.3174974322319031, train acc = 0.8849999904632568, time = 0.9400856494903564\n",
      "Training at step=97, batch=90, train loss = 0.3098585605621338, train acc = 0.8700000047683716, time = 0.9404442310333252\n",
      "Training at step=97, batch=120, train loss = 0.2838340401649475, train acc = 0.8999999761581421, time = 0.9547877311706543\n",
      "Training at step=97, batch=150, train loss = 0.2876793146133423, train acc = 0.875, time = 0.9458181858062744\n",
      "Training at step=97, batch=180, train loss = 0.24086874723434448, train acc = 0.9100000262260437, time = 0.9443657398223877\n",
      "Training at step=97, batch=210, train loss = 0.26078876852989197, train acc = 0.8999999761581421, time = 0.9397821426391602\n",
      "Training at step=97, batch=240, train loss = 0.34104758501052856, train acc = 0.8899999856948853, time = 0.9467124938964844\n",
      "Training at step=97, batch=270, train loss = 0.2864404320716858, train acc = 0.9100000262260437, time = 0.9399385452270508\n",
      "Testing at step=97, batch=0, test loss = 0.32741454243659973, test acc = 0.8799999952316284, time = 0.3370680809020996\n",
      "Testing at step=97, batch=5, test loss = 0.4387282431125641, test acc = 0.8050000071525574, time = 0.33753204345703125\n",
      "Testing at step=97, batch=10, test loss = 0.30535611510276794, test acc = 0.875, time = 0.3372206687927246\n",
      "Testing at step=97, batch=15, test loss = 0.3519315719604492, test acc = 0.8949999809265137, time = 0.3396918773651123\n",
      "Testing at step=97, batch=20, test loss = 0.44153153896331787, test acc = 0.8550000190734863, time = 0.33875441551208496\n",
      "Testing at step=97, batch=25, test loss = 0.3145574629306793, test acc = 0.875, time = 0.3395531177520752\n",
      "Testing at step=97, batch=30, test loss = 0.30455872416496277, test acc = 0.9150000214576721, time = 0.33950042724609375\n",
      "Testing at step=97, batch=35, test loss = 0.3789072036743164, test acc = 0.8550000190734863, time = 0.33846068382263184\n",
      "Testing at step=97, batch=40, test loss = 0.3026924729347229, test acc = 0.875, time = 0.3440287113189697\n",
      "Testing at step=97, batch=45, test loss = 0.32949259877204895, test acc = 0.9049999713897705, time = 0.3398432731628418\n",
      "Step 97 finished in 310.5996780395508, Train loss = 0.23448260359466075, Test loss = 0.3681489700078964; Train Acc = 0.9130999998251597, Test Acc = 0.8755999970436096\n",
      "Training at step=98, batch=0, train loss = 0.21418802440166473, train acc = 0.8999999761581421, time = 0.9378275871276855\n",
      "Training at step=98, batch=30, train loss = 0.20638227462768555, train acc = 0.9200000166893005, time = 0.9399383068084717\n",
      "Training at step=98, batch=60, train loss = 0.2072332203388214, train acc = 0.9200000166893005, time = 0.9477412700653076\n",
      "Training at step=98, batch=90, train loss = 0.2040288746356964, train acc = 0.9150000214576721, time = 0.941556453704834\n",
      "Training at step=98, batch=120, train loss = 0.1878395974636078, train acc = 0.9599999785423279, time = 0.9410734176635742\n",
      "Training at step=98, batch=150, train loss = 0.1780332326889038, train acc = 0.949999988079071, time = 0.9398653507232666\n",
      "Training at step=98, batch=180, train loss = 0.23209109902381897, train acc = 0.8849999904632568, time = 0.940079927444458\n",
      "Training at step=98, batch=210, train loss = 0.3486264944076538, train acc = 0.8600000143051147, time = 0.9402844905853271\n",
      "Training at step=98, batch=240, train loss = 0.16383346915245056, train acc = 0.9549999833106995, time = 0.9429628849029541\n",
      "Training at step=98, batch=270, train loss = 0.23864568769931793, train acc = 0.8999999761581421, time = 0.9409902095794678\n",
      "Testing at step=98, batch=0, test loss = 0.4909977614879608, test acc = 0.8450000286102295, time = 0.3599841594696045\n",
      "Testing at step=98, batch=5, test loss = 0.26406821608543396, test acc = 0.9100000262260437, time = 0.34626221656799316\n",
      "Testing at step=98, batch=10, test loss = 0.36118006706237793, test acc = 0.9049999713897705, time = 0.3404388427734375\n",
      "Testing at step=98, batch=15, test loss = 0.3850418031215668, test acc = 0.875, time = 0.34560561180114746\n",
      "Testing at step=98, batch=20, test loss = 0.2794657051563263, test acc = 0.9150000214576721, time = 0.3450441360473633\n",
      "Testing at step=98, batch=25, test loss = 0.42543333768844604, test acc = 0.8650000095367432, time = 0.341693639755249\n",
      "Testing at step=98, batch=30, test loss = 0.3401547968387604, test acc = 0.8949999809265137, time = 0.34329652786254883\n",
      "Testing at step=98, batch=35, test loss = 0.4959146976470947, test acc = 0.8399999737739563, time = 0.3452029228210449\n",
      "Testing at step=98, batch=40, test loss = 0.36878377199172974, test acc = 0.8399999737739563, time = 0.3637504577636719\n",
      "Testing at step=98, batch=45, test loss = 0.3406691253185272, test acc = 0.9049999713897705, time = 0.36486387252807617\n",
      "Step 98 finished in 310.6108639240265, Train loss = 0.23333561822772025, Test loss = 0.36288513779640197; Train Acc = 0.912900000611941, Test Acc = 0.8795999968051911\n",
      "Training at step=99, batch=0, train loss = 0.24835357069969177, train acc = 0.9150000214576721, time = 1.0466759204864502\n",
      "Training at step=99, batch=30, train loss = 0.2503511309623718, train acc = 0.9150000214576721, time = 0.9391486644744873\n",
      "Training at step=99, batch=60, train loss = 0.24201013147830963, train acc = 0.925000011920929, time = 0.9436964988708496\n",
      "Training at step=99, batch=90, train loss = 0.22617307305335999, train acc = 0.8999999761581421, time = 0.9378800392150879\n",
      "Training at step=99, batch=120, train loss = 0.18300995230674744, train acc = 0.925000011920929, time = 0.941270112991333\n",
      "Training at step=99, batch=150, train loss = 0.2441725730895996, train acc = 0.8849999904632568, time = 0.970139741897583\n",
      "Training at step=99, batch=180, train loss = 0.1731211543083191, train acc = 0.9449999928474426, time = 0.9404950141906738\n",
      "Training at step=99, batch=210, train loss = 0.2491137534379959, train acc = 0.8999999761581421, time = 0.9389448165893555\n",
      "Training at step=99, batch=240, train loss = 0.24053817987442017, train acc = 0.9200000166893005, time = 0.9399716854095459\n",
      "Training at step=99, batch=270, train loss = 0.23889243602752686, train acc = 0.9150000214576721, time = 0.9387004375457764\n",
      "Testing at step=99, batch=0, test loss = 0.36552155017852783, test acc = 0.8349999785423279, time = 0.33854198455810547\n",
      "Testing at step=99, batch=5, test loss = 0.2652474343776703, test acc = 0.9200000166893005, time = 0.34041357040405273\n",
      "Testing at step=99, batch=10, test loss = 0.30840763449668884, test acc = 0.8949999809265137, time = 0.3382754325866699\n",
      "Testing at step=99, batch=15, test loss = 0.27936550974845886, test acc = 0.8999999761581421, time = 0.3399660587310791\n",
      "Testing at step=99, batch=20, test loss = 0.3177761435508728, test acc = 0.8600000143051147, time = 0.33812403678894043\n",
      "Testing at step=99, batch=25, test loss = 0.3478063941001892, test acc = 0.8600000143051147, time = 0.3375670909881592\n",
      "Testing at step=99, batch=30, test loss = 0.33773380517959595, test acc = 0.875, time = 0.3371241092681885\n",
      "Testing at step=99, batch=35, test loss = 0.34990638494491577, test acc = 0.875, time = 0.33657050132751465\n",
      "Testing at step=99, batch=40, test loss = 0.4488687813282013, test acc = 0.824999988079071, time = 0.33667492866516113\n",
      "Testing at step=99, batch=45, test loss = 0.37878403067588806, test acc = 0.8700000047683716, time = 0.33899879455566406\n",
      "Step 99 finished in 310.8912136554718, Train loss = 0.2335457620024681, Test loss = 0.3566420865058899; Train Acc = 0.9138500010967254, Test Acc = 0.8762999987602234\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T17:06:02.381493Z",
     "iopub.status.busy": "2024-04-03T17:06:02.381278Z",
     "iopub.status.idle": "2024-04-03T17:06:02.758413Z",
     "shell.execute_reply": "2024-04-03T17:06:02.757799Z",
     "shell.execute_reply.started": "2024-04-03T17:06:02.381473Z"
    },
    "id": "U0Q0vFm7B6cg",
    "ExecuteTime": {
     "end_time": "2024-04-08T07:37:47.020472Z",
     "start_time": "2024-04-08T07:37:46.462136Z"
    }
   },
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2-fashionmnist_full_repacement.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T07:37:47.027476Z",
     "start_time": "2024-04-08T07:37:47.022098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"2-fashionmnist_full_replacement.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ],
   "outputs": [],
   "execution_count": 16
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
