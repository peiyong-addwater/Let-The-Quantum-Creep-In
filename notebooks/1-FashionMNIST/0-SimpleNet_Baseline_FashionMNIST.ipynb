{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:15:36.030435Z",
     "start_time": "2024-04-06T20:13:41.133148Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 618301,
     "status": "ok",
     "timestamp": 1712091541763,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "acHJk38WrBOi",
    "outputId": "ba1ec555-f0d0-45fd-df87-2f537a902cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: pennylane in /usr/local/lib/python3.9/dist-packages (0.35.1)\n",
      "Requirement already satisfied: cotengra in /usr/local/lib/python3.9/dist-packages (0.5.6)\n",
      "Requirement already satisfied: quimb in /usr/local/lib/python3.9/dist-packages (1.7.3)\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (1.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.10.0)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\n",
      "Requirement already satisfied: pennylane-lightning>=0.35 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.35.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\n",
      "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.6.9)\n",
      "Requirement already satisfied: autograd in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.6.2)\n",
      "Requirement already satisfied: rustworkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.14.2)\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\n",
      "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\n",
      "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.59.1)\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.12.3)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (0.11.2)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.39->quimb) (0.42.0)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio pennylane cotengra quimb torchmetrics --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:15:40.761798Z",
     "start_time": "2024-04-06T20:15:36.031982Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8020,
     "status": "ok",
     "timestamp": 1712091549780,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "FB_BuVzCrMNk",
    "outputId": "235fd337-f4c4-4ba7-9547-7b96570eac2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "\n",
    "import torch \n",
    "import torchvision \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "#REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:15:48.999706Z",
     "start_time": "2024-04-06T20:15:40.763128Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5513,
     "status": "ok",
     "timestamp": 1712091555290,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "K7wJnOiEralR",
    "outputId": "a21dab31-4609-4119-cbdd-0b84f74ac35f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n",
      "tensor([4, 1, 7, 3, 5, 8, 2, 6, 4, 6, 6, 3, 6, 9, 9, 7, 8, 7, 7, 7, 2, 7, 3, 8,\n",
      "        8, 8, 2, 2, 8, 2, 1, 5, 8, 7, 8, 2, 1, 4, 8, 7, 0, 0, 8, 9, 7, 5, 3, 6,\n",
      "        9, 3, 9, 8, 7, 1, 8, 8, 0, 7, 3, 0, 6, 4, 5, 8])\n",
      "tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.7176,\n",
      "         0.5686,  0.8588,  0.2235,  0.0902,  0.9059,  0.5529,  0.6000,  0.6078,\n",
      "         0.6392,  0.6314,  0.6157,  0.8039,  0.4039, -0.0039,  0.9922,  0.6314,\n",
      "         0.7490, -0.6235, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000])\n"
     ]
    }
   ],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:15:52.939979Z",
     "start_time": "2024-04-06T20:15:49.001868Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1712091555780,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "t6wDzOaJrmZ4",
    "outputId": "e216a519-beac-46f4-de29-cd0e9e280630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "SimpleNet(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=12544, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class SimpleNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SimpleNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=3),\n",
    "        torch.nn.Conv2d(32, 16, kernel_size=3),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(16*28*28, 10),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "net = SimpleNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:43:06.239753Z",
     "start_time": "2024-04-06T20:15:52.942021Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2289206,
     "status": "ok",
     "timestamp": 1712093844984,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "PDNHjKy4sSWZ",
    "outputId": "a0ef5fa2-455a-4017-d32f-bc0e38c7cdab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 300, Number of test batches = 50\n",
      "Print every train batch = 60, Print every test batch = 10\n",
      "Training at step=0, batch=0, train loss = 2.316196049109083, train acc = 0.09000000357627869, time = 0.18616652488708496\n",
      "Training at step=0, batch=60, train loss = 1.007979989933517, train acc = 0.7099999785423279, time = 0.015845060348510742\n",
      "Training at step=0, batch=120, train loss = 0.8442076010983364, train acc = 0.7049999833106995, time = 0.013129234313964844\n",
      "Training at step=0, batch=180, train loss = 0.5252823933986468, train acc = 0.8349999785423279, time = 0.013153314590454102\n",
      "Training at step=0, batch=240, train loss = 0.5198326100893944, train acc = 0.8299999833106995, time = 0.013137340545654297\n",
      "Testing at step=0, batch=0, test loss = 0.6626033837997689, test acc = 0.7549999952316284, time = 0.0024709701538085938\n",
      "Testing at step=0, batch=10, test loss = 0.6499353674216475, test acc = 0.7699999809265137, time = 0.002407073974609375\n",
      "Testing at step=0, batch=20, test loss = 0.5350424631408482, test acc = 0.8149999976158142, time = 0.0024034976959228516\n",
      "Testing at step=0, batch=30, test loss = 0.6097784450520648, test acc = 0.7350000143051147, time = 0.002451658248901367\n",
      "Testing at step=0, batch=40, test loss = 0.6520743495316222, test acc = 0.75, time = 0.002416372299194336\n",
      "Step 0 finished in 18.758052349090576, Train loss = 0.8901805386761411, Test loss = 0.5864420027815741; Train Acc = 0.706799997240305, Test Acc = 0.7896999955177307\n",
      "Training at step=1, batch=0, train loss = 0.6003722187235284, train acc = 0.7749999761581421, time = 0.013103485107421875\n",
      "Training at step=1, batch=60, train loss = 0.48955285753474287, train acc = 0.8450000286102295, time = 0.013152360916137695\n",
      "Training at step=1, batch=120, train loss = 0.45973032189954416, train acc = 0.8199999928474426, time = 0.012980222702026367\n",
      "Training at step=1, batch=180, train loss = 0.5104922827865747, train acc = 0.824999988079071, time = 0.012833595275878906\n",
      "Training at step=1, batch=240, train loss = 0.45434869195889926, train acc = 0.8349999785423279, time = 0.012965917587280273\n",
      "Testing at step=1, batch=0, test loss = 0.4740184524678859, test acc = 0.8550000190734863, time = 0.00247955322265625\n",
      "Testing at step=1, batch=10, test loss = 0.5353107500627251, test acc = 0.8149999976158142, time = 0.002442598342895508\n",
      "Testing at step=1, batch=20, test loss = 0.5238626854422662, test acc = 0.7850000262260437, time = 0.002416372299194336\n",
      "Testing at step=1, batch=30, test loss = 0.5192976288258511, test acc = 0.800000011920929, time = 0.0023822784423828125\n",
      "Testing at step=1, batch=40, test loss = 0.4452820687036969, test acc = 0.8450000286102295, time = 0.002434968948364258\n",
      "Step 1 finished in 13.757081985473633, Train loss = 0.5344414781971834, Test loss = 0.5341943344665332; Train Acc = 0.8118166679143906, Test Acc = 0.8098000001907348\n",
      "Training at step=2, batch=0, train loss = 0.49234585826598204, train acc = 0.8199999928474426, time = 0.013003349304199219\n",
      "Training at step=2, batch=60, train loss = 0.5405339297448348, train acc = 0.7799999713897705, time = 0.01288747787475586\n",
      "Training at step=2, batch=120, train loss = 0.4706029051958214, train acc = 0.8450000286102295, time = 0.013037919998168945\n",
      "Training at step=2, batch=180, train loss = 0.563408784088876, train acc = 0.8149999976158142, time = 0.013300180435180664\n",
      "Training at step=2, batch=240, train loss = 0.4574660160258422, train acc = 0.8550000190734863, time = 0.013008356094360352\n",
      "Testing at step=2, batch=0, test loss = 0.4829876266951229, test acc = 0.824999988079071, time = 0.002494335174560547\n",
      "Testing at step=2, batch=10, test loss = 0.4640108426880735, test acc = 0.800000011920929, time = 0.0024247169494628906\n",
      "Testing at step=2, batch=20, test loss = 0.5014267012745931, test acc = 0.8100000023841858, time = 0.0024123191833496094\n",
      "Testing at step=2, batch=30, test loss = 0.47516955733089156, test acc = 0.8299999833106995, time = 0.002428293228149414\n",
      "Testing at step=2, batch=40, test loss = 0.5483330809926629, test acc = 0.824999988079071, time = 0.00251007080078125\n",
      "Step 2 finished in 14.122387886047363, Train loss = 0.494585759206567, Test loss = 0.514898437628743; Train Acc = 0.8265000001589458, Test Acc = 0.8150999975204468\n",
      "Training at step=3, batch=0, train loss = 0.45136757486701595, train acc = 0.824999988079071, time = 0.013210773468017578\n",
      "Training at step=3, batch=60, train loss = 0.4331629511024823, train acc = 0.8399999737739563, time = 0.012857675552368164\n",
      "Training at step=3, batch=120, train loss = 0.6288604042082792, train acc = 0.824999988079071, time = 0.01311182975769043\n",
      "Training at step=3, batch=180, train loss = 0.43275631202265913, train acc = 0.8600000143051147, time = 0.013044357299804688\n",
      "Training at step=3, batch=240, train loss = 0.44890395134008493, train acc = 0.8399999737739563, time = 0.013057947158813477\n",
      "Testing at step=3, batch=0, test loss = 0.5285569867579274, test acc = 0.8050000071525574, time = 0.002424478530883789\n",
      "Testing at step=3, batch=10, test loss = 0.49309498236398697, test acc = 0.8199999928474426, time = 0.0024733543395996094\n",
      "Testing at step=3, batch=20, test loss = 0.5025752211262184, test acc = 0.824999988079071, time = 0.0024976730346679688\n",
      "Testing at step=3, batch=30, test loss = 0.4928389561757241, test acc = 0.8349999785423279, time = 0.002431631088256836\n",
      "Testing at step=3, batch=40, test loss = 0.4609093672560774, test acc = 0.8399999737739563, time = 0.0025243759155273438\n",
      "Step 3 finished in 14.267914533615112, Train loss = 0.473536693544573, Test loss = 0.4926042837319763; Train Acc = 0.8343499994277954, Test Acc = 0.8214999973773957\n",
      "Training at step=4, batch=0, train loss = 0.47031364447212326, train acc = 0.800000011920929, time = 0.01337575912475586\n",
      "Training at step=4, batch=60, train loss = 0.5026964227020688, train acc = 0.8199999928474426, time = 0.013077020645141602\n",
      "Training at step=4, batch=120, train loss = 0.4686494755860909, train acc = 0.8299999833106995, time = 0.012964963912963867\n",
      "Training at step=4, batch=180, train loss = 0.3947815705230076, train acc = 0.8450000286102295, time = 0.013321399688720703\n",
      "Training at step=4, batch=240, train loss = 0.3389778045369392, train acc = 0.8899999856948853, time = 0.013204813003540039\n",
      "Testing at step=4, batch=0, test loss = 0.38263078674908974, test acc = 0.875, time = 0.0023636817932128906\n",
      "Testing at step=4, batch=10, test loss = 0.45579286453097995, test acc = 0.8500000238418579, time = 0.0023462772369384766\n",
      "Testing at step=4, batch=20, test loss = 0.4674401164927531, test acc = 0.8399999737739563, time = 0.0023093223571777344\n",
      "Testing at step=4, batch=30, test loss = 0.5007929919743076, test acc = 0.8149999976158142, time = 0.0022804737091064453\n",
      "Testing at step=4, batch=40, test loss = 0.4942094526265869, test acc = 0.8349999785423279, time = 0.002263307571411133\n",
      "Step 4 finished in 14.331868171691895, Train loss = 0.46025129112557095, Test loss = 0.48490278882559223; Train Acc = 0.8410833354791005, Test Acc = 0.8291999971866608\n",
      "Training at step=5, batch=0, train loss = 0.44803346458742155, train acc = 0.8600000143051147, time = 0.013041019439697266\n",
      "Training at step=5, batch=60, train loss = 0.5933624643478789, train acc = 0.7599999904632568, time = 0.013065099716186523\n",
      "Training at step=5, batch=120, train loss = 0.4401650747615499, train acc = 0.8550000190734863, time = 0.013026237487792969\n",
      "Training at step=5, batch=180, train loss = 0.474384294540163, train acc = 0.8100000023841858, time = 0.012967348098754883\n",
      "Training at step=5, batch=240, train loss = 0.4165522320921312, train acc = 0.8299999833106995, time = 0.013081073760986328\n",
      "Testing at step=5, batch=0, test loss = 0.398310999480952, test acc = 0.8600000143051147, time = 0.0024683475494384766\n",
      "Testing at step=5, batch=10, test loss = 0.5537237484935784, test acc = 0.800000011920929, time = 0.002435445785522461\n",
      "Testing at step=5, batch=20, test loss = 0.3700133603324352, test acc = 0.8650000095367432, time = 0.002392292022705078\n",
      "Testing at step=5, batch=30, test loss = 0.4936453458743121, test acc = 0.8450000286102295, time = 0.002338409423828125\n",
      "Testing at step=5, batch=40, test loss = 0.4782561465037747, test acc = 0.824999988079071, time = 0.0024077892303466797\n",
      "Step 5 finished in 13.815018892288208, Train loss = 0.4527790859950089, Test loss = 0.49099687713243206; Train Acc = 0.8431666664282481, Test Acc = 0.8248000001907348\n",
      "Training at step=6, batch=0, train loss = 0.46226330938862026, train acc = 0.8149999976158142, time = 0.012964725494384766\n",
      "Training at step=6, batch=60, train loss = 0.4381271641411776, train acc = 0.8399999737739563, time = 0.013302087783813477\n",
      "Training at step=6, batch=120, train loss = 0.4154782836701394, train acc = 0.8650000095367432, time = 0.013191938400268555\n",
      "Training at step=6, batch=180, train loss = 0.4344656926120489, train acc = 0.8700000047683716, time = 0.012937068939208984\n",
      "Training at step=6, batch=240, train loss = 0.43511550267169247, train acc = 0.8399999737739563, time = 0.01299285888671875\n",
      "Testing at step=6, batch=0, test loss = 0.647260051174352, test acc = 0.7799999713897705, time = 0.0024874210357666016\n",
      "Testing at step=6, batch=10, test loss = 0.4305216535490343, test acc = 0.8700000047683716, time = 0.0024726390838623047\n",
      "Testing at step=6, batch=20, test loss = 0.571316414990698, test acc = 0.7900000214576721, time = 0.0024416446685791016\n",
      "Testing at step=6, batch=30, test loss = 0.5645283544078685, test acc = 0.7900000214576721, time = 0.0024101734161376953\n",
      "Testing at step=6, batch=40, test loss = 0.4048845248556506, test acc = 0.8700000047683716, time = 0.002547025680541992\n",
      "Step 6 finished in 13.813477516174316, Train loss = 0.44610984476217586, Test loss = 0.47958929238164577; Train Acc = 0.844633336464564, Test Acc = 0.8314000010490418\n",
      "Training at step=7, batch=0, train loss = 0.5053344684239036, train acc = 0.875, time = 0.013077259063720703\n",
      "Training at step=7, batch=60, train loss = 0.4995536867979945, train acc = 0.8500000238418579, time = 0.012718677520751953\n",
      "Training at step=7, batch=120, train loss = 0.4972078244698936, train acc = 0.8399999737739563, time = 0.013020992279052734\n",
      "Training at step=7, batch=180, train loss = 0.45561248191427856, train acc = 0.8500000238418579, time = 0.01304316520690918\n",
      "Training at step=7, batch=240, train loss = 0.4405979060925786, train acc = 0.8600000143051147, time = 0.012730836868286133\n",
      "Testing at step=7, batch=0, test loss = 0.4458783006229766, test acc = 0.8450000286102295, time = 0.002469778060913086\n",
      "Testing at step=7, batch=10, test loss = 0.41075157353059055, test acc = 0.8550000190734863, time = 0.0024340152740478516\n",
      "Testing at step=7, batch=20, test loss = 0.49717836102807894, test acc = 0.8199999928474426, time = 0.0023910999298095703\n",
      "Testing at step=7, batch=30, test loss = 0.4311008742712082, test acc = 0.8500000238418579, time = 0.002486705780029297\n",
      "Testing at step=7, batch=40, test loss = 0.42825779540289344, test acc = 0.8500000238418579, time = 0.002384185791015625\n",
      "Step 7 finished in 14.004814863204956, Train loss = 0.44128974698437956, Test loss = 0.4767296497645897; Train Acc = 0.8469500011205673, Test Acc = 0.8320000016689301\n",
      "Training at step=8, batch=0, train loss = 0.47003445481942224, train acc = 0.8100000023841858, time = 0.013081073760986328\n",
      "Training at step=8, batch=60, train loss = 0.4349143636363979, train acc = 0.8650000095367432, time = 0.012998819351196289\n",
      "Training at step=8, batch=120, train loss = 0.4204917916048266, train acc = 0.8399999737739563, time = 0.012989521026611328\n",
      "Training at step=8, batch=180, train loss = 0.4266864567828732, train acc = 0.8349999785423279, time = 0.013373374938964844\n",
      "Training at step=8, batch=240, train loss = 0.4746587858127629, train acc = 0.8349999785423279, time = 0.013035058975219727\n",
      "Testing at step=8, batch=0, test loss = 0.38612816895794205, test acc = 0.8450000286102295, time = 0.002374887466430664\n",
      "Testing at step=8, batch=10, test loss = 0.489796413601093, test acc = 0.8500000238418579, time = 0.0022547245025634766\n",
      "Testing at step=8, batch=20, test loss = 0.5868851811756076, test acc = 0.800000011920929, time = 0.002228975296020508\n",
      "Testing at step=8, batch=30, test loss = 0.4916671102269713, test acc = 0.824999988079071, time = 0.002427816390991211\n",
      "Testing at step=8, batch=40, test loss = 0.45920841236225307, test acc = 0.8399999737739563, time = 0.0023572444915771484\n",
      "Step 8 finished in 14.16636037826538, Train loss = 0.4362481644049861, Test loss = 0.4628239776938638; Train Acc = 0.8484500022729238, Test Acc = 0.8362999987602234\n",
      "Training at step=9, batch=0, train loss = 0.3568356757627388, train acc = 0.8550000190734863, time = 0.012958049774169922\n",
      "Training at step=9, batch=60, train loss = 0.29416387643491815, train acc = 0.8899999856948853, time = 0.013044357299804688\n",
      "Training at step=9, batch=120, train loss = 0.34530537393737143, train acc = 0.875, time = 0.013108491897583008\n",
      "Training at step=9, batch=180, train loss = 0.3864888494337223, train acc = 0.8799999952316284, time = 0.013291120529174805\n",
      "Training at step=9, batch=240, train loss = 0.48260729170596506, train acc = 0.824999988079071, time = 0.01301717758178711\n",
      "Testing at step=9, batch=0, test loss = 0.4987842590313614, test acc = 0.8299999833106995, time = 0.0024874210357666016\n",
      "Testing at step=9, batch=10, test loss = 0.4491004093589705, test acc = 0.8450000286102295, time = 0.002481222152709961\n",
      "Testing at step=9, batch=20, test loss = 0.48529648202794445, test acc = 0.8100000023841858, time = 0.0024843215942382812\n",
      "Testing at step=9, batch=30, test loss = 0.4983100593622911, test acc = 0.8100000023841858, time = 0.0023987293243408203\n",
      "Testing at step=9, batch=40, test loss = 0.4882490842858496, test acc = 0.824999988079071, time = 0.002434968948364258\n",
      "Step 9 finished in 14.397163391113281, Train loss = 0.4315827136051479, Test loss = 0.4665559238306778; Train Acc = 0.8500000017881394, Test Acc = 0.8366000020503997\n",
      "Training at step=10, batch=0, train loss = 0.39195598128624376, train acc = 0.8700000047683716, time = 0.013168573379516602\n",
      "Training at step=10, batch=60, train loss = 0.5005047675005152, train acc = 0.824999988079071, time = 0.013187170028686523\n",
      "Training at step=10, batch=120, train loss = 0.4272162615252246, train acc = 0.8650000095367432, time = 0.01296234130859375\n",
      "Training at step=10, batch=180, train loss = 0.5185361403697207, train acc = 0.7799999713897705, time = 0.013284921646118164\n",
      "Training at step=10, batch=240, train loss = 0.4582868136526621, train acc = 0.8450000286102295, time = 0.013034343719482422\n",
      "Testing at step=10, batch=0, test loss = 0.5007448231543706, test acc = 0.8450000286102295, time = 0.002420663833618164\n",
      "Testing at step=10, batch=10, test loss = 0.5726725339194378, test acc = 0.7799999713897705, time = 0.0024051666259765625\n",
      "Testing at step=10, batch=20, test loss = 0.47423287745065656, test acc = 0.8199999928474426, time = 0.0023856163024902344\n",
      "Testing at step=10, batch=30, test loss = 0.3772730344345628, test acc = 0.8650000095367432, time = 0.0023872852325439453\n",
      "Testing at step=10, batch=40, test loss = 0.40023051640495405, test acc = 0.8600000143051147, time = 0.0023632049560546875\n",
      "Step 10 finished in 13.860284090042114, Train loss = 0.4313887310339418, Test loss = 0.464610137638879; Train Acc = 0.8500166656573613, Test Acc = 0.8358000028133392\n",
      "Training at step=11, batch=0, train loss = 0.440708747701281, train acc = 0.8349999785423279, time = 0.013111114501953125\n",
      "Training at step=11, batch=60, train loss = 0.3988670654976022, train acc = 0.8600000143051147, time = 0.013104438781738281\n",
      "Training at step=11, batch=120, train loss = 0.3978894201646652, train acc = 0.8450000286102295, time = 0.01292872428894043\n",
      "Training at step=11, batch=180, train loss = 0.4966877466510718, train acc = 0.8199999928474426, time = 0.012969255447387695\n",
      "Training at step=11, batch=240, train loss = 0.39324976778659676, train acc = 0.875, time = 0.013132810592651367\n",
      "Testing at step=11, batch=0, test loss = 0.4045455211443706, test acc = 0.8399999737739563, time = 0.0023903846740722656\n",
      "Testing at step=11, batch=10, test loss = 0.5040504454064937, test acc = 0.7950000166893005, time = 0.0022933483123779297\n",
      "Testing at step=11, batch=20, test loss = 0.4045380766609464, test acc = 0.8450000286102295, time = 0.0023469924926757812\n",
      "Testing at step=11, batch=30, test loss = 0.4435116246381425, test acc = 0.8650000095367432, time = 0.00391697883605957\n",
      "Testing at step=11, batch=40, test loss = 0.44898810502425335, test acc = 0.8550000190734863, time = 0.0022077560424804688\n",
      "Step 11 finished in 14.582353591918945, Train loss = 0.42629323104199796, Test loss = 0.45605072070956054; Train Acc = 0.8513666677474976, Test Acc = 0.8394000017642975\n",
      "Training at step=12, batch=0, train loss = 0.3870828746310315, train acc = 0.8799999952316284, time = 0.013002872467041016\n",
      "Training at step=12, batch=60, train loss = 0.4750936109695106, train acc = 0.824999988079071, time = 0.012799501419067383\n",
      "Training at step=12, batch=120, train loss = 0.46449737964657706, train acc = 0.8399999737739563, time = 0.013166666030883789\n",
      "Training at step=12, batch=180, train loss = 0.33105424960658253, train acc = 0.875, time = 0.013086557388305664\n",
      "Training at step=12, batch=240, train loss = 0.5512861534908873, train acc = 0.7950000166893005, time = 0.013086795806884766\n",
      "Testing at step=12, batch=0, test loss = 0.437624505891811, test acc = 0.8100000023841858, time = 0.0024161338806152344\n",
      "Testing at step=12, batch=10, test loss = 0.43450780152353763, test acc = 0.8299999833106995, time = 0.0024805068969726562\n",
      "Testing at step=12, batch=20, test loss = 0.40695475648907886, test acc = 0.8450000286102295, time = 0.002306222915649414\n",
      "Testing at step=12, batch=30, test loss = 0.5224006117161061, test acc = 0.8050000071525574, time = 0.0023696422576904297\n",
      "Testing at step=12, batch=40, test loss = 0.38048076374443335, test acc = 0.8650000095367432, time = 0.0024034976959228516\n",
      "Step 12 finished in 13.89388370513916, Train loss = 0.4223349802325179, Test loss = 0.4584866634577889; Train Acc = 0.8547166681289673, Test Acc = 0.8355999982357025\n",
      "Training at step=13, batch=0, train loss = 0.5479356887070934, train acc = 0.8199999928474426, time = 0.012996196746826172\n",
      "Training at step=13, batch=60, train loss = 0.31752171819209957, train acc = 0.8949999809265137, time = 0.013126850128173828\n",
      "Training at step=13, batch=120, train loss = 0.4542899123912838, train acc = 0.8349999785423279, time = 0.013150215148925781\n",
      "Training at step=13, batch=180, train loss = 0.43357740229325364, train acc = 0.8399999737739563, time = 0.013188362121582031\n",
      "Training at step=13, batch=240, train loss = 0.3828908110741333, train acc = 0.8899999856948853, time = 0.013110876083374023\n",
      "Testing at step=13, batch=0, test loss = 0.4292337796163187, test acc = 0.8199999928474426, time = 0.002418041229248047\n",
      "Testing at step=13, batch=10, test loss = 0.5009913170476615, test acc = 0.824999988079071, time = 0.0023970603942871094\n",
      "Testing at step=13, batch=20, test loss = 0.40427391424002124, test acc = 0.8500000238418579, time = 0.002432107925415039\n",
      "Testing at step=13, batch=30, test loss = 0.43109893354099577, test acc = 0.8550000190734863, time = 0.0024738311767578125\n",
      "Testing at step=13, batch=40, test loss = 0.4639331590686947, test acc = 0.8399999737739563, time = 0.002427339553833008\n",
      "Step 13 finished in 14.012001752853394, Train loss = 0.4199268673812478, Test loss = 0.459028046610211; Train Acc = 0.8550333335002264, Test Acc = 0.8383000016212463\n",
      "Training at step=14, batch=0, train loss = 0.4174397510106229, train acc = 0.8650000095367432, time = 0.013146162033081055\n",
      "Training at step=14, batch=60, train loss = 0.2930349113679382, train acc = 0.8849999904632568, time = 0.013134956359863281\n",
      "Training at step=14, batch=120, train loss = 0.4631280555034624, train acc = 0.8399999737739563, time = 0.0129852294921875\n",
      "Training at step=14, batch=180, train loss = 0.47428668862831475, train acc = 0.8500000238418579, time = 0.012896299362182617\n",
      "Training at step=14, batch=240, train loss = 0.36098942615993146, train acc = 0.8949999809265137, time = 0.012965917587280273\n",
      "Testing at step=14, batch=0, test loss = 0.4051278995108008, test acc = 0.8849999904632568, time = 0.0024378299713134766\n",
      "Testing at step=14, batch=10, test loss = 0.34488774250250964, test acc = 0.8799999952316284, time = 0.002417325973510742\n",
      "Testing at step=14, batch=20, test loss = 0.4069572793871519, test acc = 0.8799999952316284, time = 0.0024023056030273438\n",
      "Testing at step=14, batch=30, test loss = 0.4777267414866774, test acc = 0.8299999833106995, time = 0.002404451370239258\n",
      "Testing at step=14, batch=40, test loss = 0.4430397540193222, test acc = 0.8149999976158142, time = 0.002434968948364258\n",
      "Step 14 finished in 14.262972116470337, Train loss = 0.4192895126745476, Test loss = 0.4499892347308528; Train Acc = 0.8552500011523565, Test Acc = 0.8423999977111817\n",
      "Training at step=15, batch=0, train loss = 0.4610821056386586, train acc = 0.8399999737739563, time = 0.013098001480102539\n",
      "Training at step=15, batch=60, train loss = 0.4646710337213649, train acc = 0.8199999928474426, time = 0.012748003005981445\n",
      "Training at step=15, batch=120, train loss = 0.32384790910965866, train acc = 0.8849999904632568, time = 0.013057708740234375\n",
      "Training at step=15, batch=180, train loss = 0.489155828395419, train acc = 0.8399999737739563, time = 0.012978315353393555\n",
      "Training at step=15, batch=240, train loss = 0.30317170477397604, train acc = 0.8849999904632568, time = 0.013419389724731445\n",
      "Testing at step=15, batch=0, test loss = 0.43465787703808517, test acc = 0.8450000286102295, time = 0.0024607181549072266\n",
      "Testing at step=15, batch=10, test loss = 0.5581855751188898, test acc = 0.8349999785423279, time = 0.002437591552734375\n",
      "Testing at step=15, batch=20, test loss = 0.4768726808433163, test acc = 0.8100000023841858, time = 0.0024099349975585938\n",
      "Testing at step=15, batch=30, test loss = 0.4709019431394987, test acc = 0.8399999737739563, time = 0.0023615360260009766\n",
      "Testing at step=15, batch=40, test loss = 0.44312075457583, test acc = 0.8199999928474426, time = 0.002380847930908203\n",
      "Step 15 finished in 13.885988473892212, Train loss = 0.41612323453117206, Test loss = 0.448226624474177; Train Acc = 0.8559666679302851, Test Acc = 0.842700002193451\n",
      "Training at step=16, batch=0, train loss = 0.38770574014897025, train acc = 0.875, time = 0.013085603713989258\n",
      "Training at step=16, batch=60, train loss = 0.3706739474043051, train acc = 0.8849999904632568, time = 0.013000726699829102\n",
      "Training at step=16, batch=120, train loss = 0.40143074994804295, train acc = 0.8650000095367432, time = 0.01308894157409668\n",
      "Training at step=16, batch=180, train loss = 0.4352157254057073, train acc = 0.875, time = 0.013079166412353516\n",
      "Training at step=16, batch=240, train loss = 0.3749371459706321, train acc = 0.8849999904632568, time = 0.013092994689941406\n",
      "Testing at step=16, batch=0, test loss = 0.5419037325043234, test acc = 0.8149999976158142, time = 0.002490997314453125\n",
      "Testing at step=16, batch=10, test loss = 0.5500569235234026, test acc = 0.7950000166893005, time = 0.0023963451385498047\n",
      "Testing at step=16, batch=20, test loss = 0.5209409344595222, test acc = 0.8299999833106995, time = 0.002412557601928711\n",
      "Testing at step=16, batch=30, test loss = 0.4448783519902652, test acc = 0.8399999737739563, time = 0.0024101734161376953\n",
      "Testing at step=16, batch=40, test loss = 0.4478877636196017, test acc = 0.8650000095367432, time = 0.002378225326538086\n",
      "Step 16 finished in 14.170222997665405, Train loss = 0.41345128327589087, Test loss = 0.4524616321954664; Train Acc = 0.8572666680812836, Test Acc = 0.8408000004291535\n",
      "Training at step=17, batch=0, train loss = 0.4417497413242239, train acc = 0.8600000143051147, time = 0.013243675231933594\n",
      "Training at step=17, batch=60, train loss = 0.3997242882474559, train acc = 0.8600000143051147, time = 0.013077020645141602\n",
      "Training at step=17, batch=120, train loss = 0.4347387975421765, train acc = 0.8600000143051147, time = 0.013142108917236328\n",
      "Training at step=17, batch=180, train loss = 0.457318867215223, train acc = 0.8600000143051147, time = 0.013044595718383789\n",
      "Training at step=17, batch=240, train loss = 0.5097274559861379, train acc = 0.8500000238418579, time = 0.013251781463623047\n",
      "Testing at step=17, batch=0, test loss = 0.4186939609234572, test acc = 0.8650000095367432, time = 0.0025022029876708984\n",
      "Testing at step=17, batch=10, test loss = 0.4025239166159505, test acc = 0.8550000190734863, time = 0.0024378299713134766\n",
      "Testing at step=17, batch=20, test loss = 0.46987777660031654, test acc = 0.8299999833106995, time = 0.002362966537475586\n",
      "Testing at step=17, batch=30, test loss = 0.3803461171299138, test acc = 0.8550000190734863, time = 0.0023810863494873047\n",
      "Testing at step=17, batch=40, test loss = 0.4375585763586806, test acc = 0.8399999737739563, time = 0.0023522377014160156\n",
      "Step 17 finished in 14.01060700416565, Train loss = 0.41223388369038094, Test loss = 0.45060427605729797; Train Acc = 0.8579333337148031, Test Acc = 0.8409999990463257\n",
      "Training at step=18, batch=0, train loss = 0.3563847439466079, train acc = 0.8799999952316284, time = 0.012989044189453125\n",
      "Training at step=18, batch=60, train loss = 0.5540336841950217, train acc = 0.8149999976158142, time = 0.012835264205932617\n",
      "Training at step=18, batch=120, train loss = 0.4392590648908291, train acc = 0.8500000238418579, time = 0.012966632843017578\n",
      "Training at step=18, batch=180, train loss = 0.4518960606308442, train acc = 0.8299999833106995, time = 0.012766361236572266\n",
      "Training at step=18, batch=240, train loss = 0.3544733450678263, train acc = 0.9049999713897705, time = 0.013025760650634766\n",
      "Testing at step=18, batch=0, test loss = 0.47679494307619175, test acc = 0.8450000286102295, time = 0.002393007278442383\n",
      "Testing at step=18, batch=10, test loss = 0.4348377544837528, test acc = 0.8600000143051147, time = 0.002370595932006836\n",
      "Testing at step=18, batch=20, test loss = 0.49279858858150716, test acc = 0.8450000286102295, time = 0.002405881881713867\n",
      "Testing at step=18, batch=30, test loss = 0.3745724740993241, test acc = 0.8650000095367432, time = 0.0039348602294921875\n",
      "Testing at step=18, batch=40, test loss = 0.3734701509564447, test acc = 0.8700000047683716, time = 0.002375364303588867\n",
      "Step 18 finished in 14.040527820587158, Train loss = 0.41051942064879066, Test loss = 0.4574759189001481; Train Acc = 0.8578333348035813, Test Acc = 0.8373000025749207\n",
      "Training at step=19, batch=0, train loss = 0.4554417359807848, train acc = 0.8349999785423279, time = 0.013068675994873047\n",
      "Training at step=19, batch=60, train loss = 0.45473112038138597, train acc = 0.8399999737739563, time = 0.013265848159790039\n",
      "Training at step=19, batch=120, train loss = 0.4677819592975765, train acc = 0.8399999737739563, time = 0.013062715530395508\n",
      "Training at step=19, batch=180, train loss = 0.36278939984421277, train acc = 0.8650000095367432, time = 0.013148307800292969\n",
      "Training at step=19, batch=240, train loss = 0.3394309915018088, train acc = 0.8849999904632568, time = 0.013028621673583984\n",
      "Testing at step=19, batch=0, test loss = 0.4111647894769096, test acc = 0.8650000095367432, time = 0.002454519271850586\n",
      "Testing at step=19, batch=10, test loss = 0.5499856045845836, test acc = 0.8299999833106995, time = 0.0024292469024658203\n",
      "Testing at step=19, batch=20, test loss = 0.4227474123269829, test acc = 0.8500000238418579, time = 0.0023534297943115234\n",
      "Testing at step=19, batch=30, test loss = 0.4108608540757126, test acc = 0.8600000143051147, time = 0.002404451370239258\n",
      "Testing at step=19, batch=40, test loss = 0.5017773888604876, test acc = 0.8349999785423279, time = 0.002425670623779297\n",
      "Step 19 finished in 13.903917074203491, Train loss = 0.40823653611486965, Test loss = 0.4487902625580527; Train Acc = 0.858616667787234, Test Acc = 0.8421999990940094\n",
      "Training at step=20, batch=0, train loss = 0.30189363931319, train acc = 0.8849999904632568, time = 0.013210535049438477\n",
      "Training at step=20, batch=60, train loss = 0.36456774427108973, train acc = 0.8700000047683716, time = 0.013082742691040039\n",
      "Training at step=20, batch=120, train loss = 0.3151768496691308, train acc = 0.8899999856948853, time = 0.013067960739135742\n",
      "Training at step=20, batch=180, train loss = 0.4350732891865467, train acc = 0.8550000190734863, time = 0.013158559799194336\n",
      "Training at step=20, batch=240, train loss = 0.40123132916208193, train acc = 0.8450000286102295, time = 0.012959003448486328\n",
      "Testing at step=20, batch=0, test loss = 0.5005074357798741, test acc = 0.8149999976158142, time = 0.0024602413177490234\n",
      "Testing at step=20, batch=10, test loss = 0.421019135406107, test acc = 0.8450000286102295, time = 0.002433300018310547\n",
      "Testing at step=20, batch=20, test loss = 0.43953940991770685, test acc = 0.8349999785423279, time = 0.0024132728576660156\n",
      "Testing at step=20, batch=30, test loss = 0.40132118487665885, test acc = 0.875, time = 0.0023729801177978516\n",
      "Testing at step=20, batch=40, test loss = 0.41514572039338227, test acc = 0.8849999904632568, time = 0.0023851394653320312\n",
      "Step 20 finished in 13.99406909942627, Train loss = 0.40793890524184734, Test loss = 0.4423519456120049; Train Acc = 0.8594833346207936, Test Acc = 0.8446000015735626\n",
      "Training at step=21, batch=0, train loss = 0.35637808268944504, train acc = 0.8650000095367432, time = 0.013132333755493164\n",
      "Training at step=21, batch=60, train loss = 0.5060107105861198, train acc = 0.824999988079071, time = 0.013207197189331055\n",
      "Training at step=21, batch=120, train loss = 0.4408500570452958, train acc = 0.8500000238418579, time = 0.013015985488891602\n",
      "Training at step=21, batch=180, train loss = 0.2897718051014062, train acc = 0.8999999761581421, time = 0.012782096862792969\n",
      "Training at step=21, batch=240, train loss = 0.3787070804653455, train acc = 0.875, time = 0.012793302536010742\n",
      "Testing at step=21, batch=0, test loss = 0.4313774960531807, test acc = 0.8500000238418579, time = 0.002542257308959961\n",
      "Testing at step=21, batch=10, test loss = 0.46444120436865177, test acc = 0.8399999737739563, time = 0.002440929412841797\n",
      "Testing at step=21, batch=20, test loss = 0.31852971653520085, test acc = 0.8999999761581421, time = 0.002331972122192383\n",
      "Testing at step=21, batch=30, test loss = 0.2713356499372238, test acc = 0.9150000214576721, time = 0.0024573802947998047\n",
      "Testing at step=21, batch=40, test loss = 0.5217905872466588, test acc = 0.8149999976158142, time = 0.002482891082763672\n",
      "Step 21 finished in 14.192774295806885, Train loss = 0.40588325704599487, Test loss = 0.44683891568523426; Train Acc = 0.8595333349704742, Test Acc = 0.8423000001907348\n",
      "Training at step=22, batch=0, train loss = 0.4258517991513645, train acc = 0.8650000095367432, time = 0.013440370559692383\n",
      "Training at step=22, batch=60, train loss = 0.3946411370693873, train acc = 0.8399999737739563, time = 0.013031244277954102\n",
      "Training at step=22, batch=120, train loss = 0.36278339910811264, train acc = 0.8849999904632568, time = 0.013091802597045898\n",
      "Training at step=22, batch=180, train loss = 0.3347446459448349, train acc = 0.8700000047683716, time = 0.013129711151123047\n",
      "Training at step=22, batch=240, train loss = 0.4292506167932887, train acc = 0.8500000238418579, time = 0.01300811767578125\n",
      "Testing at step=22, batch=0, test loss = 0.43400504793139577, test acc = 0.8349999785423279, time = 0.002459287643432617\n",
      "Testing at step=22, batch=10, test loss = 0.47874970221673, test acc = 0.8450000286102295, time = 0.0024313926696777344\n",
      "Testing at step=22, batch=20, test loss = 0.3766502204391362, test acc = 0.8550000190734863, time = 0.0024137496948242188\n",
      "Testing at step=22, batch=30, test loss = 0.3950756208215789, test acc = 0.8700000047683716, time = 0.002406597137451172\n",
      "Testing at step=22, batch=40, test loss = 0.5549131625871135, test acc = 0.800000011920929, time = 0.0024187564849853516\n",
      "Step 22 finished in 14.070098876953125, Train loss = 0.40473356306795183, Test loss = 0.44687032553717804; Train Acc = 0.8592000027497609, Test Acc = 0.8423000037670135\n",
      "Training at step=23, batch=0, train loss = 0.4684953513998755, train acc = 0.8450000286102295, time = 0.013099431991577148\n",
      "Training at step=23, batch=60, train loss = 0.39825235351593685, train acc = 0.8600000143051147, time = 0.013268232345581055\n",
      "Training at step=23, batch=120, train loss = 0.4391845586674843, train acc = 0.8399999737739563, time = 0.013138055801391602\n",
      "Training at step=23, batch=180, train loss = 0.48835295322113564, train acc = 0.8450000286102295, time = 0.013127326965332031\n",
      "Training at step=23, batch=240, train loss = 0.3123865286677744, train acc = 0.9049999713897705, time = 0.013158082962036133\n",
      "Testing at step=23, batch=0, test loss = 0.3832312667525963, test acc = 0.8550000190734863, time = 0.002483844757080078\n",
      "Testing at step=23, batch=10, test loss = 0.33975129096166506, test acc = 0.8799999952316284, time = 0.0024585723876953125\n",
      "Testing at step=23, batch=20, test loss = 0.4001018900663096, test acc = 0.8550000190734863, time = 0.0022704601287841797\n",
      "Testing at step=23, batch=30, test loss = 0.5621383314327343, test acc = 0.8100000023841858, time = 0.0022470951080322266\n",
      "Testing at step=23, batch=40, test loss = 0.517849658974686, test acc = 0.8349999785423279, time = 0.0023610591888427734\n",
      "Step 23 finished in 14.766527891159058, Train loss = 0.4043173051113492, Test loss = 0.450803969951839; Train Acc = 0.8603000022967656, Test Acc = 0.8390999984741211\n",
      "Training at step=24, batch=0, train loss = 0.36672249280419755, train acc = 0.8450000286102295, time = 0.015677690505981445\n",
      "Training at step=24, batch=60, train loss = 0.3524232739132545, train acc = 0.8550000190734863, time = 0.012796640396118164\n",
      "Training at step=24, batch=120, train loss = 0.3543812799548315, train acc = 0.8799999952316284, time = 0.012783527374267578\n",
      "Training at step=24, batch=180, train loss = 0.31903224050852275, train acc = 0.8949999809265137, time = 0.012911319732666016\n",
      "Training at step=24, batch=240, train loss = 0.41055901183344806, train acc = 0.8500000238418579, time = 0.012987136840820312\n",
      "Testing at step=24, batch=0, test loss = 0.48456737817269124, test acc = 0.8399999737739563, time = 0.0024454593658447266\n",
      "Testing at step=24, batch=10, test loss = 0.4903438929860884, test acc = 0.7850000262260437, time = 0.002339601516723633\n",
      "Testing at step=24, batch=20, test loss = 0.4976771601584539, test acc = 0.8600000143051147, time = 0.0023581981658935547\n",
      "Testing at step=24, batch=30, test loss = 0.39288619472131825, test acc = 0.875, time = 0.0023813247680664062\n",
      "Testing at step=24, batch=40, test loss = 0.4732536161345395, test acc = 0.824999988079071, time = 0.002347707748413086\n",
      "Step 24 finished in 13.687418222427368, Train loss = 0.40190188833831586, Test loss = 0.445898473782249; Train Acc = 0.8609500030676523, Test Acc = 0.843899998664856\n",
      "Training at step=25, batch=0, train loss = 0.28819052811787105, train acc = 0.8849999904632568, time = 0.013067007064819336\n",
      "Training at step=25, batch=60, train loss = 0.43681654512525725, train acc = 0.8500000238418579, time = 0.01313018798828125\n",
      "Training at step=25, batch=120, train loss = 0.4488454699638071, train acc = 0.824999988079071, time = 0.013128995895385742\n",
      "Training at step=25, batch=180, train loss = 0.46008885384653503, train acc = 0.8299999833106995, time = 0.013077974319458008\n",
      "Training at step=25, batch=240, train loss = 0.3912826338784241, train acc = 0.8700000047683716, time = 0.013094663619995117\n",
      "Testing at step=25, batch=0, test loss = 0.48978628550984, test acc = 0.7950000166893005, time = 0.002276897430419922\n",
      "Testing at step=25, batch=10, test loss = 0.4200395249811846, test acc = 0.875, time = 0.0022356510162353516\n",
      "Testing at step=25, batch=20, test loss = 0.4231836184122073, test acc = 0.8299999833106995, time = 0.0023779869079589844\n",
      "Testing at step=25, batch=30, test loss = 0.5358162989730673, test acc = 0.8149999976158142, time = 0.002409219741821289\n",
      "Testing at step=25, batch=40, test loss = 0.5281988230103413, test acc = 0.8299999833106995, time = 0.002414226531982422\n",
      "Step 25 finished in 14.25736403465271, Train loss = 0.3998365360108162, Test loss = 0.4411877191610516; Train Acc = 0.8623833350340525, Test Acc = 0.8443000042438507\n",
      "Training at step=26, batch=0, train loss = 0.40594023602391505, train acc = 0.8700000047683716, time = 0.013094663619995117\n",
      "Training at step=26, batch=60, train loss = 0.4231782716617573, train acc = 0.8899999856948853, time = 0.013099431991577148\n",
      "Training at step=26, batch=120, train loss = 0.42996141653174047, train acc = 0.8500000238418579, time = 0.013185977935791016\n",
      "Training at step=26, batch=180, train loss = 0.3691123198926667, train acc = 0.8600000143051147, time = 0.013020992279052734\n",
      "Training at step=26, batch=240, train loss = 0.4170170281031529, train acc = 0.8700000047683716, time = 0.013100862503051758\n",
      "Testing at step=26, batch=0, test loss = 0.4935023646400252, test acc = 0.8299999833106995, time = 0.002447843551635742\n",
      "Testing at step=26, batch=10, test loss = 0.45941126427119905, test acc = 0.8500000238418579, time = 0.0024406909942626953\n",
      "Testing at step=26, batch=20, test loss = 0.46678389332512266, test acc = 0.8349999785423279, time = 0.0023920536041259766\n",
      "Testing at step=26, batch=30, test loss = 0.4754059578014679, test acc = 0.8199999928474426, time = 0.0024154186248779297\n",
      "Testing at step=26, batch=40, test loss = 0.47818058588509765, test acc = 0.8500000238418579, time = 0.0023679733276367188\n",
      "Step 26 finished in 13.83426284790039, Train loss = 0.399875905776912, Test loss = 0.44453016972578224; Train Acc = 0.861799999276797, Test Acc = 0.8412000036239624\n",
      "Training at step=27, batch=0, train loss = 0.38775220334370675, train acc = 0.875, time = 0.012993335723876953\n",
      "Training at step=27, batch=60, train loss = 0.43134596153455684, train acc = 0.8500000238418579, time = 0.013135194778442383\n",
      "Training at step=27, batch=120, train loss = 0.3334500440657967, train acc = 0.8999999761581421, time = 0.013006925582885742\n",
      "Training at step=27, batch=180, train loss = 0.4935779710365096, train acc = 0.8349999785423279, time = 0.012875556945800781\n",
      "Training at step=27, batch=240, train loss = 0.399119887806278, train acc = 0.8799999952316284, time = 0.01288604736328125\n",
      "Testing at step=27, batch=0, test loss = 0.40705206327886145, test acc = 0.8600000143051147, time = 0.0023674964904785156\n",
      "Testing at step=27, batch=10, test loss = 0.438403184190352, test acc = 0.8500000238418579, time = 0.0023300647735595703\n",
      "Testing at step=27, batch=20, test loss = 0.3294347659720083, test acc = 0.8999999761581421, time = 0.002280712127685547\n",
      "Testing at step=27, batch=30, test loss = 0.4833499019003982, test acc = 0.8349999785423279, time = 0.0022330284118652344\n",
      "Testing at step=27, batch=40, test loss = 0.4566943286330761, test acc = 0.7850000262260437, time = 0.0022284984588623047\n",
      "Step 27 finished in 13.724594831466675, Train loss = 0.3995251985019547, Test loss = 0.4497292308058431; Train Acc = 0.8625833350419998, Test Acc = 0.8391999983787537\n",
      "Training at step=28, batch=0, train loss = 0.35489344040919446, train acc = 0.8899999856948853, time = 0.012815475463867188\n",
      "Training at step=28, batch=60, train loss = 0.3952677414831116, train acc = 0.8899999856948853, time = 0.012778997421264648\n",
      "Training at step=28, batch=120, train loss = 0.4872561177666994, train acc = 0.8050000071525574, time = 0.013216972351074219\n",
      "Training at step=28, batch=180, train loss = 0.3616398098488227, train acc = 0.8700000047683716, time = 0.013206958770751953\n",
      "Training at step=28, batch=240, train loss = 0.33230285903977186, train acc = 0.8999999761581421, time = 0.013005971908569336\n",
      "Testing at step=28, batch=0, test loss = 0.3730103113557272, test acc = 0.8899999856948853, time = 0.002511739730834961\n",
      "Testing at step=28, batch=10, test loss = 0.4736910735614145, test acc = 0.8450000286102295, time = 0.002460002899169922\n",
      "Testing at step=28, batch=20, test loss = 0.47837473976223366, test acc = 0.8199999928474426, time = 0.0024154186248779297\n",
      "Testing at step=28, batch=30, test loss = 0.4560196822281894, test acc = 0.8199999928474426, time = 0.0024116039276123047\n",
      "Testing at step=28, batch=40, test loss = 0.42977640025123853, test acc = 0.8550000190734863, time = 0.0023665428161621094\n",
      "Step 28 finished in 14.142642259597778, Train loss = 0.39726092482529735, Test loss = 0.4411881594515153; Train Acc = 0.8626000008980433, Test Acc = 0.842700001001358\n",
      "Training at step=29, batch=0, train loss = 0.33902665489763095, train acc = 0.8849999904632568, time = 0.013141870498657227\n",
      "Training at step=29, batch=60, train loss = 0.38381840765155145, train acc = 0.8799999952316284, time = 0.012984752655029297\n",
      "Training at step=29, batch=120, train loss = 0.34212149578017625, train acc = 0.8799999952316284, time = 0.013193607330322266\n",
      "Training at step=29, batch=180, train loss = 0.3717005762360755, train acc = 0.8899999856948853, time = 0.01316976547241211\n",
      "Training at step=29, batch=240, train loss = 0.3592607838566561, train acc = 0.8799999952316284, time = 0.01327824592590332\n",
      "Testing at step=29, batch=0, test loss = 0.5277321842655113, test acc = 0.8199999928474426, time = 0.002474069595336914\n",
      "Testing at step=29, batch=10, test loss = 0.3758491281798389, test acc = 0.8899999856948853, time = 0.0024039745330810547\n",
      "Testing at step=29, batch=20, test loss = 0.5710624204055632, test acc = 0.824999988079071, time = 0.0024662017822265625\n",
      "Testing at step=29, batch=30, test loss = 0.33859714112770084, test acc = 0.875, time = 0.0024480819702148438\n",
      "Testing at step=29, batch=40, test loss = 0.47689827427534537, test acc = 0.8450000286102295, time = 0.0023980140686035156\n",
      "Step 29 finished in 13.839126825332642, Train loss = 0.3965005425514891, Test loss = 0.44695130115388354; Train Acc = 0.862633334795634, Test Acc = 0.8435000026226044\n",
      "Training at step=30, batch=0, train loss = 0.3386381316158541, train acc = 0.8799999952316284, time = 0.013186454772949219\n",
      "Training at step=30, batch=60, train loss = 0.34971906918238604, train acc = 0.8650000095367432, time = 0.013092994689941406\n",
      "Training at step=30, batch=120, train loss = 0.36753366211945016, train acc = 0.8600000143051147, time = 0.013053417205810547\n",
      "Training at step=30, batch=180, train loss = 0.516735396127918, train acc = 0.8100000023841858, time = 0.012992382049560547\n",
      "Training at step=30, batch=240, train loss = 0.3980188237072153, train acc = 0.8500000238418579, time = 0.012797832489013672\n",
      "Testing at step=30, batch=0, test loss = 0.37937502644764465, test acc = 0.8600000143051147, time = 0.002368450164794922\n",
      "Testing at step=30, batch=10, test loss = 0.49102062540843056, test acc = 0.8149999976158142, time = 0.0023856163024902344\n",
      "Testing at step=30, batch=20, test loss = 0.37237011273945053, test acc = 0.8550000190734863, time = 0.0023038387298583984\n",
      "Testing at step=30, batch=30, test loss = 0.3973457906870705, test acc = 0.8799999952316284, time = 0.0022971630096435547\n",
      "Testing at step=30, batch=40, test loss = 0.31917356222260185, test acc = 0.8700000047683716, time = 0.0022611618041992188\n",
      "Step 30 finished in 13.985084295272827, Train loss = 0.39572141856310045, Test loss = 0.44412274076501773; Train Acc = 0.8627333350976308, Test Acc = 0.8435000050067901\n",
      "Training at step=31, batch=0, train loss = 0.34053756633666976, train acc = 0.8700000047683716, time = 0.012961864471435547\n",
      "Training at step=31, batch=60, train loss = 0.42733423897781136, train acc = 0.8700000047683716, time = 0.012773513793945312\n",
      "Training at step=31, batch=120, train loss = 0.3691017265744121, train acc = 0.875, time = 0.013098001480102539\n",
      "Training at step=31, batch=180, train loss = 0.2903054265799645, train acc = 0.9100000262260437, time = 0.013151884078979492\n",
      "Training at step=31, batch=240, train loss = 0.25323448947957045, train acc = 0.9150000214576721, time = 0.013089895248413086\n",
      "Testing at step=31, batch=0, test loss = 0.5095287708520011, test acc = 0.8100000023841858, time = 0.0024437904357910156\n",
      "Testing at step=31, batch=10, test loss = 0.5117553458512704, test acc = 0.8550000190734863, time = 0.0024840831756591797\n",
      "Testing at step=31, batch=20, test loss = 0.4737439144878811, test acc = 0.8299999833106995, time = 0.0024149417877197266\n",
      "Testing at step=31, batch=30, test loss = 0.4075291367410163, test acc = 0.8450000286102295, time = 0.0023462772369384766\n",
      "Testing at step=31, batch=40, test loss = 0.38037030211412903, test acc = 0.8149999976158142, time = 0.002417325973510742\n",
      "Step 31 finished in 13.828258275985718, Train loss = 0.394141242042139, Test loss = 0.44179546099456135; Train Acc = 0.8639166675011317, Test Acc = 0.8432999980449677\n",
      "Training at step=32, batch=0, train loss = 0.35212246057583213, train acc = 0.8949999809265137, time = 0.013054609298706055\n",
      "Training at step=32, batch=60, train loss = 0.3798744548027651, train acc = 0.8550000190734863, time = 0.013414859771728516\n",
      "Training at step=32, batch=120, train loss = 0.3715313192879155, train acc = 0.8600000143051147, time = 0.013102054595947266\n",
      "Training at step=32, batch=180, train loss = 0.3828620715309688, train acc = 0.8650000095367432, time = 0.013222455978393555\n",
      "Training at step=32, batch=240, train loss = 0.2751483619781141, train acc = 0.925000011920929, time = 0.013231039047241211\n",
      "Testing at step=32, batch=0, test loss = 0.34626506439617083, test acc = 0.8899999856948853, time = 0.0024788379669189453\n",
      "Testing at step=32, batch=10, test loss = 0.3522439494969444, test acc = 0.8550000190734863, time = 0.0025081634521484375\n",
      "Testing at step=32, batch=20, test loss = 0.4312044833850536, test acc = 0.824999988079071, time = 0.0024297237396240234\n",
      "Testing at step=32, batch=30, test loss = 0.35628216158110226, test acc = 0.875, time = 0.002424955368041992\n",
      "Testing at step=32, batch=40, test loss = 0.4851770670624932, test acc = 0.8450000286102295, time = 0.0023729801177978516\n",
      "Step 32 finished in 14.940492391586304, Train loss = 0.3943504353861173, Test loss = 0.4387919868770584; Train Acc = 0.8629666684071223, Test Acc = 0.8428000020980835\n",
      "Training at step=33, batch=0, train loss = 0.3988343082200313, train acc = 0.8700000047683716, time = 0.013084173202514648\n",
      "Training at step=33, batch=60, train loss = 0.33195473252097074, train acc = 0.8849999904632568, time = 0.013042688369750977\n",
      "Training at step=33, batch=120, train loss = 0.35504115667384406, train acc = 0.8849999904632568, time = 0.013010501861572266\n",
      "Training at step=33, batch=180, train loss = 0.3783836668147211, train acc = 0.8799999952316284, time = 0.012841939926147461\n",
      "Training at step=33, batch=240, train loss = 0.37153051214076, train acc = 0.8650000095367432, time = 0.012953996658325195\n",
      "Testing at step=33, batch=0, test loss = 0.41297557188336875, test acc = 0.8600000143051147, time = 0.0024199485778808594\n",
      "Testing at step=33, batch=10, test loss = 0.486083059976795, test acc = 0.8349999785423279, time = 0.0022912025451660156\n",
      "Testing at step=33, batch=20, test loss = 0.4172536217809764, test acc = 0.8349999785423279, time = 0.0023305416107177734\n",
      "Testing at step=33, batch=30, test loss = 0.3684837937626228, test acc = 0.8600000143051147, time = 0.002392292022705078\n",
      "Testing at step=33, batch=40, test loss = 0.49904600845984654, test acc = 0.8349999785423279, time = 0.0022361278533935547\n",
      "Step 33 finished in 13.712198257446289, Train loss = 0.3927819591554307, Test loss = 0.4419762778314435; Train Acc = 0.8645333337783814, Test Acc = 0.8431000018119812\n",
      "Training at step=34, batch=0, train loss = 0.4167996498862908, train acc = 0.8650000095367432, time = 0.012681245803833008\n",
      "Training at step=34, batch=60, train loss = 0.39405353313479596, train acc = 0.8600000143051147, time = 0.012714147567749023\n",
      "Training at step=34, batch=120, train loss = 0.47143205682929457, train acc = 0.8450000286102295, time = 0.013031005859375\n",
      "Training at step=34, batch=180, train loss = 0.48218694458023015, train acc = 0.8600000143051147, time = 0.013182401657104492\n",
      "Training at step=34, batch=240, train loss = 0.4553963587720403, train acc = 0.8600000143051147, time = 0.013186454772949219\n",
      "Testing at step=34, batch=0, test loss = 0.42816120731040735, test acc = 0.8450000286102295, time = 0.0024220943450927734\n",
      "Testing at step=34, batch=10, test loss = 0.4308953165403875, test acc = 0.8550000190734863, time = 0.0025022029876708984\n",
      "Testing at step=34, batch=20, test loss = 0.37478679156308475, test acc = 0.8550000190734863, time = 0.0025777816772460938\n",
      "Testing at step=34, batch=30, test loss = 0.4582173447461355, test acc = 0.8500000238418579, time = 0.0023953914642333984\n",
      "Testing at step=34, batch=40, test loss = 0.43357118172248915, test acc = 0.8500000238418579, time = 0.002404451370239258\n",
      "Step 34 finished in 13.989609003067017, Train loss = 0.39253714029585013, Test loss = 0.44099652870351636; Train Acc = 0.8641666692495346, Test Acc = 0.8449000060558319\n",
      "Training at step=35, batch=0, train loss = 0.44938858021906514, train acc = 0.8450000286102295, time = 0.013173818588256836\n",
      "Training at step=35, batch=60, train loss = 0.37018946177771356, train acc = 0.8550000190734863, time = 0.013015270233154297\n",
      "Training at step=35, batch=120, train loss = 0.4447811140595413, train acc = 0.8849999904632568, time = 0.012788057327270508\n",
      "Training at step=35, batch=180, train loss = 0.334938793094488, train acc = 0.875, time = 0.013164043426513672\n",
      "Training at step=35, batch=240, train loss = 0.30438259810936774, train acc = 0.9049999713897705, time = 0.01336669921875\n",
      "Testing at step=35, batch=0, test loss = 0.32835408376956143, test acc = 0.8849999904632568, time = 0.0024220943450927734\n",
      "Testing at step=35, batch=10, test loss = 0.505156054302368, test acc = 0.8100000023841858, time = 0.0024487972259521484\n",
      "Testing at step=35, batch=20, test loss = 0.47601565434825366, test acc = 0.824999988079071, time = 0.0024297237396240234\n",
      "Testing at step=35, batch=30, test loss = 0.371458633140348, test acc = 0.8600000143051147, time = 0.0024356842041015625\n",
      "Testing at step=35, batch=40, test loss = 0.511635619221192, test acc = 0.800000011920929, time = 0.0024261474609375\n",
      "Step 35 finished in 14.084832668304443, Train loss = 0.3910203565152536, Test loss = 0.43958251315621694; Train Acc = 0.863866668343544, Test Acc = 0.8417000031471252\n",
      "Training at step=36, batch=0, train loss = 0.3524575458862682, train acc = 0.8700000047683716, time = 0.013087987899780273\n",
      "Training at step=36, batch=60, train loss = 0.4221601905535787, train acc = 0.8149999976158142, time = 0.013127565383911133\n",
      "Training at step=36, batch=120, train loss = 0.45859338467037264, train acc = 0.8399999737739563, time = 0.01314401626586914\n",
      "Training at step=36, batch=180, train loss = 0.4259899260166915, train acc = 0.8550000190734863, time = 0.013061761856079102\n",
      "Training at step=36, batch=240, train loss = 0.35759152858343085, train acc = 0.875, time = 0.013019561767578125\n",
      "Testing at step=36, batch=0, test loss = 0.47546264608216743, test acc = 0.8100000023841858, time = 0.002352476119995117\n",
      "Testing at step=36, batch=10, test loss = 0.4612530406635233, test acc = 0.8349999785423279, time = 0.0022230148315429688\n",
      "Testing at step=36, batch=20, test loss = 0.3753964948240602, test acc = 0.8500000238418579, time = 0.002191781997680664\n",
      "Testing at step=36, batch=30, test loss = 0.5359469130961206, test acc = 0.8299999833106995, time = 0.002362489700317383\n",
      "Testing at step=36, batch=40, test loss = 0.39801484750569666, test acc = 0.8399999737739563, time = 0.0022172927856445312\n",
      "Step 36 finished in 13.915863275527954, Train loss = 0.38989229703236733, Test loss = 0.4402480772366973; Train Acc = 0.8653333348035812, Test Acc = 0.8433999979496002\n",
      "Training at step=37, batch=0, train loss = 0.289578531011151, train acc = 0.8999999761581421, time = 0.012767791748046875\n",
      "Training at step=37, batch=60, train loss = 0.43342193467530954, train acc = 0.8399999737739563, time = 0.01296234130859375\n",
      "Training at step=37, batch=120, train loss = 0.42908121185937403, train acc = 0.8349999785423279, time = 0.012893915176391602\n",
      "Training at step=37, batch=180, train loss = 0.4360569550115914, train acc = 0.8550000190734863, time = 0.013112068176269531\n",
      "Training at step=37, batch=240, train loss = 0.2914100209857966, train acc = 0.8949999809265137, time = 0.012950897216796875\n",
      "Testing at step=37, batch=0, test loss = 0.3276151032564305, test acc = 0.8949999809265137, time = 0.002389669418334961\n",
      "Testing at step=37, batch=10, test loss = 0.4514974562340932, test acc = 0.8550000190734863, time = 0.0024566650390625\n",
      "Testing at step=37, batch=20, test loss = 0.3490042832682391, test acc = 0.8550000190734863, time = 0.002460479736328125\n",
      "Testing at step=37, batch=30, test loss = 0.48354394493458325, test acc = 0.8399999737739563, time = 0.002342700958251953\n",
      "Testing at step=37, batch=40, test loss = 0.3783525738004391, test acc = 0.8600000143051147, time = 0.0023992061614990234\n",
      "Step 37 finished in 14.013349294662476, Train loss = 0.38972200105455734, Test loss = 0.4399712999387168; Train Acc = 0.864600000778834, Test Acc = 0.8436000001430511\n",
      "Training at step=38, batch=0, train loss = 0.2566755711770689, train acc = 0.9150000214576721, time = 0.013066291809082031\n",
      "Training at step=38, batch=60, train loss = 0.459382176019234, train acc = 0.824999988079071, time = 0.013008594512939453\n",
      "Training at step=38, batch=120, train loss = 0.3700379675809538, train acc = 0.8650000095367432, time = 0.013111591339111328\n",
      "Training at step=38, batch=180, train loss = 0.3845020278684593, train acc = 0.8550000190734863, time = 0.013066768646240234\n",
      "Training at step=38, batch=240, train loss = 0.3639379283552671, train acc = 0.8799999952316284, time = 0.013139486312866211\n",
      "Testing at step=38, batch=0, test loss = 0.4804840907482826, test acc = 0.8299999833106995, time = 0.0024747848510742188\n",
      "Testing at step=38, batch=10, test loss = 0.47164382977114955, test acc = 0.8199999928474426, time = 0.0024306774139404297\n",
      "Testing at step=38, batch=20, test loss = 0.38064010112913826, test acc = 0.875, time = 0.0024344921112060547\n",
      "Testing at step=38, batch=30, test loss = 0.4592603702329021, test acc = 0.8650000095367432, time = 0.0023801326751708984\n",
      "Testing at step=38, batch=40, test loss = 0.3990891021745632, test acc = 0.875, time = 0.002404451370239258\n",
      "Step 38 finished in 13.821802616119385, Train loss = 0.38863002907263544, Test loss = 0.43742334153864204; Train Acc = 0.8650166674455007, Test Acc = 0.8435000014305115\n",
      "Training at step=39, batch=0, train loss = 0.40693679328114013, train acc = 0.8450000286102295, time = 0.01311182975769043\n",
      "Training at step=39, batch=60, train loss = 0.4476775885927326, train acc = 0.8199999928474426, time = 0.012986898422241211\n",
      "Training at step=39, batch=120, train loss = 0.4052638774915334, train acc = 0.8650000095367432, time = 0.012864351272583008\n",
      "Training at step=39, batch=180, train loss = 0.41376124910026246, train acc = 0.8849999904632568, time = 0.013042926788330078\n",
      "Training at step=39, batch=240, train loss = 0.3606234773265018, train acc = 0.875, time = 0.012911558151245117\n",
      "Testing at step=39, batch=0, test loss = 0.3944560004383933, test acc = 0.8600000143051147, time = 0.0024051666259765625\n",
      "Testing at step=39, batch=10, test loss = 0.41702897167379327, test acc = 0.8399999737739563, time = 0.0023987293243408203\n",
      "Testing at step=39, batch=20, test loss = 0.3889241214148955, test acc = 0.8650000095367432, time = 0.002397298812866211\n",
      "Testing at step=39, batch=30, test loss = 0.47659971576241444, test acc = 0.8149999976158142, time = 0.0023632049560546875\n",
      "Testing at step=39, batch=40, test loss = 0.43114429839806057, test acc = 0.8650000095367432, time = 0.0024514198303222656\n",
      "Step 39 finished in 13.981921195983887, Train loss = 0.3882821692137973, Test loss = 0.43779748922942324; Train Acc = 0.8654666670163472, Test Acc = 0.8448000013828277\n",
      "Training at step=40, batch=0, train loss = 0.47983004626132825, train acc = 0.8600000143051147, time = 0.013025522232055664\n",
      "Training at step=40, batch=60, train loss = 0.32475803294318806, train acc = 0.875, time = 0.013110160827636719\n",
      "Training at step=40, batch=120, train loss = 0.47511364101341796, train acc = 0.8399999737739563, time = 0.013088464736938477\n",
      "Training at step=40, batch=180, train loss = 0.3552700290233412, train acc = 0.8899999856948853, time = 0.012977838516235352\n",
      "Training at step=40, batch=240, train loss = 0.3126638627064838, train acc = 0.8949999809265137, time = 0.013076066970825195\n",
      "Testing at step=40, batch=0, test loss = 0.47893575541148065, test acc = 0.8299999833106995, time = 0.0025920867919921875\n",
      "Testing at step=40, batch=10, test loss = 0.3971409424784239, test acc = 0.8399999737739563, time = 0.0024602413177490234\n",
      "Testing at step=40, batch=20, test loss = 0.3746804858299033, test acc = 0.875, time = 0.002452373504638672\n",
      "Testing at step=40, batch=30, test loss = 0.45159140157292454, test acc = 0.8600000143051147, time = 0.002406597137451172\n",
      "Testing at step=40, batch=40, test loss = 0.41417257924776435, test acc = 0.8399999737739563, time = 0.0024492740631103516\n",
      "Step 40 finished in 13.752137660980225, Train loss = 0.3876731271148307, Test loss = 0.4371491923552318; Train Acc = 0.8651333318154018, Test Acc = 0.8439999997615815\n",
      "Training at step=41, batch=0, train loss = 0.3245208604345383, train acc = 0.8849999904632568, time = 0.01320505142211914\n",
      "Training at step=41, batch=60, train loss = 0.39076426465081604, train acc = 0.8600000143051147, time = 0.013156890869140625\n",
      "Training at step=41, batch=120, train loss = 0.3443325886751497, train acc = 0.8650000095367432, time = 0.012964487075805664\n",
      "Training at step=41, batch=180, train loss = 0.4318828134768037, train acc = 0.8799999952316284, time = 0.013103961944580078\n",
      "Training at step=41, batch=240, train loss = 0.4688976933125012, train acc = 0.8700000047683716, time = 0.012993335723876953\n",
      "Testing at step=41, batch=0, test loss = 0.3892642606351247, test acc = 0.8650000095367432, time = 0.002429962158203125\n",
      "Testing at step=41, batch=10, test loss = 0.4136945405033811, test acc = 0.8600000143051147, time = 0.002401113510131836\n",
      "Testing at step=41, batch=20, test loss = 0.42476480660373533, test acc = 0.8550000190734863, time = 0.0024080276489257812\n",
      "Testing at step=41, batch=30, test loss = 0.531412101290015, test acc = 0.8299999833106995, time = 0.0024132728576660156\n",
      "Testing at step=41, batch=40, test loss = 0.4506950526506957, test acc = 0.8199999928474426, time = 0.002417325973510742\n",
      "Step 41 finished in 13.936925411224365, Train loss = 0.38731743904564414, Test loss = 0.445810318515436; Train Acc = 0.8651666682958603, Test Acc = 0.8427999997138977\n",
      "Training at step=42, batch=0, train loss = 0.4199814550896146, train acc = 0.8700000047683716, time = 0.013077259063720703\n",
      "Training at step=42, batch=60, train loss = 0.3301449358419349, train acc = 0.8849999904632568, time = 0.013057708740234375\n",
      "Training at step=42, batch=120, train loss = 0.40677605524233734, train acc = 0.8650000095367432, time = 0.01306772232055664\n",
      "Training at step=42, batch=180, train loss = 0.4167620710800987, train acc = 0.8450000286102295, time = 0.012953758239746094\n",
      "Training at step=42, batch=240, train loss = 0.271986800024548, train acc = 0.9150000214576721, time = 0.01276707649230957\n",
      "Testing at step=42, batch=0, test loss = 0.4081060086093723, test acc = 0.8500000238418579, time = 0.002430438995361328\n",
      "Testing at step=42, batch=10, test loss = 0.468107283192925, test acc = 0.8349999785423279, time = 0.0024106502532958984\n",
      "Testing at step=42, batch=20, test loss = 0.47678106670650144, test acc = 0.8199999928474426, time = 0.0024182796478271484\n",
      "Testing at step=42, batch=30, test loss = 0.5183516943594791, test acc = 0.7900000214576721, time = 0.002424001693725586\n",
      "Testing at step=42, batch=40, test loss = 0.40214971315275894, test acc = 0.8349999785423279, time = 0.002410411834716797\n",
      "Step 42 finished in 13.966249227523804, Train loss = 0.3868119289821684, Test loss = 0.43834985734782683; Train Acc = 0.8654166688521703, Test Acc = 0.8441000032424927\n",
      "Training at step=43, batch=0, train loss = 0.3989738081570983, train acc = 0.8600000143051147, time = 0.013054847717285156\n",
      "Training at step=43, batch=60, train loss = 0.41747268829152345, train acc = 0.8550000190734863, time = 0.012978553771972656\n",
      "Training at step=43, batch=120, train loss = 0.32936424627679045, train acc = 0.875, time = 0.013035774230957031\n",
      "Training at step=43, batch=180, train loss = 0.4943604457138051, train acc = 0.8149999976158142, time = 0.012963533401489258\n",
      "Training at step=43, batch=240, train loss = 0.28955514245714026, train acc = 0.9100000262260437, time = 0.012873172760009766\n",
      "Testing at step=43, batch=0, test loss = 0.5589844071039285, test acc = 0.7950000166893005, time = 0.0024695396423339844\n",
      "Testing at step=43, batch=10, test loss = 0.42596759472529105, test acc = 0.8349999785423279, time = 0.0025167465209960938\n",
      "Testing at step=43, batch=20, test loss = 0.41331363799075027, test acc = 0.8399999737739563, time = 0.002418041229248047\n",
      "Testing at step=43, batch=30, test loss = 0.34972771669541686, test acc = 0.8849999904632568, time = 0.0024292469024658203\n",
      "Testing at step=43, batch=40, test loss = 0.4097480511126185, test acc = 0.8550000190734863, time = 0.002424001693725586\n",
      "Step 43 finished in 13.816490888595581, Train loss = 0.38536423686095705, Test loss = 0.4410891446672231; Train Acc = 0.8667500017086665, Test Acc = 0.840400002002716\n",
      "Training at step=44, batch=0, train loss = 0.37126271491402635, train acc = 0.875, time = 0.01319265365600586\n",
      "Training at step=44, batch=60, train loss = 0.45821358554091673, train acc = 0.8349999785423279, time = 0.012836694717407227\n",
      "Training at step=44, batch=120, train loss = 0.433375120646839, train acc = 0.8500000238418579, time = 0.01302957534790039\n",
      "Training at step=44, batch=180, train loss = 0.36177368015145456, train acc = 0.8799999952316284, time = 0.01309657096862793\n",
      "Training at step=44, batch=240, train loss = 0.34684683083011664, train acc = 0.8700000047683716, time = 0.01303243637084961\n",
      "Testing at step=44, batch=0, test loss = 0.31281351065889607, test acc = 0.9100000262260437, time = 0.002435445785522461\n",
      "Testing at step=44, batch=10, test loss = 0.5226569802540917, test acc = 0.8050000071525574, time = 0.0024144649505615234\n",
      "Testing at step=44, batch=20, test loss = 0.44266822826858876, test acc = 0.8399999737739563, time = 0.002428293228149414\n",
      "Testing at step=44, batch=30, test loss = 0.43737211805066567, test acc = 0.8600000143051147, time = 0.0024237632751464844\n",
      "Testing at step=44, batch=40, test loss = 0.4127652769484866, test acc = 0.8399999737739563, time = 0.002379179000854492\n",
      "Step 44 finished in 14.249703407287598, Train loss = 0.3852615828816122, Test loss = 0.43677322604144103; Train Acc = 0.8659166673819224, Test Acc = 0.8437999999523162\n",
      "Training at step=45, batch=0, train loss = 0.35549997648595266, train acc = 0.8550000190734863, time = 0.013139963150024414\n",
      "Training at step=45, batch=60, train loss = 0.42834534612136543, train acc = 0.8450000286102295, time = 0.012976408004760742\n",
      "Training at step=45, batch=120, train loss = 0.40948318158288044, train acc = 0.8650000095367432, time = 0.01299905776977539\n",
      "Training at step=45, batch=180, train loss = 0.4620353735889156, train acc = 0.8550000190734863, time = 0.01296854019165039\n",
      "Training at step=45, batch=240, train loss = 0.42313231797459905, train acc = 0.8399999737739563, time = 0.012939929962158203\n",
      "Testing at step=45, batch=0, test loss = 0.39608815425443217, test acc = 0.8799999952316284, time = 0.0024373531341552734\n",
      "Testing at step=45, batch=10, test loss = 0.40930604307366286, test acc = 0.875, time = 0.0023708343505859375\n",
      "Testing at step=45, batch=20, test loss = 0.44243238674615315, test acc = 0.8500000238418579, time = 0.0022394657135009766\n",
      "Testing at step=45, batch=30, test loss = 0.46971389732873087, test acc = 0.8349999785423279, time = 0.002317190170288086\n",
      "Testing at step=45, batch=40, test loss = 0.39369918987488967, test acc = 0.8450000286102295, time = 0.002330780029296875\n",
      "Step 45 finished in 13.72249984741211, Train loss = 0.3842840525353923, Test loss = 0.43569380661742024; Train Acc = 0.8670000022649765, Test Acc = 0.8453000032901764\n",
      "Training at step=46, batch=0, train loss = 0.3455147639426762, train acc = 0.8700000047683716, time = 0.012864828109741211\n",
      "Training at step=46, batch=60, train loss = 0.40741950441914954, train acc = 0.8299999833106995, time = 0.012758493423461914\n",
      "Training at step=46, batch=120, train loss = 0.38687983143000826, train acc = 0.8500000238418579, time = 0.013402700424194336\n",
      "Training at step=46, batch=180, train loss = 0.3626520781359541, train acc = 0.8849999904632568, time = 0.01299285888671875\n",
      "Training at step=46, batch=240, train loss = 0.42907364072300636, train acc = 0.8600000143051147, time = 0.013129234313964844\n",
      "Testing at step=46, batch=0, test loss = 0.4946853395736995, test acc = 0.824999988079071, time = 0.0023686885833740234\n",
      "Testing at step=46, batch=10, test loss = 0.3806605748070022, test acc = 0.8650000095367432, time = 0.0022912025451660156\n",
      "Testing at step=46, batch=20, test loss = 0.3169662161479513, test acc = 0.8999999761581421, time = 0.002343416213989258\n",
      "Testing at step=46, batch=30, test loss = 0.4107530405700064, test acc = 0.8550000190734863, time = 0.0023870468139648438\n",
      "Testing at step=46, batch=40, test loss = 0.4675182983586804, test acc = 0.8550000190734863, time = 0.002374887466430664\n",
      "Step 46 finished in 13.909696578979492, Train loss = 0.3841607195756369, Test loss = 0.4399159892364009; Train Acc = 0.8660000004371007, Test Acc = 0.8449000000953675\n",
      "Training at step=47, batch=0, train loss = 0.3108935026975708, train acc = 0.8849999904632568, time = 0.013218164443969727\n",
      "Training at step=47, batch=60, train loss = 0.3034316379251835, train acc = 0.875, time = 0.012987136840820312\n",
      "Training at step=47, batch=120, train loss = 0.387188669862918, train acc = 0.8600000143051147, time = 0.012981891632080078\n",
      "Training at step=47, batch=180, train loss = 0.40178077627425085, train acc = 0.8600000143051147, time = 0.013094902038574219\n",
      "Training at step=47, batch=240, train loss = 0.5050411306252008, train acc = 0.8149999976158142, time = 0.013258934020996094\n",
      "Testing at step=47, batch=0, test loss = 0.5565069544640555, test acc = 0.7950000166893005, time = 0.0025718212127685547\n",
      "Testing at step=47, batch=10, test loss = 0.3993924467955337, test acc = 0.8450000286102295, time = 0.0023469924926757812\n",
      "Testing at step=47, batch=20, test loss = 0.48373236183154744, test acc = 0.824999988079071, time = 0.0024275779724121094\n",
      "Testing at step=47, batch=30, test loss = 0.3415029307249918, test acc = 0.8700000047683716, time = 0.002411365509033203\n",
      "Testing at step=47, batch=40, test loss = 0.5021249821143315, test acc = 0.8349999785423279, time = 0.002424955368041992\n",
      "Step 47 finished in 14.033777475357056, Train loss = 0.3838449196722224, Test loss = 0.4374648711126498; Train Acc = 0.8659000013271968, Test Acc = 0.8439000010490417\n",
      "Training at step=48, batch=0, train loss = 0.2865603254468516, train acc = 0.8899999856948853, time = 0.01300811767578125\n",
      "Training at step=48, batch=60, train loss = 0.3325997962010264, train acc = 0.8700000047683716, time = 0.012981414794921875\n",
      "Training at step=48, batch=120, train loss = 0.4580928510193126, train acc = 0.8199999928474426, time = 0.013067007064819336\n",
      "Training at step=48, batch=180, train loss = 0.31214919548213865, train acc = 0.8999999761581421, time = 0.013020992279052734\n",
      "Training at step=48, batch=240, train loss = 0.3640887072957357, train acc = 0.8650000095367432, time = 0.013074636459350586\n",
      "Testing at step=48, batch=0, test loss = 0.4127006072410338, test acc = 0.8550000190734863, time = 0.0024271011352539062\n",
      "Testing at step=48, batch=10, test loss = 0.3780136117958033, test acc = 0.8899999856948853, time = 0.002405405044555664\n",
      "Testing at step=48, batch=20, test loss = 0.4617841408413551, test acc = 0.8349999785423279, time = 0.002372264862060547\n",
      "Testing at step=48, batch=30, test loss = 0.32658178495941764, test acc = 0.8550000190734863, time = 0.0023517608642578125\n",
      "Testing at step=48, batch=40, test loss = 0.5033637871477356, test acc = 0.8550000190734863, time = 0.0024094581604003906\n",
      "Step 48 finished in 13.826442003250122, Train loss = 0.3823989582194583, Test loss = 0.44219161722123773; Train Acc = 0.8669499981403351, Test Acc = 0.8421000015735626\n",
      "Training at step=49, batch=0, train loss = 0.400417732747417, train acc = 0.8399999737739563, time = 0.012986898422241211\n",
      "Training at step=49, batch=60, train loss = 0.28538835016730224, train acc = 0.9049999713897705, time = 0.013191699981689453\n",
      "Training at step=49, batch=120, train loss = 0.45591761933129626, train acc = 0.8299999833106995, time = 0.013087034225463867\n",
      "Training at step=49, batch=180, train loss = 0.3887853128142719, train acc = 0.875, time = 0.013002872467041016\n",
      "Training at step=49, batch=240, train loss = 0.3507108915285202, train acc = 0.8949999809265137, time = 0.012732505798339844\n",
      "Testing at step=49, batch=0, test loss = 0.4195706133719149, test acc = 0.8550000190734863, time = 0.002428770065307617\n",
      "Testing at step=49, batch=10, test loss = 0.4005510957667985, test acc = 0.9049999713897705, time = 0.0022809505462646484\n",
      "Testing at step=49, batch=20, test loss = 0.45378314074476483, test acc = 0.8349999785423279, time = 0.002240896224975586\n",
      "Testing at step=49, batch=30, test loss = 0.38175278099393756, test acc = 0.8550000190734863, time = 0.0022895336151123047\n",
      "Testing at step=49, batch=40, test loss = 0.5208596188364149, test acc = 0.8199999928474426, time = 0.002415180206298828\n",
      "Step 49 finished in 14.216615438461304, Train loss = 0.38177084968950975, Test loss = 0.440029842007078; Train Acc = 0.8678333338101705, Test Acc = 0.8441000032424927\n",
      "Training at step=50, batch=0, train loss = 0.3555899532802927, train acc = 0.8600000143051147, time = 0.013067483901977539\n",
      "Training at step=50, batch=60, train loss = 0.32798643232586044, train acc = 0.8999999761581421, time = 0.013083934783935547\n",
      "Training at step=50, batch=120, train loss = 0.4172911187425924, train acc = 0.8500000238418579, time = 0.01307535171508789\n",
      "Training at step=50, batch=180, train loss = 0.32780408670099864, train acc = 0.8799999952316284, time = 0.013174295425415039\n",
      "Training at step=50, batch=240, train loss = 0.35483971244794893, train acc = 0.8899999856948853, time = 0.012907266616821289\n",
      "Testing at step=50, batch=0, test loss = 0.4869627657316592, test acc = 0.8450000286102295, time = 0.0024518966674804688\n",
      "Testing at step=50, batch=10, test loss = 0.4061204202014557, test acc = 0.8799999952316284, time = 0.0024466514587402344\n",
      "Testing at step=50, batch=20, test loss = 0.4014231594395852, test acc = 0.8600000143051147, time = 0.00237274169921875\n",
      "Testing at step=50, batch=30, test loss = 0.35171678776654863, test acc = 0.8500000238418579, time = 0.002582073211669922\n",
      "Testing at step=50, batch=40, test loss = 0.44185502291320417, test acc = 0.8450000286102295, time = 0.0024132728576660156\n",
      "Step 50 finished in 13.955594778060913, Train loss = 0.38199230682181623, Test loss = 0.4406767911215519; Train Acc = 0.8670500010251999, Test Acc = 0.843600001335144\n",
      "Training at step=51, batch=0, train loss = 0.3309373171595136, train acc = 0.925000011920929, time = 0.01326608657836914\n",
      "Training at step=51, batch=60, train loss = 0.4653494869485636, train acc = 0.800000011920929, time = 0.012938261032104492\n",
      "Training at step=51, batch=120, train loss = 0.4447305518557518, train acc = 0.824999988079071, time = 0.012926578521728516\n",
      "Training at step=51, batch=180, train loss = 0.4159227675199937, train acc = 0.8349999785423279, time = 0.01295161247253418\n",
      "Training at step=51, batch=240, train loss = 0.3071027233002094, train acc = 0.8899999856948853, time = 0.013213396072387695\n",
      "Testing at step=51, batch=0, test loss = 0.3955063030843116, test acc = 0.8600000143051147, time = 0.002213001251220703\n",
      "Testing at step=51, batch=10, test loss = 0.4347979048205027, test acc = 0.8550000190734863, time = 0.002299785614013672\n",
      "Testing at step=51, batch=20, test loss = 0.54115499721461, test acc = 0.800000011920929, time = 0.0023140907287597656\n",
      "Testing at step=51, batch=30, test loss = 0.35805376537461614, test acc = 0.8799999952316284, time = 0.002210855484008789\n",
      "Testing at step=51, batch=40, test loss = 0.5497700683529702, test acc = 0.8299999833106995, time = 0.002412557601928711\n",
      "Step 51 finished in 14.130235195159912, Train loss = 0.38130211572063244, Test loss = 0.4445032877057072; Train Acc = 0.8675666677951813, Test Acc = 0.8421000039577484\n",
      "Training at step=52, batch=0, train loss = 0.3090047642816785, train acc = 0.8999999761581421, time = 0.013020753860473633\n",
      "Training at step=52, batch=60, train loss = 0.4968357572733735, train acc = 0.8650000095367432, time = 0.01312708854675293\n",
      "Training at step=52, batch=120, train loss = 0.4170409835260855, train acc = 0.8500000238418579, time = 0.013000249862670898\n",
      "Training at step=52, batch=180, train loss = 0.4063659474538279, train acc = 0.875, time = 0.013089418411254883\n",
      "Training at step=52, batch=240, train loss = 0.3915102574896441, train acc = 0.8349999785423279, time = 0.013255834579467773\n",
      "Testing at step=52, batch=0, test loss = 0.4850086609096385, test acc = 0.8349999785423279, time = 0.0024929046630859375\n",
      "Testing at step=52, batch=10, test loss = 0.46107469567738646, test acc = 0.8299999833106995, time = 0.002424478530883789\n",
      "Testing at step=52, batch=20, test loss = 0.4870864354794026, test acc = 0.8299999833106995, time = 0.0023865699768066406\n",
      "Testing at step=52, batch=30, test loss = 0.39025591774345236, test acc = 0.8650000095367432, time = 0.00266265869140625\n",
      "Testing at step=52, batch=40, test loss = 0.5008685535713461, test acc = 0.8450000286102295, time = 0.002419710159301758\n",
      "Step 52 finished in 13.822397470474243, Train loss = 0.38136828339270307, Test loss = 0.4360258992607926; Train Acc = 0.8684333342313767, Test Acc = 0.8430000054836273\n",
      "Training at step=53, batch=0, train loss = 0.4747473515463381, train acc = 0.8349999785423279, time = 0.012999534606933594\n",
      "Training at step=53, batch=60, train loss = 0.417424319150343, train acc = 0.8349999785423279, time = 0.013146638870239258\n",
      "Training at step=53, batch=120, train loss = 0.38024737686068205, train acc = 0.8600000143051147, time = 0.01332545280456543\n",
      "Training at step=53, batch=180, train loss = 0.34935926118823785, train acc = 0.875, time = 0.013115644454956055\n",
      "Training at step=53, batch=240, train loss = 0.362309958416759, train acc = 0.875, time = 0.013103008270263672\n",
      "Testing at step=53, batch=0, test loss = 0.3761116812046669, test acc = 0.8600000143051147, time = 0.0024585723876953125\n",
      "Testing at step=53, batch=10, test loss = 0.42986141218103063, test acc = 0.8700000047683716, time = 0.002420186996459961\n",
      "Testing at step=53, batch=20, test loss = 0.37418016224016454, test acc = 0.8849999904632568, time = 0.0024487972259521484\n",
      "Testing at step=53, batch=30, test loss = 0.35699901708462956, test acc = 0.8849999904632568, time = 0.00241851806640625\n",
      "Testing at step=53, batch=40, test loss = 0.5395796175030169, test acc = 0.8100000023841858, time = 0.002367258071899414\n",
      "Step 53 finished in 13.812856435775757, Train loss = 0.3799356065803812, Test loss = 0.43915286694574485; Train Acc = 0.868183334072431, Test Acc = 0.8457999992370605\n",
      "Training at step=54, batch=0, train loss = 0.2677280930597756, train acc = 0.925000011920929, time = 0.013009071350097656\n",
      "Training at step=54, batch=60, train loss = 0.35693992943132485, train acc = 0.875, time = 0.012951374053955078\n",
      "Training at step=54, batch=120, train loss = 0.45780233449098745, train acc = 0.8600000143051147, time = 0.013146638870239258\n",
      "Training at step=54, batch=180, train loss = 0.34861324567697716, train acc = 0.8650000095367432, time = 0.012870073318481445\n",
      "Training at step=54, batch=240, train loss = 0.40930479532957903, train acc = 0.8299999833106995, time = 0.012887954711914062\n",
      "Testing at step=54, batch=0, test loss = 0.3986441314019033, test acc = 0.8299999833106995, time = 0.002428770065307617\n",
      "Testing at step=54, batch=10, test loss = 0.3816715894032494, test acc = 0.875, time = 0.0024309158325195312\n",
      "Testing at step=54, batch=20, test loss = 0.5251769219941864, test acc = 0.800000011920929, time = 0.002489805221557617\n",
      "Testing at step=54, batch=30, test loss = 0.3749751947620574, test acc = 0.8700000047683716, time = 0.002409696578979492\n",
      "Testing at step=54, batch=40, test loss = 0.5217681168063285, test acc = 0.824999988079071, time = 0.002446413040161133\n",
      "Step 54 finished in 15.142976760864258, Train loss = 0.3807389944035436, Test loss = 0.43904336273765415; Train Acc = 0.8675166686375936, Test Acc = 0.8403999984264374\n",
      "Training at step=55, batch=0, train loss = 0.443132405092941, train acc = 0.8650000095367432, time = 0.013151884078979492\n",
      "Training at step=55, batch=60, train loss = 0.4334881431301041, train acc = 0.875, time = 0.012674331665039062\n",
      "Training at step=55, batch=120, train loss = 0.3637110744834622, train acc = 0.8650000095367432, time = 0.01265716552734375\n",
      "Training at step=55, batch=180, train loss = 0.3110774724164343, train acc = 0.8700000047683716, time = 0.013280868530273438\n",
      "Training at step=55, batch=240, train loss = 0.3684042937108202, train acc = 0.8999999761581421, time = 0.012955427169799805\n",
      "Testing at step=55, batch=0, test loss = 0.5428085706462634, test acc = 0.7950000166893005, time = 0.0024254322052001953\n",
      "Testing at step=55, batch=10, test loss = 0.40282570410129026, test acc = 0.8550000190734863, time = 0.0024433135986328125\n",
      "Testing at step=55, batch=20, test loss = 0.35096959248823034, test acc = 0.8600000143051147, time = 0.0023741722106933594\n",
      "Testing at step=55, batch=30, test loss = 0.5024920636535473, test acc = 0.7950000166893005, time = 0.0023849010467529297\n",
      "Testing at step=55, batch=40, test loss = 0.6071466201774876, test acc = 0.7950000166893005, time = 0.0024764537811279297\n",
      "Step 55 finished in 13.806544303894043, Train loss = 0.3794069057109665, Test loss = 0.442126588285002; Train Acc = 0.8679500003655751, Test Acc = 0.8442000091075897\n",
      "Training at step=56, batch=0, train loss = 0.30543428385686733, train acc = 0.8949999809265137, time = 0.013259172439575195\n",
      "Training at step=56, batch=60, train loss = 0.46856725364196394, train acc = 0.8450000286102295, time = 0.013113260269165039\n",
      "Training at step=56, batch=120, train loss = 0.4075617282301504, train acc = 0.8500000238418579, time = 0.012772798538208008\n",
      "Training at step=56, batch=180, train loss = 0.3787380098627088, train acc = 0.8500000238418579, time = 0.013336896896362305\n",
      "Training at step=56, batch=240, train loss = 0.31192742352006436, train acc = 0.8999999761581421, time = 0.013233423233032227\n",
      "Testing at step=56, batch=0, test loss = 0.47069683828763736, test acc = 0.8349999785423279, time = 0.002410888671875\n",
      "Testing at step=56, batch=10, test loss = 0.4394726817673755, test acc = 0.8399999737739563, time = 0.0024166107177734375\n",
      "Testing at step=56, batch=20, test loss = 0.3029225599482435, test acc = 0.8799999952316284, time = 0.0022361278533935547\n",
      "Testing at step=56, batch=30, test loss = 0.34712661623945484, test acc = 0.8799999952316284, time = 0.002318143844604492\n",
      "Testing at step=56, batch=40, test loss = 0.4213794103561738, test acc = 0.8500000238418579, time = 0.002290964126586914\n",
      "Step 56 finished in 14.102836847305298, Train loss = 0.37816919778668406, Test loss = 0.4373617193916218; Train Acc = 0.8687000018358231, Test Acc = 0.8434999990463257\n",
      "Training at step=57, batch=0, train loss = 0.4046869310451084, train acc = 0.8600000143051147, time = 0.013127565383911133\n",
      "Training at step=57, batch=60, train loss = 0.4081270467752647, train acc = 0.8799999952316284, time = 0.013193845748901367\n",
      "Training at step=57, batch=120, train loss = 0.4430400188016407, train acc = 0.8500000238418579, time = 0.012894392013549805\n",
      "Training at step=57, batch=180, train loss = 0.4013250720446092, train acc = 0.8949999809265137, time = 0.012783527374267578\n",
      "Training at step=57, batch=240, train loss = 0.4037875904223654, train acc = 0.8700000047683716, time = 0.01308441162109375\n",
      "Testing at step=57, batch=0, test loss = 0.36606274939764416, test acc = 0.875, time = 0.002482891082763672\n",
      "Testing at step=57, batch=10, test loss = 0.3465882849800983, test acc = 0.8949999809265137, time = 0.002403736114501953\n",
      "Testing at step=57, batch=20, test loss = 0.5162614096618112, test acc = 0.8149999976158142, time = 0.0023899078369140625\n",
      "Testing at step=57, batch=30, test loss = 0.4520667760484327, test acc = 0.8450000286102295, time = 0.0023500919342041016\n",
      "Testing at step=57, batch=40, test loss = 0.41899180783821605, test acc = 0.8399999737739563, time = 0.002398252487182617\n",
      "Step 57 finished in 13.864753723144531, Train loss = 0.37847608728461285, Test loss = 0.43846443214313907; Train Acc = 0.8678833341598511, Test Acc = 0.8437999975681305\n",
      "Training at step=58, batch=0, train loss = 0.31283263090022173, train acc = 0.8999999761581421, time = 0.013088464736938477\n",
      "Training at step=58, batch=60, train loss = 0.34105948005646963, train acc = 0.8999999761581421, time = 0.013067483901977539\n",
      "Training at step=58, batch=120, train loss = 0.4247602090502258, train acc = 0.8700000047683716, time = 0.013308048248291016\n",
      "Training at step=58, batch=180, train loss = 0.43091289270152033, train acc = 0.8399999737739563, time = 0.01309061050415039\n",
      "Training at step=58, batch=240, train loss = 0.4253457669066427, train acc = 0.8450000286102295, time = 0.012844085693359375\n",
      "Testing at step=58, batch=0, test loss = 0.5077937848294768, test acc = 0.8149999976158142, time = 0.002374410629272461\n",
      "Testing at step=58, batch=10, test loss = 0.4994884123747483, test acc = 0.8199999928474426, time = 0.002361774444580078\n",
      "Testing at step=58, batch=20, test loss = 0.3708661704903499, test acc = 0.8650000095367432, time = 0.0023806095123291016\n",
      "Testing at step=58, batch=30, test loss = 0.574047395838544, test acc = 0.7850000262260437, time = 0.002351045608520508\n",
      "Testing at step=58, batch=40, test loss = 0.3667121333637071, test acc = 0.8899999856948853, time = 0.002366781234741211\n",
      "Step 58 finished in 14.149572610855103, Train loss = 0.3771098482622702, Test loss = 0.4366676115119188; Train Acc = 0.8683666670322419, Test Acc = 0.8438000011444092\n",
      "Training at step=59, batch=0, train loss = 0.3835310084459918, train acc = 0.8450000286102295, time = 0.013476371765136719\n",
      "Training at step=59, batch=60, train loss = 0.37121863967908114, train acc = 0.8349999785423279, time = 0.013014554977416992\n",
      "Training at step=59, batch=120, train loss = 0.3515583044559543, train acc = 0.8799999952316284, time = 0.01301264762878418\n",
      "Training at step=59, batch=180, train loss = 0.35959495706605776, train acc = 0.8600000143051147, time = 0.013042449951171875\n",
      "Training at step=59, batch=240, train loss = 0.4356208448797318, train acc = 0.8550000190734863, time = 0.013002157211303711\n",
      "Testing at step=59, batch=0, test loss = 0.4214034792053555, test acc = 0.8500000238418579, time = 0.0024073123931884766\n",
      "Testing at step=59, batch=10, test loss = 0.502609231363583, test acc = 0.8500000238418579, time = 0.002354145050048828\n",
      "Testing at step=59, batch=20, test loss = 0.5574598346852432, test acc = 0.7850000262260437, time = 0.0024073123931884766\n",
      "Testing at step=59, batch=30, test loss = 0.4119919555885967, test acc = 0.8450000286102295, time = 0.0024056434631347656\n",
      "Testing at step=59, batch=40, test loss = 0.5007528862185862, test acc = 0.8500000238418579, time = 0.002357959747314453\n",
      "Step 59 finished in 13.647765874862671, Train loss = 0.376952235762896, Test loss = 0.4388034877945179; Train Acc = 0.8682000001271566, Test Acc = 0.8424000024795533\n",
      "Training at step=60, batch=0, train loss = 0.45766331378404695, train acc = 0.8299999833106995, time = 0.013129949569702148\n",
      "Training at step=60, batch=60, train loss = 0.41217819290610325, train acc = 0.8399999737739563, time = 0.012916326522827148\n",
      "Training at step=60, batch=120, train loss = 0.35769396743040416, train acc = 0.8650000095367432, time = 0.012616157531738281\n",
      "Training at step=60, batch=180, train loss = 0.4133404675860112, train acc = 0.8600000143051147, time = 0.012773275375366211\n",
      "Training at step=60, batch=240, train loss = 0.4066547287944184, train acc = 0.875, time = 0.012674808502197266\n",
      "Testing at step=60, batch=0, test loss = 0.41487959785674916, test acc = 0.8450000286102295, time = 0.0024139881134033203\n",
      "Testing at step=60, batch=10, test loss = 0.42847410439821226, test acc = 0.8450000286102295, time = 0.002321958541870117\n",
      "Testing at step=60, batch=20, test loss = 0.34476674939521196, test acc = 0.875, time = 0.0023865699768066406\n",
      "Testing at step=60, batch=30, test loss = 0.4936427855599035, test acc = 0.8050000071525574, time = 0.002446413040161133\n",
      "Testing at step=60, batch=40, test loss = 0.421742804454301, test acc = 0.8349999785423279, time = 0.0024297237396240234\n",
      "Step 60 finished in 13.540079832077026, Train loss = 0.37735271443126245, Test loss = 0.4420439771829367; Train Acc = 0.8682000005245208, Test Acc = 0.8400000035762787\n",
      "Training at step=61, batch=0, train loss = 0.3269029185592521, train acc = 0.8600000143051147, time = 0.013154745101928711\n",
      "Training at step=61, batch=60, train loss = 0.37794267761842065, train acc = 0.8799999952316284, time = 0.013014554977416992\n",
      "Training at step=61, batch=120, train loss = 0.4667259638586472, train acc = 0.824999988079071, time = 0.01309823989868164\n",
      "Training at step=61, batch=180, train loss = 0.3828197530117481, train acc = 0.8450000286102295, time = 0.013049125671386719\n",
      "Training at step=61, batch=240, train loss = 0.34840524525923977, train acc = 0.8899999856948853, time = 0.01300191879272461\n",
      "Testing at step=61, batch=0, test loss = 0.47628120455881406, test acc = 0.8349999785423279, time = 0.002346515655517578\n",
      "Testing at step=61, batch=10, test loss = 0.3343809200211141, test acc = 0.8799999952316284, time = 0.0024194717407226562\n",
      "Testing at step=61, batch=20, test loss = 0.4463738471153284, test acc = 0.8450000286102295, time = 0.0023703575134277344\n",
      "Testing at step=61, batch=30, test loss = 0.5171266106805615, test acc = 0.7950000166893005, time = 0.0023691654205322266\n",
      "Testing at step=61, batch=40, test loss = 0.6699849811538009, test acc = 0.7950000166893005, time = 0.0023658275604248047\n",
      "Step 61 finished in 14.128434181213379, Train loss = 0.37702249341542093, Test loss = 0.4396994761736239; Train Acc = 0.8683666672309239, Test Acc = 0.841300002336502\n",
      "Training at step=62, batch=0, train loss = 0.4418174924074201, train acc = 0.8550000190734863, time = 0.013072013854980469\n",
      "Training at step=62, batch=60, train loss = 0.4427073971669592, train acc = 0.8450000286102295, time = 0.012899398803710938\n",
      "Training at step=62, batch=120, train loss = 0.5050071443165538, train acc = 0.8399999737739563, time = 0.01288461685180664\n",
      "Training at step=62, batch=180, train loss = 0.3476617242473614, train acc = 0.8799999952316284, time = 0.012779474258422852\n",
      "Training at step=62, batch=240, train loss = 0.4496696372339273, train acc = 0.8199999928474426, time = 0.012967824935913086\n",
      "Testing at step=62, batch=0, test loss = 0.4493962815248322, test acc = 0.824999988079071, time = 0.002371072769165039\n",
      "Testing at step=62, batch=10, test loss = 0.5037416625253652, test acc = 0.8349999785423279, time = 0.0023779869079589844\n",
      "Testing at step=62, batch=20, test loss = 0.4011258228839797, test acc = 0.8550000190734863, time = 0.0023610591888427734\n",
      "Testing at step=62, batch=30, test loss = 0.4360887883526199, test acc = 0.824999988079071, time = 0.0023436546325683594\n",
      "Testing at step=62, batch=40, test loss = 0.38566998217576703, test acc = 0.8650000095367432, time = 0.0023703575134277344\n",
      "Step 62 finished in 13.656200408935547, Train loss = 0.3761888553714344, Test loss = 0.43797011398648145; Train Acc = 0.8688833345969518, Test Acc = 0.8436000037193299\n",
      "Training at step=63, batch=0, train loss = 0.38362944877142063, train acc = 0.8500000238418579, time = 0.013091802597045898\n",
      "Training at step=63, batch=60, train loss = 0.3836609906906475, train acc = 0.8550000190734863, time = 0.012927532196044922\n",
      "Training at step=63, batch=120, train loss = 0.3848556285110521, train acc = 0.8600000143051147, time = 0.012964487075805664\n",
      "Training at step=63, batch=180, train loss = 0.3366880154867459, train acc = 0.8899999856948853, time = 0.012896299362182617\n",
      "Training at step=63, batch=240, train loss = 0.3417770767128825, train acc = 0.8899999856948853, time = 0.012883901596069336\n",
      "Testing at step=63, batch=0, test loss = 0.441639024710103, test acc = 0.8500000238418579, time = 0.0023839473724365234\n",
      "Testing at step=63, batch=10, test loss = 0.4062375183183297, test acc = 0.8600000143051147, time = 0.002231597900390625\n",
      "Testing at step=63, batch=20, test loss = 0.3635258030709074, test acc = 0.875, time = 0.0022745132446289062\n",
      "Testing at step=63, batch=30, test loss = 0.39714693662131334, test acc = 0.8550000190734863, time = 0.002201080322265625\n",
      "Testing at step=63, batch=40, test loss = 0.5152737156754512, test acc = 0.7900000214576721, time = 0.0022869110107421875\n",
      "Step 63 finished in 13.880471467971802, Train loss = 0.37574837896811336, Test loss = 0.43706570060844313; Train Acc = 0.8687833336989085, Test Acc = 0.8423000025749207\n",
      "Training at step=64, batch=0, train loss = 0.4308412917112674, train acc = 0.875, time = 0.012861013412475586\n",
      "Training at step=64, batch=60, train loss = 0.3274086161228273, train acc = 0.8799999952316284, time = 0.012854576110839844\n",
      "Training at step=64, batch=120, train loss = 0.3076968959326862, train acc = 0.8899999856948853, time = 0.012989044189453125\n",
      "Training at step=64, batch=180, train loss = 0.3687627535097046, train acc = 0.8550000190734863, time = 0.013161897659301758\n",
      "Training at step=64, batch=240, train loss = 0.30575804567776815, train acc = 0.8849999904632568, time = 0.01336359977722168\n",
      "Testing at step=64, batch=0, test loss = 0.47852037296035016, test acc = 0.8399999737739563, time = 0.002435445785522461\n",
      "Testing at step=64, batch=10, test loss = 0.42389340024394784, test acc = 0.8399999737739563, time = 0.002427339553833008\n",
      "Testing at step=64, batch=20, test loss = 0.4705325701093069, test acc = 0.8500000238418579, time = 0.0024557113647460938\n",
      "Testing at step=64, batch=30, test loss = 0.3804171647360033, test acc = 0.8650000095367432, time = 0.0024671554565429688\n",
      "Testing at step=64, batch=40, test loss = 0.2992579093964037, test acc = 0.8799999952316284, time = 0.0023107528686523438\n",
      "Step 64 finished in 13.701069116592407, Train loss = 0.37537488253049434, Test loss = 0.43837616346131036; Train Acc = 0.8690833340088526, Test Acc = 0.8442000031471253\n",
      "Training at step=65, batch=0, train loss = 0.3943265949583119, train acc = 0.8399999737739563, time = 0.013152360916137695\n",
      "Training at step=65, batch=60, train loss = 0.4001385884443253, train acc = 0.8500000238418579, time = 0.013102293014526367\n",
      "Training at step=65, batch=120, train loss = 0.3438387112580875, train acc = 0.8650000095367432, time = 0.013129472732543945\n",
      "Training at step=65, batch=180, train loss = 0.32607617415505274, train acc = 0.8899999856948853, time = 0.013136863708496094\n",
      "Training at step=65, batch=240, train loss = 0.4681046532163701, train acc = 0.8450000286102295, time = 0.013252735137939453\n",
      "Testing at step=65, batch=0, test loss = 0.5484327676731445, test acc = 0.8149999976158142, time = 0.002421140670776367\n",
      "Testing at step=65, batch=10, test loss = 0.3608034102429835, test acc = 0.8650000095367432, time = 0.0024657249450683594\n",
      "Testing at step=65, batch=20, test loss = 0.36579975736669396, test acc = 0.8550000190734863, time = 0.0023827552795410156\n",
      "Testing at step=65, batch=30, test loss = 0.4083557614269842, test acc = 0.8299999833106995, time = 0.002376556396484375\n",
      "Testing at step=65, batch=40, test loss = 0.5129459566599207, test acc = 0.824999988079071, time = 0.0022830963134765625\n",
      "Step 65 finished in 14.062120914459229, Train loss = 0.37434479122996833, Test loss = 0.43833668765865064; Train Acc = 0.8691000020503998, Test Acc = 0.843600002527237\n",
      "Training at step=66, batch=0, train loss = 0.346111130000646, train acc = 0.8700000047683716, time = 0.013120174407958984\n",
      "Training at step=66, batch=60, train loss = 0.42599034879081865, train acc = 0.8550000190734863, time = 0.013021469116210938\n",
      "Training at step=66, batch=120, train loss = 0.4792904200320604, train acc = 0.8199999928474426, time = 0.013013601303100586\n",
      "Training at step=66, batch=180, train loss = 0.3690089740114741, train acc = 0.8700000047683716, time = 0.01313924789428711\n",
      "Training at step=66, batch=240, train loss = 0.32194508368328184, train acc = 0.8550000190734863, time = 0.012972593307495117\n",
      "Testing at step=66, batch=0, test loss = 0.4102273454363494, test acc = 0.8450000286102295, time = 0.002390623092651367\n",
      "Testing at step=66, batch=10, test loss = 0.4620213597440443, test acc = 0.8450000286102295, time = 0.0024034976959228516\n",
      "Testing at step=66, batch=20, test loss = 0.3533869228705944, test acc = 0.9049999713897705, time = 0.0023453235626220703\n",
      "Testing at step=66, batch=30, test loss = 0.3958821560127908, test acc = 0.8500000238418579, time = 0.002392292022705078\n",
      "Testing at step=66, batch=40, test loss = 0.3590210477770587, test acc = 0.8650000095367432, time = 0.002397298812866211\n",
      "Step 66 finished in 13.849178791046143, Train loss = 0.374446207927062, Test loss = 0.43789218687411285; Train Acc = 0.8691333339611689, Test Acc = 0.8454000008106232\n",
      "Training at step=67, batch=0, train loss = 0.4372336467272973, train acc = 0.8500000238418579, time = 0.013036489486694336\n",
      "Training at step=67, batch=60, train loss = 0.3557488264239012, train acc = 0.8450000286102295, time = 0.012828588485717773\n",
      "Training at step=67, batch=120, train loss = 0.3282955121112468, train acc = 0.8899999856948853, time = 0.01282811164855957\n",
      "Training at step=67, batch=180, train loss = 0.3801288952232998, train acc = 0.8550000190734863, time = 0.013148784637451172\n",
      "Training at step=67, batch=240, train loss = 0.4330138386633685, train acc = 0.8799999952316284, time = 0.012877702713012695\n",
      "Testing at step=67, batch=0, test loss = 0.5175686635149522, test acc = 0.824999988079071, time = 0.002397298812866211\n",
      "Testing at step=67, batch=10, test loss = 0.3630130053085716, test acc = 0.8450000286102295, time = 0.002437591552734375\n",
      "Testing at step=67, batch=20, test loss = 0.3938155872095244, test acc = 0.8500000238418579, time = 0.002404451370239258\n",
      "Testing at step=67, batch=30, test loss = 0.4642887983924263, test acc = 0.8149999976158142, time = 0.0022406578063964844\n",
      "Testing at step=67, batch=40, test loss = 0.37375493153585937, test acc = 0.8600000143051147, time = 0.002388477325439453\n",
      "Step 67 finished in 14.538819074630737, Train loss = 0.3741419418119918, Test loss = 0.4370749434883308; Train Acc = 0.8695333329836528, Test Acc = 0.8430000078678132\n",
      "Training at step=68, batch=0, train loss = 0.3491688249601336, train acc = 0.8700000047683716, time = 0.013399839401245117\n",
      "Training at step=68, batch=60, train loss = 0.31096234257575595, train acc = 0.8949999809265137, time = 0.013164758682250977\n",
      "Training at step=68, batch=120, train loss = 0.30500264725613335, train acc = 0.8999999761581421, time = 0.01314401626586914\n",
      "Training at step=68, batch=180, train loss = 0.33708411853042536, train acc = 0.8899999856948853, time = 0.012840747833251953\n",
      "Training at step=68, batch=240, train loss = 0.405152769274607, train acc = 0.8399999737739563, time = 0.013092041015625\n",
      "Testing at step=68, batch=0, test loss = 0.3884291539521342, test acc = 0.8550000190734863, time = 0.002411365509033203\n",
      "Testing at step=68, batch=10, test loss = 0.38603739538628995, test acc = 0.8550000190734863, time = 0.002340555191040039\n",
      "Testing at step=68, batch=20, test loss = 0.41494248832052166, test acc = 0.8450000286102295, time = 0.002391815185546875\n",
      "Testing at step=68, batch=30, test loss = 0.3901767988754473, test acc = 0.8700000047683716, time = 0.002368450164794922\n",
      "Testing at step=68, batch=40, test loss = 0.39612040040749114, test acc = 0.8799999952316284, time = 0.002335071563720703\n",
      "Step 68 finished in 14.107151746749878, Train loss = 0.3741457205735778, Test loss = 0.44123165957895893; Train Acc = 0.8691833319266637, Test Acc = 0.8433000004291534\n",
      "Training at step=69, batch=0, train loss = 0.29981295169118627, train acc = 0.9150000214576721, time = 0.013014554977416992\n",
      "Training at step=69, batch=60, train loss = 0.34525179600587824, train acc = 0.875, time = 0.013166189193725586\n",
      "Training at step=69, batch=120, train loss = 0.5073352773867267, train acc = 0.8299999833106995, time = 0.012968778610229492\n",
      "Training at step=69, batch=180, train loss = 0.3266122371978082, train acc = 0.8600000143051147, time = 0.013022661209106445\n",
      "Training at step=69, batch=240, train loss = 0.36003319714930704, train acc = 0.8799999952316284, time = 0.013030290603637695\n",
      "Testing at step=69, batch=0, test loss = 0.34975978997623464, test acc = 0.875, time = 0.0024149417877197266\n",
      "Testing at step=69, batch=10, test loss = 0.3134516864619292, test acc = 0.8849999904632568, time = 0.0023086071014404297\n",
      "Testing at step=69, batch=20, test loss = 0.4561816496826787, test acc = 0.8399999737739563, time = 0.0023374557495117188\n",
      "Testing at step=69, batch=30, test loss = 0.4616853122366637, test acc = 0.8299999833106995, time = 0.002352476119995117\n",
      "Testing at step=69, batch=40, test loss = 0.4887793688572985, test acc = 0.8399999737739563, time = 0.0023114681243896484\n",
      "Step 69 finished in 13.942866086959839, Train loss = 0.37282221361502704, Test loss = 0.4408839504210882; Train Acc = 0.8698666663964589, Test Acc = 0.842700002193451\n",
      "Training at step=70, batch=0, train loss = 0.31326089832296317, train acc = 0.8700000047683716, time = 0.0131072998046875\n",
      "Training at step=70, batch=60, train loss = 0.3380122533040991, train acc = 0.8600000143051147, time = 0.01304006576538086\n",
      "Training at step=70, batch=120, train loss = 0.46476491618470284, train acc = 0.8550000190734863, time = 0.012963294982910156\n",
      "Training at step=70, batch=180, train loss = 0.3646817171070003, train acc = 0.8799999952316284, time = 0.012938261032104492\n",
      "Training at step=70, batch=240, train loss = 0.39966721056663895, train acc = 0.8399999737739563, time = 0.0131072998046875\n",
      "Testing at step=70, batch=0, test loss = 0.3453092118026161, test acc = 0.8650000095367432, time = 0.0024518966674804688\n",
      "Testing at step=70, batch=10, test loss = 0.496957348239006, test acc = 0.824999988079071, time = 0.0022711753845214844\n",
      "Testing at step=70, batch=20, test loss = 0.4897274624444601, test acc = 0.8500000238418579, time = 0.0023784637451171875\n",
      "Testing at step=70, batch=30, test loss = 0.40965794065757394, test acc = 0.8450000286102295, time = 0.002448558807373047\n",
      "Testing at step=70, batch=40, test loss = 0.4403173528640776, test acc = 0.8299999833106995, time = 0.0024411678314208984\n",
      "Step 70 finished in 13.907348394393921, Train loss = 0.3736943768536596, Test loss = 0.4393678868477799; Train Acc = 0.8697000010808309, Test Acc = 0.8416000056266785\n",
      "Training at step=71, batch=0, train loss = 0.35881157269803393, train acc = 0.875, time = 0.013161420822143555\n",
      "Training at step=71, batch=60, train loss = 0.3697063120557651, train acc = 0.8500000238418579, time = 0.01305246353149414\n",
      "Training at step=71, batch=120, train loss = 0.3840716698702276, train acc = 0.8899999856948853, time = 0.012987375259399414\n",
      "Training at step=71, batch=180, train loss = 0.3469029701713944, train acc = 0.8650000095367432, time = 0.012966632843017578\n",
      "Training at step=71, batch=240, train loss = 0.44803809742956796, train acc = 0.8600000143051147, time = 0.012984752655029297\n",
      "Testing at step=71, batch=0, test loss = 0.2893147256803957, test acc = 0.8999999761581421, time = 0.002290487289428711\n",
      "Testing at step=71, batch=10, test loss = 0.37834734365933415, test acc = 0.8600000143051147, time = 0.002332448959350586\n",
      "Testing at step=71, batch=20, test loss = 0.4620507236209398, test acc = 0.8500000238418579, time = 0.0022237300872802734\n",
      "Testing at step=71, batch=30, test loss = 0.4741699805286116, test acc = 0.8199999928474426, time = 0.002229928970336914\n",
      "Testing at step=71, batch=40, test loss = 0.35660020552328014, test acc = 0.8500000238418579, time = 0.002266407012939453\n",
      "Step 71 finished in 13.818110704421997, Train loss = 0.3727217784080598, Test loss = 0.4390271269618469; Train Acc = 0.8686333354314169, Test Acc = 0.8420000016689301\n",
      "Training at step=72, batch=0, train loss = 0.44185120656390175, train acc = 0.8650000095367432, time = 0.013094425201416016\n",
      "Training at step=72, batch=60, train loss = 0.4353043679497901, train acc = 0.8450000286102295, time = 0.01303243637084961\n",
      "Training at step=72, batch=120, train loss = 0.33406950484313674, train acc = 0.8849999904632568, time = 0.012908220291137695\n",
      "Training at step=72, batch=180, train loss = 0.32099055900429413, train acc = 0.8949999809265137, time = 0.012679576873779297\n",
      "Training at step=72, batch=240, train loss = 0.36879324891202514, train acc = 0.875, time = 0.01276397705078125\n",
      "Testing at step=72, batch=0, test loss = 0.5101661007313623, test acc = 0.8500000238418579, time = 0.002228260040283203\n",
      "Testing at step=72, batch=10, test loss = 0.460466673327001, test acc = 0.824999988079071, time = 0.0022478103637695312\n",
      "Testing at step=72, batch=20, test loss = 0.5110433641744426, test acc = 0.8299999833106995, time = 0.0023148059844970703\n",
      "Testing at step=72, batch=30, test loss = 0.3556319840214838, test acc = 0.8700000047683716, time = 0.0023348331451416016\n",
      "Testing at step=72, batch=40, test loss = 0.453179611116093, test acc = 0.824999988079071, time = 0.002333402633666992\n",
      "Step 72 finished in 13.767843246459961, Train loss = 0.3720020096285747, Test loss = 0.43906051401752594; Train Acc = 0.8697333341836929, Test Acc = 0.844000004529953\n",
      "Training at step=73, batch=0, train loss = 0.4247392297602807, train acc = 0.8450000286102295, time = 0.013033628463745117\n",
      "Training at step=73, batch=60, train loss = 0.35301683983951676, train acc = 0.824999988079071, time = 0.012836217880249023\n",
      "Training at step=73, batch=120, train loss = 0.35997044659240385, train acc = 0.8849999904632568, time = 0.012681245803833008\n",
      "Training at step=73, batch=180, train loss = 0.454789935624461, train acc = 0.824999988079071, time = 0.012944459915161133\n",
      "Training at step=73, batch=240, train loss = 0.3748256580606084, train acc = 0.8650000095367432, time = 0.013092994689941406\n",
      "Testing at step=73, batch=0, test loss = 0.4609091741091824, test acc = 0.8199999928474426, time = 0.002426624298095703\n",
      "Testing at step=73, batch=10, test loss = 0.28501893928836536, test acc = 0.875, time = 0.0023851394653320312\n",
      "Testing at step=73, batch=20, test loss = 0.3611814602791696, test acc = 0.8600000143051147, time = 0.0023779869079589844\n",
      "Testing at step=73, batch=30, test loss = 0.4399826541231877, test acc = 0.8299999833106995, time = 0.002403736114501953\n",
      "Testing at step=73, batch=40, test loss = 0.5403768826929111, test acc = 0.824999988079071, time = 0.0023641586303710938\n",
      "Step 73 finished in 13.673279047012329, Train loss = 0.3722804763764617, Test loss = 0.4470223752530779; Train Acc = 0.8696833324432373, Test Acc = 0.8388000023365021\n",
      "Training at step=74, batch=0, train loss = 0.3786897240075631, train acc = 0.8799999952316284, time = 0.013123512268066406\n",
      "Training at step=74, batch=60, train loss = 0.3411915168335924, train acc = 0.875, time = 0.01322317123413086\n",
      "Training at step=74, batch=120, train loss = 0.4245547336726816, train acc = 0.8550000190734863, time = 0.013030767440795898\n",
      "Training at step=74, batch=180, train loss = 0.4022018421728427, train acc = 0.8600000143051147, time = 0.012953996658325195\n",
      "Training at step=74, batch=240, train loss = 0.4048238977592191, train acc = 0.8550000190734863, time = 0.013025522232055664\n",
      "Testing at step=74, batch=0, test loss = 0.43145844160446545, test acc = 0.8700000047683716, time = 0.0023686885833740234\n",
      "Testing at step=74, batch=10, test loss = 0.5224973606607067, test acc = 0.8050000071525574, time = 0.002415180206298828\n",
      "Testing at step=74, batch=20, test loss = 0.46436767225734404, test acc = 0.8450000286102295, time = 0.002349853515625\n",
      "Testing at step=74, batch=30, test loss = 0.42749009040859226, test acc = 0.8399999737739563, time = 0.0023708343505859375\n",
      "Testing at step=74, batch=40, test loss = 0.4004120418168979, test acc = 0.8700000047683716, time = 0.0023984909057617188\n",
      "Step 74 finished in 13.841827392578125, Train loss = 0.37177235028539984, Test loss = 0.4386289214072648; Train Acc = 0.8700166682402293, Test Acc = 0.8428000009059906\n",
      "Training at step=75, batch=0, train loss = 0.39109107510780716, train acc = 0.8550000190734863, time = 0.012994050979614258\n",
      "Training at step=75, batch=60, train loss = 0.4487943026398289, train acc = 0.8450000286102295, time = 0.013161420822143555\n",
      "Training at step=75, batch=120, train loss = 0.3700126043506519, train acc = 0.8550000190734863, time = 0.013082504272460938\n",
      "Training at step=75, batch=180, train loss = 0.3527087914943789, train acc = 0.8949999809265137, time = 0.012906074523925781\n",
      "Training at step=75, batch=240, train loss = 0.40274300122547585, train acc = 0.8600000143051147, time = 0.012905597686767578\n",
      "Testing at step=75, batch=0, test loss = 0.5206466472832845, test acc = 0.8050000071525574, time = 0.0022513866424560547\n",
      "Testing at step=75, batch=10, test loss = 0.3631825555030613, test acc = 0.8450000286102295, time = 0.002279996871948242\n",
      "Testing at step=75, batch=20, test loss = 0.422188083408189, test acc = 0.8550000190734863, time = 0.0023636817932128906\n",
      "Testing at step=75, batch=30, test loss = 0.47922161694040893, test acc = 0.8399999737739563, time = 0.002197742462158203\n",
      "Testing at step=75, batch=40, test loss = 0.4499025591274549, test acc = 0.8399999737739563, time = 0.0022430419921875\n",
      "Step 75 finished in 14.098671913146973, Train loss = 0.37082382363430044, Test loss = 0.4416300117417347; Train Acc = 0.8703333330154419, Test Acc = 0.8415000021457673\n",
      "Training at step=76, batch=0, train loss = 0.3088274537244211, train acc = 0.8949999809265137, time = 0.012678146362304688\n",
      "Training at step=76, batch=60, train loss = 0.3830055365051577, train acc = 0.8600000143051147, time = 0.012939453125\n",
      "Training at step=76, batch=120, train loss = 0.4332694712537501, train acc = 0.8600000143051147, time = 0.012934446334838867\n",
      "Training at step=76, batch=180, train loss = 0.5209557169424455, train acc = 0.800000011920929, time = 0.013029813766479492\n",
      "Training at step=76, batch=240, train loss = 0.41851521611106945, train acc = 0.8849999904632568, time = 0.012991189956665039\n",
      "Testing at step=76, batch=0, test loss = 0.3503458597608964, test acc = 0.8700000047683716, time = 0.002363920211791992\n",
      "Testing at step=76, batch=10, test loss = 0.6248199419605899, test acc = 0.7900000214576721, time = 0.002643108367919922\n",
      "Testing at step=76, batch=20, test loss = 0.33901330756687087, test acc = 0.8949999809265137, time = 0.002498149871826172\n",
      "Testing at step=76, batch=30, test loss = 0.34088521245877645, test acc = 0.8450000286102295, time = 0.002446889877319336\n",
      "Testing at step=76, batch=40, test loss = 0.28645264718741437, test acc = 0.8849999904632568, time = 0.002501964569091797\n",
      "Step 76 finished in 13.878151416778564, Train loss = 0.37083089199887903, Test loss = 0.4386078605751909; Train Acc = 0.8699333345890046, Test Acc = 0.8446000027656555\n",
      "Training at step=77, batch=0, train loss = 0.44688736172890925, train acc = 0.8299999833106995, time = 0.013341188430786133\n",
      "Training at step=77, batch=60, train loss = 0.44218048292121304, train acc = 0.8349999785423279, time = 0.013120412826538086\n",
      "Training at step=77, batch=120, train loss = 0.34007206464092804, train acc = 0.8849999904632568, time = 0.013200521469116211\n",
      "Training at step=77, batch=180, train loss = 0.3810004976329995, train acc = 0.875, time = 0.01335287094116211\n",
      "Training at step=77, batch=240, train loss = 0.36450048308510596, train acc = 0.9100000262260437, time = 0.013094186782836914\n",
      "Testing at step=77, batch=0, test loss = 0.3827707317739727, test acc = 0.8349999785423279, time = 0.0024139881134033203\n",
      "Testing at step=77, batch=10, test loss = 0.34522809358825685, test acc = 0.875, time = 0.0023148059844970703\n",
      "Testing at step=77, batch=20, test loss = 0.5578964096133586, test acc = 0.7900000214576721, time = 0.0023865699768066406\n",
      "Testing at step=77, batch=30, test loss = 0.37136501694151464, test acc = 0.875, time = 0.002431631088256836\n",
      "Testing at step=77, batch=40, test loss = 0.4583203668274751, test acc = 0.8450000286102295, time = 0.0023882389068603516\n",
      "Step 77 finished in 14.787535190582275, Train loss = 0.3708915929242876, Test loss = 0.44043354729344364; Train Acc = 0.870450000166893, Test Acc = 0.8459000027179718\n",
      "Training at step=78, batch=0, train loss = 0.43600483374742965, train acc = 0.8650000095367432, time = 0.013180971145629883\n",
      "Training at step=78, batch=60, train loss = 0.25960312045430534, train acc = 0.9100000262260437, time = 0.012993335723876953\n",
      "Training at step=78, batch=120, train loss = 0.38519661322138143, train acc = 0.8550000190734863, time = 0.01293182373046875\n",
      "Training at step=78, batch=180, train loss = 0.5356841281565339, train acc = 0.8149999976158142, time = 0.012922048568725586\n",
      "Training at step=78, batch=240, train loss = 0.3237466966952151, train acc = 0.8999999761581421, time = 0.013149023056030273\n",
      "Testing at step=78, batch=0, test loss = 0.47475009062712786, test acc = 0.8349999785423279, time = 0.0023927688598632812\n",
      "Testing at step=78, batch=10, test loss = 0.30729152480667826, test acc = 0.8999999761581421, time = 0.002394437789916992\n",
      "Testing at step=78, batch=20, test loss = 0.41282522269377947, test acc = 0.8600000143051147, time = 0.0023670196533203125\n",
      "Testing at step=78, batch=30, test loss = 0.47860418337788696, test acc = 0.8100000023841858, time = 0.0023615360260009766\n",
      "Testing at step=78, batch=40, test loss = 0.38492783769124833, test acc = 0.8650000095367432, time = 0.0023665428161621094\n",
      "Step 78 finished in 13.825611114501953, Train loss = 0.37092147554755894, Test loss = 0.43927280737063923; Train Acc = 0.8699833349386851, Test Acc = 0.8443000030517578\n",
      "Training at step=79, batch=0, train loss = 0.3072642033654855, train acc = 0.8899999856948853, time = 0.013091325759887695\n",
      "Training at step=79, batch=60, train loss = 0.4576764127741012, train acc = 0.8450000286102295, time = 0.012911319732666016\n",
      "Training at step=79, batch=120, train loss = 0.2744706949598754, train acc = 0.9100000262260437, time = 0.01320791244506836\n",
      "Training at step=79, batch=180, train loss = 0.26202417697331015, train acc = 0.9100000262260437, time = 0.013007164001464844\n",
      "Training at step=79, batch=240, train loss = 0.35346545885446234, train acc = 0.8550000190734863, time = 0.012877464294433594\n",
      "Testing at step=79, batch=0, test loss = 0.3877513589704382, test acc = 0.8500000238418579, time = 0.002402067184448242\n",
      "Testing at step=79, batch=10, test loss = 0.37790993145564494, test acc = 0.8349999785423279, time = 0.0023508071899414062\n",
      "Testing at step=79, batch=20, test loss = 0.47319100707630113, test acc = 0.8399999737739563, time = 0.002298116683959961\n",
      "Testing at step=79, batch=30, test loss = 0.5004788465359844, test acc = 0.8550000190734863, time = 0.002357959747314453\n",
      "Testing at step=79, batch=40, test loss = 0.6089687804362427, test acc = 0.7850000262260437, time = 0.00234222412109375\n",
      "Step 79 finished in 13.928720951080322, Train loss = 0.37114479900573866, Test loss = 0.44356141605151267; Train Acc = 0.8698499997456869, Test Acc = 0.8401000022888183\n",
      "Training at step=80, batch=0, train loss = 0.40099761797317895, train acc = 0.8650000095367432, time = 0.013040781021118164\n",
      "Training at step=80, batch=60, train loss = 0.42957543540292525, train acc = 0.8399999737739563, time = 0.013071298599243164\n",
      "Training at step=80, batch=120, train loss = 0.31022845315527625, train acc = 0.8799999952316284, time = 0.013081789016723633\n",
      "Training at step=80, batch=180, train loss = 0.3891215478538229, train acc = 0.8550000190734863, time = 0.013088703155517578\n",
      "Training at step=80, batch=240, train loss = 0.36763211014547276, train acc = 0.8500000238418579, time = 0.01310420036315918\n",
      "Testing at step=80, batch=0, test loss = 0.4552452630513438, test acc = 0.8349999785423279, time = 0.00241851806640625\n",
      "Testing at step=80, batch=10, test loss = 0.4166969148251557, test acc = 0.875, time = 0.0023980140686035156\n",
      "Testing at step=80, batch=20, test loss = 0.4328193381179712, test acc = 0.8349999785423279, time = 0.002415180206298828\n",
      "Testing at step=80, batch=30, test loss = 0.4107125749853934, test acc = 0.824999988079071, time = 0.0023658275604248047\n",
      "Testing at step=80, batch=40, test loss = 0.5212869076802811, test acc = 0.8349999785423279, time = 0.002450704574584961\n",
      "Step 80 finished in 13.887831449508667, Train loss = 0.3697720550205518, Test loss = 0.442672274051543; Train Acc = 0.8701333336035411, Test Acc = 0.8430999958515167\n",
      "Training at step=81, batch=0, train loss = 0.3264812482352262, train acc = 0.8849999904632568, time = 0.01311802864074707\n",
      "Training at step=81, batch=60, train loss = 0.4079663256525022, train acc = 0.875, time = 0.013048410415649414\n",
      "Training at step=81, batch=120, train loss = 0.35439926405361905, train acc = 0.875, time = 0.013077020645141602\n",
      "Training at step=81, batch=180, train loss = 0.4029094565495599, train acc = 0.8500000238418579, time = 0.013068914413452148\n",
      "Training at step=81, batch=240, train loss = 0.3365347076296643, train acc = 0.9049999713897705, time = 0.012997627258300781\n",
      "Testing at step=81, batch=0, test loss = 0.4853395442548791, test acc = 0.8199999928474426, time = 0.0024156570434570312\n",
      "Testing at step=81, batch=10, test loss = 0.39876603003675837, test acc = 0.8550000190734863, time = 0.0024080276489257812\n",
      "Testing at step=81, batch=20, test loss = 0.4410176590814603, test acc = 0.8399999737739563, time = 0.0023581981658935547\n",
      "Testing at step=81, batch=30, test loss = 0.41772047130380285, test acc = 0.8500000238418579, time = 0.002401590347290039\n",
      "Testing at step=81, batch=40, test loss = 0.304991137054082, test acc = 0.8700000047683716, time = 0.0023195743560791016\n",
      "Step 81 finished in 13.818471908569336, Train loss = 0.3702262485933227, Test loss = 0.43933439253721956; Train Acc = 0.8702000004053115, Test Acc = 0.8425000035762786\n",
      "Training at step=82, batch=0, train loss = 0.44110446469588427, train acc = 0.8550000190734863, time = 0.013103246688842773\n",
      "Training at step=82, batch=60, train loss = 0.4388557610987063, train acc = 0.8500000238418579, time = 0.012839555740356445\n",
      "Training at step=82, batch=120, train loss = 0.31474317270204266, train acc = 0.875, time = 0.012867450714111328\n",
      "Training at step=82, batch=180, train loss = 0.2869274749910683, train acc = 0.8849999904632568, time = 0.012850522994995117\n",
      "Training at step=82, batch=240, train loss = 0.2545668605765334, train acc = 0.8949999809265137, time = 0.013106107711791992\n",
      "Testing at step=82, batch=0, test loss = 0.4620831441258584, test acc = 0.8550000190734863, time = 0.0024285316467285156\n",
      "Testing at step=82, batch=10, test loss = 0.496682468263119, test acc = 0.800000011920929, time = 0.002346038818359375\n",
      "Testing at step=82, batch=20, test loss = 0.42092261721519286, test acc = 0.8399999737739563, time = 0.0023546218872070312\n",
      "Testing at step=82, batch=30, test loss = 0.4191869780957366, test acc = 0.8450000286102295, time = 0.002409696578979492\n",
      "Testing at step=82, batch=40, test loss = 0.5000477084142119, test acc = 0.8450000286102295, time = 0.0024404525756835938\n",
      "Step 82 finished in 13.924907922744751, Train loss = 0.36977877575355955, Test loss = 0.43983236279842053; Train Acc = 0.8707666659355163, Test Acc = 0.8416000008583069\n",
      "Training at step=83, batch=0, train loss = 0.4302089014308227, train acc = 0.8600000143051147, time = 0.013205766677856445\n",
      "Training at step=83, batch=60, train loss = 0.3254334484668665, train acc = 0.8799999952316284, time = 0.012943267822265625\n",
      "Training at step=83, batch=120, train loss = 0.38982080635445526, train acc = 0.8650000095367432, time = 0.013268470764160156\n",
      "Training at step=83, batch=180, train loss = 0.3875373975168831, train acc = 0.8799999952316284, time = 0.013179540634155273\n",
      "Training at step=83, batch=240, train loss = 0.3356515504861832, train acc = 0.8949999809265137, time = 0.01339864730834961\n",
      "Testing at step=83, batch=0, test loss = 0.46580001482160005, test acc = 0.8349999785423279, time = 0.0025141239166259766\n",
      "Testing at step=83, batch=10, test loss = 0.3669784871959865, test acc = 0.8650000095367432, time = 0.0024764537811279297\n",
      "Testing at step=83, batch=20, test loss = 0.4882864553681163, test acc = 0.8050000071525574, time = 0.0024843215942382812\n",
      "Testing at step=83, batch=30, test loss = 0.5387621328237112, test acc = 0.824999988079071, time = 0.0024063587188720703\n",
      "Testing at step=83, batch=40, test loss = 0.3740035011123679, test acc = 0.8600000143051147, time = 0.00246429443359375\n",
      "Step 83 finished in 13.975316047668457, Train loss = 0.36869529938260004, Test loss = 0.4400897717542114; Train Acc = 0.8707666661341985, Test Acc = 0.8445000004768372\n",
      "Training at step=84, batch=0, train loss = 0.412834481237997, train acc = 0.8349999785423279, time = 0.013329505920410156\n",
      "Training at step=84, batch=60, train loss = 0.40660545061068776, train acc = 0.8899999856948853, time = 0.012961387634277344\n",
      "Training at step=84, batch=120, train loss = 0.3585052107289199, train acc = 0.8949999809265137, time = 0.012883424758911133\n",
      "Training at step=84, batch=180, train loss = 0.36747745121587805, train acc = 0.8949999809265137, time = 0.012993335723876953\n",
      "Training at step=84, batch=240, train loss = 0.32392128464017106, train acc = 0.8700000047683716, time = 0.013097763061523438\n",
      "Testing at step=84, batch=0, test loss = 0.3696355487302922, test acc = 0.8650000095367432, time = 0.0024666786193847656\n",
      "Testing at step=84, batch=10, test loss = 0.4132297782683016, test acc = 0.824999988079071, time = 0.002377033233642578\n",
      "Testing at step=84, batch=20, test loss = 0.43243817466213047, test acc = 0.8650000095367432, time = 0.0023987293243408203\n",
      "Testing at step=84, batch=30, test loss = 0.4290941315310974, test acc = 0.8500000238418579, time = 0.0024204254150390625\n",
      "Testing at step=84, batch=40, test loss = 0.46483966793644654, test acc = 0.8299999833106995, time = 0.002423524856567383\n",
      "Step 84 finished in 14.167789936065674, Train loss = 0.3690167235439731, Test loss = 0.4405700253215108; Train Acc = 0.8707333344221115, Test Acc = 0.842900003194809\n",
      "Training at step=85, batch=0, train loss = 0.3088744150946935, train acc = 0.9100000262260437, time = 0.01289820671081543\n",
      "Training at step=85, batch=60, train loss = 0.34044162384835586, train acc = 0.8650000095367432, time = 0.013026952743530273\n",
      "Training at step=85, batch=120, train loss = 0.27494039091839534, train acc = 0.9100000262260437, time = 0.012980461120605469\n",
      "Training at step=85, batch=180, train loss = 0.3477664674273647, train acc = 0.875, time = 0.013104915618896484\n",
      "Training at step=85, batch=240, train loss = 0.3876703494409472, train acc = 0.875, time = 0.013098955154418945\n",
      "Testing at step=85, batch=0, test loss = 0.5602706276744303, test acc = 0.824999988079071, time = 0.0023713111877441406\n",
      "Testing at step=85, batch=10, test loss = 0.4583125739126552, test acc = 0.8299999833106995, time = 0.0023169517517089844\n",
      "Testing at step=85, batch=20, test loss = 0.46263868220195364, test acc = 0.8450000286102295, time = 0.0022406578063964844\n",
      "Testing at step=85, batch=30, test loss = 0.5201600202125398, test acc = 0.8399999737739563, time = 0.0022363662719726562\n",
      "Testing at step=85, batch=40, test loss = 0.36825551478738083, test acc = 0.8600000143051147, time = 0.0022819042205810547\n",
      "Step 85 finished in 13.623806238174438, Train loss = 0.36840157552041386, Test loss = 0.4418875923612275; Train Acc = 0.8717666659752528, Test Acc = 0.8430999994277955\n",
      "Training at step=86, batch=0, train loss = 0.31934508135038603, train acc = 0.9100000262260437, time = 0.012979269027709961\n",
      "Training at step=86, batch=60, train loss = 0.28867347649894515, train acc = 0.925000011920929, time = 0.012951850891113281\n",
      "Training at step=86, batch=120, train loss = 0.349014677943074, train acc = 0.8450000286102295, time = 0.013118743896484375\n",
      "Training at step=86, batch=180, train loss = 0.36673311376642376, train acc = 0.8500000238418579, time = 0.012959718704223633\n",
      "Training at step=86, batch=240, train loss = 0.3960221817115108, train acc = 0.8500000238418579, time = 0.013103485107421875\n",
      "Testing at step=86, batch=0, test loss = 0.47519852493397324, test acc = 0.8450000286102295, time = 0.0024509429931640625\n",
      "Testing at step=86, batch=10, test loss = 0.5151654318746114, test acc = 0.800000011920929, time = 0.002431154251098633\n",
      "Testing at step=86, batch=20, test loss = 0.4527485956343131, test acc = 0.8299999833106995, time = 0.0025043487548828125\n",
      "Testing at step=86, batch=30, test loss = 0.45803984608254905, test acc = 0.8199999928474426, time = 0.002405405044555664\n",
      "Testing at step=86, batch=40, test loss = 0.4237890134718365, test acc = 0.8349999785423279, time = 0.0024271011352539062\n",
      "Step 86 finished in 14.20241641998291, Train loss = 0.36796803227931024, Test loss = 0.44416940407069666; Train Acc = 0.8709833325942358, Test Acc = 0.8390999960899354\n",
      "Training at step=87, batch=0, train loss = 0.34548928550384395, train acc = 0.8650000095367432, time = 0.013132333755493164\n",
      "Training at step=87, batch=60, train loss = 0.32849363008788884, train acc = 0.8700000047683716, time = 0.013123273849487305\n",
      "Training at step=87, batch=120, train loss = 0.3495724670842813, train acc = 0.8899999856948853, time = 0.013103246688842773\n",
      "Training at step=87, batch=180, train loss = 0.3147520460998809, train acc = 0.8999999761581421, time = 0.013193130493164062\n",
      "Training at step=87, batch=240, train loss = 0.30398837698403813, train acc = 0.8899999856948853, time = 0.012960433959960938\n",
      "Testing at step=87, batch=0, test loss = 0.38749742129201287, test acc = 0.8700000047683716, time = 0.002465486526489258\n",
      "Testing at step=87, batch=10, test loss = 0.46383596367946867, test acc = 0.8149999976158142, time = 0.0024194717407226562\n",
      "Testing at step=87, batch=20, test loss = 0.4119344402116256, test acc = 0.8600000143051147, time = 0.002414226531982422\n",
      "Testing at step=87, batch=30, test loss = 0.38964839772716353, test acc = 0.8650000095367432, time = 0.0023946762084960938\n",
      "Testing at step=87, batch=40, test loss = 0.40603668658310155, test acc = 0.8500000238418579, time = 0.002437591552734375\n",
      "Step 87 finished in 13.884594440460205, Train loss = 0.36823606022442124, Test loss = 0.44410307590699466; Train Acc = 0.870916666984558, Test Acc = 0.8414000010490418\n",
      "Training at step=88, batch=0, train loss = 0.3219276376309568, train acc = 0.8650000095367432, time = 0.013057708740234375\n",
      "Training at step=88, batch=60, train loss = 0.37634442949399344, train acc = 0.8399999737739563, time = 0.013110637664794922\n",
      "Training at step=88, batch=120, train loss = 0.430539596162313, train acc = 0.8399999737739563, time = 0.01313328742980957\n",
      "Training at step=88, batch=180, train loss = 0.2836443018021124, train acc = 0.9150000214576721, time = 0.013244390487670898\n",
      "Training at step=88, batch=240, train loss = 0.3012899319944399, train acc = 0.8949999809265137, time = 0.012996435165405273\n",
      "Testing at step=88, batch=0, test loss = 0.48980362886881446, test acc = 0.8399999737739563, time = 0.002413034439086914\n",
      "Testing at step=88, batch=10, test loss = 0.3876625218204042, test acc = 0.8700000047683716, time = 0.0024073123931884766\n",
      "Testing at step=88, batch=20, test loss = 0.4567768924225764, test acc = 0.824999988079071, time = 0.0024154186248779297\n",
      "Testing at step=88, batch=30, test loss = 0.4383322207559474, test acc = 0.8500000238418579, time = 0.0024127960205078125\n",
      "Testing at step=88, batch=40, test loss = 0.3992220186501143, test acc = 0.8299999833106995, time = 0.0024132728576660156\n",
      "Step 88 finished in 13.849678754806519, Train loss = 0.36761768632798086, Test loss = 0.4411567569356262; Train Acc = 0.8711166667938233, Test Acc = 0.8436999988555908\n",
      "Training at step=89, batch=0, train loss = 0.37533859529517705, train acc = 0.8799999952316284, time = 0.013042449951171875\n",
      "Training at step=89, batch=60, train loss = 0.3393751880399891, train acc = 0.8949999809265137, time = 0.012940645217895508\n",
      "Training at step=89, batch=120, train loss = 0.36726111132978195, train acc = 0.8700000047683716, time = 0.013131380081176758\n",
      "Training at step=89, batch=180, train loss = 0.36456783934490017, train acc = 0.8700000047683716, time = 0.013033390045166016\n",
      "Training at step=89, batch=240, train loss = 0.37000112118865125, train acc = 0.8500000238418579, time = 0.013236045837402344\n",
      "Testing at step=89, batch=0, test loss = 0.5427025492229047, test acc = 0.8399999737739563, time = 0.0024819374084472656\n",
      "Testing at step=89, batch=10, test loss = 0.4834054715900454, test acc = 0.8399999737739563, time = 0.0025141239166259766\n",
      "Testing at step=89, batch=20, test loss = 0.33339843915798517, test acc = 0.8799999952316284, time = 0.0024721622467041016\n",
      "Testing at step=89, batch=30, test loss = 0.4105324080027895, test acc = 0.8349999785423279, time = 0.0024509429931640625\n",
      "Testing at step=89, batch=40, test loss = 0.4292729715149051, test acc = 0.8550000190734863, time = 0.0024306774139404297\n",
      "Step 89 finished in 14.159249544143677, Train loss = 0.3677134551884453, Test loss = 0.4444018202836312; Train Acc = 0.871399998664856, Test Acc = 0.8422999989986419\n",
      "Training at step=90, batch=0, train loss = 0.3734157770810903, train acc = 0.8700000047683716, time = 0.01334834098815918\n",
      "Training at step=90, batch=60, train loss = 0.3609831509428977, train acc = 0.8550000190734863, time = 0.013002157211303711\n",
      "Training at step=90, batch=120, train loss = 0.4308847187041942, train acc = 0.8700000047683716, time = 0.013135671615600586\n",
      "Training at step=90, batch=180, train loss = 0.35831927590838747, train acc = 0.8949999809265137, time = 0.012901544570922852\n",
      "Training at step=90, batch=240, train loss = 0.3712703942374526, train acc = 0.8849999904632568, time = 0.012983083724975586\n",
      "Testing at step=90, batch=0, test loss = 0.4888448323812817, test acc = 0.824999988079071, time = 0.00243377685546875\n",
      "Testing at step=90, batch=10, test loss = 0.37997164471119865, test acc = 0.8650000095367432, time = 0.002445220947265625\n",
      "Testing at step=90, batch=20, test loss = 0.4002959643221551, test acc = 0.8650000095367432, time = 0.002408742904663086\n",
      "Testing at step=90, batch=30, test loss = 0.47074512740546576, test acc = 0.824999988079071, time = 0.0023746490478515625\n",
      "Testing at step=90, batch=40, test loss = 0.3265219729606177, test acc = 0.8849999904632568, time = 0.0024557113647460938\n",
      "Step 90 finished in 13.811470985412598, Train loss = 0.3677124052829755, Test loss = 0.4405501823528038; Train Acc = 0.871316667397817, Test Acc = 0.8421000003814697\n",
      "Training at step=91, batch=0, train loss = 0.43113817237927116, train acc = 0.8650000095367432, time = 0.012979507446289062\n",
      "Training at step=91, batch=60, train loss = 0.33023375861831694, train acc = 0.8899999856948853, time = 0.013183832168579102\n",
      "Training at step=91, batch=120, train loss = 0.28779663440540015, train acc = 0.9100000262260437, time = 0.013004064559936523\n",
      "Training at step=91, batch=180, train loss = 0.4006396944396309, train acc = 0.8849999904632568, time = 0.013168096542358398\n",
      "Training at step=91, batch=240, train loss = 0.36617943131958547, train acc = 0.8650000095367432, time = 0.013087987899780273\n",
      "Testing at step=91, batch=0, test loss = 0.3641995115703837, test acc = 0.875, time = 0.002347707748413086\n",
      "Testing at step=91, batch=10, test loss = 0.5166451812959394, test acc = 0.8149999976158142, time = 0.0023527145385742188\n",
      "Testing at step=91, batch=20, test loss = 0.49263366483961024, test acc = 0.7950000166893005, time = 0.002357006072998047\n",
      "Testing at step=91, batch=30, test loss = 0.5194380791420624, test acc = 0.8149999976158142, time = 0.0023725032806396484\n",
      "Testing at step=91, batch=40, test loss = 0.39506172714184973, test acc = 0.8650000095367432, time = 0.00240325927734375\n",
      "Step 91 finished in 14.002149105072021, Train loss = 0.36744647604305036, Test loss = 0.44089228109271256; Train Acc = 0.8717166660229365, Test Acc = 0.8429000008106232\n",
      "Training at step=92, batch=0, train loss = 0.35119484276789575, train acc = 0.875, time = 0.012912511825561523\n",
      "Training at step=92, batch=60, train loss = 0.38894734195597425, train acc = 0.8399999737739563, time = 0.012926340103149414\n",
      "Training at step=92, batch=120, train loss = 0.3183271354156767, train acc = 0.8849999904632568, time = 0.013082504272460938\n",
      "Training at step=92, batch=180, train loss = 0.3428552344376772, train acc = 0.8999999761581421, time = 0.01323843002319336\n",
      "Training at step=92, batch=240, train loss = 0.3339646077603553, train acc = 0.8899999856948853, time = 0.01326298713684082\n",
      "Testing at step=92, batch=0, test loss = 0.36212710371068546, test acc = 0.8650000095367432, time = 0.002432584762573242\n",
      "Testing at step=92, batch=10, test loss = 0.36562846854747927, test acc = 0.8650000095367432, time = 0.002408742904663086\n",
      "Testing at step=92, batch=20, test loss = 0.42470776370036895, test acc = 0.875, time = 0.002415895462036133\n",
      "Testing at step=92, batch=30, test loss = 0.39246736006712196, test acc = 0.875, time = 0.0022361278533935547\n",
      "Testing at step=92, batch=40, test loss = 0.4132362900835769, test acc = 0.8650000095367432, time = 0.0025217533111572266\n",
      "Step 92 finished in 13.725455284118652, Train loss = 0.3668319592725905, Test loss = 0.44069770031224187; Train Acc = 0.8710333327452342, Test Acc = 0.8425000023841858\n",
      "Training at step=93, batch=0, train loss = 0.3742252225290794, train acc = 0.8650000095367432, time = 0.013030290603637695\n",
      "Training at step=93, batch=60, train loss = 0.2641465515249857, train acc = 0.8999999761581421, time = 0.013044118881225586\n",
      "Training at step=93, batch=120, train loss = 0.3900465328603062, train acc = 0.8450000286102295, time = 0.01304006576538086\n",
      "Training at step=93, batch=180, train loss = 0.3044692342525674, train acc = 0.8999999761581421, time = 0.013112068176269531\n",
      "Training at step=93, batch=240, train loss = 0.39174839319803406, train acc = 0.8600000143051147, time = 0.013025283813476562\n",
      "Testing at step=93, batch=0, test loss = 0.5056117760594825, test acc = 0.8100000023841858, time = 0.002420663833618164\n",
      "Testing at step=93, batch=10, test loss = 0.43959916859070475, test acc = 0.8299999833106995, time = 0.002388477325439453\n",
      "Testing at step=93, batch=20, test loss = 0.47630788117305684, test acc = 0.8500000238418579, time = 0.0024552345275878906\n",
      "Testing at step=93, batch=30, test loss = 0.3467369458603695, test acc = 0.8650000095367432, time = 0.002395153045654297\n",
      "Testing at step=93, batch=40, test loss = 0.5138988079949626, test acc = 0.8100000023841858, time = 0.0024368762969970703\n",
      "Step 93 finished in 14.60533618927002, Train loss = 0.3667440013948221, Test loss = 0.44089302683950926; Train Acc = 0.8715166668097178, Test Acc = 0.842900003194809\n",
      "Training at step=94, batch=0, train loss = 0.3981059660053585, train acc = 0.8899999856948853, time = 0.013072729110717773\n",
      "Training at step=94, batch=60, train loss = 0.29691129718211207, train acc = 0.9100000262260437, time = 0.013081550598144531\n",
      "Training at step=94, batch=120, train loss = 0.3524844486867577, train acc = 0.8600000143051147, time = 0.013137340545654297\n",
      "Training at step=94, batch=180, train loss = 0.40672389408132015, train acc = 0.8650000095367432, time = 0.013162374496459961\n",
      "Training at step=94, batch=240, train loss = 0.35947502035621576, train acc = 0.8650000095367432, time = 0.013052940368652344\n",
      "Testing at step=94, batch=0, test loss = 0.43051160904122726, test acc = 0.8700000047683716, time = 0.0024220943450927734\n",
      "Testing at step=94, batch=10, test loss = 0.502170852187637, test acc = 0.8349999785423279, time = 0.0024633407592773438\n",
      "Testing at step=94, batch=20, test loss = 0.46622143854778186, test acc = 0.8500000238418579, time = 0.0024166107177734375\n",
      "Testing at step=94, batch=30, test loss = 0.3972893809342189, test acc = 0.8799999952316284, time = 0.002401590347290039\n",
      "Testing at step=94, batch=40, test loss = 0.4091459759336902, test acc = 0.8450000286102295, time = 0.0024373531341552734\n",
      "Step 94 finished in 14.679129838943481, Train loss = 0.36659049219479245, Test loss = 0.43993903288882996; Train Acc = 0.8717833334207534, Test Acc = 0.8442000043392182\n",
      "Training at step=95, batch=0, train loss = 0.40627832973693373, train acc = 0.8399999737739563, time = 0.013062238693237305\n",
      "Training at step=95, batch=60, train loss = 0.38607920699770565, train acc = 0.8550000190734863, time = 0.013019084930419922\n",
      "Training at step=95, batch=120, train loss = 0.2826133098202054, train acc = 0.8849999904632568, time = 0.012903928756713867\n",
      "Training at step=95, batch=180, train loss = 0.2691862691691867, train acc = 0.9049999713897705, time = 0.012987136840820312\n",
      "Training at step=95, batch=240, train loss = 0.43690446207515427, train acc = 0.8450000286102295, time = 0.013013839721679688\n",
      "Testing at step=95, batch=0, test loss = 0.38771405193490077, test acc = 0.8700000047683716, time = 0.002415180206298828\n",
      "Testing at step=95, batch=10, test loss = 0.438738621860425, test acc = 0.8450000286102295, time = 0.0024003982543945312\n",
      "Testing at step=95, batch=20, test loss = 0.5099354059078464, test acc = 0.8399999737739563, time = 0.0024347305297851562\n",
      "Testing at step=95, batch=30, test loss = 0.44435567858946745, test acc = 0.824999988079071, time = 0.0024139881134033203\n",
      "Testing at step=95, batch=40, test loss = 0.47473086860204466, test acc = 0.8149999976158142, time = 0.002443552017211914\n",
      "Step 95 finished in 14.179752111434937, Train loss = 0.36600148390286835, Test loss = 0.44227542421602206; Train Acc = 0.8711833329995473, Test Acc = 0.8437000012397766\n",
      "Training at step=96, batch=0, train loss = 0.3563691760818747, train acc = 0.8799999952316284, time = 0.013087034225463867\n",
      "Training at step=96, batch=60, train loss = 0.3421834853720413, train acc = 0.8899999856948853, time = 0.013096332550048828\n",
      "Training at step=96, batch=120, train loss = 0.3889291652093583, train acc = 0.8399999737739563, time = 0.013201236724853516\n",
      "Training at step=96, batch=180, train loss = 0.4310280843639896, train acc = 0.8450000286102295, time = 0.013197898864746094\n",
      "Training at step=96, batch=240, train loss = 0.4203175462760344, train acc = 0.8500000238418579, time = 0.012987613677978516\n",
      "Testing at step=96, batch=0, test loss = 0.41137206883223143, test acc = 0.8349999785423279, time = 0.002454996109008789\n",
      "Testing at step=96, batch=10, test loss = 0.4128632899213344, test acc = 0.8600000143051147, time = 0.002422332763671875\n",
      "Testing at step=96, batch=20, test loss = 0.3774145343883973, test acc = 0.8650000095367432, time = 0.002375364303588867\n",
      "Testing at step=96, batch=30, test loss = 0.3982822819997903, test acc = 0.8500000238418579, time = 0.0023648738861083984\n",
      "Testing at step=96, batch=40, test loss = 0.48018014522398045, test acc = 0.824999988079071, time = 0.002399921417236328\n",
      "Step 96 finished in 14.10643482208252, Train loss = 0.36595333685111825, Test loss = 0.44154241453160303; Train Acc = 0.8715333340565363, Test Acc = 0.8429000020027161\n",
      "Training at step=97, batch=0, train loss = 0.3713880963680406, train acc = 0.8700000047683716, time = 0.01309967041015625\n",
      "Training at step=97, batch=60, train loss = 0.4677522875318835, train acc = 0.8650000095367432, time = 0.013114690780639648\n",
      "Training at step=97, batch=120, train loss = 0.3301328500667966, train acc = 0.8700000047683716, time = 0.013142108917236328\n",
      "Training at step=97, batch=180, train loss = 0.36523168341181017, train acc = 0.8849999904632568, time = 0.01304316520690918\n",
      "Training at step=97, batch=240, train loss = 0.37026347058393677, train acc = 0.8700000047683716, time = 0.012984037399291992\n",
      "Testing at step=97, batch=0, test loss = 0.4362551908466938, test acc = 0.8349999785423279, time = 0.002382040023803711\n",
      "Testing at step=97, batch=10, test loss = 0.4091231354480827, test acc = 0.8500000238418579, time = 0.0024340152740478516\n",
      "Testing at step=97, batch=20, test loss = 0.42461033964542066, test acc = 0.8550000190734863, time = 0.002373218536376953\n",
      "Testing at step=97, batch=30, test loss = 0.4000035751672716, test acc = 0.8700000047683716, time = 0.002392292022705078\n",
      "Testing at step=97, batch=40, test loss = 0.5577229624713663, test acc = 0.8050000071525574, time = 0.0023729801177978516\n",
      "Step 97 finished in 13.891828775405884, Train loss = 0.36552810717775147, Test loss = 0.4406693224048258; Train Acc = 0.8716833345095316, Test Acc = 0.8445000040531159\n",
      "Training at step=98, batch=0, train loss = 0.46095993267976326, train acc = 0.8450000286102295, time = 0.013141155242919922\n",
      "Training at step=98, batch=60, train loss = 0.3951105138567484, train acc = 0.824999988079071, time = 0.013084173202514648\n",
      "Training at step=98, batch=120, train loss = 0.4293869824332376, train acc = 0.8700000047683716, time = 0.013164043426513672\n",
      "Training at step=98, batch=180, train loss = 0.3744393949039663, train acc = 0.8550000190734863, time = 0.014383077621459961\n",
      "Training at step=98, batch=240, train loss = 0.4009588708266501, train acc = 0.8550000190734863, time = 0.013084888458251953\n",
      "Testing at step=98, batch=0, test loss = 0.6000651671438701, test acc = 0.8199999928474426, time = 0.0023288726806640625\n",
      "Testing at step=98, batch=10, test loss = 0.49800650545126923, test acc = 0.8399999737739563, time = 0.002378702163696289\n",
      "Testing at step=98, batch=20, test loss = 0.48621116426883765, test acc = 0.8600000143051147, time = 0.0023779869079589844\n",
      "Testing at step=98, batch=30, test loss = 0.32637588785266575, test acc = 0.8849999904632568, time = 0.002278566360473633\n",
      "Testing at step=98, batch=40, test loss = 0.4526438486607085, test acc = 0.8349999785423279, time = 0.002307891845703125\n",
      "Step 98 finished in 13.996253252029419, Train loss = 0.36564207630445256, Test loss = 0.441969342221355; Train Acc = 0.8717499983310699, Test Acc = 0.8439000034332276\n",
      "Training at step=99, batch=0, train loss = 0.387251492978061, train acc = 0.8650000095367432, time = 0.01319265365600586\n",
      "Training at step=99, batch=60, train loss = 0.4159263578408774, train acc = 0.8349999785423279, time = 0.013018608093261719\n",
      "Training at step=99, batch=120, train loss = 0.2948007087544115, train acc = 0.875, time = 0.012813806533813477\n",
      "Training at step=99, batch=180, train loss = 0.3977652077398507, train acc = 0.8600000143051147, time = 0.012990236282348633\n",
      "Training at step=99, batch=240, train loss = 0.4559787582812842, train acc = 0.8500000238418579, time = 0.013086080551147461\n",
      "Testing at step=99, batch=0, test loss = 0.3768268503995267, test acc = 0.875, time = 0.002606630325317383\n",
      "Testing at step=99, batch=10, test loss = 0.43634608974243294, test acc = 0.8349999785423279, time = 0.002275228500366211\n",
      "Testing at step=99, batch=20, test loss = 0.37124244411985835, test acc = 0.875, time = 0.0022830963134765625\n",
      "Testing at step=99, batch=30, test loss = 0.560041403411303, test acc = 0.8450000286102295, time = 0.002255678176879883\n",
      "Testing at step=99, batch=40, test loss = 0.5357132714744112, test acc = 0.8050000071525574, time = 0.002326488494873047\n",
      "Step 99 finished in 13.901341438293457, Train loss = 0.36569347232465266, Test loss = 0.4435763383830323; Train Acc = 0.8720500006278356, Test Acc = 0.8432000017166138\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.2\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:47:09.750633Z",
     "start_time": "2024-04-06T20:47:09.194520Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 953,
     "status": "ok",
     "timestamp": 1712093845925,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "V7YYHs33saIx",
    "outputId": "928859bd-acd3-46ea-be13-39efadae9e50"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"0-fashionmnist_simple_cnn.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:43:06.955805Z",
     "start_time": "2024-04-06T20:43:06.810295Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"0-fashionmnist_simple_cnn.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
