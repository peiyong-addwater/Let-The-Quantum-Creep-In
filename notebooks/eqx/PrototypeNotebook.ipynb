{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1zOPcUIKgLHn7WRRRP9h95TkPBkzALMh5","authorship_tag":"ABX9TyOcwlTX36VjIk5KJQdhbs44"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eO_B09CrdTyK","executionInfo":{"status":"ok","timestamp":1713881604211,"user_tz":-600,"elapsed":6198,"user":{"displayName":"P. W.","userId":"06457912707533471190"}},"outputId":"f007ba8d-945d-481f-cb38-a7618a430569"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pennylane in /usr/local/lib/python3.10/dist-packages (0.35.1)\n","Requirement already satisfied: pennylane-lightning in /usr/local/lib/python3.10/dist-packages (0.35.1)\n","Requirement already satisfied: cotengra in /usr/local/lib/python3.10/dist-packages (0.6.0)\n","Requirement already satisfied: quimb in /usr/local/lib/python3.10/dist-packages (1.8.0)\n","Requirement already satisfied: equinox in /usr/local/lib/python3.10/dist-packages (0.11.4)\n","Requirement already satisfied: jaxtyping in /usr/local/lib/python3.10/dist-packages (0.2.28)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.11.4)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.3)\n","Requirement already satisfied: rustworkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.14.2)\n","Requirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n","Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.10.0)\n","Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.6.9)\n","Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.11.0)\n","Requirement already satisfied: pennylane-lightning-gpu in /usr/local/lib/python3.10/dist-packages (from pennylane-lightning) (0.35.1)\n","Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from quimb) (0.12.3)\n","Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.10/dist-packages (from quimb) (0.58.1)\n","Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from quimb) (5.9.5)\n","Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.10/dist-packages (from quimb) (4.66.2)\n","Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from equinox) (0.4.26)\n","Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping) (2.13.3)\n","Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.1)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->equinox) (0.2.0)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->equinox) (3.3.0)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.39->quimb) (0.41.1)\n","Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.2.2)\n"]}],"source":["!pip install pennylane pennylane-lightning pennylane-lightning[gpu] cotengra quimb equinox jaxtyping --upgrade"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import sys\n","import pandas as pd\n","import jax\n","import time\n","\n","from jaxtyping import Array, Float, Int, PyTree\n","import jax.numpy as jnp\n","import numpy as np\n","\n","import equinox as eqx\n","\n","import optax  # optimization using jax\n","\n","import torch  # https://pytorch.org\n","import torchvision  # https://pytorch.org\n","\n","import pennylane as qml\n","import pennylane.numpy as pnp\n","\n","import functools, itertools\n","\n","from jax.lib import xla_bridge\n","\n","#jax.config.update(\"jax_enable_x64\", True)\n","\n","def set_jax_platform():\n","    # Check if TPU is available\n","    try:\n","        tpu_backend = xla_bridge.get_backend('tpu')\n","        if tpu_backend and tpu_backend.device_count() > 0:\n","            # Set platform to TPU\n","            jax.config.update('jax_platform_name', 'tpu')\n","            print(\"Set platform to TPU\")\n","            return\n","    except RuntimeError:\n","        pass  # No TPU found, move on to check for GPU\n","\n","    # Check if GPU is available\n","    try:\n","      gpu_backend = xla_bridge.get_backend('gpu')\n","      if gpu_backend and gpu_backend.device_count() > 0:\n","          # Set platform to CUDA (GPU)\n","          jax.config.update('jax_platform_name', 'gpu')\n","          print(\"Set platform to GPU\")\n","    except RuntimeError:\n","          # Set platform to CPU\n","          jax.config.update('jax_platform_name', 'cpu')\n","          print(\"Set platform to CPU\")\n","\n","# Call the function to set the platform\n","set_jax_platform()\n","\n","seed = 1701\n","jrng_key = jax.random.PRNGKey(seed)\n","torch.manual_seed(seed)\n","print(jax.devices())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yYQfe8pdj9B4","executionInfo":{"status":"ok","timestamp":1713881609403,"user_tz":-600,"elapsed":5194,"user":{"displayName":"P. W.","userId":"06457912707533471190"}},"outputId":"6e38d06f-dc28-4a10-c91e-8df2358a0a1d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Set platform to GPU\n","[cuda(id=0)]\n"]}]},{"cell_type":"code","source":["# some training configuration\n","DATA = \"CIFAR10\" # \"MNIST\", \"FashionMNIST\", \"CIFAR10\"\n","REPLACEMENT_LVL = 2 # 0, 1, or 2\n","BATCH_SIZE = 200\n","EPOCHS = 100\n","LEARNING_RATE = 3E-4\n","REPEATS = 5"],"metadata":{"id":"DeNaIbvnlbzr","executionInfo":{"status":"ok","timestamp":1713881609403,"user_tz":-600,"elapsed":4,"user":{"displayName":"P. W.","userId":"06457912707533471190"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# utilities\n","ket = {\n","    '0':jnp.array([1,0]),\n","    '1':jnp.array([0,1]),\n","    '+':(jnp.array([1,0]) + jnp.array([0,1]))/jnp.sqrt(2),\n","    '-':(jnp.array([1,0]) - jnp.array([0,1]))/jnp.sqrt(2)\n","}\n","\n","pauli = {\n","    'I':jnp.array([[1,0],[0,1]]),\n","    'X':jnp.array([[0,1],[1,0]]),\n","    'Y':jnp.array([[0, -1j],[1j, 0]]),\n","    'Z':jnp.array([[1,0],[0,-1]])\n","}\n","\n","def tensor_product(*args):\n","  input_list = [a for a in args]\n","  return functools.reduce(jnp.kron, input_list)\n","\n","def multi_qubit_identity(n_qubits:int)->jnp.ndarray:\n","  assert n_qubits>0\n","  if n_qubits == 1:\n","    return pauli['I']\n","  else:\n","    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n","\n","def pauli_dict_func(key):\n","    return pauli[key]\n","\n","def pauli_dict_func_multiple_keys(keys):\n","    return list(map(pauli_dict_func, keys))\n","\n","def pauli_string_tensor_prod(pauli_string:str):\n","    paulis_char = list(pauli_string)\n","    paulis_mat = pauli_dict_func_multiple_keys(paulis_char)\n","    return tensor_product(*paulis_mat)\n","\n","def generate_nqubit_pauli_strings(n_qubits:int):\n","    assert n_qubits>0\n","    pauli_labels = ['I', 'X', 'Y', 'Z']\n","    pauli_strings = []\n","    for labels in itertools.product(pauli_labels, repeat=n_qubits):\n","        pauli_str = \"\".join(labels)\n","        if pauli_str != 'I'*n_qubits:\n","            pauli_strings.append(pauli_str)\n","    return pauli_strings\n","\n","def generate_pauli_tensor_list(pauli_strings:list):\n","    return list(map(pauli_string_tensor_prod, pauli_strings))\n","\n","su4_generators = generate_pauli_tensor_list(\n","    generate_nqubit_pauli_strings(2)\n",")\n","\n","su32_generators = generate_pauli_tensor_list(\n","    generate_nqubit_pauli_strings(5)\n",")\n","\n","su8_generators = generate_pauli_tensor_list(\n","    generate_nqubit_pauli_strings(3)\n",")\n","\n","su16_generators = generate_pauli_tensor_list(\n","    generate_nqubit_pauli_strings(4)\n",")\n","\n","def su32_op(\n","    params:jnp.ndarray\n","):\n","    generator = jnp.einsum(\"i, ijk - >jk\", params, jnp.asarray(su32_generators))\n","    return jax.scipy.linalg.expm(1j*generator)\n","\n","def su4_op(\n","    params:jnp.ndarray\n","):\n","    generator = jnp.einsum(\"i, ijk - >jk\", params, jnp.asarray(su4_generators))\n","    return jax.scipy.linalg.expm(1j*generator)\n","\n","def su8_op(\n","    params:jnp.ndarray\n","):\n","    generator = jnp.einsum(\"i, ijk - >jk\", params, jnp.asarray(su8_generators))\n","    return jax.scipy.linalg.expm(1j*generator)\n","\n","def su16_op(\n","    params:jnp.ndarray\n","):\n","    generator = jnp.einsum(\"i, ijk - >jk\", params, jnp.asarray(su16_generators))\n","    return jax.scipy.linalg.expm(1j*generator)\n","\n","def measure_sv(\n","    state:jnp.ndarray,\n","    observable:jnp.ndarray\n","    ):\n","  \"\"\"\n","  Measure a statevector with a Hermitian observable.\n","  Note: No checking Hermitianicity of the observable or whether the observable\n","  has all real eigenvalues or not\n","  \"\"\"\n","  expectation_value = jnp.dot(jnp.conj(state.T), jnp.dot(observable, state))\n","  return jnp.real(expectation_value)\n","\n","def measure_dm(\n","    rho:jnp.ndarray,\n","    observable:jnp.ndarray\n","):\n","  \"\"\"\n","  Measure a density matrix with a Hermitian observable.\n","  Note: No checking Hermitianicity of the observable or whether the observable\n","  has all real eigenvalues or not.\n","  \"\"\"\n","  product = jnp.dot(rho, observable)\n","\n","  # Calculate the trace, which is the sum of diagonal elements\n","  trace = jnp.trace(product)\n","\n","  # The expectation value should be real for physical observables\n","  return jnp.real(trace)\n","\n","# assuming the input patch (hermitianized) has shape (c, h, w)\n","# assuming the input set statevectors has shape (c, 2**n)\n","# assuming we have a list of (state, observable) pairs\n","vmap_measure_sv_ob_pairs = jax.vmap(lambda pair: measure_sv(pair[0], pair[1]), in_axes=0, out_axes=0)\n","# assuming the input set desnity matrices has shape (c, 2**n, 2**n)\n","# assuming we have a list of (rho, observable) pairs\n","vmap_measure_dm_ob_pairs = jax.vmap(lambda pair: measure_dm(pair[0], pair[1]), in_axes=0, out_axes=0)\n","\n","# vmap through different observables\n","vmap_measure_sv = jax.vmap(measure_sv, in_axes=(None, 0), out_axes=0)\n","vmap_measure_dm = jax.vmap(measure_dm, in_axes=(None, 0), out_axes=0)\n","\n","def bitstring_to_state(bitstring:str):\n","  \"\"\"\n","  Convert a bit string, like '0101001' or '+-+-101'\n","  to a statevector. Each character in the bitstring must be among\n","  0, 1, + and -\n","  \"\"\"\n","  assert len(bitstring)>0\n","  for c in bitstring:\n","    assert c in ['0', '1', '+', '-']\n","  single_qubit_states = [ket[c] for c in bitstring]\n","  return tensor_product(*single_qubit_states)\n","\n","\n","# utilities for the flipped quanvolution kernel\n","def extract_patches(image, patch_size, stride, padding=None):\n","    \"\"\"\n","    Extracts patches from an image with multiple input channels and optional custom padding.\n","\n","    Args:\n","        image (jnp.ndarray): Input image tensor of shape (in_channels, height, width).\n","        patch_size (int): Size of the square patches to extract.\n","        stride (int): Stride between patches.\n","        padding (tuple): Padding value(s) for each dimension.\n","\n","    Returns:\n","        jnp.ndarray: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n","    \"\"\"\n","\n","    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n","\n","    pad_h, pad_w = padding if padding is not None else (0, 0)\n","\n","\n","    image = jnp.pad(image, [(0, 0), (pad_h, pad_h), (pad_w, pad_w)], mode='constant') if padding is not None else image\n","\n","\n","    _, height, width = image.shape\n","\n","\n","    num_patches_h = (height - patch_size) // stride + 1\n","    num_patches_w = (width - patch_size) // stride + 1\n","\n","    patch_indices = [(i, j) for i in range(num_patches_h) for j in range(num_patches_w)]\n","\n","    patches = jnp.stack([image[:, i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n","                         for i, j in patch_indices])\n","\n","    return patches\n","\n","\n","def generate_2q_param_state(theta):\n","  state = bitstring_to_state('00')\n","  state = jnp.dot(\n","      su4_op(theta),\n","      state\n","  )\n","  return state\n","\n","vmap_generate_2q_param_state = jax.vmap(generate_2q_param_state, in_axes=0, out_axes = 0)\n","\n","# FlippedQuanv3x3 kernel\n","def single_kernel_op(thetas, patch):\n","  # patch has shape (c_in, h, w)\n","  # thetas has shape (c_in, 4^2-1) for SU4 gates\n","  n_theta = thetas.shape[0]\n","  n_channel = patch.shape[0]\n","  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n","  states = vmap_generate_2q_param_state(thetas)\n","  patch = jnp.pad(patch, [(0,0),(0,1),(0,1)], mode='constant')\n","  herm_patch = (jnp.einsum(\"ijk->ikj\", patch)+patch)/2\n","  channel_out = vmap_measure_sv_ob_pairs([states, herm_patch])\n","  return jnp.sum(channel_out, axis = 0)/n_theta\n","\n","vmap_single_kernel_op_through_extracted_patches = jax.vmap(single_kernel_op, in_axes=(None, 0), out_axes=0)\n","\n","# For multiple channel output\n","# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n","vmap_vmap_single_kernel_op_through_extracted_patches = jax.vmap(vmap_single_kernel_op_through_extracted_patches, in_axes=(0, None), out_axes=0)\n","\n","# Quantum version of the linear layer\n","# Realized with data reuploading and Hamiltonian embedding\n","# for input dimension D\n","# the quantum linear layer is a n = ceil(log4(D+1))-qubit quantum circuit\n","# Both the data encoding and the parameterised cirucit are achieved via the SU(2^n) unitary\n","\n","def data_encode_unitary(padded_data, t):\n","    #original_dim = 4**7 # fix to 7 qubits #padded_data.shape[-1]\n","    #new_dim = jnp.sqrt(original_dim).astype(jnp.int_)\n","    data = jnp.reshape(padded_data, (2**7, 2**7))\n","    generator = (data + jnp.einsum('...jk->...kj', data))/2\n","    return jax.scipy.linalg.expm(1.0j*t*generator)\n","\n","def su_n(params, pauli_string_tensor_list):\n","    paulis = jnp.asarray(pauli_string_tensor_list)\n","    generator = jnp.einsum(\"i, ijk -> jk\", params, paulis)\n","    return jax.scipy.linalg.expm(1.0j*generator)\n","\n","def linear_layer_func(\n","        padded_data,\n","        params,\n","        pauli_string_tensor_list,\n","        observables,\n","        n_qubits\n","):\n","    n_rep = params.shape[0]\n","    state = bitstring_to_state(\"+\" * n_qubits)\n","    data_unitary = data_encode_unitary(padded_data, 1.0/n_rep)\n","    for i in range(n_rep):\n","        state = jnp.dot(data_unitary, state)\n","        state = jnp.dot(su_n(params[i], pauli_string_tensor_list), state)\n","    return vmap_measure_sv(state, observables)\n","\n","vmap_batch_linear_layer_func = jax.vmap(linear_layer_func, in_axes=(0, None, None, None, None), out_axes=0)"],"metadata":{"id":"0tm-dsBaqjx0","executionInfo":{"status":"ok","timestamp":1713881610690,"user_tz":-600,"elapsed":1290,"user":{"displayName":"P. W.","userId":"06457912707533471190"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# models\n","class FlippedQuanv3x3(eqx.Module):\n","  weight: jax.Array\n","  bias: jax.Array\n","  stride: int\n","  pad: tuple|None\n","  pad_h: int\n","  pad_w: int\n","\n","  def __init__(self, in_channels, out_channels, stride, padding, key):\n","    wkey, bkey = jax.random.split(key,2)\n","    self.weight = jax.random.normal(shape=[out_channels, in_channels, 15], key=wkey)\n","    self.bias = jax.random.normal(shape=[out_channels, 1], key=bkey)\n","    self.stride = stride\n","    self.pad = padding\n","    self.pad_h, self.pad_w = padding if padding is not None else (0,0)\n","\n","  def __call__(self, x):\n","    # x has shape ( ,c_in, h, w)\n","    # weight has shape (c_out, c_in, 15)\n","    # bias has shape (c_out, 1)\n","    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n","    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n","    h_out = (h_in-3+2*self.pad_h)//self.stride +1\n","    w_out = (w_in-3+2*self.pad_w)//self.stride +1\n","    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n","    out = out + self.bias\n","    return out.reshape((-1, h_out, w_out))\n","\n","class DataReUploadingLinear(eqx.Module):\n","    weight: jax.Array\n","    bias: jax.Array\n","    n_reps:int\n","    in_dim:int\n","    out_dim:int\n","    n_qubits:int\n","\n","    def __init__(self, n_reps, key):\n","        #assert 2 ** n_qubits >= out_dim\n","        #assert 4 ** n_qubits >= in_dim\n","        wkey, bkey = jax.random.split(key, 2)\n","        self.in_dim = 16 * 28 * 28\n","        self.out_dim = 10\n","        self.n_qubits = 7\n","        # in_dim is 16 * 28 * 28\n","        # out_dim is 10\n","        self.n_reps = n_reps\n","        param_dim = 4 ** self.n_qubits - 1\n","        self.weight = jax.random.normal(shape=[self.n_reps, param_dim], key=wkey)\n","        self.bias = jax.random.normal(shape=[self.out_dim], key=bkey)\n","\n","    def generate_observables(self):\n","        observables = []\n","        for i in range(self.out_dim):\n","            temp_bitstring = '{0:b}'.format(i).zfill(self.n_qubits)\n","            ob = jnp.outer(bitstring_to_state(temp_bitstring), bitstring_to_state(temp_bitstring))\n","            observables.append(ob)\n","        return jnp.asarray(observables)\n","\n","    def get_pauli_string_tensor_list(self):\n","        return generate_pauli_tensor_list(generate_nqubit_pauli_strings(self.n_qubits))\n","\n","    #def get_pad_size(self):\n","    #    return 4 ** self.n_qubits - self.in_dim\n","\n","    def __call__(self, x):\n","        # x has size (batchsize, in_dim)\n","        # pad x\n","        x = jnp.pad(x, (0, 4 ** self.n_qubits - self.in_dim))\n","\n","        out = linear_layer_func(\n","            padded_data=x,\n","            params=self.weight,\n","            pauli_string_tensor_list=self.get_pauli_string_tensor_list(),\n","            observables=self.generate_observables(),\n","            n_qubits=self.n_qubits\n","        )\n","        out = out + self.bias\n","        return out\n","\n","class HybridNet(eqx.Module):\n","    #in_channels:int\n","    replacement_lvl:int\n","    layers: list\n","    def __init__(self, replacement_lvl, key):\n","        assert replacement_lvl in [0,1,2]\n","        #self.in_channels = in_channels\n","        self.replacement_lvl = replacement_lvl\n","        key1, key2, key3 = jax.random.split(key, 3)\n","        if self.replacement_lvl == 0:\n","            self.layers = [\n","                eqx.nn.Conv2d(3, 32, kernel_size=3, padding=0, key=key1),\n","                eqx.nn.Conv2d(32, 16, kernel_size=3, padding=0, key=key2),\n","                jnp.ravel,\n","                eqx.nn.Linear(16 * 28 * 28, 10, key=key3),\n","            ]\n","        elif self.replacement_lvl == 1:\n","            self.layers = [\n","                FlippedQuanv3x3(3, 32, stride=1, padding=(0, 0), key=key1),\n","                FlippedQuanv3x3(32, 16, stride=1, padding=(0, 0), key=key2),\n","                jnp.ravel,\n","                eqx.nn.Linear(16 * 28 * 28, 10, key=key3),\n","            ]\n","        elif self.replacement_lvl == 2:\n","            self.layers = [\n","                FlippedQuanv3x3(3, 32, stride=1, padding=(0, 0), key=key1),\n","                FlippedQuanv3x3(32, 16, stride=1, padding=(0, 0), key=key2),\n","                jnp.ravel,\n","                DataReUploadingLinear(n_reps=1, key=key3),\n","            ]\n","        else:\n","            raise ValueError(\"replacement_lvl should be 0, 1, or 2\")\n","\n","    def __call__(self, x:Float[Array, \"3 h w\"])->Float[Array, \"10\"]:\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x"],"metadata":{"id":"PMrF21Gbshok","executionInfo":{"status":"ok","timestamp":1713881610691,"user_tz":-600,"elapsed":4,"user":{"displayName":"P. W.","userId":"06457912707533471190"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["test_img = jnp.stack([jnp.arange(32*32*1, dtype=jnp.float_).reshape((32, 32))]*3*BATCH_SIZE, axis = 0).reshape((BATCH_SIZE,3,32,32))\n","print(test_img.shape)\n","for rpl_lvl in [0,1,2]:\n","  print(f\"Replacement lvl={rpl_lvl}\")\n","  start = time.time()\n","  model = HybridNet(replacement_lvl=rpl_lvl, key=jrng_key)\n","  print(model)\n","  test_out = jax.vmap(model)(test_img)\n","  print(test_out.shape)\n","  end = time.time()\n","  print(f\"Time taken: {end-start}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LISwYLcZtJ8q","executionInfo":{"status":"ok","timestamp":1713881631411,"user_tz":-600,"elapsed":20723,"user":{"displayName":"P. W.","userId":"06457912707533471190"}},"outputId":"c9ed0d73-b9ca-4f46-af7d-a5e2ea190b35"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-88142b8978e0>:1: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in arange is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n","  test_img = jnp.stack([jnp.arange(32*32*1, dtype=jnp.float_).reshape((32, 32))]*3*BATCH_SIZE, axis = 0).reshape((BATCH_SIZE,3,32,32))\n"]},{"output_type":"stream","name":"stdout","text":["(200, 3, 32, 32)\n","Replacement lvl=0\n","HybridNet(\n","  replacement_lvl=0,\n","  layers=[\n","    Conv2d(\n","      num_spatial_dims=2,\n","      weight=f32[32,3,3,3],\n","      bias=f32[32,1,1],\n","      in_channels=3,\n","      out_channels=32,\n","      kernel_size=(3, 3),\n","      stride=(1, 1),\n","      padding=((0, 0), (0, 0)),\n","      dilation=(1, 1),\n","      groups=1,\n","      use_bias=True,\n","      padding_mode='ZEROS'\n","    ),\n","    Conv2d(\n","      num_spatial_dims=2,\n","      weight=f32[16,32,3,3],\n","      bias=f32[16,1,1],\n","      in_channels=32,\n","      out_channels=16,\n","      kernel_size=(3, 3),\n","      stride=(1, 1),\n","      padding=((0, 0), (0, 0)),\n","      dilation=(1, 1),\n","      groups=1,\n","      use_bias=True,\n","      padding_mode='ZEROS'\n","    ),\n","    <wrapped function ravel>,\n","    Linear(\n","      weight=f32[10,12544],\n","      bias=f32[10],\n","      in_features=12544,\n","      out_features=10,\n","      use_bias=True\n","    )\n","  ]\n",")\n","(200, 10)\n","Time taken: 1.7253785133361816\n","Replacement lvl=1\n","HybridNet(\n","  replacement_lvl=1,\n","  layers=[\n","    FlippedQuanv3x3(\n","      weight=f32[32,3,15],\n","      bias=f32[32,1],\n","      stride=1,\n","      pad=(0, 0),\n","      pad_h=0,\n","      pad_w=0\n","    ),\n","    FlippedQuanv3x3(\n","      weight=f32[16,32,15],\n","      bias=f32[16,1],\n","      stride=1,\n","      pad=(0, 0),\n","      pad_h=0,\n","      pad_w=0\n","    ),\n","    <wrapped function ravel>,\n","    Linear(\n","      weight=f32[10,12544],\n","      bias=f32[10],\n","      in_features=12544,\n","      out_features=10,\n","      use_bias=True\n","    )\n","  ]\n",")\n","(200, 10)\n","Time taken: 7.141312837600708\n","Replacement lvl=2\n","HybridNet(\n","  replacement_lvl=2,\n","  layers=[\n","    FlippedQuanv3x3(\n","      weight=f32[32,3,15],\n","      bias=f32[32,1],\n","      stride=1,\n","      pad=(0, 0),\n","      pad_h=0,\n","      pad_w=0\n","    ),\n","    FlippedQuanv3x3(\n","      weight=f32[16,32,15],\n","      bias=f32[16,1],\n","      stride=1,\n","      pad=(0, 0),\n","      pad_h=0,\n","      pad_w=0\n","    ),\n","    <wrapped function ravel>,\n","    DataReUploadingLinear(\n","      weight=f32[1,16383],\n","      bias=f32[10],\n","      n_reps=1,\n","      in_dim=12544,\n","      out_dim=10,\n","      n_qubits=7\n","    )\n","  ]\n",")\n","(200, 10)\n","Time taken: 11.431694269180298\n"]}]},{"cell_type":"code","source":["# training utilities\n","start_compile = time.time()\n","@eqx.filter_jit\n","def compute_out(\n","    model:HybridNet,\n","    x:Float[Array, \"batch 3 32 32\"]\n",") -> Float[Array, \"batch 10\"]:\n","    return jax.vmap(model)(x)\n","\n","model = HybridNet(replacement_lvl=2, key=jrng_key)\n","test_out = compute_out(model, test_img)\n","end = time.time()\n","print(f\"Time taken: {end-start_compile}\")\n","print(test_out.shape)\n","\n","test_img2 = jnp.stack([jnp.arange(32*32*1, dtype=jnp.float_).reshape((32, 32))]*3*BATCH_SIZE, axis = 0).reshape((BATCH_SIZE,3,32,32))\n","print(test_img2.shape)\n","start = time.time()\n","test_out2 = compute_out(model, test_img2)\n","end = time.time()\n","print(f\"Time taken: {end-start}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sRQJk8PEPVLK","executionInfo":{"status":"ok","timestamp":1713882328117,"user_tz":-600,"elapsed":696714,"user":{"displayName":"P. W.","userId":"06457912707533471190"}},"outputId":"9d787d87-cc6b-4019-c74d-925e52feb746"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Time taken: 696.3979589939117\n","(200, 10)\n","(200, 3, 32, 32)\n","Time taken: 0.12333202362060547\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-7-efb4fe96703e>:16: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in arange is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n","  test_img2 = jnp.stack([jnp.arange(32*32*1, dtype=jnp.float_).reshape((32, 32))]*3*BATCH_SIZE, axis = 0).reshape((BATCH_SIZE,3,32,32))\n"]}]},{"cell_type":"code","source":["def loss_fn(\n","    pred_y:Float[Array, \"batch 10\"],\n","    y:Int[Array, \"batch\"]\n","):\n","  return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(pred_y, y))\n","\n","def accuracy_fn(\n","    pred_y:Float[Array, \"batch 10\"],\n","    y:Int[Array, \"batch\"]\n","):\n","  pred = jnp.argmax(pred_y, axis=1)\n","  return jnp.sum(jnp.array(pred == y).astype(int)) / len(pred_y)\n","\n","@eqx.filter_jit\n","def compute_loss(\n","    model: HybridNet, x: Float[Array, \"batch 3 32 32\"], y: Int[Array, \" batch\"]\n","):\n","  pred_y = compute_out(model, x)\n","  return loss_fn(pred_y, y)\n","\n","@eqx.filter_jit\n","def compute_accuracy(\n","    model: HybridNet, x: Float[Array, \"batch 3 32 32\"], y: Int[Array, \" batch\"]\n","):\n","  pred_y = compute_out(model, x)\n","  return accuracy_fn(pred_y, y)\n","\n","def evaluate(\n","    model: HybridNet,\n","    test_loader: torch.utils.data.DataLoader,\n","):\n","  avg_loss = 0\n","  avg_acc = 0\n","  for x, y in test_loader:\n","    x = x.numpy()\n","    y = y.numpy()\n","    avg_loss += compute_loss(model, x, y)\n","    avg_acc += compute_accuracy(model, x, y)\n","  return avg_loss / len(test_loader), avg_acc / len(test_loader)"],"metadata":{"id":"iE06OcdCIN5k","executionInfo":{"status":"ok","timestamp":1713882328117,"user_tz":-600,"elapsed":3,"user":{"displayName":"P. W.","userId":"06457912707533471190"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# training utilities\n","def get_train_test_data(name = \"MNIST\"):\n","  assert name in [\"MNIST\", \"CIFAR10\", \"FashionMNIST\"]\n","  if name  == \"CIFAR10\":\n","    preprocess = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize((0.5,), (0.5,))\n","    ])\n","  else:\n","    preprocess =  torchvision.transforms.Compose([\n","    torchvision.transforms.Pad(2),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize((0.5,), (0.5,))\n","    ])\n","  if name == \"MNIST\":\n","    train = torchvision.datasets.MNIST(\n","        root=\"MNIST\",\n","        train=True,\n","        transform=preprocess,\n","        download=True)\n","    test = torchvision.datasets.MNIST(\n","        root=\"MNIST\",\n","        train=False,\n","        transform=preprocess,\n","        download=True)\n","  elif name == \"CIFAR10\":\n","    train = torchvision.datasets.CIFAR10(\n","        root=\"CIFAR10\",\n","        train=True,\n","        transform=preprocess,\n","        download=True)\n","    test = torchvision.datasets.CIFAR10(\n","        root=\"CIFAR10\",\n","        train=False,\n","        transform=preprocess,\n","        download=True)\n","  elif name == \"FashionMNIST\":\n","    train = torchvision.datasets.FashionMNIST(\n","        root=\"FashionMNIST\",\n","        train=True,\n","        transform=preprocess,\n","        download=True)\n","    test = torchvision.datasets.FashionMNIST(\n","        root=\"FashionMNIST\",\n","        train=False,\n","        transform=preprocess,\n","        download=True)\n","  else:\n","    raise ValueError(\"name should be MNIST, CIFAR10, or FashionMNIST\")\n","  return train, test\n","\n","def train(\n","      model: HybridNet,\n","      data_name: str,\n","      batchsize: int,\n","      epochs: int,\n","      optim: optax.GradientTransformation,\n","  ):\n","\n","  train_loss = []\n","  train_acc = []\n","  test_loss = []\n","  test_acc = []\n","\n","  opt_state = optim.init(eqx.filter(model, eqx.is_array))\n","  train_data, test_data = get_train_test_data(data_name)\n","  trainloader = torch.utils.data.DataLoader(\n","      train_data, batch_size=batchsize, shuffle=True)\n","  test_loader = torch.utils.data.DataLoader(\n","      test_data, batch_size=batchsize, shuffle=False)\n","  if data_name == \"CIFAR10\":\n","    @eqx.filter_jit\n","    def make_step(\n","      model: HybridNet,\n","      opt_state: PyTree,\n","      x: Float[Array, \"batch 3 32 32\"],\n","      y: Int[Array, \"batch\"],\n","    ):\n","      loss_value, grads = eqx.filter_value_and_grad(compute_loss)(model, x, y)\n","      updates, opt_state = optim.update(grads, opt_state, model)\n","      model = eqx.apply_updates(model, updates)\n","      acc = compute_accuracy(model, x, y)\n","      return model, opt_state, loss_value, acc\n","\n","    @eqx.filter_jit\n","    def make_test_step(\n","        model:HybridNet,\n","        x: Float[Array, \"batch 3 32 32\"],\n","        y: Int[Array, \"batch\"],\n","    ):\n","      out = compute_out(model, x)\n","      loss_value = loss_fn(out, y)\n","      acc = accuracy_fn(out, y)\n","      return loss_value, acc\n","  else:\n","    @eqx.filter_jit\n","    def make_step(\n","      model: HybridNet,\n","      opt_state: PyTree,\n","      x: Float[Array, \"batch 1 32 32\"],\n","      y: Int[Array, \"batch\"],\n","    ):\n","      loss_value, grads = eqx.filter_value_and_grad(compute_loss)(model, x, y)\n","      updates, opt_state = optim.update(grads, opt_state, model)\n","      model = eqx.apply_updates(model, updates)\n","      acc = compute_accuracy(model, x, y)\n","      return model, opt_state, loss_value, acc\n","\n","    @eqx.filter_jit\n","    def make_test_step(\n","        model:HybridNet,\n","        x: Float[Array, \"batch 1 32 32\"],\n","        y: Int[Array, \"batch\"],\n","    ):\n","      out = compute_out(model, x)\n","      loss_value = loss_fn(out, y)\n","      acc = accuracy_fn(out, y)\n","      return loss_value, acc\n","\n","\n","  for step in range(epochs):\n","    step_start = time.time()\n","    batch_train_loss = []\n","    batch_train_acc = []\n","    batch_test_loss = []\n","    batch_test_acc = []\n","    for batchidx, (x, y) in enumerate(trainloader):\n","      x = x.numpy()\n","      y = y.numpy()\n","      model, opt_state, loss_value_batch, acc_value_batch = make_step(model, opt_state, x, y)\n","      batch_train_loss.append(loss_value_batch)\n","      batch_train_acc.append(acc_value_batch)\n","    loss_value = np.mean(batch_train_loss)\n","    acc_value = np.mean(batch_train_acc)\n","    train_loss.append(loss_value)\n","    train_acc.append(acc_value)\n","    train_time = time.time() - step_start\n","    print(f\"Train step {step}, loss = {loss_value:.4f}, acc = {acc_value:.4f}; train time = {train_time:.4f} seconds\")\n","\n","    for batchidx, (x, y) in enumerate(test_loader):\n","      x = x.numpy()\n","      y = y.numpy()\n","      loss_value_batch, acc_value_batch = make_test_step(model, x, y)\n","      batch_test_loss.append(loss_value_batch)\n","      batch_test_acc.append(acc_value_batch)\n","\n","    test_loss_value, test_acc_value = np.mean(batch_test_loss), np.mean(batch_test_acc)\n","    test_loss.append(test_loss_value)\n","    test_acc.append(test_acc_value)\n","    test_time = time.time() - step_start - train_time\n","    print(f\"Test loss = {test_loss_value:.4f}, acc = {test_acc_value:.4f}; test time = {test_time:.4f} seconds\")\n","    print(f\"Total time = {time.time() - step_start:.4f} seconds\")\n","\n","  return model, train_loss, train_acc, test_loss, test_acc"],"metadata":{"id":"1m05AZJS_h4l","executionInfo":{"status":"ok","timestamp":1713882328117,"user_tz":-600,"elapsed":2,"user":{"displayName":"P. W.","userId":"06457912707533471190"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["model = HybridNet(replacement_lvl=2, key = jrng_key)\n","optim = optax.sgd(LEARNING_RATE)\n","model, train_loss, train_acc, test_loss, test_acc = train(model, \"CIFAR10\", BATCH_SIZE, 10, optim)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"cjiqAvqnrpJm","executionInfo":{"status":"error","timestamp":1713883411256,"user_tz":-600,"elapsed":1083141,"user":{"displayName":"P. W.","userId":"06457912707533471190"}},"outputId":"8f087f13-2cfb-4455-cada-bc68460be2a8"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py:2740: ComplexWarning: Casting complex values to real discards the imaginary part\n","  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n","/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py:2740: ComplexWarning: Casting complex values to real discards the imaginary part\n","  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n","/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py:2740: ComplexWarning: Casting complex values to real discards the imaginary part\n","  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n","/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py:2740: ComplexWarning: Casting complex values to real discards the imaginary part\n","  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n"]},{"output_type":"stream","name":"stdout","text":["Train step 0, loss = 2.6655, acc = 0.1000; train time = 1081.4233 seconds\n"]},{"output_type":"error","ename":"ValueError","evalue":"Incompatible shapes for broadcasting: shapes=[(200, 32, 32), (200,)]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/util.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/util.py\u001b[0m in \u001b[0;36mcached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_broadcast_shapes_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_broadcast_shapes_uncached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mresult_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Incompatible shapes for broadcasting: shapes={list(shapes)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(200, 32, 32), (200,)]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-899d61c90650>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHybridNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplacement_lvl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjrng_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CIFAR10\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-6c82bd3250a8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_name, batchsize, epochs, optim)\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m       \u001b[0mloss_value_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_value_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_test_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m       \u001b[0mbatch_test_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m       \u001b[0mbatch_test_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_value_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n","\u001b[0;32m<ipython-input-9-6c82bd3250a8>\u001b[0m in \u001b[0;36mmake_test_step\u001b[0;34m(model, x, y)\u001b[0m\n\u001b[1;32m     91\u001b[0m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m       \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-abe50ff288df>\u001b[0m in \u001b[0;36maccuracy_fn\u001b[0;34m(pred_y, y)\u001b[0m\n\u001b[1;32m     10\u001b[0m ):\n\u001b[1;32m     11\u001b[0m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0meqx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_jit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mop\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_forward_operator_to_aval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"_{name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;31m# Note: don't use isinstance here, because we don't want to raise for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/ufuncs.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlax_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpromote_args_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlax_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpromote_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m   \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"jax.numpy.{numpy_fn.__name__}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/util.py\u001b[0m in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0m_check_no_float0s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m   \u001b[0mcheck_for_prngkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpromote_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mpromote_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/numpy/util.py\u001b[0m in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_rank_promotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"allow\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m           \u001b[0m_rank_promotion_warning_or_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mresult_rank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         return [_broadcast_to(arg, (1,) * (result_rank - len(shp)) + shp)\n\u001b[1;32m    252\u001b[0m                 for arg, shp in zip(args, shapes)]\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mresult_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_try_broadcast_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mresult_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Incompatible shapes for broadcasting: shapes={list(shapes)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(200, 32, 32), (200,)]"]}]}]}