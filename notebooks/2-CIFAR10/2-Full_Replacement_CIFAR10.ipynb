{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:22:52.948872Z",
     "iopub.status.busy": "2024-04-03T08:22:52.948409Z",
     "iopub.status.idle": "2024-04-03T08:24:31.832368Z",
     "shell.execute_reply": "2024-04-03T08:24:31.831751Z",
     "shell.execute_reply.started": "2024-04-03T08:22:52.948838Z"
    },
    "id": "5ebnFsErY0EM",
    "outputId": "e8932b66-715c-467f-912a-8b449c518214",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.17.2)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pennylane in /usr/local/lib/python3.9/dist-packages (0.35.1)\n",
      "Requirement already satisfied: cotengra in /usr/local/lib/python3.9/dist-packages (0.5.6)\n",
      "Requirement already satisfied: quimb in /usr/local/lib/python3.9/dist-packages (1.7.3)\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (1.3.2)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.6.9)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.23.4)\n",
      "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\n",
      "Requirement already satisfied: autograd in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.6.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pennylane) (4.11.0)\n",
      "Requirement already satisfied: rustworkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.14.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (3.0)\n",
      "Requirement already satisfied: pennylane-lightning>=0.35 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.35.1)\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.4.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\n",
      "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\n",
      "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.59.1)\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.12.3)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (0.11.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.2.2)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.39->quimb) (0.42.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2023.1.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (3.9.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.127)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!pip install torch torchvision torchaudio --upgrade\n",
    "!pip install pennylane cotengra quimb torchmetrics --upgrade\n",
    "#!pip install -U \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:25:45.093069Z",
     "iopub.status.busy": "2024-04-03T08:25:45.092790Z",
     "iopub.status.idle": "2024-04-03T08:26:54.096113Z",
     "shell.execute_reply": "2024-04-03T08:26:54.095258Z",
     "shell.execute_reply.started": "2024-04-03T08:25:45.093044Z"
    },
    "id": "VN2wD-U7bGse",
    "outputId": "efc73948-0832-402c-eafc-b0e6adce4a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "\n",
    "\n",
    "#import pennylane as qml\n",
    "#import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "#prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "#torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WmQPQuLbSFw"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:54.097750Z",
     "iopub.status.busy": "2024-04-03T08:26:54.097431Z",
     "iopub.status.idle": "2024-04-03T08:26:54.405968Z",
     "shell.execute_reply": "2024-04-03T08:26:54.405273Z",
     "shell.execute_reply.started": "2024-04-03T08:26:54.097729Z"
    },
    "id": "rUhQUP2dbTja",
    "outputId": "0d3641db-a2ff-4689-8b7e-32e0e8b313d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32])\n",
      "tensor([7, 2, 7, 6, 4, 5, 7, 1, 8, 1, 1, 0, 8, 9, 7, 3, 1, 4, 6, 8, 2, 6, 5, 1,\n",
      "        5, 9, 7, 2, 6, 5, 5, 9])\n",
      "tensor([ 0.0275+0.j, -0.0196+0.j, -0.0353+0.j, -0.6078+0.j, -0.7961+0.j, -0.8118+0.j,\n",
      "        -0.8196+0.j, -0.7569+0.j, -0.7490+0.j, -0.7176+0.j, -0.7490+0.j, -0.7569+0.j,\n",
      "        -0.7647+0.j, -0.8118+0.j, -0.7569+0.j, -0.7412+0.j, -0.7804+0.j, -0.6157+0.j,\n",
      "         0.0118+0.j,  0.1294+0.j,  0.0902+0.j,  0.0902+0.j,  0.0196+0.j,  0.0196+0.j,\n",
      "         0.0745+0.j,  0.0745+0.j,  0.0667+0.j,  0.0824+0.j,  0.0431+0.j,  0.0039+0.j,\n",
      "        -0.0118+0.j,  0.0118+0.j])\n"
     ]
    }
   ],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    #torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    \"CIFAR10\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQI8WWY6byQq"
   },
   "source": [
    "# Some Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:54.407720Z",
     "iopub.status.busy": "2024-04-03T08:26:54.407518Z",
     "iopub.status.idle": "2024-04-03T08:26:56.071027Z",
     "shell.execute_reply": "2024-04-03T08:26:56.069162Z",
     "shell.execute_reply.started": "2024-04-03T08:26:54.407700Z"
    },
    "id": "7B4DTncWbwQl",
    "outputId": "38975b7c-66c4-49f5-e5fd-158e55028cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-0.j, 0.+0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j,  0.+0.j, -1.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-0.j, 0.+0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j,  0.+0.j, -1.+0.j],\n",
      "        [ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.-0.j, 0.-1.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  0.-0.j,  0.-0.j, -1.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j,  1.-0.j,  0.-0.j],\n",
      "        [ 0.+0.j,  1.-0.j,  0.+0.j,  0.-0.j],\n",
      "        [-1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, -0.+0.j, 0.-0.j, 0.+1.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, -0.-1.j, 0.+0.j, -0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -0.+0.j, -1.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -0.+0.j, -1.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-0.j, -0.+0.j, 0.+1.j],\n",
      "        [0.+0.j, 0.+0.j, -0.-1.j, -0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j, -0.+0.j,  1.-0.j]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "ket = {\n",
    "    '0':torch.tensor([1.,0.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '1':torch.tensor([0.,1.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '+':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) + torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device)),\n",
    "    '-':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) - torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device))\n",
    "}\n",
    "\n",
    "pauli = {\n",
    "    'I':torch.tensor([[1.,0.],[0.,1.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'X':torch.tensor([[0.,1.],[1.,0.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Y':torch.tensor([[0., -1.j],[1.j, 0]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Z':torch.tensor([[1.,0.],[0.,-1.]], dtype = COMPLEX_DTYPE, device=device)\n",
    "}\n",
    "\n",
    "def tensor_product(*args):\n",
    "  input_list = [a for a in args]\n",
    "  return functools.reduce(torch.kron, input_list)\n",
    "\n",
    "def multi_qubit_identity(n_qubits:int):\n",
    "  assert n_qubits>0\n",
    "  if n_qubits == 1:\n",
    "    return pauli['I']\n",
    "  else:\n",
    "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
    "\n",
    "pauli_words_su4 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
    "      #print(key1, key2)\n",
    "      #print(pauli[key1])\n",
    "      #print(pauli[key2])\n",
    "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
    "      #print(pauli_words_su4[key1+key2])\n",
    "\n",
    "pauli_words_su8 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      if not key1+key2+key3 == 'III':\n",
    "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
    "\n",
    "pauli_words_su16 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        if not key1+key2+key3+key4 == 'IIII':\n",
    "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
    "              pauli[key1],\n",
    "              pauli[key2],\n",
    "              pauli[key3],\n",
    "              pauli[key4]\n",
    "          )\n",
    "\n",
    "pauli_words_su32 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        for key5 in pauli.keys():\n",
    "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
    "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
    "                pauli[key1],\n",
    "                pauli[key2],\n",
    "                pauli[key3],\n",
    "                pauli[key4],\n",
    "                pauli[key5]\n",
    "            )\n",
    "\n",
    "def su32_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = list(pauli_words_su32.values())\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, torch.stack(dict_values))\n",
    "  #print(generator.shape)\n",
    "  return torch.matrix_exp(1j*generator)\n",
    "\n",
    "def su4_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = torch.stack(list(pauli_words_su4.values()))\n",
    "  #print(dict_values.shape)\n",
    "  generator = torch.einsum(\"i, ijk -> jk\", params, dict_values)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "def pauli_dict_func(key):\n",
    "    return pauli[key]\n",
    "\n",
    "def pauli_dict_func_multiple_keys(keys):\n",
    "    return list(map(pauli_dict_func, keys))\n",
    "\n",
    "def pauli_string_tensor_prod(pauli_string:str):\n",
    "    paulis_char = list(pauli_string)\n",
    "    paulis_mat = pauli_dict_func_multiple_keys(paulis_char)\n",
    "    return tensor_product(*paulis_mat)\n",
    "\n",
    "def generate_nqubit_pauli_strings(n_qubits:int):\n",
    "    assert n_qubits>0\n",
    "    pauli_labels = ['I', 'X', 'Y', 'Z']\n",
    "    pauli_strings = []\n",
    "    for labels in itertools.product(pauli_labels, repeat=n_qubits):\n",
    "        pauli_str = \"\".join(labels)\n",
    "        if pauli_str != 'I'*n_qubits:\n",
    "            pauli_strings.append(pauli_str)\n",
    "    return pauli_strings\n",
    "\n",
    "def generate_pauli_tensor_list(pauli_strings:list):\n",
    "    return list(map(pauli_string_tensor_prod, pauli_strings))\n",
    "\n",
    "\n",
    "print(\n",
    "    generate_pauli_tensor_list(generate_nqubit_pauli_strings(2))\n",
    ")\n",
    "\n",
    "\n",
    "#test_params = torch.randn(4**2-1,  device=device).type(COMPLEX_DTYPE)\n",
    "#print(test_params.shape)\n",
    "#test_op = su4_op(test_params)\n",
    "#print(test_op)\n",
    "#print(qml.is_unitary(qml.QubitUnitary(test_op.cpu().numpy(), wires=[0,1]))) # will be false if not torch.cdouble\n",
    "\n",
    "#rx = torch.linalg.matrix_exp(1j*torch.pi*pauli[\"X\"]/2)\n",
    "#print(qml.is_unitary(qml.QubitUnitary(rx.cpu().numpy(), wires=[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.073077Z",
     "iopub.status.busy": "2024-04-03T08:26:56.072615Z",
     "iopub.status.idle": "2024-04-03T08:26:56.079092Z",
     "shell.execute_reply": "2024-04-03T08:26:56.078390Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.073053Z"
    },
    "id": "Xs0c2F1eBnGc"
   },
   "outputs": [],
   "source": [
    "def measure_sv(\n",
    "    state,\n",
    "    observable\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Measure a statevector with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not\n",
    "  \"\"\"\n",
    "  expectation_value = torch.conj(state)@observable@state\n",
    "  return torch.real(expectation_value)\n",
    "\n",
    "def measure_dm(\n",
    "    rho,\n",
    "    observable\n",
    "):\n",
    "  \"\"\"\n",
    "  Measure a density matrix with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not.\n",
    "  \"\"\"\n",
    "  product = torch.matmul(rho, observable)\n",
    "\n",
    "  # Calculate the trace, which is the sum of diagonal elements\n",
    "  trace = torch.trace(product)\n",
    "\n",
    "  # The expectation value should be real for physical observables\n",
    "  return torch.real(trace)\n",
    "\n",
    "vmap_measure_sv = torch.vmap(measure_sv, in_dims=(None, 0), out_dims=0)\n",
    "vmap_measure_dm = torch.vmap(measure_dm, in_dims=(None, 0), out_dims=0)\n",
    "\n",
    "# assuming the input patch observables (hermitianized) has shape (batchsize,n_patches, c, h, w)\n",
    "# assuming the input set statevectors has shape (c, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_sv_batched_ob = torch.vmap(measure_sv, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "# assuming the input set desnity matrices has shape (batchsize, n_patches, c, 2**n, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_dm_batched_ob = torch.vmap(measure_dm, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "def bitstring_to_state(bitstring:str):\n",
    "  \"\"\"\n",
    "  Convert a bit string, like '0101001' or '+-+-101'\n",
    "  to a statevector. Each character in the bitstring must be among\n",
    "  0, 1, + and -\n",
    "  \"\"\"\n",
    "  assert len(bitstring)>0\n",
    "  for c in bitstring:\n",
    "    assert c in ['0', '1', '+', '-']\n",
    "  single_qubit_states = [ket[c] for c in bitstring]\n",
    "  return tensor_product(*single_qubit_states)\n",
    "\n",
    "\n",
    "#test_patch = torch.randn(size=[5, 2, 3, 4,4], dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_herm_patch = (torch.einsum(\"...jk->...kj\", test_patch)+test_patch)/2\n",
    "#print(test_herm_patch)\n",
    "#print(\"all patches shape\", test_herm_patch.shape)\n",
    "#test_sv = torch.stack([bitstring_to_state('++')]*3, axis = 0)\n",
    "#print(test_sv)\n",
    "#print(\"all sv shape\", test_sv.shape)\n",
    "#res = vmap_measure_channel_sv_batched_ob(test_sv, test_herm_patch)\n",
    "#print(res)\n",
    "#print(\"res shape\",res.shape)\n",
    "\n",
    "#for batchid in range(test_patch.shape[0]):\n",
    "#  batchitem = test_patch[batchid]\n",
    "#  for patch_idx in range(batchitem.shape[0]):\n",
    "#    patch = batchitem[patch_idx]\n",
    "#    for ch_idx in range(patch.shape[0]):\n",
    "#      print(f\"batchid={batchid}; patchid = {patch_idx}; chid = {ch_idx}\")\n",
    "#      ch = patch[ch_idx]\n",
    "#      #print(ch.shape)\n",
    "#      sv_ch = test_sv[ch_idx]\n",
    "#      #print(sv_ch.shape)\n",
    "#      print(measure_sv(sv_ch, ch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFZqT6bpElaL"
   },
   "source": [
    "# Flipped Quanvolution Kernel\n",
    "\n",
    "First, let's define the function to extract patches for a single kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.080443Z",
     "iopub.status.busy": "2024-04-03T08:26:56.079871Z",
     "iopub.status.idle": "2024-04-03T08:26:56.085801Z",
     "shell.execute_reply": "2024-04-03T08:26:56.085133Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.080420Z"
    },
    "id": "He4HdMRHC7T6"
   },
   "outputs": [],
   "source": [
    "def extract_patches(image, patch_size, stride, padding=None):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image with multiple input channels and optional custom padding.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (in_channels, height, width).\n",
    "        patch_size (int): Size of the square patches to extract.\n",
    "        stride (int): Stride between patches.\n",
    "        padding (tuple): Padding value(s) for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n",
    "    pad_l, pad_r, pad_t, pad_b = padding if padding is not None else (0,0,0,0)\n",
    "\n",
    "    if padding is not None:\n",
    "        image = torch.nn.functional.pad(image, (pad_l, pad_r, pad_t, pad_b), mode='constant')\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    height, width = image.shape[-2],  image.shape[-1]\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = image[..., i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    patches = torch.stack(patches, dim=-4)\n",
    "    return patches # has shape (batchsize, n_patches, channel, h, w)\n",
    "\n",
    "\n",
    "#single_img = torch.stack([torch.arange(32*32*1,dtype = torch.double, device=device).reshape((32,32)).type(torch.cdouble)]*3)\n",
    "\n",
    "#test_img = torch.stack([single_img]*2)\n",
    "#print(test_img[0][...,:4,:4])\n",
    "#patches = extract_patches(test_img, patch_size=4, stride=2,padding=(0,0,0,0))\n",
    "#print(patches.shape)\n",
    "#print(patches[0].shape)\n",
    "#print(patches[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRqflooEqKN"
   },
   "source": [
    "## A Three-by-Three Quanv Kernel Function\n",
    "\n",
    "For 3 by 3 patches, we only need a 2-qubit parameterised state, generated by the SU4 gate applied to the $|00\\rangle$ state. The first thing to do is to pad the 3 by 3 patches to 4 by 4. However, for input with size\n",
    "\n",
    "$$\n",
    "D_1\\times H_1 \\times W_1\n",
    "$$\n",
    "\n",
    "where $D_1$ is the number of input channels, we will need $D_1$ different two-qubit states, generated by $D_1$ different parameterised circuits. These circuits will produce $D_1$ expectation values. Suppose each circuit is parameterised by $\\theta_i, i\\in \\{1,2,\\cdots, D_1\\}$, the parameterised (pure) state can be written as\n",
    "\n",
    "$$\n",
    "|\\psi (\\theta_i)⟩ = SU4(\\theta_i)|00⟩\n",
    "$$\n",
    "\n",
    "For a channel $M_i$ of patch $M$ with shape $4\\times 4$, the expectation value is calculated by\n",
    "\n",
    "$$\n",
    "⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$\n",
    "\n",
    "Then, a single pixel value of the output channel corresponding this single kernel is\n",
    "\n",
    "$$\n",
    "\\frac{1}{D_1}\\sum_{i=1}^{D_1}⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.086697Z",
     "iopub.status.busy": "2024-04-03T08:26:56.086517Z",
     "iopub.status.idle": "2024-04-03T08:26:56.091796Z",
     "shell.execute_reply": "2024-04-03T08:26:56.091178Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.086679Z"
    },
    "id": "Yzn4KEt5ErG7"
   },
   "outputs": [],
   "source": [
    "def generate_2q_param_state(theta):\n",
    "  state = bitstring_to_state('00')\n",
    "  state = torch.matmul(su4_op(theta), state)\n",
    "  return state\n",
    "\n",
    "vmap_generate_2q_param_state = torch.vmap(generate_2q_param_state, in_dims=0, out_dims = 0)\n",
    "\n",
    "def single_kernel_op_over_batched_patches(thetas, patch):\n",
    "  # patch has shape (c_in, h, w)\n",
    "  # thetas has shape (c_in, 4^2-1) for SU4 gates\n",
    "  n_theta = thetas.shape[-2]\n",
    "  n_channel = patch.shape[-3]\n",
    "  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n",
    "  states = vmap_generate_2q_param_state(thetas)\n",
    "  #print(\"States shape\", states.shape)\n",
    "  patch = torch.nn.functional.pad(patch, (0, 1, 0, 1), mode='constant')\n",
    "  patch_t = torch.einsum(\"...jk->...kj\", patch)\n",
    "  herm_patch = (patch_t+patch)/2 # has dim (batchsize, num_patches, c, h, w)\n",
    "  #print(\"Herm patch shape\", herm_patch.shape)\n",
    "  channel_out = vmap_measure_channel_sv_batched_ob(states, herm_patch) # has dim (batchsize,n_patches, c)\n",
    "  return torch.sum(channel_out, axis = -1)/n_theta # has dim (batchszie, n_patches)\n",
    "\n",
    "\n",
    "#test_params = torch.randn((1,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_img = dummy_x.to(device)#.type(torch.cdouble)\n",
    "#print(test_img.shape)\n",
    "#test_patches = extract_patches(test_img, patch_size=3, stride=1,padding=(1,1,1,1))\n",
    "#print(test_patches.shape)\n",
    "#print(test_params.shape)\n",
    "#print(vmap_generate_2q_param_state(test_params))\n",
    "#test_out = single_kernel_op_over_batched_patches(test_params, test_patches)\n",
    "#print(test_out.shape)\n",
    "#h = (32-3+1*2)//1 +1\n",
    "#h**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4BXEKd8I67_"
   },
   "source": [
    "## Multiple Output Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.092668Z",
     "iopub.status.busy": "2024-04-03T08:26:56.092483Z",
     "iopub.status.idle": "2024-04-03T08:26:56.095952Z",
     "shell.execute_reply": "2024-04-03T08:26:56.095319Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.092650Z"
    },
    "id": "72vkHV_BI80l"
   },
   "outputs": [],
   "source": [
    "# For multiple channel output\n",
    "# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n",
    "vmap_vmap_single_kernel_op_through_extracted_patches = torch.vmap(single_kernel_op_over_batched_patches, in_dims=(0, None), out_dims=-2) # output has dim (batchsize, c_out, n_patches)\n",
    "\n",
    "#c_in = 1\n",
    "#c_out = 2\n",
    "\n",
    "#test_params2 = torch.randn((c_out, c_in,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_out2 = vmap_vmap_single_kernel_op_through_extracted_patches(test_params2, test_patches)\n",
    "#print(test_out2.shape)\n",
    "#test_out_2_features = test_out2.reshape((-1,c_out, 32, 32))\n",
    "#print(test_out_2_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuZ6G5-sKK_J"
   },
   "source": [
    "## Pytorch Module for Three-by-Three Flipped Quanv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.096785Z",
     "iopub.status.busy": "2024-04-03T08:26:56.096604Z",
     "iopub.status.idle": "2024-04-03T08:26:56.103091Z",
     "shell.execute_reply": "2024-04-03T08:26:56.102542Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.096768Z"
    },
    "id": "Gww_XdJ5KPJt"
   },
   "outputs": [],
   "source": [
    "class FlippedQuanv3x3(torch.nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, padding):\n",
    "    super(FlippedQuanv3x3, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.pad_l, self.pad_r, self.pad_t, self.pad_b = padding if padding is not None else (0,0,0,0)\n",
    "    self.pad = (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n",
    "    self.weight= torch.nn.Parameter(torch.randn((out_channels, in_channels, 4**2-1)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn((out_channels, 1)).type(COMPLEX_DTYPE))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batchsize ,c_in, h, w)\n",
    "    # weight has shape (c_out, c_in, 15)\n",
    "    # bias has shape (c_out, 1)\n",
    "    x = x.type(COMPLEX_DTYPE)\n",
    "    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n",
    "    h_out = (h_in-3+(self.pad_t+self.pad_b))//self.stride +1\n",
    "    w_out = (w_in-3+(self.pad_l+self.pad_r))//self.stride +1\n",
    "\n",
    "    #print(self.weight.shape)\n",
    "    #print(patches.shape)\n",
    "\n",
    "    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n",
    "\n",
    "\n",
    "    out = out + self.bias\n",
    "    return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_channels={self.in_channels}, out_channels={self.out_channels}, stride={self.stride}, padding={self.padding}'\n",
    "\n",
    "#test_module = FlippedQuanv3x3(in_channels=1, out_channels=2, stride=1, padding=(1,1,1,1)).to(device)\n",
    "#print(dummy_x.shape)\n",
    "#test_out = test_module(dummy_x.to(device))\n",
    "#print(test_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jY9sQZ7Ynla"
   },
   "source": [
    "# Linear Layer\n",
    "\n",
    "For input dimension $D$, the quantum linear layer is a $n = \\lceil \\log_4(D+1)⌉$-qubit quantum circuit. Both the data encoding and parameterised circuit is achieved via the $SU(2^n)$ unitary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.103883Z",
     "iopub.status.busy": "2024-04-03T08:26:56.103708Z",
     "iopub.status.idle": "2024-04-03T08:26:57.562457Z",
     "shell.execute_reply": "2024-04-03T08:26:57.561682Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.103865Z"
    },
    "id": "AXxNIObFYnPW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0847, 0.6649, 0.0675, 0.1829], device='cuda:0')\n",
      "tensor(1.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def data_encode_unitary(padded_data, t):\n",
    "  original_dim = padded_data.shape[-1]\n",
    "  new_dim = torch.sqrt(torch.tensor(original_dim)).type(torch.int)\n",
    "  data = torch.reshape(padded_data, (new_dim, new_dim))\n",
    "  generator = (data + torch.einsum(\"...jk->...kj\", data))/2\n",
    "  return torch.linalg.matrix_exp(1.0j*generator*t)\n",
    "\n",
    "def su_n(params, pauli_string_tensor_list):\n",
    "  # params has dim 4**n-1\n",
    "  paulis = torch.stack(pauli_string_tensor_list)\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, paulis)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "def linear_layer_func(padded_data, params, pauli_string_tensor_list, observables, n_qubits):\n",
    "  n_rep = params.shape[0]\n",
    "  state = bitstring_to_state(\"+\"*n_qubits)\n",
    "  for i in range(n_rep):\n",
    "    data_unitary = data_encode_unitary(padded_data, 1/n_rep)\n",
    "    #print(data_unitary.shape)\n",
    "    #print(state.shape)\n",
    "    state = torch.matmul(\n",
    "      data_unitary,\n",
    "      state\n",
    "    )\n",
    "    sun_gate = su_n(params[i], pauli_string_tensor_list)\n",
    "    #print(sun_gate.shape)\n",
    "    state = torch.matmul(\n",
    "      sun_gate,\n",
    "      state\n",
    "    )\n",
    "  return vmap_measure_sv(state, observables)\n",
    "\n",
    "test_params = torch.randn((3, 4**2-1),device=device).type(COMPLEX_DTYPE)\n",
    "test_data = torch.randn(4**2, device=device).type(COMPLEX_DTYPE)\n",
    "test_obs = torch.stack([torch.outer(bitstring_to_state('00'), bitstring_to_state('00')),\n",
    "                        torch.outer(bitstring_to_state('01'), bitstring_to_state('01')),\n",
    "                        torch.outer(bitstring_to_state('10'), bitstring_to_state('10')),\n",
    "                         torch.outer(bitstring_to_state('11'), bitstring_to_state('11'))])\n",
    "test_pauli_string_tensor_list = generate_pauli_tensor_list(generate_nqubit_pauli_strings(2))\n",
    "\n",
    "\n",
    "test_out = linear_layer_func(test_data, test_params, test_pauli_string_tensor_list, test_obs, 2)\n",
    "print(test_out)\n",
    "print(torch.sum(test_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:57.564844Z",
     "iopub.status.busy": "2024-04-03T08:26:57.564417Z",
     "iopub.status.idle": "2024-04-03T08:26:57.576557Z",
     "shell.execute_reply": "2024-04-03T08:26:57.575906Z",
     "shell.execute_reply.started": "2024-04-03T08:26:57.564823Z"
    },
    "id": "2F4_SBgIYnMv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3414, 0.4328, 0.0295, 0.1964],\n",
      "        [0.4175, 0.1313, 0.2182, 0.2330],\n",
      "        [0.1889, 0.5187, 0.1613, 0.1311]], device='cuda:0')\n",
      "tensor(3.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "vmap_batch_linear_layer_func = torch.vmap(linear_layer_func, in_dims=(0, None, None, None, None), out_dims=0)\n",
    "\n",
    "test_data = torch.randn((3, 4**2), device=device).type(COMPLEX_DTYPE)\n",
    "test_out = vmap_batch_linear_layer_func(test_data, test_params, test_pauli_string_tensor_list, test_obs, 2)\n",
    "print(test_out)\n",
    "print(torch.sum(test_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryROGtnC_3po"
   },
   "source": [
    "## PyTorch Module for the Quantum Version of Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:57.577462Z",
     "iopub.status.busy": "2024-04-03T08:26:57.577280Z",
     "iopub.status.idle": "2024-04-03T08:26:57.605899Z",
     "shell.execute_reply": "2024-04-03T08:26:57.605187Z",
     "shell.execute_reply.started": "2024-04-03T08:26:57.577443Z"
    },
    "id": "RlTC952w_8VW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 45])\n",
      "torch.Size([16, 6])\n",
      "DataReUploadingLinear(in_dim=45, out_dim=6, n_qubits=3, n_reps=10)\n"
     ]
    }
   ],
   "source": [
    "class DataReUploadingLinear(torch.nn.Module):\n",
    "  def __init__(self, in_dim, out_dim, n_qubits, n_reps):\n",
    "    super(DataReUploadingLinear, self).__init__()\n",
    "    assert 2**n_qubits >= out_dim\n",
    "    assert 4**n_qubits >= in_dim\n",
    "    self.in_dim = in_dim\n",
    "    self.out_dim = out_dim\n",
    "    self.n_qubits = n_qubits\n",
    "    self.n_reps = n_reps\n",
    "    self.pauli_string_tensor_list = generate_pauli_tensor_list(generate_nqubit_pauli_strings(n_qubits))\n",
    "    self.param_dim = 4**n_qubits-1\n",
    "    self.params = torch.nn.Parameter(torch.randn((self.n_reps,self.param_dim)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn(out_dim).type(REAL_DTYPE))\n",
    "    self.observables = self.generate_observables()\n",
    "    self.pad_size = 4**n_qubits-self.in_dim\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has size (batchsize, in_dim)\n",
    "    # pad x\n",
    "    x = torch.nn.functional.pad(x, (0, self.pad_size)).type(COMPLEX_DTYPE)\n",
    "    out = vmap_batch_linear_layer_func(x, self.params, self.pauli_string_tensor_list, self.observables, self.n_qubits)\n",
    "    out = out.type(REAL_DTYPE) + self.bias\n",
    "    return out\n",
    "\n",
    "  def generate_observables(self):\n",
    "    observables = []\n",
    "    for i in range(self.out_dim):\n",
    "      temp_bitstring = '{0:b}'.format(i).zfill(self.n_qubits)\n",
    "      ob = torch.outer(bitstring_to_state(temp_bitstring), bitstring_to_state(temp_bitstring))\n",
    "      observables.append(ob)\n",
    "    return torch.stack(observables)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_dim={self.in_dim}, out_dim={self.out_dim}, n_qubits={self.n_qubits}, n_reps={self.n_reps}'\n",
    "\n",
    "test_linear_module = DataReUploadingLinear(in_dim=45, out_dim=6, n_qubits=3, n_reps = 10).to(device)\n",
    "test_data = torch.randn((16, 45), device=device).type(COMPLEX_DTYPE)\n",
    "print(test_data.shape)\n",
    "test_out = test_linear_module(test_data.to(device))\n",
    "print(test_out.shape)\n",
    "print(test_linear_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yumZTjDVu63"
   },
   "source": [
    "# Replacing Classical Layers with Quantum Layers\n",
    "\n",
    "Let's replace `Conv2d` with `FlippedQuanv3x3`, and `Linear` with `QuantLinear`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:57.606924Z",
     "iopub.status.busy": "2024-04-03T08:26:57.606727Z",
     "iopub.status.idle": "2024-04-03T08:26:59.000203Z",
     "shell.execute_reply": "2024-04-03T08:26:58.999449Z",
     "shell.execute_reply.started": "2024-04-03T08:26:57.606905Z"
    },
    "id": "W4RC5ou-VzbE",
    "outputId": "9eb0b35d-ed18-431b-8a53-ba5216d3fd06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n",
      "HybridNet(\n",
      "  (layers): Sequential(\n",
      "    (0): FlippedQuanv3x3(in_channels=3, out_channels=32, stride=1, padding=None)\n",
      "    (1): FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): DataReUploadingLinear(in_dim=12544, out_dim=10, n_qubits=7, n_reps=10)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_509/4168075707.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n",
      "/tmp/ipykernel_509/1684779817.py:30: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:301.)\n",
      "  return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "class HybridNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HybridNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        FlippedQuanv3x3(in_channels=3, out_channels=32, stride=1, padding=None), # 32->30\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None), # 30->28\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        DataReUploadingLinear(16*28*28, 10, 7, 10)\n",
    "        #torch.nn.Linear(32*14*14, 10).type(REAL_DTYPE)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "\n",
    "net = HybridNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:59.001280Z",
     "iopub.status.busy": "2024-04-03T08:26:59.001080Z",
     "iopub.status.idle": "2024-04-03T17:06:02.379965Z",
     "shell.execute_reply": "2024-04-03T17:06:02.379200Z",
     "shell.execute_reply.started": "2024-04-03T08:26:59.001260Z"
    },
    "id": "x4ZY59q1WS4x",
    "outputId": "6532aea1-47a3-49f3-fe36-b774aee1905f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 250, Number of test batches = 50\n",
      "Print every train batch = 25, Print every test batch = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_509/4168075707.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at step=0, batch=0, train loss = 2.482003688812256, train acc = 0.11500000208616257, time = 1.5793821811676025\n",
      "Training at step=0, batch=25, train loss = 2.5056371688842773, train acc = 0.09000000357627869, time = 0.9443156719207764\n",
      "Training at step=0, batch=50, train loss = 2.5312769412994385, train acc = 0.06499999761581421, time = 0.9501950740814209\n",
      "Training at step=0, batch=75, train loss = 2.4974148273468018, train acc = 0.10999999940395355, time = 0.954216480255127\n",
      "Training at step=0, batch=100, train loss = 2.4977495670318604, train acc = 0.05999999865889549, time = 0.9341530799865723\n",
      "Training at step=0, batch=125, train loss = 2.4864206314086914, train acc = 0.08500000089406967, time = 0.9513311386108398\n",
      "Training at step=0, batch=150, train loss = 2.518573045730591, train acc = 0.08500000089406967, time = 0.9657611846923828\n",
      "Training at step=0, batch=175, train loss = 2.4448115825653076, train acc = 0.09000000357627869, time = 0.954348087310791\n",
      "Training at step=0, batch=200, train loss = 2.4454076290130615, train acc = 0.08500000089406967, time = 0.959580659866333\n",
      "Training at step=0, batch=225, train loss = 2.4599859714508057, train acc = 0.10499999672174454, time = 0.9625117778778076\n",
      "Testing at step=0, batch=0, test loss = 2.4198813438415527, test acc = 0.11999999731779099, time = 0.36157703399658203\n",
      "Testing at step=0, batch=5, test loss = 2.3729348182678223, test acc = 0.12999999523162842, time = 0.35395026206970215\n",
      "Testing at step=0, batch=10, test loss = 2.515612840652466, test acc = 0.04500000178813934, time = 0.3585829734802246\n",
      "Testing at step=0, batch=15, test loss = 2.411358118057251, test acc = 0.10999999940395355, time = 0.3696939945220947\n",
      "Testing at step=0, batch=20, test loss = 2.3988778591156006, test acc = 0.1550000011920929, time = 0.35257411003112793\n",
      "Testing at step=0, batch=25, test loss = 2.43394136428833, test acc = 0.11500000208616257, time = 0.3572959899902344\n",
      "Testing at step=0, batch=30, test loss = 2.4694883823394775, test acc = 0.08500000089406967, time = 0.36751651763916016\n",
      "Testing at step=0, batch=35, test loss = 2.4145820140838623, test acc = 0.10499999672174454, time = 0.35398077964782715\n",
      "Testing at step=0, batch=40, test loss = 2.4289565086364746, test acc = 0.11500000208616257, time = 0.3595879077911377\n",
      "Testing at step=0, batch=45, test loss = 2.478095293045044, test acc = 0.07999999821186066, time = 0.36161351203918457\n",
      "Step 0 finished in 264.75222635269165, Train loss = 2.4676976175308227, Test loss = 2.4440946912765504; Train Acc = 0.10000000017881393, Test Acc = 0.09999999985098838\n",
      "Training at step=1, batch=0, train loss = 2.3971002101898193, train acc = 0.11500000208616257, time = 0.9394400119781494\n",
      "Training at step=1, batch=25, train loss = 2.4058775901794434, train acc = 0.10999999940395355, time = 0.9511370658874512\n",
      "Training at step=1, batch=50, train loss = 2.4294021129608154, train acc = 0.10000000149011612, time = 0.9491384029388428\n",
      "Training at step=1, batch=75, train loss = 2.3687543869018555, train acc = 0.125, time = 0.9317367076873779\n",
      "Training at step=1, batch=100, train loss = 2.4466629028320312, train acc = 0.08500000089406967, time = 0.9380366802215576\n",
      "Training at step=1, batch=125, train loss = 2.407461643218994, train acc = 0.0949999988079071, time = 0.9399480819702148\n",
      "Training at step=1, batch=150, train loss = 2.3741326332092285, train acc = 0.125, time = 0.9355251789093018\n",
      "Training at step=1, batch=175, train loss = 2.374891519546509, train acc = 0.07999999821186066, time = 0.940415620803833\n",
      "Training at step=1, batch=200, train loss = 2.338127851486206, train acc = 0.0949999988079071, time = 0.9319720268249512\n",
      "Training at step=1, batch=225, train loss = 2.337405204772949, train acc = 0.10000000149011612, time = 0.9457602500915527\n",
      "Testing at step=1, batch=0, test loss = 2.290987014770508, test acc = 0.09000000357627869, time = 0.3716757297515869\n",
      "Testing at step=1, batch=5, test loss = 2.33966064453125, test acc = 0.09000000357627869, time = 0.3554720878601074\n",
      "Testing at step=1, batch=10, test loss = 2.303344964981079, test acc = 0.10499999672174454, time = 0.35009002685546875\n",
      "Testing at step=1, batch=15, test loss = 2.33920955657959, test acc = 0.08500000089406967, time = 0.34735560417175293\n",
      "Testing at step=1, batch=20, test loss = 2.3628830909729004, test acc = 0.07500000298023224, time = 0.35738420486450195\n",
      "Testing at step=1, batch=25, test loss = 2.2938544750213623, test acc = 0.14499999582767487, time = 0.36461496353149414\n",
      "Testing at step=1, batch=30, test loss = 2.31693959236145, test acc = 0.11999999731779099, time = 0.35480332374572754\n",
      "Testing at step=1, batch=35, test loss = 2.3454558849334717, test acc = 0.05999999865889549, time = 0.36466431617736816\n",
      "Testing at step=1, batch=40, test loss = 2.310751438140869, test acc = 0.10499999672174454, time = 0.36528778076171875\n",
      "Testing at step=1, batch=45, test loss = 2.319082021713257, test acc = 0.11500000208616257, time = 0.3572413921356201\n",
      "Step 1 finished in 263.6706483364105, Train loss = 2.396209144592285, Test loss = 2.326269655227661; Train Acc = 0.10000000007450581, Test Acc = 0.09999999962747097\n",
      "Training at step=2, batch=0, train loss = 2.266371726989746, train acc = 0.15000000596046448, time = 0.9440875053405762\n",
      "Training at step=2, batch=25, train loss = 2.296004056930542, train acc = 0.10999999940395355, time = 0.9393808841705322\n",
      "Training at step=2, batch=50, train loss = 2.3199193477630615, train acc = 0.10000000149011612, time = 0.9404287338256836\n",
      "Training at step=2, batch=75, train loss = 2.2706854343414307, train acc = 0.11500000208616257, time = 0.9359831809997559\n",
      "Training at step=2, batch=100, train loss = 2.284825325012207, train acc = 0.11999999731779099, time = 0.9419424533843994\n",
      "Training at step=2, batch=125, train loss = 2.3130877017974854, train acc = 0.13500000536441803, time = 0.9488921165466309\n",
      "Training at step=2, batch=150, train loss = 2.31557297706604, train acc = 0.10000000149011612, time = 0.9487507343292236\n",
      "Training at step=2, batch=175, train loss = 2.2918670177459717, train acc = 0.12999999523162842, time = 0.9535341262817383\n",
      "Training at step=2, batch=200, train loss = 2.239683151245117, train acc = 0.1550000011920929, time = 0.9427423477172852\n",
      "Training at step=2, batch=225, train loss = 2.2689032554626465, train acc = 0.13500000536441803, time = 0.9428260326385498\n",
      "Testing at step=2, batch=0, test loss = 2.2602577209472656, test acc = 0.1599999964237213, time = 0.35889196395874023\n",
      "Testing at step=2, batch=5, test loss = 2.2767419815063477, test acc = 0.10499999672174454, time = 0.3545238971710205\n",
      "Testing at step=2, batch=10, test loss = 2.2509899139404297, test acc = 0.125, time = 0.3524937629699707\n",
      "Testing at step=2, batch=15, test loss = 2.2172603607177734, test acc = 0.15000000596046448, time = 0.35172009468078613\n",
      "Testing at step=2, batch=20, test loss = 2.166656732559204, test acc = 0.22499999403953552, time = 0.35228419303894043\n",
      "Testing at step=2, batch=25, test loss = 2.2786240577697754, test acc = 0.125, time = 0.35279107093811035\n",
      "Testing at step=2, batch=30, test loss = 2.2840938568115234, test acc = 0.15000000596046448, time = 0.3538215160369873\n",
      "Testing at step=2, batch=35, test loss = 2.2276673316955566, test acc = 0.15000000596046448, time = 0.35033106803894043\n",
      "Testing at step=2, batch=40, test loss = 2.231388568878174, test acc = 0.1599999964237213, time = 0.3483095169067383\n",
      "Testing at step=2, batch=45, test loss = 2.216672658920288, test acc = 0.1899999976158142, time = 0.35878920555114746\n",
      "Step 2 finished in 263.26112842559814, Train loss = 2.292599104881287, Test loss = 2.244076657295227; Train Acc = 0.12148000031709671, Test Acc = 0.15500000044703482\n",
      "Training at step=3, batch=0, train loss = 2.240493059158325, train acc = 0.1550000011920929, time = 1.005474328994751\n",
      "Training at step=3, batch=25, train loss = 2.254366874694824, train acc = 0.14499999582767487, time = 0.9415292739868164\n",
      "Training at step=3, batch=50, train loss = 2.213803768157959, train acc = 0.20499999821186066, time = 0.9492177963256836\n",
      "Training at step=3, batch=75, train loss = 2.223625421524048, train acc = 0.1899999976158142, time = 0.943934440612793\n",
      "Training at step=3, batch=100, train loss = 2.159566640853882, train acc = 0.22499999403953552, time = 0.9368338584899902\n",
      "Training at step=3, batch=125, train loss = 2.1815567016601562, train acc = 0.18000000715255737, time = 0.9388344287872314\n",
      "Training at step=3, batch=150, train loss = 2.12229061126709, train acc = 0.24500000476837158, time = 0.9473879337310791\n",
      "Training at step=3, batch=175, train loss = 2.2115161418914795, train acc = 0.17000000178813934, time = 0.9458467960357666\n",
      "Training at step=3, batch=200, train loss = 2.0981221199035645, train acc = 0.25, time = 0.9383702278137207\n",
      "Training at step=3, batch=225, train loss = 2.070051670074463, train acc = 0.30000001192092896, time = 0.9425625801086426\n",
      "Testing at step=3, batch=0, test loss = 2.0277862548828125, test acc = 0.2750000059604645, time = 0.36126208305358887\n",
      "Testing at step=3, batch=5, test loss = 2.1107733249664307, test acc = 0.23499999940395355, time = 0.35646581649780273\n",
      "Testing at step=3, batch=10, test loss = 2.129338026046753, test acc = 0.23999999463558197, time = 0.35587120056152344\n",
      "Testing at step=3, batch=15, test loss = 2.10616397857666, test acc = 0.2549999952316284, time = 0.3602759838104248\n",
      "Testing at step=3, batch=20, test loss = 2.141113042831421, test acc = 0.2199999988079071, time = 0.35427403450012207\n",
      "Testing at step=3, batch=25, test loss = 2.012638568878174, test acc = 0.2549999952316284, time = 0.3619377613067627\n",
      "Testing at step=3, batch=30, test loss = 2.0862226486206055, test acc = 0.26499998569488525, time = 0.35146141052246094\n",
      "Testing at step=3, batch=35, test loss = 2.1128015518188477, test acc = 0.2800000011920929, time = 0.3625972270965576\n",
      "Testing at step=3, batch=40, test loss = 2.1246840953826904, test acc = 0.23499999940395355, time = 0.3492114543914795\n",
      "Testing at step=3, batch=45, test loss = 2.0794179439544678, test acc = 0.23499999940395355, time = 0.3495349884033203\n",
      "Step 3 finished in 263.7848906517029, Train loss = 2.1750702919960023, Test loss = 2.0936432695388794; Train Acc = 0.21095999884605407, Test Acc = 0.2496999990940094\n",
      "Training at step=4, batch=0, train loss = 2.1101410388946533, train acc = 0.23499999940395355, time = 0.9484250545501709\n",
      "Training at step=4, batch=25, train loss = 2.0955183506011963, train acc = 0.25, time = 0.945124626159668\n",
      "Training at step=4, batch=50, train loss = 2.088956832885742, train acc = 0.2549999952316284, time = 0.9539399147033691\n",
      "Training at step=4, batch=75, train loss = 2.0561373233795166, train acc = 0.2750000059604645, time = 0.9643993377685547\n",
      "Training at step=4, batch=100, train loss = 2.1482338905334473, train acc = 0.22499999403953552, time = 0.9574215412139893\n",
      "Training at step=4, batch=125, train loss = 2.1014516353607178, train acc = 0.26499998569488525, time = 0.9397549629211426\n",
      "Training at step=4, batch=150, train loss = 2.1152541637420654, train acc = 0.22499999403953552, time = 0.9522731304168701\n",
      "Training at step=4, batch=175, train loss = 2.0476458072662354, train acc = 0.20999999344348907, time = 0.9524867534637451\n",
      "Training at step=4, batch=200, train loss = 1.978918433189392, train acc = 0.28999999165534973, time = 1.0274462699890137\n",
      "Training at step=4, batch=225, train loss = 1.9855902194976807, train acc = 0.28999999165534973, time = 0.9433567523956299\n",
      "Testing at step=4, batch=0, test loss = 1.9550046920776367, test acc = 0.32499998807907104, time = 0.3505978584289551\n",
      "Testing at step=4, batch=5, test loss = 2.0075933933258057, test acc = 0.28999999165534973, time = 0.3780248165130615\n",
      "Testing at step=4, batch=10, test loss = 1.9783568382263184, test acc = 0.2849999964237213, time = 0.36313438415527344\n",
      "Testing at step=4, batch=15, test loss = 2.0368313789367676, test acc = 0.29499998688697815, time = 0.35430312156677246\n",
      "Testing at step=4, batch=20, test loss = 2.0258994102478027, test acc = 0.2549999952316284, time = 0.3668966293334961\n",
      "Testing at step=4, batch=25, test loss = 2.0181195735931396, test acc = 0.26499998569488525, time = 0.3642549514770508\n",
      "Testing at step=4, batch=30, test loss = 2.0028820037841797, test acc = 0.27000001072883606, time = 0.36103105545043945\n",
      "Testing at step=4, batch=35, test loss = 1.9844857454299927, test acc = 0.26499998569488525, time = 0.35770082473754883\n",
      "Testing at step=4, batch=40, test loss = 1.8885000944137573, test acc = 0.3700000047683716, time = 0.3684499263763428\n",
      "Testing at step=4, batch=45, test loss = 2.021162271499634, test acc = 0.23499999940395355, time = 0.36429357528686523\n",
      "Step 4 finished in 264.0896739959717, Train loss = 2.038031951904297, Test loss = 1.988630039691925; Train Acc = 0.26919999837875364, Test Acc = 0.2932999995350838\n",
      "Training at step=5, batch=0, train loss = 2.020305633544922, train acc = 0.23999999463558197, time = 0.9456851482391357\n",
      "Training at step=5, batch=25, train loss = 2.0620615482330322, train acc = 0.27000001072883606, time = 0.9399914741516113\n",
      "Training at step=5, batch=50, train loss = 1.984819769859314, train acc = 0.2849999964237213, time = 0.9368979930877686\n",
      "Training at step=5, batch=75, train loss = 1.971153736114502, train acc = 0.3100000023841858, time = 0.9432613849639893\n",
      "Training at step=5, batch=100, train loss = 2.0263686180114746, train acc = 0.2849999964237213, time = 0.9411089420318604\n",
      "Training at step=5, batch=125, train loss = 1.9022095203399658, train acc = 0.3100000023841858, time = 0.9501516819000244\n",
      "Training at step=5, batch=150, train loss = 1.9969754219055176, train acc = 0.32499998807907104, time = 0.9656085968017578\n",
      "Training at step=5, batch=175, train loss = 2.0096468925476074, train acc = 0.3149999976158142, time = 0.9536676406860352\n",
      "Training at step=5, batch=200, train loss = 1.9710408449172974, train acc = 0.3149999976158142, time = 0.9601655006408691\n",
      "Training at step=5, batch=225, train loss = 1.9071764945983887, train acc = 0.29499998688697815, time = 0.9412932395935059\n",
      "Testing at step=5, batch=0, test loss = 2.053877830505371, test acc = 0.2800000011920929, time = 0.3507266044616699\n",
      "Testing at step=5, batch=5, test loss = 1.9681897163391113, test acc = 0.2849999964237213, time = 0.34861063957214355\n",
      "Testing at step=5, batch=10, test loss = 2.0025529861450195, test acc = 0.33000001311302185, time = 0.3596527576446533\n",
      "Testing at step=5, batch=15, test loss = 1.9284454584121704, test acc = 0.28999999165534973, time = 0.35523414611816406\n",
      "Testing at step=5, batch=20, test loss = 2.030045509338379, test acc = 0.22499999403953552, time = 0.35689663887023926\n",
      "Testing at step=5, batch=25, test loss = 1.935867428779602, test acc = 0.29499998688697815, time = 0.355334997177124\n",
      "Testing at step=5, batch=30, test loss = 1.9908514022827148, test acc = 0.29499998688697815, time = 0.3601961135864258\n",
      "Testing at step=5, batch=35, test loss = 1.9044886827468872, test acc = 0.33500000834465027, time = 0.35642313957214355\n",
      "Testing at step=5, batch=40, test loss = 1.8485773801803589, test acc = 0.36500000953674316, time = 0.36478352546691895\n",
      "Testing at step=5, batch=45, test loss = 1.9718328714370728, test acc = 0.3100000023841858, time = 0.3556697368621826\n",
      "Step 5 finished in 263.978440284729, Train loss = 1.9692030191421508, Test loss = 1.9540439677238464; Train Acc = 0.2993399980664253, Test Acc = 0.3\n",
      "Training at step=6, batch=0, train loss = 1.9080703258514404, train acc = 0.3100000023841858, time = 0.9389197826385498\n",
      "Training at step=6, batch=25, train loss = 1.8876930475234985, train acc = 0.3449999988079071, time = 0.9396193027496338\n",
      "Training at step=6, batch=50, train loss = 2.0505852699279785, train acc = 0.23999999463558197, time = 0.9429566860198975\n",
      "Training at step=6, batch=75, train loss = 1.9127615690231323, train acc = 0.3499999940395355, time = 0.9432339668273926\n",
      "Training at step=6, batch=100, train loss = 1.8905390501022339, train acc = 0.3449999988079071, time = 0.9378092288970947\n",
      "Training at step=6, batch=125, train loss = 1.8590797185897827, train acc = 0.3449999988079071, time = 0.9481451511383057\n",
      "Training at step=6, batch=150, train loss = 1.983060598373413, train acc = 0.33000001311302185, time = 0.9506494998931885\n",
      "Training at step=6, batch=175, train loss = 1.9155845642089844, train acc = 0.30000001192092896, time = 0.943840503692627\n",
      "Training at step=6, batch=200, train loss = 1.867435336112976, train acc = 0.2800000011920929, time = 0.9562256336212158\n",
      "Training at step=6, batch=225, train loss = 1.9456312656402588, train acc = 0.3050000071525574, time = 0.9576823711395264\n",
      "Testing at step=6, batch=0, test loss = 1.9845664501190186, test acc = 0.27000001072883606, time = 0.36203932762145996\n",
      "Testing at step=6, batch=5, test loss = 1.937516689300537, test acc = 0.2849999964237213, time = 0.3628709316253662\n",
      "Testing at step=6, batch=10, test loss = 1.8974031209945679, test acc = 0.33500000834465027, time = 0.3688223361968994\n",
      "Testing at step=6, batch=15, test loss = 1.8925029039382935, test acc = 0.32499998807907104, time = 0.3714265823364258\n",
      "Testing at step=6, batch=20, test loss = 1.9188495874404907, test acc = 0.38999998569488525, time = 0.3569648265838623\n",
      "Testing at step=6, batch=25, test loss = 1.9088183641433716, test acc = 0.3199999928474426, time = 0.3688316345214844\n",
      "Testing at step=6, batch=30, test loss = 1.9823524951934814, test acc = 0.2849999964237213, time = 0.36061525344848633\n",
      "Testing at step=6, batch=35, test loss = 1.987200140953064, test acc = 0.30000001192092896, time = 0.3542442321777344\n",
      "Testing at step=6, batch=40, test loss = 1.8968915939331055, test acc = 0.3050000071525574, time = 0.35613250732421875\n",
      "Testing at step=6, batch=45, test loss = 1.9189825057983398, test acc = 0.29499998688697815, time = 0.35530662536621094\n",
      "Step 6 finished in 265.0530478954315, Train loss = 1.9259148144721985, Test loss = 1.9117099833488465; Train Acc = 0.31546000003814695, Test Acc = 0.3225\n",
      "Training at step=7, batch=0, train loss = 1.9210827350616455, train acc = 0.29499998688697815, time = 0.9485814571380615\n",
      "Training at step=7, batch=25, train loss = 1.799135684967041, train acc = 0.32499998807907104, time = 1.0377216339111328\n",
      "Training at step=7, batch=50, train loss = 1.8764829635620117, train acc = 0.36500000953674316, time = 0.939589262008667\n",
      "Training at step=7, batch=75, train loss = 1.9971497058868408, train acc = 0.28999999165534973, time = 0.9423465728759766\n",
      "Training at step=7, batch=100, train loss = 1.8818317651748657, train acc = 0.3149999976158142, time = 0.937748908996582\n",
      "Training at step=7, batch=125, train loss = 1.8877862691879272, train acc = 0.3799999952316284, time = 1.0067331790924072\n",
      "Training at step=7, batch=150, train loss = 1.9225146770477295, train acc = 0.3050000071525574, time = 0.9425849914550781\n",
      "Training at step=7, batch=175, train loss = 1.784774661064148, train acc = 0.38499999046325684, time = 0.949561357498169\n",
      "Training at step=7, batch=200, train loss = 1.9491676092147827, train acc = 0.3100000023841858, time = 0.9412767887115479\n",
      "Training at step=7, batch=225, train loss = 1.8184154033660889, train acc = 0.3499999940395355, time = 0.9615168571472168\n",
      "Testing at step=7, batch=0, test loss = 1.9405391216278076, test acc = 0.3050000071525574, time = 0.3544631004333496\n",
      "Testing at step=7, batch=5, test loss = 1.9360791444778442, test acc = 0.3400000035762787, time = 0.3671083450317383\n",
      "Testing at step=7, batch=10, test loss = 1.8719966411590576, test acc = 0.3449999988079071, time = 0.3830239772796631\n",
      "Testing at step=7, batch=15, test loss = 1.841088891029358, test acc = 0.36500000953674316, time = 0.3607752323150635\n",
      "Testing at step=7, batch=20, test loss = 1.9092174768447876, test acc = 0.3050000071525574, time = 0.35642147064208984\n",
      "Testing at step=7, batch=25, test loss = 1.7960249185562134, test acc = 0.3499999940395355, time = 0.35819315910339355\n",
      "Testing at step=7, batch=30, test loss = 1.8114393949508667, test acc = 0.4000000059604645, time = 0.37215471267700195\n",
      "Testing at step=7, batch=35, test loss = 1.8312655687332153, test acc = 0.35499998927116394, time = 0.3682413101196289\n",
      "Testing at step=7, batch=40, test loss = 1.896918773651123, test acc = 0.3149999976158142, time = 0.3527646064758301\n",
      "Testing at step=7, batch=45, test loss = 1.8347140550613403, test acc = 0.36000001430511475, time = 0.3698239326477051\n",
      "Step 7 finished in 265.6194052696228, Train loss = 1.8920394682884216, Test loss = 1.8763657236099243; Train Acc = 0.3296199991703033, Test Acc = 0.3342000013589859\n",
      "Training at step=8, batch=0, train loss = 1.8845847845077515, train acc = 0.35499998927116394, time = 0.9632344245910645\n",
      "Training at step=8, batch=25, train loss = 2.0070769786834717, train acc = 0.30000001192092896, time = 0.9423747062683105\n",
      "Training at step=8, batch=50, train loss = 1.8904962539672852, train acc = 0.30000001192092896, time = 0.9465720653533936\n",
      "Training at step=8, batch=75, train loss = 1.8674514293670654, train acc = 0.35499998927116394, time = 0.9415714740753174\n",
      "Training at step=8, batch=100, train loss = 1.789526343345642, train acc = 0.39500001072883606, time = 0.9454283714294434\n",
      "Training at step=8, batch=125, train loss = 1.8769891262054443, train acc = 0.3449999988079071, time = 0.9494860172271729\n",
      "Training at step=8, batch=150, train loss = 1.9154490232467651, train acc = 0.33000001311302185, time = 0.9518682956695557\n",
      "Training at step=8, batch=175, train loss = 1.7611066102981567, train acc = 0.4000000059604645, time = 0.9434969425201416\n",
      "Training at step=8, batch=200, train loss = 1.7828694581985474, train acc = 0.36000001430511475, time = 0.9385452270507812\n",
      "Training at step=8, batch=225, train loss = 1.8882142305374146, train acc = 0.3050000071525574, time = 0.9453098773956299\n",
      "Testing at step=8, batch=0, test loss = 1.8251540660858154, test acc = 0.3449999988079071, time = 0.34869885444641113\n",
      "Testing at step=8, batch=5, test loss = 1.8712801933288574, test acc = 0.3799999952316284, time = 0.35790014266967773\n",
      "Testing at step=8, batch=10, test loss = 1.757340431213379, test acc = 0.4099999964237213, time = 0.35041165351867676\n",
      "Testing at step=8, batch=15, test loss = 1.9564610719680786, test acc = 0.33000001311302185, time = 0.34744930267333984\n",
      "Testing at step=8, batch=20, test loss = 1.776796579360962, test acc = 0.3799999952316284, time = 0.34794187545776367\n",
      "Testing at step=8, batch=25, test loss = 1.8663312196731567, test acc = 0.3499999940395355, time = 0.3728771209716797\n",
      "Testing at step=8, batch=30, test loss = 1.8167288303375244, test acc = 0.33000001311302185, time = 0.35037899017333984\n",
      "Testing at step=8, batch=35, test loss = 1.9692862033843994, test acc = 0.26499998569488525, time = 0.35520076751708984\n",
      "Testing at step=8, batch=40, test loss = 1.9229081869125366, test acc = 0.3199999928474426, time = 0.36509227752685547\n",
      "Testing at step=8, batch=45, test loss = 2.0029923915863037, test acc = 0.3100000023841858, time = 0.3655269145965576\n",
      "Step 8 finished in 264.3163616657257, Train loss = 1.858884045124054, Test loss = 1.8728119564056396; Train Acc = 0.33968000042438506, Test Acc = 0.3375999999046326\n",
      "Training at step=9, batch=0, train loss = 1.8691527843475342, train acc = 0.33500000834465027, time = 0.9502542018890381\n",
      "Training at step=9, batch=25, train loss = 1.8624769449234009, train acc = 0.3400000035762787, time = 0.9372580051422119\n",
      "Training at step=9, batch=50, train loss = 1.8378844261169434, train acc = 0.33000001311302185, time = 0.9528374671936035\n",
      "Training at step=9, batch=75, train loss = 1.8421218395233154, train acc = 0.3149999976158142, time = 0.9400959014892578\n",
      "Training at step=9, batch=100, train loss = 1.8726807832717896, train acc = 0.3199999928474426, time = 0.9368288516998291\n",
      "Training at step=9, batch=125, train loss = 1.8142906427383423, train acc = 0.32499998807907104, time = 0.9394567012786865\n",
      "Training at step=9, batch=150, train loss = 1.820370078086853, train acc = 0.33500000834465027, time = 0.9519407749176025\n",
      "Training at step=9, batch=175, train loss = 1.7515056133270264, train acc = 0.4300000071525574, time = 0.9489624500274658\n",
      "Training at step=9, batch=200, train loss = 1.8308978080749512, train acc = 0.39500001072883606, time = 0.9514756202697754\n",
      "Training at step=9, batch=225, train loss = 1.8898106813430786, train acc = 0.32499998807907104, time = 0.947474479675293\n",
      "Testing at step=9, batch=0, test loss = 1.8554768562316895, test acc = 0.29499998688697815, time = 0.3540980815887451\n",
      "Testing at step=9, batch=5, test loss = 1.8543685674667358, test acc = 0.3499999940395355, time = 0.35417771339416504\n",
      "Testing at step=9, batch=10, test loss = 1.8553509712219238, test acc = 0.36000001430511475, time = 0.35185837745666504\n",
      "Testing at step=9, batch=15, test loss = 1.8663129806518555, test acc = 0.38499999046325684, time = 0.3566875457763672\n",
      "Testing at step=9, batch=20, test loss = 1.784056544303894, test acc = 0.375, time = 0.35846924781799316\n",
      "Testing at step=9, batch=25, test loss = 1.7593705654144287, test acc = 0.39500001072883606, time = 0.34787821769714355\n",
      "Testing at step=9, batch=30, test loss = 1.87776517868042, test acc = 0.3199999928474426, time = 0.35389280319213867\n",
      "Testing at step=9, batch=35, test loss = 1.760701298713684, test acc = 0.38999998569488525, time = 0.35217714309692383\n",
      "Testing at step=9, batch=40, test loss = 1.7425950765609741, test acc = 0.4050000011920929, time = 0.3567237854003906\n",
      "Testing at step=9, batch=45, test loss = 1.8220610618591309, test acc = 0.3700000047683716, time = 0.3495216369628906\n",
      "Step 9 finished in 263.9021027088165, Train loss = 1.8335071716308593, Test loss = 1.8143751001358033; Train Acc = 0.3488000003695488, Test Acc = 0.36359999895095824\n",
      "Training at step=10, batch=0, train loss = 1.7508413791656494, train acc = 0.3499999940395355, time = 0.9396231174468994\n",
      "Training at step=10, batch=25, train loss = 1.8969541788101196, train acc = 0.30000001192092896, time = 0.9539952278137207\n",
      "Training at step=10, batch=50, train loss = 1.8656392097473145, train acc = 0.3700000047683716, time = 0.9611470699310303\n",
      "Training at step=10, batch=75, train loss = 1.7778046131134033, train acc = 0.35499998927116394, time = 0.940178632736206\n",
      "Training at step=10, batch=100, train loss = 1.773421049118042, train acc = 0.36500000953674316, time = 0.9447135925292969\n",
      "Training at step=10, batch=125, train loss = 1.8010284900665283, train acc = 0.32499998807907104, time = 0.9775078296661377\n",
      "Training at step=10, batch=150, train loss = 1.8182721138000488, train acc = 0.3499999940395355, time = 0.9468438625335693\n",
      "Training at step=10, batch=175, train loss = 1.7851202487945557, train acc = 0.3449999988079071, time = 0.9422504901885986\n",
      "Training at step=10, batch=200, train loss = 1.7152289152145386, train acc = 0.4099999964237213, time = 0.9408836364746094\n",
      "Training at step=10, batch=225, train loss = 1.7485709190368652, train acc = 0.4099999964237213, time = 0.9431860446929932\n",
      "Testing at step=10, batch=0, test loss = 1.7277165651321411, test acc = 0.41999998688697815, time = 0.34991979598999023\n",
      "Testing at step=10, batch=5, test loss = 1.841455101966858, test acc = 0.33000001311302185, time = 0.3646688461303711\n",
      "Testing at step=10, batch=10, test loss = 1.8044800758361816, test acc = 0.3799999952316284, time = 0.3515176773071289\n",
      "Testing at step=10, batch=15, test loss = 1.8327925205230713, test acc = 0.36500000953674316, time = 0.3486785888671875\n",
      "Testing at step=10, batch=20, test loss = 1.847288966178894, test acc = 0.35499998927116394, time = 0.36072492599487305\n",
      "Testing at step=10, batch=25, test loss = 1.675628662109375, test acc = 0.4300000071525574, time = 0.36124134063720703\n",
      "Testing at step=10, batch=30, test loss = 1.8048977851867676, test acc = 0.3499999940395355, time = 0.3598616123199463\n",
      "Testing at step=10, batch=35, test loss = 1.7968394756317139, test acc = 0.3199999928474426, time = 0.3525400161743164\n",
      "Testing at step=10, batch=40, test loss = 1.76080322265625, test acc = 0.38999998569488525, time = 0.35476016998291016\n",
      "Testing at step=10, batch=45, test loss = 1.8153965473175049, test acc = 0.3449999988079071, time = 0.36261558532714844\n",
      "Step 10 finished in 263.734210729599, Train loss = 1.8051041378974915, Test loss = 1.7837292957305908; Train Acc = 0.36003999948501586, Test Acc = 0.3698000001907349\n",
      "Training at step=11, batch=0, train loss = 1.7862434387207031, train acc = 0.3400000035762787, time = 0.9517619609832764\n",
      "Training at step=11, batch=25, train loss = 1.7849297523498535, train acc = 0.33500000834465027, time = 0.9431562423706055\n",
      "Training at step=11, batch=50, train loss = 1.8136178255081177, train acc = 0.36000001430511475, time = 1.0659377574920654\n",
      "Training at step=11, batch=75, train loss = 1.681566596031189, train acc = 0.4300000071525574, time = 0.943181037902832\n",
      "Training at step=11, batch=100, train loss = 1.7378981113433838, train acc = 0.39500001072883606, time = 0.9547202587127686\n",
      "Training at step=11, batch=125, train loss = 1.7741334438323975, train acc = 0.36000001430511475, time = 0.9505736827850342\n",
      "Training at step=11, batch=150, train loss = 1.8459181785583496, train acc = 0.3700000047683716, time = 1.0089800357818604\n",
      "Training at step=11, batch=175, train loss = 1.8086280822753906, train acc = 0.4050000011920929, time = 0.9431536197662354\n",
      "Training at step=11, batch=200, train loss = 1.8752025365829468, train acc = 0.35499998927116394, time = 0.9502665996551514\n",
      "Training at step=11, batch=225, train loss = 1.8097586631774902, train acc = 0.3700000047683716, time = 0.9424922466278076\n",
      "Testing at step=11, batch=0, test loss = 1.7798913717269897, test acc = 0.38499999046325684, time = 0.3593144416809082\n",
      "Testing at step=11, batch=5, test loss = 1.866304874420166, test acc = 0.35499998927116394, time = 0.36885976791381836\n",
      "Testing at step=11, batch=10, test loss = 1.7809268236160278, test acc = 0.3499999940395355, time = 0.35102105140686035\n",
      "Testing at step=11, batch=15, test loss = 1.8385766744613647, test acc = 0.3149999976158142, time = 0.36545276641845703\n",
      "Testing at step=11, batch=20, test loss = 1.6762778759002686, test acc = 0.4350000023841858, time = 0.3484528064727783\n",
      "Testing at step=11, batch=25, test loss = 1.674443244934082, test acc = 0.4050000011920929, time = 0.35155272483825684\n",
      "Testing at step=11, batch=30, test loss = 1.6697306632995605, test acc = 0.41499999165534973, time = 0.36461949348449707\n",
      "Testing at step=11, batch=35, test loss = 1.683005928993225, test acc = 0.4300000071525574, time = 0.35540103912353516\n",
      "Testing at step=11, batch=40, test loss = 1.859554648399353, test acc = 0.33000001311302185, time = 0.3478562831878662\n",
      "Testing at step=11, batch=45, test loss = 1.7820324897766113, test acc = 0.4000000059604645, time = 0.3639254570007324\n",
      "Step 11 finished in 263.6204640865326, Train loss = 1.7797206363677978, Test loss = 1.7950274515151978; Train Acc = 0.36890000104904175, Test Acc = 0.3681000006198883\n",
      "Training at step=12, batch=0, train loss = 1.8634397983551025, train acc = 0.3449999988079071, time = 0.9432022571563721\n",
      "Training at step=12, batch=25, train loss = 1.8031105995178223, train acc = 0.3400000035762787, time = 0.9442217350006104\n",
      "Training at step=12, batch=50, train loss = 1.7934449911117554, train acc = 0.36500000953674316, time = 0.9521417617797852\n",
      "Training at step=12, batch=75, train loss = 1.7507967948913574, train acc = 0.3499999940395355, time = 0.945502758026123\n",
      "Training at step=12, batch=100, train loss = 1.8192218542099, train acc = 0.32499998807907104, time = 0.9374656677246094\n",
      "Training at step=12, batch=125, train loss = 1.7442924976348877, train acc = 0.375, time = 0.9356904029846191\n",
      "Training at step=12, batch=150, train loss = 1.7156176567077637, train acc = 0.41999998688697815, time = 0.9405136108398438\n",
      "Training at step=12, batch=175, train loss = 1.7646883726119995, train acc = 0.36000001430511475, time = 0.9396154880523682\n",
      "Training at step=12, batch=200, train loss = 1.627943754196167, train acc = 0.4050000011920929, time = 0.9453730583190918\n",
      "Training at step=12, batch=225, train loss = 1.7622283697128296, train acc = 0.38999998569488525, time = 0.9436919689178467\n",
      "Testing at step=12, batch=0, test loss = 1.7449157238006592, test acc = 0.3050000071525574, time = 0.52663254737854\n",
      "Testing at step=12, batch=5, test loss = 1.706235647201538, test acc = 0.4000000059604645, time = 0.3516535758972168\n",
      "Testing at step=12, batch=10, test loss = 1.7942805290222168, test acc = 0.3499999940395355, time = 0.3654768466949463\n",
      "Testing at step=12, batch=15, test loss = 1.7013369798660278, test acc = 0.3799999952316284, time = 0.36740899085998535\n",
      "Testing at step=12, batch=20, test loss = 1.6999698877334595, test acc = 0.39500001072883606, time = 0.371488094329834\n",
      "Testing at step=12, batch=25, test loss = 1.7937082052230835, test acc = 0.3799999952316284, time = 0.3663344383239746\n",
      "Testing at step=12, batch=30, test loss = 1.652033805847168, test acc = 0.4050000011920929, time = 0.36282992362976074\n",
      "Testing at step=12, batch=35, test loss = 1.7076127529144287, test acc = 0.32499998807907104, time = 0.35561108589172363\n",
      "Testing at step=12, batch=40, test loss = 1.7846157550811768, test acc = 0.39500001072883606, time = 0.36817359924316406\n",
      "Testing at step=12, batch=45, test loss = 1.7650728225708008, test acc = 0.4050000011920929, time = 0.37502264976501465\n",
      "Step 12 finished in 263.9546048641205, Train loss = 1.7643143520355224, Test loss = 1.7450505447387696; Train Acc = 0.3752600008249283, Test Acc = 0.3774000006914139\n",
      "Training at step=13, batch=0, train loss = 1.6809440851211548, train acc = 0.41499999165534973, time = 0.955132246017456\n",
      "Training at step=13, batch=25, train loss = 1.6717852354049683, train acc = 0.4300000071525574, time = 0.947845458984375\n",
      "Training at step=13, batch=50, train loss = 1.7237571477890015, train acc = 0.4099999964237213, time = 0.950178861618042\n",
      "Training at step=13, batch=75, train loss = 1.8217653036117554, train acc = 0.375, time = 0.9485640525817871\n",
      "Training at step=13, batch=100, train loss = 1.658603549003601, train acc = 0.38499999046325684, time = 0.9412946701049805\n",
      "Training at step=13, batch=125, train loss = 1.8311160802841187, train acc = 0.3400000035762787, time = 0.9507300853729248\n",
      "Training at step=13, batch=150, train loss = 1.7200188636779785, train acc = 0.42500001192092896, time = 0.9461367130279541\n",
      "Training at step=13, batch=175, train loss = 1.6719225645065308, train acc = 0.4050000011920929, time = 0.9440350532531738\n",
      "Training at step=13, batch=200, train loss = 1.7164406776428223, train acc = 0.46000000834465027, time = 0.9508204460144043\n",
      "Training at step=13, batch=225, train loss = 1.724525809288025, train acc = 0.3799999952316284, time = 0.9328324794769287\n",
      "Testing at step=13, batch=0, test loss = 1.7512071132659912, test acc = 0.375, time = 0.35837626457214355\n",
      "Testing at step=13, batch=5, test loss = 1.7308564186096191, test acc = 0.38999998569488525, time = 0.36606764793395996\n",
      "Testing at step=13, batch=10, test loss = 1.7487658262252808, test acc = 0.41999998688697815, time = 0.35279130935668945\n",
      "Testing at step=13, batch=15, test loss = 1.831830620765686, test acc = 0.33500000834465027, time = 0.3526930809020996\n",
      "Testing at step=13, batch=20, test loss = 1.697494626045227, test acc = 0.41499999165534973, time = 0.3503074645996094\n",
      "Testing at step=13, batch=25, test loss = 1.7984083890914917, test acc = 0.35499998927116394, time = 0.3466639518737793\n",
      "Testing at step=13, batch=30, test loss = 1.763537883758545, test acc = 0.38999998569488525, time = 0.34841227531433105\n",
      "Testing at step=13, batch=35, test loss = 1.7974138259887695, test acc = 0.3700000047683716, time = 0.35178041458129883\n",
      "Testing at step=13, batch=40, test loss = 1.865403652191162, test acc = 0.35499998927116394, time = 0.3513469696044922\n",
      "Testing at step=13, batch=45, test loss = 1.753705620765686, test acc = 0.4350000023841858, time = 0.3605763912200928\n",
      "Step 13 finished in 263.77495670318604, Train loss = 1.7487258257865905, Test loss = 1.7834693741798402; Train Acc = 0.38106000059843065, Test Acc = 0.36839999973773957\n",
      "Training at step=14, batch=0, train loss = 1.7611613273620605, train acc = 0.41999998688697815, time = 0.9419870376586914\n",
      "Training at step=14, batch=25, train loss = 1.71881103515625, train acc = 0.38499999046325684, time = 0.9465928077697754\n",
      "Training at step=14, batch=50, train loss = 1.6729910373687744, train acc = 0.42500001192092896, time = 0.9354851245880127\n",
      "Training at step=14, batch=75, train loss = 1.6745747327804565, train acc = 0.4050000011920929, time = 0.9553816318511963\n",
      "Training at step=14, batch=100, train loss = 1.7935662269592285, train acc = 0.3700000047683716, time = 0.9602105617523193\n",
      "Training at step=14, batch=125, train loss = 1.6580880880355835, train acc = 0.4300000071525574, time = 0.941504716873169\n",
      "Training at step=14, batch=150, train loss = 1.7566022872924805, train acc = 0.3400000035762787, time = 0.9497482776641846\n",
      "Training at step=14, batch=175, train loss = 1.7607461214065552, train acc = 0.3700000047683716, time = 0.9413979053497314\n",
      "Training at step=14, batch=200, train loss = 1.6986057758331299, train acc = 0.41499999165534973, time = 0.9439985752105713\n",
      "Training at step=14, batch=225, train loss = 1.836276650428772, train acc = 0.3700000047683716, time = 0.9461891651153564\n",
      "Testing at step=14, batch=0, test loss = 1.7277276515960693, test acc = 0.4050000011920929, time = 0.3532860279083252\n",
      "Testing at step=14, batch=5, test loss = 1.7354298830032349, test acc = 0.36500000953674316, time = 0.3648509979248047\n",
      "Testing at step=14, batch=10, test loss = 1.7548829317092896, test acc = 0.36000001430511475, time = 0.35236239433288574\n",
      "Testing at step=14, batch=15, test loss = 1.856697678565979, test acc = 0.3449999988079071, time = 0.3569066524505615\n",
      "Testing at step=14, batch=20, test loss = 1.6231480836868286, test acc = 0.41499999165534973, time = 0.3560147285461426\n",
      "Testing at step=14, batch=25, test loss = 1.6392062902450562, test acc = 0.4699999988079071, time = 0.3518834114074707\n",
      "Testing at step=14, batch=30, test loss = 1.7528423070907593, test acc = 0.3700000047683716, time = 0.3510768413543701\n",
      "Testing at step=14, batch=35, test loss = 1.7056198120117188, test acc = 0.41499999165534973, time = 0.3502938747406006\n",
      "Testing at step=14, batch=40, test loss = 1.7387762069702148, test acc = 0.4050000011920929, time = 0.3732106685638428\n",
      "Testing at step=14, batch=45, test loss = 1.750266671180725, test acc = 0.38999998569488525, time = 0.34375643730163574\n",
      "Step 14 finished in 263.5457730293274, Train loss = 1.7367006692886353, Test loss = 1.7378918051719665; Train Acc = 0.3838199999332428, Test Acc = 0.3818999981880188\n",
      "Training at step=15, batch=0, train loss = 1.6060950756072998, train acc = 0.41999998688697815, time = 0.9500763416290283\n",
      "Training at step=15, batch=25, train loss = 1.7021507024765015, train acc = 0.38999998569488525, time = 0.9395081996917725\n",
      "Training at step=15, batch=50, train loss = 1.7163474559783936, train acc = 0.39500001072883606, time = 0.9427106380462646\n",
      "Training at step=15, batch=75, train loss = 1.638403058052063, train acc = 0.39500001072883606, time = 0.9341869354248047\n",
      "Training at step=15, batch=100, train loss = 1.6282695531845093, train acc = 0.41499999165534973, time = 0.9486541748046875\n",
      "Training at step=15, batch=125, train loss = 1.7089240550994873, train acc = 0.38499999046325684, time = 0.9538874626159668\n",
      "Training at step=15, batch=150, train loss = 1.6614428758621216, train acc = 0.42500001192092896, time = 0.951725959777832\n",
      "Training at step=15, batch=175, train loss = 1.778149127960205, train acc = 0.3400000035762787, time = 0.9550466537475586\n",
      "Training at step=15, batch=200, train loss = 1.6704270839691162, train acc = 0.4050000011920929, time = 0.9497132301330566\n",
      "Training at step=15, batch=225, train loss = 1.6533998250961304, train acc = 0.38999998569488525, time = 0.9403262138366699\n",
      "Testing at step=15, batch=0, test loss = 1.8001527786254883, test acc = 0.3799999952316284, time = 0.3559303283691406\n",
      "Testing at step=15, batch=5, test loss = 1.6588777303695679, test acc = 0.41499999165534973, time = 0.35588932037353516\n",
      "Testing at step=15, batch=10, test loss = 1.7591102123260498, test acc = 0.3499999940395355, time = 0.35585832595825195\n",
      "Testing at step=15, batch=15, test loss = 1.9297113418579102, test acc = 0.375, time = 0.3547508716583252\n",
      "Testing at step=15, batch=20, test loss = 1.7382694482803345, test acc = 0.38999998569488525, time = 0.35134148597717285\n",
      "Testing at step=15, batch=25, test loss = 1.6690869331359863, test acc = 0.42500001192092896, time = 0.35820698738098145\n",
      "Testing at step=15, batch=30, test loss = 1.778633713722229, test acc = 0.3400000035762787, time = 0.35369014739990234\n",
      "Testing at step=15, batch=35, test loss = 1.7556109428405762, test acc = 0.4000000059604645, time = 0.35175418853759766\n",
      "Testing at step=15, batch=40, test loss = 1.7686476707458496, test acc = 0.38999998569488525, time = 0.35294437408447266\n",
      "Testing at step=15, batch=45, test loss = 1.7046711444854736, test acc = 0.3700000047683716, time = 0.3523843288421631\n",
      "Step 15 finished in 263.6799352169037, Train loss = 1.7138587470054627, Test loss = 1.716925482749939; Train Acc = 0.39251999962329864, Test Acc = 0.38790000081062315\n",
      "Training at step=16, batch=0, train loss = 1.6946535110473633, train acc = 0.4300000071525574, time = 0.9457252025604248\n",
      "Training at step=16, batch=25, train loss = 1.7832746505737305, train acc = 0.33500000834465027, time = 0.9511551856994629\n",
      "Training at step=16, batch=50, train loss = 1.6564216613769531, train acc = 0.3799999952316284, time = 0.9397878646850586\n",
      "Training at step=16, batch=75, train loss = 1.6294796466827393, train acc = 0.42500001192092896, time = 0.9871432781219482\n",
      "Training at step=16, batch=100, train loss = 1.689133644104004, train acc = 0.4000000059604645, time = 0.9519460201263428\n",
      "Training at step=16, batch=125, train loss = 1.784967064857483, train acc = 0.3499999940395355, time = 0.9430053234100342\n",
      "Training at step=16, batch=150, train loss = 1.6343175172805786, train acc = 0.38999998569488525, time = 0.951056718826294\n",
      "Training at step=16, batch=175, train loss = 1.7642954587936401, train acc = 0.3449999988079071, time = 0.9687392711639404\n",
      "Training at step=16, batch=200, train loss = 1.7791575193405151, train acc = 0.33000001311302185, time = 0.9465973377227783\n",
      "Training at step=16, batch=225, train loss = 1.6263684034347534, train acc = 0.45500001311302185, time = 0.9411256313323975\n",
      "Testing at step=16, batch=0, test loss = 1.6724541187286377, test acc = 0.39500001072883606, time = 0.3518526554107666\n",
      "Testing at step=16, batch=5, test loss = 1.751583218574524, test acc = 0.3499999940395355, time = 0.358184814453125\n",
      "Testing at step=16, batch=10, test loss = 1.6795027256011963, test acc = 0.39500001072883606, time = 0.34859347343444824\n",
      "Testing at step=16, batch=15, test loss = 1.5402100086212158, test acc = 0.4350000023841858, time = 0.35007667541503906\n",
      "Testing at step=16, batch=20, test loss = 1.7048717737197876, test acc = 0.3799999952316284, time = 0.35464048385620117\n",
      "Testing at step=16, batch=25, test loss = 1.764082908630371, test acc = 0.42500001192092896, time = 0.3557124137878418\n",
      "Testing at step=16, batch=30, test loss = 1.6458075046539307, test acc = 0.3799999952316284, time = 0.34978485107421875\n",
      "Testing at step=16, batch=35, test loss = 1.6841984987258911, test acc = 0.39500001072883606, time = 0.35881948471069336\n",
      "Testing at step=16, batch=40, test loss = 1.7057647705078125, test acc = 0.35499998927116394, time = 0.34633660316467285\n",
      "Testing at step=16, batch=45, test loss = 1.756066918373108, test acc = 0.38499999046325684, time = 0.3607301712036133\n",
      "Step 16 finished in 263.31865668296814, Train loss = 1.7005628995895385, Test loss = 1.7110478329658507; Train Acc = 0.3981399995088577, Test Acc = 0.3916999989748001\n",
      "Training at step=17, batch=0, train loss = 1.7258039712905884, train acc = 0.41499999165534973, time = 0.9513914585113525\n",
      "Training at step=17, batch=25, train loss = 1.7476310729980469, train acc = 0.38999998569488525, time = 0.9435689449310303\n",
      "Training at step=17, batch=50, train loss = 1.766579270362854, train acc = 0.3400000035762787, time = 0.9746768474578857\n",
      "Training at step=17, batch=75, train loss = 1.7915825843811035, train acc = 0.38999998569488525, time = 0.9325547218322754\n",
      "Training at step=17, batch=100, train loss = 1.7029008865356445, train acc = 0.375, time = 0.9347398281097412\n",
      "Training at step=17, batch=125, train loss = 1.6625752449035645, train acc = 0.4000000059604645, time = 0.9385437965393066\n",
      "Training at step=17, batch=150, train loss = 1.6775437593460083, train acc = 0.4749999940395355, time = 0.9363555908203125\n",
      "Training at step=17, batch=175, train loss = 1.743082880973816, train acc = 0.36500000953674316, time = 1.0121350288391113\n",
      "Training at step=17, batch=200, train loss = 1.7743864059448242, train acc = 0.3700000047683716, time = 0.9325494766235352\n",
      "Training at step=17, batch=225, train loss = 1.74898362159729, train acc = 0.375, time = 0.9605152606964111\n",
      "Testing at step=17, batch=0, test loss = 1.6910884380340576, test acc = 0.38499999046325684, time = 0.3666675090789795\n",
      "Testing at step=17, batch=5, test loss = 1.6312501430511475, test acc = 0.41499999165534973, time = 0.35369181632995605\n",
      "Testing at step=17, batch=10, test loss = 1.713447093963623, test acc = 0.4099999964237213, time = 0.360992431640625\n",
      "Testing at step=17, batch=15, test loss = 1.703584909439087, test acc = 0.39500001072883606, time = 0.351456880569458\n",
      "Testing at step=17, batch=20, test loss = 1.70499587059021, test acc = 0.3799999952316284, time = 0.3590717315673828\n",
      "Testing at step=17, batch=25, test loss = 1.6834030151367188, test acc = 0.39500001072883606, time = 0.35800695419311523\n",
      "Testing at step=17, batch=30, test loss = 1.7339900732040405, test acc = 0.38999998569488525, time = 0.3603835105895996\n",
      "Testing at step=17, batch=35, test loss = 1.6918237209320068, test acc = 0.4300000071525574, time = 0.35268354415893555\n",
      "Testing at step=17, batch=40, test loss = 1.863335371017456, test acc = 0.3149999976158142, time = 0.35569214820861816\n",
      "Testing at step=17, batch=45, test loss = 1.6591566801071167, test acc = 0.42500001192092896, time = 0.3555736541748047\n",
      "Step 17 finished in 262.80333399772644, Train loss = 1.69364346075058, Test loss = 1.6882777643203735; Train Acc = 0.40173999893665313, Test Acc = 0.4047000002861023\n",
      "Training at step=18, batch=0, train loss = 1.5771479606628418, train acc = 0.41499999165534973, time = 0.9403913021087646\n",
      "Training at step=18, batch=25, train loss = 1.5940301418304443, train acc = 0.4650000035762787, time = 0.9397962093353271\n",
      "Training at step=18, batch=50, train loss = 1.7584805488586426, train acc = 0.41499999165534973, time = 0.9462125301361084\n",
      "Training at step=18, batch=75, train loss = 1.6135964393615723, train acc = 0.41999998688697815, time = 0.9462831020355225\n",
      "Training at step=18, batch=100, train loss = 1.7201461791992188, train acc = 0.36500000953674316, time = 0.9458503723144531\n",
      "Training at step=18, batch=125, train loss = 1.7643458843231201, train acc = 0.38499999046325684, time = 0.9370150566101074\n",
      "Training at step=18, batch=150, train loss = 1.7450366020202637, train acc = 0.35499998927116394, time = 0.9456000328063965\n",
      "Training at step=18, batch=175, train loss = 1.6703811883926392, train acc = 0.4000000059604645, time = 0.9393510818481445\n",
      "Training at step=18, batch=200, train loss = 1.6045043468475342, train acc = 0.45500001311302185, time = 0.9609479904174805\n",
      "Training at step=18, batch=225, train loss = 1.6768786907196045, train acc = 0.41499999165534973, time = 0.9490232467651367\n",
      "Testing at step=18, batch=0, test loss = 1.6454963684082031, test acc = 0.3700000047683716, time = 0.36440420150756836\n",
      "Testing at step=18, batch=5, test loss = 1.8271510601043701, test acc = 0.35499998927116394, time = 0.3477046489715576\n",
      "Testing at step=18, batch=10, test loss = 1.5671298503875732, test acc = 0.4399999976158142, time = 0.35610151290893555\n",
      "Testing at step=18, batch=15, test loss = 1.772347092628479, test acc = 0.3700000047683716, time = 0.35276055335998535\n",
      "Testing at step=18, batch=20, test loss = 1.7510796785354614, test acc = 0.3700000047683716, time = 0.35617828369140625\n",
      "Testing at step=18, batch=25, test loss = 1.6344962120056152, test acc = 0.4050000011920929, time = 0.3579857349395752\n",
      "Testing at step=18, batch=30, test loss = 1.655092716217041, test acc = 0.4000000059604645, time = 0.359346866607666\n",
      "Testing at step=18, batch=35, test loss = 1.5756924152374268, test acc = 0.4449999928474426, time = 0.35720038414001465\n",
      "Testing at step=18, batch=40, test loss = 1.5645222663879395, test acc = 0.4350000023841858, time = 0.36216068267822266\n",
      "Testing at step=18, batch=45, test loss = 1.6649574041366577, test acc = 0.39500001072883606, time = 0.3499631881713867\n",
      "Step 18 finished in 263.38514256477356, Train loss = 1.674844769001007, Test loss = 1.6880758309364319; Train Acc = 0.40663999879360196, Test Acc = 0.39990000128746034\n",
      "Training at step=19, batch=0, train loss = 1.744583010673523, train acc = 0.4000000059604645, time = 0.9524486064910889\n",
      "Training at step=19, batch=25, train loss = 1.6629406213760376, train acc = 0.4300000071525574, time = 0.9344432353973389\n",
      "Training at step=19, batch=50, train loss = 1.5833433866500854, train acc = 0.4350000023841858, time = 0.9515950679779053\n",
      "Training at step=19, batch=75, train loss = 1.539429783821106, train acc = 0.5, time = 0.9326786994934082\n",
      "Training at step=19, batch=100, train loss = 1.6814639568328857, train acc = 0.38999998569488525, time = 0.9385476112365723\n",
      "Training at step=19, batch=125, train loss = 1.7083830833435059, train acc = 0.39500001072883606, time = 0.9483270645141602\n",
      "Training at step=19, batch=150, train loss = 1.6549632549285889, train acc = 0.4350000023841858, time = 0.9383320808410645\n",
      "Training at step=19, batch=175, train loss = 1.7155506610870361, train acc = 0.3499999940395355, time = 0.932478666305542\n",
      "Training at step=19, batch=200, train loss = 1.6408473253250122, train acc = 0.4099999964237213, time = 0.9460909366607666\n",
      "Training at step=19, batch=225, train loss = 1.6449748277664185, train acc = 0.4099999964237213, time = 0.9506502151489258\n",
      "Testing at step=19, batch=0, test loss = 1.7096426486968994, test acc = 0.3499999940395355, time = 0.36128973960876465\n",
      "Testing at step=19, batch=5, test loss = 1.7214590311050415, test acc = 0.41999998688697815, time = 0.35877490043640137\n",
      "Testing at step=19, batch=10, test loss = 1.717461347579956, test acc = 0.4050000011920929, time = 0.3498561382293701\n",
      "Testing at step=19, batch=15, test loss = 1.566494345664978, test acc = 0.41499999165534973, time = 0.3552584648132324\n",
      "Testing at step=19, batch=20, test loss = 1.7209830284118652, test acc = 0.3400000035762787, time = 0.3538789749145508\n",
      "Testing at step=19, batch=25, test loss = 1.6936583518981934, test acc = 0.41499999165534973, time = 0.3498554229736328\n",
      "Testing at step=19, batch=30, test loss = 1.5981773138046265, test acc = 0.42500001192092896, time = 0.3546774387359619\n",
      "Testing at step=19, batch=35, test loss = 1.5576211214065552, test acc = 0.41499999165534973, time = 0.35738158226013184\n",
      "Testing at step=19, batch=40, test loss = 1.6962612867355347, test acc = 0.39500001072883606, time = 0.3510427474975586\n",
      "Testing at step=19, batch=45, test loss = 1.662206768989563, test acc = 0.39500001072883606, time = 0.34838056564331055\n",
      "Step 19 finished in 263.3919925689697, Train loss = 1.668829981803894, Test loss = 1.6768689155578613; Train Acc = 0.40694000041484835, Test Acc = 0.403199999332428\n",
      "Training at step=20, batch=0, train loss = 1.629449486732483, train acc = 0.4050000011920929, time = 0.9475970268249512\n",
      "Training at step=20, batch=25, train loss = 1.6041357517242432, train acc = 0.41499999165534973, time = 0.9368805885314941\n",
      "Training at step=20, batch=50, train loss = 1.6096516847610474, train acc = 0.45500001311302185, time = 0.9588124752044678\n",
      "Training at step=20, batch=75, train loss = 1.6930891275405884, train acc = 0.38499999046325684, time = 0.9599297046661377\n",
      "Training at step=20, batch=100, train loss = 1.5120600461959839, train acc = 0.4749999940395355, time = 0.9527385234832764\n",
      "Training at step=20, batch=125, train loss = 1.6936944723129272, train acc = 0.4000000059604645, time = 0.9559934139251709\n",
      "Training at step=20, batch=150, train loss = 1.4726868867874146, train acc = 0.4449999928474426, time = 0.9423580169677734\n",
      "Training at step=20, batch=175, train loss = 1.7190978527069092, train acc = 0.41999998688697815, time = 0.9434647560119629\n",
      "Training at step=20, batch=200, train loss = 1.6560026407241821, train acc = 0.4000000059604645, time = 0.9450056552886963\n",
      "Training at step=20, batch=225, train loss = 1.6671985387802124, train acc = 0.4099999964237213, time = 0.9594323635101318\n",
      "Testing at step=20, batch=0, test loss = 1.6973377466201782, test acc = 0.41499999165534973, time = 0.35933828353881836\n",
      "Testing at step=20, batch=5, test loss = 1.6035891771316528, test acc = 0.4350000023841858, time = 0.3538081645965576\n",
      "Testing at step=20, batch=10, test loss = 1.6921989917755127, test acc = 0.38999998569488525, time = 0.3549060821533203\n",
      "Testing at step=20, batch=15, test loss = 1.6470730304718018, test acc = 0.3799999952316284, time = 0.34879231452941895\n",
      "Testing at step=20, batch=20, test loss = 1.716895341873169, test acc = 0.38999998569488525, time = 0.35227203369140625\n",
      "Testing at step=20, batch=25, test loss = 1.7650818824768066, test acc = 0.35499998927116394, time = 0.3542613983154297\n",
      "Testing at step=20, batch=30, test loss = 1.6743760108947754, test acc = 0.4050000011920929, time = 0.35066986083984375\n",
      "Testing at step=20, batch=35, test loss = 1.6374484300613403, test acc = 0.3700000047683716, time = 0.3497030735015869\n",
      "Testing at step=20, batch=40, test loss = 1.795823097229004, test acc = 0.4050000011920929, time = 0.3541295528411865\n",
      "Testing at step=20, batch=45, test loss = 1.4958947896957397, test acc = 0.5, time = 0.350266695022583\n",
      "Step 20 finished in 263.7455174922943, Train loss = 1.6595758743286133, Test loss = 1.6675587487220764; Train Acc = 0.4122199990749359, Test Acc = 0.4054999965429306\n",
      "Training at step=21, batch=0, train loss = 1.6109544038772583, train acc = 0.41499999165534973, time = 0.9438958168029785\n",
      "Training at step=21, batch=25, train loss = 1.6552472114562988, train acc = 0.4050000011920929, time = 0.9513635635375977\n",
      "Training at step=21, batch=50, train loss = 1.671034574508667, train acc = 0.41999998688697815, time = 0.9424126148223877\n",
      "Training at step=21, batch=75, train loss = 1.689988613128662, train acc = 0.41499999165534973, time = 0.9541606903076172\n",
      "Training at step=21, batch=100, train loss = 1.7357237339019775, train acc = 0.38499999046325684, time = 0.9479398727416992\n",
      "Training at step=21, batch=125, train loss = 1.6656767129898071, train acc = 0.4000000059604645, time = 0.941002607345581\n",
      "Training at step=21, batch=150, train loss = 1.6885476112365723, train acc = 0.41499999165534973, time = 0.9615597724914551\n",
      "Training at step=21, batch=175, train loss = 1.6139171123504639, train acc = 0.4749999940395355, time = 0.937509298324585\n",
      "Training at step=21, batch=200, train loss = 1.6060503721237183, train acc = 0.41499999165534973, time = 0.9461636543273926\n",
      "Training at step=21, batch=225, train loss = 1.603452444076538, train acc = 0.41499999165534973, time = 0.9544112682342529\n",
      "Testing at step=21, batch=0, test loss = 1.5744199752807617, test acc = 0.4300000071525574, time = 0.35166239738464355\n",
      "Testing at step=21, batch=5, test loss = 1.6940900087356567, test acc = 0.39500001072883606, time = 0.3519597053527832\n",
      "Testing at step=21, batch=10, test loss = 1.6253423690795898, test acc = 0.4399999976158142, time = 0.3658158779144287\n",
      "Testing at step=21, batch=15, test loss = 1.683385968208313, test acc = 0.375, time = 0.36635518074035645\n",
      "Testing at step=21, batch=20, test loss = 1.6528161764144897, test acc = 0.4000000059604645, time = 0.36197733879089355\n",
      "Testing at step=21, batch=25, test loss = 1.5918588638305664, test acc = 0.4399999976158142, time = 0.36051249504089355\n",
      "Testing at step=21, batch=30, test loss = 1.6658830642700195, test acc = 0.4399999976158142, time = 0.35630154609680176\n",
      "Testing at step=21, batch=35, test loss = 1.5172041654586792, test acc = 0.4650000035762787, time = 0.35753822326660156\n",
      "Testing at step=21, batch=40, test loss = 1.6908197402954102, test acc = 0.44999998807907104, time = 0.3511481285095215\n",
      "Testing at step=21, batch=45, test loss = 1.7189794778823853, test acc = 0.3700000047683716, time = 0.35282039642333984\n",
      "Step 21 finished in 264.0886995792389, Train loss = 1.6425236206054687, Test loss = 1.6403304839134216; Train Acc = 0.41748000025749205, Test Acc = 0.4168999993801117\n",
      "Training at step=22, batch=0, train loss = 1.6502970457077026, train acc = 0.44999998807907104, time = 0.9892652034759521\n",
      "Training at step=22, batch=25, train loss = 1.5750319957733154, train acc = 0.47999998927116394, time = 0.9516751766204834\n",
      "Training at step=22, batch=50, train loss = 1.6105103492736816, train acc = 0.41499999165534973, time = 0.9359574317932129\n",
      "Training at step=22, batch=75, train loss = 1.6494622230529785, train acc = 0.4449999928474426, time = 0.936286211013794\n",
      "Training at step=22, batch=100, train loss = 1.676939845085144, train acc = 0.36000001430511475, time = 0.9401810169219971\n",
      "Training at step=22, batch=125, train loss = 1.738233208656311, train acc = 0.36500000953674316, time = 0.9583945274353027\n",
      "Training at step=22, batch=150, train loss = 1.519356369972229, train acc = 0.46000000834465027, time = 0.9593150615692139\n",
      "Training at step=22, batch=175, train loss = 1.6606857776641846, train acc = 0.4399999976158142, time = 0.9509947299957275\n",
      "Training at step=22, batch=200, train loss = 1.7099380493164062, train acc = 0.36500000953674316, time = 0.939542293548584\n",
      "Training at step=22, batch=225, train loss = 1.653703212738037, train acc = 0.4099999964237213, time = 0.9487597942352295\n",
      "Testing at step=22, batch=0, test loss = 1.5131027698516846, test acc = 0.4449999928474426, time = 0.35588884353637695\n",
      "Testing at step=22, batch=5, test loss = 1.7142268419265747, test acc = 0.36500000953674316, time = 0.35176825523376465\n",
      "Testing at step=22, batch=10, test loss = 1.7091485261917114, test acc = 0.4000000059604645, time = 0.3724675178527832\n",
      "Testing at step=22, batch=15, test loss = 1.7791128158569336, test acc = 0.375, time = 0.3640754222869873\n",
      "Testing at step=22, batch=20, test loss = 1.5878599882125854, test acc = 0.4050000011920929, time = 0.3627321720123291\n",
      "Testing at step=22, batch=25, test loss = 1.6869145631790161, test acc = 0.38499999046325684, time = 0.36104869842529297\n",
      "Testing at step=22, batch=30, test loss = 1.6484266519546509, test acc = 0.4650000035762787, time = 0.3665435314178467\n",
      "Testing at step=22, batch=35, test loss = 1.5649524927139282, test acc = 0.44999998807907104, time = 0.36469197273254395\n",
      "Testing at step=22, batch=40, test loss = 1.624334692955017, test acc = 0.41999998688697815, time = 0.3618481159210205\n",
      "Testing at step=22, batch=45, test loss = 1.532088041305542, test acc = 0.48500001430511475, time = 0.36458492279052734\n",
      "Step 22 finished in 264.2734396457672, Train loss = 1.6338165435791017, Test loss = 1.656495943069458; Train Acc = 0.41866000080108645, Test Acc = 0.40989999830722806\n",
      "Training at step=23, batch=0, train loss = 1.663101315498352, train acc = 0.42500001192092896, time = 0.9457392692565918\n",
      "Training at step=23, batch=25, train loss = 1.579525351524353, train acc = 0.4699999988079071, time = 0.9366958141326904\n",
      "Training at step=23, batch=50, train loss = 1.6769410371780396, train acc = 0.41999998688697815, time = 0.9482560157775879\n",
      "Training at step=23, batch=75, train loss = 1.7228431701660156, train acc = 0.33000001311302185, time = 0.9416563510894775\n",
      "Training at step=23, batch=100, train loss = 1.6518430709838867, train acc = 0.41999998688697815, time = 0.9607300758361816\n",
      "Training at step=23, batch=125, train loss = 1.5403906106948853, train acc = 0.4650000035762787, time = 0.9482123851776123\n",
      "Training at step=23, batch=150, train loss = 1.5957227945327759, train acc = 0.38999998569488525, time = 0.9532685279846191\n",
      "Training at step=23, batch=175, train loss = 1.7430520057678223, train acc = 0.36500000953674316, time = 0.9338841438293457\n",
      "Training at step=23, batch=200, train loss = 1.717962622642517, train acc = 0.3799999952316284, time = 0.955003023147583\n",
      "Training at step=23, batch=225, train loss = 1.650199294090271, train acc = 0.45500001311302185, time = 0.9509584903717041\n",
      "Testing at step=23, batch=0, test loss = 1.7402000427246094, test acc = 0.375, time = 0.3588600158691406\n",
      "Testing at step=23, batch=5, test loss = 1.595902681350708, test acc = 0.42500001192092896, time = 0.3521420955657959\n",
      "Testing at step=23, batch=10, test loss = 1.7201752662658691, test acc = 0.4300000071525574, time = 0.3564267158508301\n",
      "Testing at step=23, batch=15, test loss = 1.7367454767227173, test acc = 0.38999998569488525, time = 0.36223459243774414\n",
      "Testing at step=23, batch=20, test loss = 1.5842454433441162, test acc = 0.4050000011920929, time = 0.35360074043273926\n",
      "Testing at step=23, batch=25, test loss = 1.777827262878418, test acc = 0.35499998927116394, time = 0.34947896003723145\n",
      "Testing at step=23, batch=30, test loss = 1.5426955223083496, test acc = 0.4650000035762787, time = 0.34899091720581055\n",
      "Testing at step=23, batch=35, test loss = 1.6820645332336426, test acc = 0.41499999165534973, time = 0.3683943748474121\n",
      "Testing at step=23, batch=40, test loss = 1.7425105571746826, test acc = 0.3499999940395355, time = 0.351668119430542\n",
      "Testing at step=23, batch=45, test loss = 1.5707359313964844, test acc = 0.45500001311302185, time = 0.36490416526794434\n",
      "Step 23 finished in 263.90853691101074, Train loss = 1.628800971031189, Test loss = 1.6580419421195984; Train Acc = 0.4204999997615814, Test Acc = 0.41149999797344206\n",
      "Training at step=24, batch=0, train loss = 1.717972755432129, train acc = 0.41499999165534973, time = 0.9545528888702393\n",
      "Training at step=24, batch=25, train loss = 1.6471959352493286, train acc = 0.4300000071525574, time = 0.9427757263183594\n",
      "Training at step=24, batch=50, train loss = 1.63358473777771, train acc = 0.41999998688697815, time = 0.9574499130249023\n",
      "Training at step=24, batch=75, train loss = 1.6012132167816162, train acc = 0.4300000071525574, time = 0.953930139541626\n",
      "Training at step=24, batch=100, train loss = 1.6616536378860474, train acc = 0.4000000059604645, time = 0.950721263885498\n",
      "Training at step=24, batch=125, train loss = 1.628594994544983, train acc = 0.38999998569488525, time = 0.9494850635528564\n",
      "Training at step=24, batch=150, train loss = 1.486464500427246, train acc = 0.46000000834465027, time = 0.9375219345092773\n",
      "Training at step=24, batch=175, train loss = 1.4936230182647705, train acc = 0.4699999988079071, time = 0.9461686611175537\n",
      "Training at step=24, batch=200, train loss = 1.7660911083221436, train acc = 0.38999998569488525, time = 0.9335281848907471\n",
      "Training at step=24, batch=225, train loss = 1.5759615898132324, train acc = 0.46000000834465027, time = 0.9367921352386475\n",
      "Testing at step=24, batch=0, test loss = 1.615352988243103, test acc = 0.4350000023841858, time = 0.35368919372558594\n",
      "Testing at step=24, batch=5, test loss = 1.6510882377624512, test acc = 0.41499999165534973, time = 0.3551933765411377\n",
      "Testing at step=24, batch=10, test loss = 1.4893871545791626, test acc = 0.5099999904632568, time = 0.3517036437988281\n",
      "Testing at step=24, batch=15, test loss = 1.6653854846954346, test acc = 0.3799999952316284, time = 0.3590819835662842\n",
      "Testing at step=24, batch=20, test loss = 1.6438381671905518, test acc = 0.41499999165534973, time = 0.37404775619506836\n",
      "Testing at step=24, batch=25, test loss = 1.735988736152649, test acc = 0.36500000953674316, time = 0.36122751235961914\n",
      "Testing at step=24, batch=30, test loss = 1.6527305841445923, test acc = 0.4050000011920929, time = 0.36617016792297363\n",
      "Testing at step=24, batch=35, test loss = 1.5952333211898804, test acc = 0.4300000071525574, time = 0.3574867248535156\n",
      "Testing at step=24, batch=40, test loss = 1.6246205568313599, test acc = 0.41499999165534973, time = 0.3519399166107178\n",
      "Testing at step=24, batch=45, test loss = 1.7287782430648804, test acc = 0.39500001072883606, time = 0.3662683963775635\n",
      "Step 24 finished in 263.7998278141022, Train loss = 1.6154707903862, Test loss = 1.6317360663414002; Train Acc = 0.42709999895095824, Test Acc = 0.4233999985456467\n",
      "Training at step=25, batch=0, train loss = 1.5183043479919434, train acc = 0.4399999976158142, time = 0.967414379119873\n",
      "Training at step=25, batch=25, train loss = 1.4560356140136719, train acc = 0.47999998927116394, time = 0.9476528167724609\n",
      "Training at step=25, batch=50, train loss = 1.6598588228225708, train acc = 0.375, time = 0.9481627941131592\n",
      "Training at step=25, batch=75, train loss = 1.6838634014129639, train acc = 0.4300000071525574, time = 0.9434223175048828\n",
      "Training at step=25, batch=100, train loss = 1.7798782587051392, train acc = 0.38999998569488525, time = 0.9742710590362549\n",
      "Training at step=25, batch=125, train loss = 1.4641140699386597, train acc = 0.4950000047683716, time = 0.9422216415405273\n",
      "Training at step=25, batch=150, train loss = 1.6376720666885376, train acc = 0.4749999940395355, time = 0.9625914096832275\n",
      "Training at step=25, batch=175, train loss = 1.7566406726837158, train acc = 0.375, time = 0.9400439262390137\n",
      "Training at step=25, batch=200, train loss = 1.6494718790054321, train acc = 0.44999998807907104, time = 0.9375813007354736\n",
      "Training at step=25, batch=225, train loss = 1.534225583076477, train acc = 0.4399999976158142, time = 0.9486491680145264\n",
      "Testing at step=25, batch=0, test loss = 1.7046644687652588, test acc = 0.3700000047683716, time = 0.36070990562438965\n",
      "Testing at step=25, batch=5, test loss = 1.716884970664978, test acc = 0.38499999046325684, time = 0.3581845760345459\n",
      "Testing at step=25, batch=10, test loss = 1.6385698318481445, test acc = 0.36000001430511475, time = 0.36646318435668945\n",
      "Testing at step=25, batch=15, test loss = 1.6290004253387451, test acc = 0.42500001192092896, time = 0.35189127922058105\n",
      "Testing at step=25, batch=20, test loss = 1.7520297765731812, test acc = 0.3799999952316284, time = 0.3619520664215088\n",
      "Testing at step=25, batch=25, test loss = 1.7417473793029785, test acc = 0.38999998569488525, time = 0.3672158718109131\n",
      "Testing at step=25, batch=30, test loss = 1.6246380805969238, test acc = 0.41499999165534973, time = 0.3589742183685303\n",
      "Testing at step=25, batch=35, test loss = 1.6757616996765137, test acc = 0.375, time = 0.37134885787963867\n",
      "Testing at step=25, batch=40, test loss = 1.713112473487854, test acc = 0.41999998688697815, time = 0.35515570640563965\n",
      "Testing at step=25, batch=45, test loss = 1.630416750907898, test acc = 0.4000000059604645, time = 0.36791133880615234\n",
      "Step 25 finished in 264.0362284183502, Train loss = 1.610533106327057, Test loss = 1.627988681793213; Train Acc = 0.4286599994897842, Test Acc = 0.4161000007390976\n",
      "Training at step=26, batch=0, train loss = 1.6149511337280273, train acc = 0.36500000953674316, time = 0.9484157562255859\n",
      "Training at step=26, batch=25, train loss = 1.4557429552078247, train acc = 0.5249999761581421, time = 0.9530582427978516\n",
      "Training at step=26, batch=50, train loss = 1.4955073595046997, train acc = 0.4699999988079071, time = 0.9362623691558838\n",
      "Training at step=26, batch=75, train loss = 1.5496339797973633, train acc = 0.42500001192092896, time = 0.9434800148010254\n",
      "Training at step=26, batch=100, train loss = 1.572170376777649, train acc = 0.41499999165534973, time = 0.9491205215454102\n",
      "Training at step=26, batch=125, train loss = 1.791602611541748, train acc = 0.4050000011920929, time = 0.9399361610412598\n",
      "Training at step=26, batch=150, train loss = 1.6175026893615723, train acc = 0.41499999165534973, time = 0.9385178089141846\n",
      "Training at step=26, batch=175, train loss = 1.8277009725570679, train acc = 0.3499999940395355, time = 0.9479970932006836\n",
      "Training at step=26, batch=200, train loss = 1.6873174905776978, train acc = 0.41999998688697815, time = 0.9329702854156494\n",
      "Training at step=26, batch=225, train loss = 1.5420869588851929, train acc = 0.44999998807907104, time = 0.9461715221405029\n",
      "Testing at step=26, batch=0, test loss = 1.68100905418396, test acc = 0.44999998807907104, time = 0.3625366687774658\n",
      "Testing at step=26, batch=5, test loss = 1.718569040298462, test acc = 0.4000000059604645, time = 0.35737109184265137\n",
      "Testing at step=26, batch=10, test loss = 1.5419833660125732, test acc = 0.44999998807907104, time = 0.36801886558532715\n",
      "Testing at step=26, batch=15, test loss = 1.6623327732086182, test acc = 0.42500001192092896, time = 0.3544647693634033\n",
      "Testing at step=26, batch=20, test loss = 1.693688154220581, test acc = 0.3799999952316284, time = 0.36305928230285645\n",
      "Testing at step=26, batch=25, test loss = 1.5404132604599, test acc = 0.45500001311302185, time = 0.35794496536254883\n",
      "Testing at step=26, batch=30, test loss = 1.6198786497116089, test acc = 0.4300000071525574, time = 0.36290431022644043\n",
      "Testing at step=26, batch=35, test loss = 1.7610962390899658, test acc = 0.36500000953674316, time = 0.35976290702819824\n",
      "Testing at step=26, batch=40, test loss = 1.637555718421936, test acc = 0.4099999964237213, time = 0.3543417453765869\n",
      "Testing at step=26, batch=45, test loss = 1.651183009147644, test acc = 0.4650000035762787, time = 0.3568558692932129\n",
      "Step 26 finished in 264.0223286151886, Train loss = 1.6010499329566956, Test loss = 1.6312423038482666; Train Acc = 0.4317999995946884, Test Acc = 0.42450000047683717\n",
      "Training at step=27, batch=0, train loss = 1.5881760120391846, train acc = 0.4300000071525574, time = 0.951453447341919\n",
      "Training at step=27, batch=25, train loss = 1.4824552536010742, train acc = 0.4650000035762787, time = 0.942979097366333\n",
      "Training at step=27, batch=50, train loss = 1.4990075826644897, train acc = 0.41499999165534973, time = 0.9378187656402588\n",
      "Training at step=27, batch=75, train loss = 1.701583981513977, train acc = 0.4000000059604645, time = 0.9425392150878906\n",
      "Training at step=27, batch=100, train loss = 1.6519209146499634, train acc = 0.45500001311302185, time = 0.9437017440795898\n",
      "Training at step=27, batch=125, train loss = 1.4700742959976196, train acc = 0.48500001430511475, time = 1.0807414054870605\n",
      "Training at step=27, batch=150, train loss = 1.6856426000595093, train acc = 0.4050000011920929, time = 0.9448175430297852\n",
      "Training at step=27, batch=175, train loss = 1.6244620084762573, train acc = 0.4650000035762787, time = 0.9432644844055176\n",
      "Training at step=27, batch=200, train loss = 1.6882998943328857, train acc = 0.4000000059604645, time = 0.9459652900695801\n",
      "Training at step=27, batch=225, train loss = 1.5745903253555298, train acc = 0.45500001311302185, time = 1.0091123580932617\n",
      "Testing at step=27, batch=0, test loss = 1.7510924339294434, test acc = 0.35499998927116394, time = 0.3478507995605469\n",
      "Testing at step=27, batch=5, test loss = 1.538520336151123, test acc = 0.4099999964237213, time = 0.3511083126068115\n",
      "Testing at step=27, batch=10, test loss = 1.5928345918655396, test acc = 0.4650000035762787, time = 0.35468053817749023\n",
      "Testing at step=27, batch=15, test loss = 1.669232964515686, test acc = 0.4350000023841858, time = 0.3596537113189697\n",
      "Testing at step=27, batch=20, test loss = 1.4817651510238647, test acc = 0.5099999904632568, time = 0.351637601852417\n",
      "Testing at step=27, batch=25, test loss = 1.635999321937561, test acc = 0.4399999976158142, time = 0.34906005859375\n",
      "Testing at step=27, batch=30, test loss = 1.7391232252120972, test acc = 0.36000001430511475, time = 0.36723971366882324\n",
      "Testing at step=27, batch=35, test loss = 1.622029423713684, test acc = 0.39500001072883606, time = 0.3484008312225342\n",
      "Testing at step=27, batch=40, test loss = 1.589249610900879, test acc = 0.39500001072883606, time = 0.35067009925842285\n",
      "Testing at step=27, batch=45, test loss = 1.606162667274475, test acc = 0.4000000059604645, time = 0.35512256622314453\n",
      "Step 27 finished in 264.0074248313904, Train loss = 1.594391674041748, Test loss = 1.6285302114486695; Train Acc = 0.4352000004053116, Test Acc = 0.4110000020265579\n",
      "Training at step=28, batch=0, train loss = 1.5778589248657227, train acc = 0.41499999165534973, time = 0.9634289741516113\n",
      "Training at step=28, batch=25, train loss = 1.578628659248352, train acc = 0.4099999964237213, time = 0.9485263824462891\n",
      "Training at step=28, batch=50, train loss = 1.5942667722702026, train acc = 0.46000000834465027, time = 0.9429256916046143\n",
      "Training at step=28, batch=75, train loss = 1.6659260988235474, train acc = 0.39500001072883606, time = 0.9387376308441162\n",
      "Training at step=28, batch=100, train loss = 1.5929641723632812, train acc = 0.4350000023841858, time = 0.9393365383148193\n",
      "Training at step=28, batch=125, train loss = 1.6369682550430298, train acc = 0.4350000023841858, time = 0.9570503234863281\n",
      "Training at step=28, batch=150, train loss = 1.603161334991455, train acc = 0.41999998688697815, time = 0.9405465126037598\n",
      "Training at step=28, batch=175, train loss = 1.6487371921539307, train acc = 0.375, time = 0.9456126689910889\n",
      "Training at step=28, batch=200, train loss = 1.653702735900879, train acc = 0.39500001072883606, time = 0.9450228214263916\n",
      "Training at step=28, batch=225, train loss = 1.6179126501083374, train acc = 0.4350000023841858, time = 0.9539093971252441\n",
      "Testing at step=28, batch=0, test loss = 1.5453191995620728, test acc = 0.4749999940395355, time = 0.3514580726623535\n",
      "Testing at step=28, batch=5, test loss = 1.6994123458862305, test acc = 0.3700000047683716, time = 0.3507089614868164\n",
      "Testing at step=28, batch=10, test loss = 1.6771690845489502, test acc = 0.36500000953674316, time = 0.3576326370239258\n",
      "Testing at step=28, batch=15, test loss = 1.6598823070526123, test acc = 0.35499998927116394, time = 0.35011982917785645\n",
      "Testing at step=28, batch=20, test loss = 1.6941262483596802, test acc = 0.41499999165534973, time = 0.3520941734313965\n",
      "Testing at step=28, batch=25, test loss = 1.665528416633606, test acc = 0.41499999165534973, time = 0.3466637134552002\n",
      "Testing at step=28, batch=30, test loss = 1.7353858947753906, test acc = 0.3400000035762787, time = 0.34885191917419434\n",
      "Testing at step=28, batch=35, test loss = 1.7103586196899414, test acc = 0.35499998927116394, time = 0.34874963760375977\n",
      "Testing at step=28, batch=40, test loss = 1.5161281824111938, test acc = 0.4650000035762787, time = 0.34787988662719727\n",
      "Testing at step=28, batch=45, test loss = 1.6728699207305908, test acc = 0.39500001072883606, time = 0.3554854393005371\n",
      "Step 28 finished in 263.35548758506775, Train loss = 1.5838951287269591, Test loss = 1.6156271409988403; Train Acc = 0.43810000050067904, Test Acc = 0.4244000005722046\n",
      "Training at step=29, batch=0, train loss = 1.6911554336547852, train acc = 0.36000001430511475, time = 0.9442572593688965\n",
      "Training at step=29, batch=25, train loss = 1.644241452217102, train acc = 0.4300000071525574, time = 0.9691026210784912\n",
      "Training at step=29, batch=50, train loss = 1.5150444507598877, train acc = 0.38499999046325684, time = 0.9489810466766357\n",
      "Training at step=29, batch=75, train loss = 1.4774880409240723, train acc = 0.4950000047683716, time = 0.9496915340423584\n",
      "Training at step=29, batch=100, train loss = 1.6280895471572876, train acc = 0.42500001192092896, time = 0.9411389827728271\n",
      "Training at step=29, batch=125, train loss = 1.6117416620254517, train acc = 0.41999998688697815, time = 0.9505698680877686\n",
      "Training at step=29, batch=150, train loss = 1.5169504880905151, train acc = 0.4350000023841858, time = 0.962709903717041\n",
      "Training at step=29, batch=175, train loss = 1.8217297792434692, train acc = 0.28999999165534973, time = 0.9351649284362793\n",
      "Training at step=29, batch=200, train loss = 1.4916446208953857, train acc = 0.4449999928474426, time = 0.9485740661621094\n",
      "Training at step=29, batch=225, train loss = 1.5602699518203735, train acc = 0.46000000834465027, time = 0.9505689144134521\n",
      "Testing at step=29, batch=0, test loss = 1.6042898893356323, test acc = 0.46000000834465027, time = 0.3690063953399658\n",
      "Testing at step=29, batch=5, test loss = 1.483812689781189, test acc = 0.4650000035762787, time = 0.3533475399017334\n",
      "Testing at step=29, batch=10, test loss = 1.6289077997207642, test acc = 0.4050000011920929, time = 0.3690786361694336\n",
      "Testing at step=29, batch=15, test loss = 1.5999200344085693, test acc = 0.4050000011920929, time = 0.4875335693359375\n",
      "Testing at step=29, batch=20, test loss = 1.505293846130371, test acc = 0.4749999940395355, time = 0.36193013191223145\n",
      "Testing at step=29, batch=25, test loss = 1.5284419059753418, test acc = 0.4300000071525574, time = 0.35716962814331055\n",
      "Testing at step=29, batch=30, test loss = 1.642424464225769, test acc = 0.39500001072883606, time = 0.3663341999053955\n",
      "Testing at step=29, batch=35, test loss = 1.6964908838272095, test acc = 0.4300000071525574, time = 0.36464667320251465\n",
      "Testing at step=29, batch=40, test loss = 1.628791093826294, test acc = 0.38499999046325684, time = 0.35431909561157227\n",
      "Testing at step=29, batch=45, test loss = 1.4882283210754395, test acc = 0.4449999928474426, time = 0.3627598285675049\n",
      "Step 29 finished in 263.81317377090454, Train loss = 1.576656864643097, Test loss = 1.602135100364685; Train Acc = 0.44030000078678133, Test Acc = 0.43170000076293946\n",
      "Training at step=30, batch=0, train loss = 1.594617486000061, train acc = 0.45500001311302185, time = 0.9373831748962402\n",
      "Training at step=30, batch=25, train loss = 1.458750605583191, train acc = 0.5049999952316284, time = 0.9476010799407959\n",
      "Training at step=30, batch=50, train loss = 1.795536756515503, train acc = 0.39500001072883606, time = 0.9360692501068115\n",
      "Training at step=30, batch=75, train loss = 1.7062896490097046, train acc = 0.41499999165534973, time = 0.9562561511993408\n",
      "Training at step=30, batch=100, train loss = 1.726000189781189, train acc = 0.38499999046325684, time = 0.9496073722839355\n",
      "Training at step=30, batch=125, train loss = 1.5769107341766357, train acc = 0.38999998569488525, time = 0.9404916763305664\n",
      "Training at step=30, batch=150, train loss = 1.6190690994262695, train acc = 0.38499999046325684, time = 0.9382343292236328\n",
      "Training at step=30, batch=175, train loss = 1.6160006523132324, train acc = 0.41499999165534973, time = 0.9348545074462891\n",
      "Training at step=30, batch=200, train loss = 1.7187265157699585, train acc = 0.3449999988079071, time = 0.9503340721130371\n",
      "Training at step=30, batch=225, train loss = 1.6486248970031738, train acc = 0.41499999165534973, time = 0.9367821216583252\n",
      "Testing at step=30, batch=0, test loss = 1.635867714881897, test acc = 0.4399999976158142, time = 0.3615899085998535\n",
      "Testing at step=30, batch=5, test loss = 1.6458343267440796, test acc = 0.4300000071525574, time = 0.3487880229949951\n",
      "Testing at step=30, batch=10, test loss = 1.5508677959442139, test acc = 0.42500001192092896, time = 0.40519022941589355\n",
      "Testing at step=30, batch=15, test loss = 1.5384242534637451, test acc = 0.4449999928474426, time = 0.35169529914855957\n",
      "Testing at step=30, batch=20, test loss = 1.7674914598464966, test acc = 0.35499998927116394, time = 0.34453487396240234\n",
      "Testing at step=30, batch=25, test loss = 1.5900144577026367, test acc = 0.5, time = 0.3471686840057373\n",
      "Testing at step=30, batch=30, test loss = 1.600740909576416, test acc = 0.44999998807907104, time = 0.3490626811981201\n",
      "Testing at step=30, batch=35, test loss = 1.6412100791931152, test acc = 0.41999998688697815, time = 0.35541200637817383\n",
      "Testing at step=30, batch=40, test loss = 1.5691800117492676, test acc = 0.5, time = 0.35313916206359863\n",
      "Testing at step=30, batch=45, test loss = 1.615222454071045, test acc = 0.38499999046325684, time = 0.3580472469329834\n",
      "Step 30 finished in 263.7035505771637, Train loss = 1.572932635307312, Test loss = 1.6032391571998597; Train Acc = 0.44252000021934507, Test Acc = 0.42689999878406526\n",
      "Training at step=31, batch=0, train loss = 1.5287100076675415, train acc = 0.38999998569488525, time = 0.942678689956665\n",
      "Training at step=31, batch=25, train loss = 1.5265374183654785, train acc = 0.41999998688697815, time = 0.9438235759735107\n",
      "Training at step=31, batch=50, train loss = 1.604322910308838, train acc = 0.39500001072883606, time = 0.9392983913421631\n",
      "Training at step=31, batch=75, train loss = 1.5776283740997314, train acc = 0.4399999976158142, time = 0.9492185115814209\n",
      "Training at step=31, batch=100, train loss = 1.7044782638549805, train acc = 0.38499999046325684, time = 0.9525599479675293\n",
      "Training at step=31, batch=125, train loss = 1.6148741245269775, train acc = 0.38999998569488525, time = 0.9350221157073975\n",
      "Training at step=31, batch=150, train loss = 1.5443339347839355, train acc = 0.46000000834465027, time = 0.9549119472503662\n",
      "Training at step=31, batch=175, train loss = 1.5217163562774658, train acc = 0.48500001430511475, time = 0.9494287967681885\n",
      "Training at step=31, batch=200, train loss = 1.4940904378890991, train acc = 0.46000000834465027, time = 0.9397134780883789\n",
      "Training at step=31, batch=225, train loss = 1.4593818187713623, train acc = 0.48500001430511475, time = 0.9334685802459717\n",
      "Testing at step=31, batch=0, test loss = 1.6103997230529785, test acc = 0.41499999165534973, time = 0.36292171478271484\n",
      "Testing at step=31, batch=5, test loss = 1.5438401699066162, test acc = 0.46000000834465027, time = 0.35274291038513184\n",
      "Testing at step=31, batch=10, test loss = 1.6127275228500366, test acc = 0.45500001311302185, time = 0.3543415069580078\n",
      "Testing at step=31, batch=15, test loss = 1.7227261066436768, test acc = 0.4000000059604645, time = 0.35654687881469727\n",
      "Testing at step=31, batch=20, test loss = 1.5033690929412842, test acc = 0.48500001430511475, time = 0.3553898334503174\n",
      "Testing at step=31, batch=25, test loss = 1.553347110748291, test acc = 0.4699999988079071, time = 0.37564635276794434\n",
      "Testing at step=31, batch=30, test loss = 1.7426402568817139, test acc = 0.3449999988079071, time = 0.35720133781433105\n",
      "Testing at step=31, batch=35, test loss = 1.7236114740371704, test acc = 0.36000001430511475, time = 0.3632664680480957\n",
      "Testing at step=31, batch=40, test loss = 1.6448768377304077, test acc = 0.4399999976158142, time = 0.36994123458862305\n",
      "Testing at step=31, batch=45, test loss = 1.6837621927261353, test acc = 0.38999998569488525, time = 0.35445356369018555\n",
      "Step 31 finished in 264.25074100494385, Train loss = 1.5683210310935973, Test loss = 1.5992059803009033; Train Acc = 0.44380000019073484, Test Acc = 0.4300999993085861\n",
      "Training at step=32, batch=0, train loss = 1.4145519733428955, train acc = 0.46000000834465027, time = 0.9391570091247559\n",
      "Training at step=32, batch=25, train loss = 1.6389023065567017, train acc = 0.4300000071525574, time = 0.9534971714019775\n",
      "Training at step=32, batch=50, train loss = 1.4226880073547363, train acc = 0.5149999856948853, time = 0.9430420398712158\n",
      "Training at step=32, batch=75, train loss = 1.510927677154541, train acc = 0.46000000834465027, time = 0.9450702667236328\n",
      "Training at step=32, batch=100, train loss = 1.627327561378479, train acc = 0.4350000023841858, time = 0.948371410369873\n",
      "Training at step=32, batch=125, train loss = 1.538810133934021, train acc = 0.46000000834465027, time = 0.9558684825897217\n",
      "Training at step=32, batch=150, train loss = 1.5069079399108887, train acc = 0.45500001311302185, time = 1.1097919940948486\n",
      "Training at step=32, batch=175, train loss = 1.5895915031433105, train acc = 0.46000000834465027, time = 0.9461872577667236\n",
      "Training at step=32, batch=200, train loss = 1.5986236333847046, train acc = 0.4300000071525574, time = 0.944460391998291\n",
      "Training at step=32, batch=225, train loss = 1.5288329124450684, train acc = 0.4300000071525574, time = 0.9532589912414551\n",
      "Testing at step=32, batch=0, test loss = 1.651168942451477, test acc = 0.4000000059604645, time = 0.36270928382873535\n",
      "Testing at step=32, batch=5, test loss = 1.513112187385559, test acc = 0.46000000834465027, time = 0.35765743255615234\n",
      "Testing at step=32, batch=10, test loss = 1.555221676826477, test acc = 0.48500001430511475, time = 0.35430097579956055\n",
      "Testing at step=32, batch=15, test loss = 1.586000680923462, test acc = 0.4950000047683716, time = 0.34888243675231934\n",
      "Testing at step=32, batch=20, test loss = 1.6199674606323242, test acc = 0.41499999165534973, time = 0.34894442558288574\n",
      "Testing at step=32, batch=25, test loss = 1.6372404098510742, test acc = 0.41499999165534973, time = 0.35189199447631836\n",
      "Testing at step=32, batch=30, test loss = 1.537886142730713, test acc = 0.4399999976158142, time = 0.3541371822357178\n",
      "Testing at step=32, batch=35, test loss = 1.4060496091842651, test acc = 0.5249999761581421, time = 0.3553884029388428\n",
      "Testing at step=32, batch=40, test loss = 1.4969308376312256, test acc = 0.49000000953674316, time = 0.34919238090515137\n",
      "Testing at step=32, batch=45, test loss = 1.5649467706680298, test acc = 0.48500001430511475, time = 0.3617575168609619\n",
      "Step 32 finished in 263.5190839767456, Train loss = 1.5607749972343445, Test loss = 1.5837886118888855; Train Acc = 0.4464800007343292, Test Acc = 0.44470000207424165\n",
      "Training at step=33, batch=0, train loss = 1.5635097026824951, train acc = 0.46000000834465027, time = 0.943035364151001\n",
      "Training at step=33, batch=25, train loss = 1.6693894863128662, train acc = 0.41999998688697815, time = 0.9529614448547363\n",
      "Training at step=33, batch=50, train loss = 1.6806012392044067, train acc = 0.4000000059604645, time = 0.9401466846466064\n",
      "Training at step=33, batch=75, train loss = 1.4777004718780518, train acc = 0.5149999856948853, time = 0.9501767158508301\n",
      "Training at step=33, batch=100, train loss = 1.4339238405227661, train acc = 0.47999998927116394, time = 0.9446399211883545\n",
      "Training at step=33, batch=125, train loss = 1.5246516466140747, train acc = 0.4650000035762787, time = 0.9543988704681396\n",
      "Training at step=33, batch=150, train loss = 1.6426845788955688, train acc = 0.41499999165534973, time = 0.9446516036987305\n",
      "Training at step=33, batch=175, train loss = 1.542746901512146, train acc = 0.45500001311302185, time = 0.9439404010772705\n",
      "Training at step=33, batch=200, train loss = 1.5287699699401855, train acc = 0.4449999928474426, time = 0.9431579113006592\n",
      "Training at step=33, batch=225, train loss = 1.560616135597229, train acc = 0.4699999988079071, time = 0.934760332107544\n",
      "Testing at step=33, batch=0, test loss = 1.6360843181610107, test acc = 0.4449999928474426, time = 0.35355162620544434\n",
      "Testing at step=33, batch=5, test loss = 1.6196262836456299, test acc = 0.45500001311302185, time = 0.3502352237701416\n",
      "Testing at step=33, batch=10, test loss = 1.7626475095748901, test acc = 0.3799999952316284, time = 0.3564481735229492\n",
      "Testing at step=33, batch=15, test loss = 1.603562593460083, test acc = 0.42500001192092896, time = 0.3544464111328125\n",
      "Testing at step=33, batch=20, test loss = 1.6109119653701782, test acc = 0.4399999976158142, time = 0.3521406650543213\n",
      "Testing at step=33, batch=25, test loss = 1.677607774734497, test acc = 0.39500001072883606, time = 0.3546617031097412\n",
      "Testing at step=33, batch=30, test loss = 1.5313818454742432, test acc = 0.4350000023841858, time = 0.351426362991333\n",
      "Testing at step=33, batch=35, test loss = 1.5467807054519653, test acc = 0.46000000834465027, time = 0.34988880157470703\n",
      "Testing at step=33, batch=40, test loss = 1.5081498622894287, test acc = 0.4650000035762787, time = 0.35281848907470703\n",
      "Testing at step=33, batch=45, test loss = 1.6770576238632202, test acc = 0.41999998688697815, time = 0.35784101486206055\n",
      "Step 33 finished in 263.59906935691833, Train loss = 1.5560909943580628, Test loss = 1.58985520362854; Train Acc = 0.4498000005483627, Test Acc = 0.4390000003576279\n",
      "Training at step=34, batch=0, train loss = 1.5577281713485718, train acc = 0.4350000023841858, time = 0.9593713283538818\n",
      "Training at step=34, batch=25, train loss = 1.6176214218139648, train acc = 0.4399999976158142, time = 0.9476213455200195\n",
      "Training at step=34, batch=50, train loss = 1.4921754598617554, train acc = 0.4699999988079071, time = 0.9539318084716797\n",
      "Training at step=34, batch=75, train loss = 1.6085689067840576, train acc = 0.4050000011920929, time = 0.9408993721008301\n",
      "Training at step=34, batch=100, train loss = 1.5294805765151978, train acc = 0.46000000834465027, time = 0.9393692016601562\n",
      "Training at step=34, batch=125, train loss = 1.5206267833709717, train acc = 0.5, time = 0.9321238994598389\n",
      "Training at step=34, batch=150, train loss = 1.5959283113479614, train acc = 0.4449999928474426, time = 0.9406144618988037\n",
      "Training at step=34, batch=175, train loss = 1.6501426696777344, train acc = 0.4000000059604645, time = 0.9427130222320557\n",
      "Training at step=34, batch=200, train loss = 1.6002763509750366, train acc = 0.4449999928474426, time = 0.9405879974365234\n",
      "Training at step=34, batch=225, train loss = 1.6120731830596924, train acc = 0.46000000834465027, time = 0.9432597160339355\n",
      "Testing at step=34, batch=0, test loss = 1.570357084274292, test acc = 0.46000000834465027, time = 0.36204051971435547\n",
      "Testing at step=34, batch=5, test loss = 1.4604171514511108, test acc = 0.4300000071525574, time = 0.3586885929107666\n",
      "Testing at step=34, batch=10, test loss = 1.4572079181671143, test acc = 0.5149999856948853, time = 0.3546741008758545\n",
      "Testing at step=34, batch=15, test loss = 1.7670718431472778, test acc = 0.3700000047683716, time = 0.3643152713775635\n",
      "Testing at step=34, batch=20, test loss = 1.6579794883728027, test acc = 0.4050000011920929, time = 0.36893248558044434\n",
      "Testing at step=34, batch=25, test loss = 1.6675376892089844, test acc = 0.41999998688697815, time = 0.3671436309814453\n",
      "Testing at step=34, batch=30, test loss = 1.6592216491699219, test acc = 0.38999998569488525, time = 0.34980154037475586\n",
      "Testing at step=34, batch=35, test loss = 1.6773240566253662, test acc = 0.41499999165534973, time = 0.35484790802001953\n",
      "Testing at step=34, batch=40, test loss = 1.5497655868530273, test acc = 0.41499999165534973, time = 0.3458433151245117\n",
      "Testing at step=34, batch=45, test loss = 1.7033534049987793, test acc = 0.4050000011920929, time = 0.3521108627319336\n",
      "Step 34 finished in 263.82424545288086, Train loss = 1.5462051181793213, Test loss = 1.5847921752929688; Train Acc = 0.45087999999523165, Test Acc = 0.4376000010967255\n",
      "Training at step=35, batch=0, train loss = 1.6574852466583252, train acc = 0.39500001072883606, time = 0.9517695903778076\n",
      "Training at step=35, batch=25, train loss = 1.6002488136291504, train acc = 0.41999998688697815, time = 0.9515619277954102\n",
      "Training at step=35, batch=50, train loss = 1.491665005683899, train acc = 0.5149999856948853, time = 0.9562630653381348\n",
      "Training at step=35, batch=75, train loss = 1.5646798610687256, train acc = 0.4399999976158142, time = 0.9421424865722656\n",
      "Training at step=35, batch=100, train loss = 1.4174911975860596, train acc = 0.5149999856948853, time = 0.9487690925598145\n",
      "Training at step=35, batch=125, train loss = 1.5106942653656006, train acc = 0.4350000023841858, time = 0.9396264553070068\n",
      "Training at step=35, batch=150, train loss = 1.5936553478240967, train acc = 0.4300000071525574, time = 0.9418447017669678\n",
      "Training at step=35, batch=175, train loss = 1.5264559984207153, train acc = 0.4650000035762787, time = 0.9520924091339111\n",
      "Training at step=35, batch=200, train loss = 1.422908902168274, train acc = 0.5049999952316284, time = 0.9450981616973877\n",
      "Training at step=35, batch=225, train loss = 1.4092755317687988, train acc = 0.5149999856948853, time = 0.9512984752655029\n",
      "Testing at step=35, batch=0, test loss = 1.545363187789917, test acc = 0.46000000834465027, time = 0.35431790351867676\n",
      "Testing at step=35, batch=5, test loss = 1.5688350200653076, test acc = 0.45500001311302185, time = 0.3543388843536377\n",
      "Testing at step=35, batch=10, test loss = 1.6189717054367065, test acc = 0.41499999165534973, time = 0.3543365001678467\n",
      "Testing at step=35, batch=15, test loss = 1.5962271690368652, test acc = 0.41999998688697815, time = 0.35234951972961426\n",
      "Testing at step=35, batch=20, test loss = 1.4933336973190308, test acc = 0.4950000047683716, time = 0.35924649238586426\n",
      "Testing at step=35, batch=25, test loss = 1.6430842876434326, test acc = 0.4000000059604645, time = 0.36286163330078125\n",
      "Testing at step=35, batch=30, test loss = 1.6845670938491821, test acc = 0.4650000035762787, time = 0.3493199348449707\n",
      "Testing at step=35, batch=35, test loss = 1.729761004447937, test acc = 0.36500000953674316, time = 0.36498069763183594\n",
      "Testing at step=35, batch=40, test loss = 1.5114353895187378, test acc = 0.4699999988079071, time = 0.3557732105255127\n",
      "Testing at step=35, batch=45, test loss = 1.6399965286254883, test acc = 0.39500001072883606, time = 0.356201171875\n",
      "Step 35 finished in 263.8963670730591, Train loss = 1.539224078655243, Test loss = 1.5873003244400024; Train Acc = 0.4547199989557266, Test Acc = 0.4359999990463257\n",
      "Training at step=36, batch=0, train loss = 1.7098357677459717, train acc = 0.42500001192092896, time = 0.9490594863891602\n",
      "Training at step=36, batch=25, train loss = 1.4875712394714355, train acc = 0.45500001311302185, time = 0.9428095817565918\n",
      "Training at step=36, batch=50, train loss = 1.6375008821487427, train acc = 0.38999998569488525, time = 0.9402120113372803\n",
      "Training at step=36, batch=75, train loss = 1.6542412042617798, train acc = 0.42500001192092896, time = 0.9400064945220947\n",
      "Training at step=36, batch=100, train loss = 1.725927472114563, train acc = 0.4050000011920929, time = 0.9412505626678467\n",
      "Training at step=36, batch=125, train loss = 1.5770045518875122, train acc = 0.4449999928474426, time = 0.9598686695098877\n",
      "Training at step=36, batch=150, train loss = 1.550447940826416, train acc = 0.4950000047683716, time = 0.957146406173706\n",
      "Training at step=36, batch=175, train loss = 1.538385033607483, train acc = 0.46000000834465027, time = 0.9434943199157715\n",
      "Training at step=36, batch=200, train loss = 1.6011179685592651, train acc = 0.39500001072883606, time = 0.9443089962005615\n",
      "Training at step=36, batch=225, train loss = 1.5989654064178467, train acc = 0.4650000035762787, time = 0.9427614212036133\n",
      "Testing at step=36, batch=0, test loss = 1.5226225852966309, test acc = 0.44999998807907104, time = 0.35784077644348145\n",
      "Testing at step=36, batch=5, test loss = 1.5237841606140137, test acc = 0.44999998807907104, time = 0.35705018043518066\n",
      "Testing at step=36, batch=10, test loss = 1.5998197793960571, test acc = 0.41499999165534973, time = 0.3561367988586426\n",
      "Testing at step=36, batch=15, test loss = 1.4952337741851807, test acc = 0.4699999988079071, time = 0.3812751770019531\n",
      "Testing at step=36, batch=20, test loss = 1.6445074081420898, test acc = 0.45500001311302185, time = 0.35306715965270996\n",
      "Testing at step=36, batch=25, test loss = 1.6503205299377441, test acc = 0.41499999165534973, time = 0.347898006439209\n",
      "Testing at step=36, batch=30, test loss = 1.5451087951660156, test acc = 0.4000000059604645, time = 0.35256052017211914\n",
      "Testing at step=36, batch=35, test loss = 1.5561009645462036, test acc = 0.45500001311302185, time = 0.3524947166442871\n",
      "Testing at step=36, batch=40, test loss = 1.5255683660507202, test acc = 0.45500001311302185, time = 0.3465282917022705\n",
      "Testing at step=36, batch=45, test loss = 1.6296483278274536, test acc = 0.41999998688697815, time = 0.35177040100097656\n",
      "Step 36 finished in 264.11860275268555, Train loss = 1.5359392809867858, Test loss = 1.5760415148735047; Train Acc = 0.45714000046253206, Test Acc = 0.43909999787807463\n",
      "Training at step=37, batch=0, train loss = 1.6943037509918213, train acc = 0.3700000047683716, time = 0.9425733089447021\n",
      "Training at step=37, batch=25, train loss = 1.5347256660461426, train acc = 0.4350000023841858, time = 0.9625113010406494\n",
      "Training at step=37, batch=50, train loss = 1.603116750717163, train acc = 0.44999998807907104, time = 0.9376697540283203\n",
      "Training at step=37, batch=75, train loss = 1.540686011314392, train acc = 0.4650000035762787, time = 0.9356880187988281\n",
      "Training at step=37, batch=100, train loss = 1.513675332069397, train acc = 0.4650000035762787, time = 0.9385960102081299\n",
      "Training at step=37, batch=125, train loss = 1.546897292137146, train acc = 0.4749999940395355, time = 0.9425764083862305\n",
      "Training at step=37, batch=150, train loss = 1.5544581413269043, train acc = 0.4300000071525574, time = 0.9584348201751709\n",
      "Training at step=37, batch=175, train loss = 1.4299894571304321, train acc = 0.4950000047683716, time = 1.1283740997314453\n",
      "Training at step=37, batch=200, train loss = 1.5596661567687988, train acc = 0.4449999928474426, time = 0.9430904388427734\n",
      "Training at step=37, batch=225, train loss = 1.3680732250213623, train acc = 0.5400000214576721, time = 0.9415881633758545\n",
      "Testing at step=37, batch=0, test loss = 1.5024293661117554, test acc = 0.42500001192092896, time = 0.40184617042541504\n",
      "Testing at step=37, batch=5, test loss = 1.6839441061019897, test acc = 0.39500001072883606, time = 0.35384058952331543\n",
      "Testing at step=37, batch=10, test loss = 1.614549994468689, test acc = 0.38999998569488525, time = 0.35328221321105957\n",
      "Testing at step=37, batch=15, test loss = 1.6545523405075073, test acc = 0.38999998569488525, time = 0.35260701179504395\n",
      "Testing at step=37, batch=20, test loss = 1.6357992887496948, test acc = 0.44999998807907104, time = 0.34821391105651855\n",
      "Testing at step=37, batch=25, test loss = 1.5861003398895264, test acc = 0.4399999976158142, time = 0.3485276699066162\n",
      "Testing at step=37, batch=30, test loss = 1.5479463338851929, test acc = 0.4449999928474426, time = 0.3620717525482178\n",
      "Testing at step=37, batch=35, test loss = 1.5707801580429077, test acc = 0.4399999976158142, time = 0.3545076847076416\n",
      "Testing at step=37, batch=40, test loss = 1.6154826879501343, test acc = 0.47999998927116394, time = 0.3662893772125244\n",
      "Testing at step=37, batch=45, test loss = 1.5973223447799683, test acc = 0.41499999165534973, time = 0.35531115531921387\n",
      "Step 37 finished in 263.7408616542816, Train loss = 1.5296355900764464, Test loss = 1.5764065527915954; Train Acc = 0.4560400000810623, Test Acc = 0.44139999747276304\n",
      "Training at step=38, batch=0, train loss = 1.5777713060379028, train acc = 0.46000000834465027, time = 0.9525938034057617\n",
      "Training at step=38, batch=25, train loss = 1.6776325702667236, train acc = 0.38999998569488525, time = 0.9413871765136719\n",
      "Training at step=38, batch=50, train loss = 1.482609748840332, train acc = 0.42500001192092896, time = 0.9477322101593018\n",
      "Training at step=38, batch=75, train loss = 1.4118056297302246, train acc = 0.550000011920929, time = 0.9459984302520752\n",
      "Training at step=38, batch=100, train loss = 1.457423448562622, train acc = 0.4699999988079071, time = 0.9531946182250977\n",
      "Training at step=38, batch=125, train loss = 1.5032975673675537, train acc = 0.5049999952316284, time = 0.9368865489959717\n",
      "Training at step=38, batch=150, train loss = 1.5495381355285645, train acc = 0.4699999988079071, time = 0.9611589908599854\n",
      "Training at step=38, batch=175, train loss = 1.4848881959915161, train acc = 0.47999998927116394, time = 0.9501533508300781\n",
      "Training at step=38, batch=200, train loss = 1.4705651998519897, train acc = 0.4950000047683716, time = 0.9507551193237305\n",
      "Training at step=38, batch=225, train loss = 1.5556614398956299, train acc = 0.42500001192092896, time = 0.9354202747344971\n",
      "Testing at step=38, batch=0, test loss = 1.6147956848144531, test acc = 0.41999998688697815, time = 0.3557431697845459\n",
      "Testing at step=38, batch=5, test loss = 1.5417479276657104, test acc = 0.44999998807907104, time = 0.34625792503356934\n",
      "Testing at step=38, batch=10, test loss = 1.6915873289108276, test acc = 0.42500001192092896, time = 0.349825382232666\n",
      "Testing at step=38, batch=15, test loss = 1.5809544324874878, test acc = 0.4650000035762787, time = 0.3520665168762207\n",
      "Testing at step=38, batch=20, test loss = 1.6003249883651733, test acc = 0.4350000023841858, time = 0.35562610626220703\n",
      "Testing at step=38, batch=25, test loss = 1.6606918573379517, test acc = 0.4350000023841858, time = 0.3480520248413086\n",
      "Testing at step=38, batch=30, test loss = 1.5334725379943848, test acc = 0.4650000035762787, time = 0.3566560745239258\n",
      "Testing at step=38, batch=35, test loss = 1.653714895248413, test acc = 0.38999998569488525, time = 0.35929322242736816\n",
      "Testing at step=38, batch=40, test loss = 1.6100207567214966, test acc = 0.42500001192092896, time = 0.3652012348175049\n",
      "Testing at step=38, batch=45, test loss = 1.5883172750473022, test acc = 0.41999998688697815, time = 0.3502192497253418\n",
      "Step 38 finished in 263.46956157684326, Train loss = 1.5288350472450256, Test loss = 1.56331636428833; Train Acc = 0.45766000032424925, Test Acc = 0.44949999868869783\n",
      "Training at step=39, batch=0, train loss = 1.4068199396133423, train acc = 0.5099999904632568, time = 0.9447853565216064\n",
      "Training at step=39, batch=25, train loss = 1.5318139791488647, train acc = 0.48500001430511475, time = 0.9537262916564941\n",
      "Training at step=39, batch=50, train loss = 1.42793607711792, train acc = 0.47999998927116394, time = 0.942101001739502\n",
      "Training at step=39, batch=75, train loss = 1.5733730792999268, train acc = 0.41499999165534973, time = 0.9411284923553467\n",
      "Training at step=39, batch=100, train loss = 1.4421837329864502, train acc = 0.47999998927116394, time = 0.9518551826477051\n",
      "Training at step=39, batch=125, train loss = 1.516455888748169, train acc = 0.46000000834465027, time = 0.9410474300384521\n",
      "Training at step=39, batch=150, train loss = 1.5565268993377686, train acc = 0.45500001311302185, time = 0.9430086612701416\n",
      "Training at step=39, batch=175, train loss = 1.601186990737915, train acc = 0.46000000834465027, time = 0.94960618019104\n",
      "Training at step=39, batch=200, train loss = 1.5146442651748657, train acc = 0.4350000023841858, time = 0.9393215179443359\n",
      "Training at step=39, batch=225, train loss = 1.4493249654769897, train acc = 0.5149999856948853, time = 0.9381816387176514\n",
      "Testing at step=39, batch=0, test loss = 1.5486433506011963, test acc = 0.45500001311302185, time = 0.3541269302368164\n",
      "Testing at step=39, batch=5, test loss = 1.5450749397277832, test acc = 0.46000000834465027, time = 0.35355138778686523\n",
      "Testing at step=39, batch=10, test loss = 1.6429252624511719, test acc = 0.4300000071525574, time = 0.35123157501220703\n",
      "Testing at step=39, batch=15, test loss = 1.471757411956787, test acc = 0.5, time = 0.35442304611206055\n",
      "Testing at step=39, batch=20, test loss = 1.6983507871627808, test acc = 0.39500001072883606, time = 0.3555474281311035\n",
      "Testing at step=39, batch=25, test loss = 1.6669217348098755, test acc = 0.4300000071525574, time = 0.3525121212005615\n",
      "Testing at step=39, batch=30, test loss = 1.7766735553741455, test acc = 0.38499999046325684, time = 0.3490426540374756\n",
      "Testing at step=39, batch=35, test loss = 1.6110001802444458, test acc = 0.41499999165534973, time = 0.352372407913208\n",
      "Testing at step=39, batch=40, test loss = 1.6022180318832397, test acc = 0.4350000023841858, time = 0.3483302593231201\n",
      "Testing at step=39, batch=45, test loss = 1.6159192323684692, test acc = 0.4000000059604645, time = 0.3459455966949463\n",
      "Step 39 finished in 263.21797132492065, Train loss = 1.522265555858612, Test loss = 1.575491452217102; Train Acc = 0.4578200000524521, Test Acc = 0.4469000005722046\n",
      "Training at step=40, batch=0, train loss = 1.5003418922424316, train acc = 0.5, time = 0.9454615116119385\n",
      "Training at step=40, batch=25, train loss = 1.4258050918579102, train acc = 0.48500001430511475, time = 0.9382224082946777\n",
      "Training at step=40, batch=50, train loss = 1.5375022888183594, train acc = 0.45500001311302185, time = 0.951650857925415\n",
      "Training at step=40, batch=75, train loss = 1.6111875772476196, train acc = 0.41999998688697815, time = 0.940500020980835\n",
      "Training at step=40, batch=100, train loss = 1.7088367938995361, train acc = 0.39500001072883606, time = 0.9402639865875244\n",
      "Training at step=40, batch=125, train loss = 1.4857988357543945, train acc = 0.4699999988079071, time = 0.9485642910003662\n",
      "Training at step=40, batch=150, train loss = 1.6287205219268799, train acc = 0.4399999976158142, time = 0.9355037212371826\n",
      "Training at step=40, batch=175, train loss = 1.5665439367294312, train acc = 0.4350000023841858, time = 0.9430572986602783\n",
      "Training at step=40, batch=200, train loss = 1.4942481517791748, train acc = 0.44999998807907104, time = 0.9421908855438232\n",
      "Training at step=40, batch=225, train loss = 1.4881325960159302, train acc = 0.5049999952316284, time = 0.935387372970581\n",
      "Testing at step=40, batch=0, test loss = 1.5012491941452026, test acc = 0.4650000035762787, time = 0.37058186531066895\n",
      "Testing at step=40, batch=5, test loss = 1.4939273595809937, test acc = 0.46000000834465027, time = 0.34781813621520996\n",
      "Testing at step=40, batch=10, test loss = 1.5176435708999634, test acc = 0.42500001192092896, time = 0.3505973815917969\n",
      "Testing at step=40, batch=15, test loss = 1.5578268766403198, test acc = 0.48500001430511475, time = 0.34526515007019043\n",
      "Testing at step=40, batch=20, test loss = 1.592301607131958, test acc = 0.45500001311302185, time = 0.3550710678100586\n",
      "Testing at step=40, batch=25, test loss = 1.6371957063674927, test acc = 0.4099999964237213, time = 0.34870147705078125\n",
      "Testing at step=40, batch=30, test loss = 1.4448275566101074, test acc = 0.5099999904632568, time = 0.35681986808776855\n",
      "Testing at step=40, batch=35, test loss = 1.7184348106384277, test acc = 0.4000000059604645, time = 0.35457777976989746\n",
      "Testing at step=40, batch=40, test loss = 1.5739541053771973, test acc = 0.4699999988079071, time = 0.35077643394470215\n",
      "Testing at step=40, batch=45, test loss = 1.3887381553649902, test acc = 0.5149999856948853, time = 0.3552868366241455\n",
      "Step 40 finished in 263.49369645118713, Train loss = 1.5138721475601196, Test loss = 1.5765341448783874; Train Acc = 0.46417999935150145, Test Acc = 0.43989999949932096\n",
      "Training at step=41, batch=0, train loss = 1.3988815546035767, train acc = 0.5249999761581421, time = 0.9483873844146729\n",
      "Training at step=41, batch=25, train loss = 1.5249110460281372, train acc = 0.45500001311302185, time = 0.942899227142334\n",
      "Training at step=41, batch=50, train loss = 1.5725101232528687, train acc = 0.4650000035762787, time = 0.9401836395263672\n",
      "Training at step=41, batch=75, train loss = 1.4722155332565308, train acc = 0.4950000047683716, time = 0.9421849250793457\n",
      "Training at step=41, batch=100, train loss = 1.4417872428894043, train acc = 0.4749999940395355, time = 0.9999902248382568\n",
      "Training at step=41, batch=125, train loss = 1.5596750974655151, train acc = 0.4650000035762787, time = 0.9597735404968262\n",
      "Training at step=41, batch=150, train loss = 1.473036766052246, train acc = 0.46000000834465027, time = 0.9405868053436279\n",
      "Training at step=41, batch=175, train loss = 1.6330270767211914, train acc = 0.42500001192092896, time = 0.9598736763000488\n",
      "Training at step=41, batch=200, train loss = 1.5631966590881348, train acc = 0.47999998927116394, time = 0.9481921195983887\n",
      "Training at step=41, batch=225, train loss = 1.4704185724258423, train acc = 0.4950000047683716, time = 0.9474797248840332\n",
      "Testing at step=41, batch=0, test loss = 1.6337482929229736, test acc = 0.4449999928474426, time = 0.35410642623901367\n",
      "Testing at step=41, batch=5, test loss = 1.538210153579712, test acc = 0.46000000834465027, time = 0.3573789596557617\n",
      "Testing at step=41, batch=10, test loss = 1.4958364963531494, test acc = 0.41999998688697815, time = 0.35404276847839355\n",
      "Testing at step=41, batch=15, test loss = 1.4898358583450317, test acc = 0.4749999940395355, time = 0.35462427139282227\n",
      "Testing at step=41, batch=20, test loss = 1.490879774093628, test acc = 0.4699999988079071, time = 0.35192346572875977\n",
      "Testing at step=41, batch=25, test loss = 1.5772902965545654, test acc = 0.44999998807907104, time = 0.3496663570404053\n",
      "Testing at step=41, batch=30, test loss = 1.5981587171554565, test acc = 0.4650000035762787, time = 0.3491992950439453\n",
      "Testing at step=41, batch=35, test loss = 1.6293483972549438, test acc = 0.3799999952316284, time = 0.3536202907562256\n",
      "Testing at step=41, batch=40, test loss = 1.486315131187439, test acc = 0.5149999856948853, time = 0.34833860397338867\n",
      "Testing at step=41, batch=45, test loss = 1.6193596124649048, test acc = 0.4300000071525574, time = 0.34804463386535645\n",
      "Step 41 finished in 263.4127297401428, Train loss = 1.5065003252029419, Test loss = 1.5632406282424927; Train Acc = 0.46481999933719637, Test Acc = 0.4467000007629395\n",
      "Training at step=42, batch=0, train loss = 1.4175835847854614, train acc = 0.4749999940395355, time = 1.0391490459442139\n",
      "Training at step=42, batch=25, train loss = 1.5113511085510254, train acc = 0.45500001311302185, time = 0.963763952255249\n",
      "Training at step=42, batch=50, train loss = 1.6226292848587036, train acc = 0.4350000023841858, time = 0.9563078880310059\n",
      "Training at step=42, batch=75, train loss = 1.5685205459594727, train acc = 0.45500001311302185, time = 0.9431369304656982\n",
      "Training at step=42, batch=100, train loss = 1.5185723304748535, train acc = 0.4699999988079071, time = 0.9331092834472656\n",
      "Training at step=42, batch=125, train loss = 1.4995266199111938, train acc = 0.48500001430511475, time = 0.9473719596862793\n",
      "Training at step=42, batch=150, train loss = 1.585383415222168, train acc = 0.4449999928474426, time = 0.9337365627288818\n",
      "Training at step=42, batch=175, train loss = 1.5604069232940674, train acc = 0.4699999988079071, time = 0.9395756721496582\n",
      "Training at step=42, batch=200, train loss = 1.5687429904937744, train acc = 0.39500001072883606, time = 0.9419360160827637\n",
      "Training at step=42, batch=225, train loss = 1.548032522201538, train acc = 0.4300000071525574, time = 0.9386472702026367\n",
      "Testing at step=42, batch=0, test loss = 1.3534302711486816, test acc = 0.5299999713897705, time = 0.3514280319213867\n",
      "Testing at step=42, batch=5, test loss = 1.6156867742538452, test acc = 0.4099999964237213, time = 0.3639671802520752\n",
      "Testing at step=42, batch=10, test loss = 1.7327401638031006, test acc = 0.38999998569488525, time = 0.3514120578765869\n",
      "Testing at step=42, batch=15, test loss = 1.5064276456832886, test acc = 0.4449999928474426, time = 0.34937167167663574\n",
      "Testing at step=42, batch=20, test loss = 1.4541957378387451, test acc = 0.47999998927116394, time = 0.3524303436279297\n",
      "Testing at step=42, batch=25, test loss = 1.493463158607483, test acc = 0.5, time = 0.3483285903930664\n",
      "Testing at step=42, batch=30, test loss = 1.4898298978805542, test acc = 0.4749999940395355, time = 0.36496734619140625\n",
      "Testing at step=42, batch=35, test loss = 1.6684128046035767, test acc = 0.44999998807907104, time = 0.3622446060180664\n",
      "Testing at step=42, batch=40, test loss = 1.564552664756775, test acc = 0.4449999928474426, time = 0.35369229316711426\n",
      "Testing at step=42, batch=45, test loss = 1.5665720701217651, test acc = 0.4449999928474426, time = 0.34859490394592285\n",
      "Step 42 finished in 264.25605177879333, Train loss = 1.5087044100761413, Test loss = 1.5716846942901612; Train Acc = 0.4642199996709824, Test Acc = 0.4454999977350235\n",
      "Training at step=43, batch=0, train loss = 1.369235873222351, train acc = 0.5099999904632568, time = 0.9375698566436768\n",
      "Training at step=43, batch=25, train loss = 1.3894433975219727, train acc = 0.5249999761581421, time = 0.9500424861907959\n",
      "Training at step=43, batch=50, train loss = 1.5827445983886719, train acc = 0.41499999165534973, time = 0.9424374103546143\n",
      "Training at step=43, batch=75, train loss = 1.5088125467300415, train acc = 0.42500001192092896, time = 0.939284086227417\n",
      "Training at step=43, batch=100, train loss = 1.3925204277038574, train acc = 0.5199999809265137, time = 0.9376239776611328\n",
      "Training at step=43, batch=125, train loss = 1.5778112411499023, train acc = 0.4350000023841858, time = 0.9487969875335693\n",
      "Training at step=43, batch=150, train loss = 1.5055230855941772, train acc = 0.4449999928474426, time = 0.950239896774292\n",
      "Training at step=43, batch=175, train loss = 1.6261848211288452, train acc = 0.4000000059604645, time = 0.944312334060669\n",
      "Training at step=43, batch=200, train loss = 1.323340892791748, train acc = 0.5649999976158142, time = 0.9513669013977051\n",
      "Training at step=43, batch=225, train loss = 1.5203701257705688, train acc = 0.46000000834465027, time = 0.9424173831939697\n",
      "Testing at step=43, batch=0, test loss = 1.4544291496276855, test acc = 0.45500001311302185, time = 0.37061405181884766\n",
      "Testing at step=43, batch=5, test loss = 1.581994891166687, test acc = 0.4099999964237213, time = 0.36183595657348633\n",
      "Testing at step=43, batch=10, test loss = 1.4672737121582031, test acc = 0.4749999940395355, time = 0.3527696132659912\n",
      "Testing at step=43, batch=15, test loss = 1.6321316957473755, test acc = 0.41999998688697815, time = 0.36539721488952637\n",
      "Testing at step=43, batch=20, test loss = 1.6703778505325317, test acc = 0.41999998688697815, time = 0.3636493682861328\n",
      "Testing at step=43, batch=25, test loss = 1.5054728984832764, test acc = 0.46000000834465027, time = 0.35484910011291504\n",
      "Testing at step=43, batch=30, test loss = 1.646065354347229, test acc = 0.38999998569488525, time = 0.3619687557220459\n",
      "Testing at step=43, batch=35, test loss = 1.633712887763977, test acc = 0.4099999964237213, time = 0.3674886226654053\n",
      "Testing at step=43, batch=40, test loss = 1.7002122402191162, test acc = 0.3700000047683716, time = 0.3495347499847412\n",
      "Testing at step=43, batch=45, test loss = 1.514238715171814, test acc = 0.4749999940395355, time = 0.3643062114715576\n",
      "Step 43 finished in 264.2044334411621, Train loss = 1.5011042685508729, Test loss = 1.5663446593284607; Train Acc = 0.46556000006198883, Test Acc = 0.4393999993801117\n",
      "Training at step=44, batch=0, train loss = 1.5419062376022339, train acc = 0.4950000047683716, time = 0.9413044452667236\n",
      "Training at step=44, batch=25, train loss = 1.3475590944290161, train acc = 0.5649999976158142, time = 0.9397704601287842\n",
      "Training at step=44, batch=50, train loss = 1.4751417636871338, train acc = 0.4950000047683716, time = 0.9511115550994873\n",
      "Training at step=44, batch=75, train loss = 1.47703218460083, train acc = 0.4650000035762787, time = 0.9475302696228027\n",
      "Training at step=44, batch=100, train loss = 1.5608530044555664, train acc = 0.45500001311302185, time = 0.949042797088623\n",
      "Training at step=44, batch=125, train loss = 1.4436349868774414, train acc = 0.5299999713897705, time = 0.9400758743286133\n",
      "Training at step=44, batch=150, train loss = 1.5936903953552246, train acc = 0.45500001311302185, time = 0.9367866516113281\n",
      "Training at step=44, batch=175, train loss = 1.459681749343872, train acc = 0.5049999952316284, time = 0.9406633377075195\n",
      "Training at step=44, batch=200, train loss = 1.7231777906417847, train acc = 0.39500001072883606, time = 0.9391744136810303\n",
      "Training at step=44, batch=225, train loss = 1.440688133239746, train acc = 0.4650000035762787, time = 0.9440338611602783\n",
      "Testing at step=44, batch=0, test loss = 1.4184914827346802, test acc = 0.4950000047683716, time = 0.3566727638244629\n",
      "Testing at step=44, batch=5, test loss = 1.5374884605407715, test acc = 0.45500001311302185, time = 0.352567195892334\n",
      "Testing at step=44, batch=10, test loss = 1.6709589958190918, test acc = 0.41499999165534973, time = 0.35070085525512695\n",
      "Testing at step=44, batch=15, test loss = 1.5204333066940308, test acc = 0.48500001430511475, time = 0.35152220726013184\n",
      "Testing at step=44, batch=20, test loss = 1.4777839183807373, test acc = 0.48500001430511475, time = 0.35854029655456543\n",
      "Testing at step=44, batch=25, test loss = 1.548851728439331, test acc = 0.41999998688697815, time = 0.3514890670776367\n",
      "Testing at step=44, batch=30, test loss = 1.6250320672988892, test acc = 0.36500000953674316, time = 0.3544151782989502\n",
      "Testing at step=44, batch=35, test loss = 1.4651408195495605, test acc = 0.5199999809265137, time = 0.3477776050567627\n",
      "Testing at step=44, batch=40, test loss = 1.6245760917663574, test acc = 0.4449999928474426, time = 0.35309791564941406\n",
      "Testing at step=44, batch=45, test loss = 1.537725567817688, test acc = 0.45500001311302185, time = 0.35053277015686035\n",
      "Step 44 finished in 263.28499364852905, Train loss = 1.5052279477119446, Test loss = 1.56175799369812; Train Acc = 0.4661199998855591, Test Acc = 0.4476999998092651\n",
      "Training at step=45, batch=0, train loss = 1.6219369173049927, train acc = 0.4099999964237213, time = 0.9379086494445801\n",
      "Training at step=45, batch=25, train loss = 1.6188421249389648, train acc = 0.4399999976158142, time = 0.9483134746551514\n",
      "Training at step=45, batch=50, train loss = 1.3748877048492432, train acc = 0.5249999761581421, time = 0.9534690380096436\n",
      "Training at step=45, batch=75, train loss = 1.485026240348816, train acc = 0.5099999904632568, time = 0.9528384208679199\n",
      "Training at step=45, batch=100, train loss = 1.359899878501892, train acc = 0.5049999952316284, time = 0.9514694213867188\n",
      "Training at step=45, batch=125, train loss = 1.5430721044540405, train acc = 0.46000000834465027, time = 0.9335708618164062\n",
      "Training at step=45, batch=150, train loss = 1.6224435567855835, train acc = 0.41999998688697815, time = 0.9574575424194336\n",
      "Training at step=45, batch=175, train loss = 1.3891574144363403, train acc = 0.47999998927116394, time = 0.9436962604522705\n",
      "Training at step=45, batch=200, train loss = 1.5545369386672974, train acc = 0.4749999940395355, time = 0.9391694068908691\n",
      "Training at step=45, batch=225, train loss = 1.5080490112304688, train acc = 0.4449999928474426, time = 0.9428215026855469\n",
      "Testing at step=45, batch=0, test loss = 1.428950548171997, test acc = 0.5249999761581421, time = 0.3566169738769531\n",
      "Testing at step=45, batch=5, test loss = 1.5298092365264893, test acc = 0.38499999046325684, time = 0.35109686851501465\n",
      "Testing at step=45, batch=10, test loss = 1.6093977689743042, test acc = 0.4650000035762787, time = 0.3461451530456543\n",
      "Testing at step=45, batch=15, test loss = 1.5726934671401978, test acc = 0.44999998807907104, time = 0.3556349277496338\n",
      "Testing at step=45, batch=20, test loss = 1.5713871717453003, test acc = 0.4449999928474426, time = 0.34647059440612793\n",
      "Testing at step=45, batch=25, test loss = 1.6737240552902222, test acc = 0.38999998569488525, time = 0.34999942779541016\n",
      "Testing at step=45, batch=30, test loss = 1.6988389492034912, test acc = 0.3799999952316284, time = 0.3500833511352539\n",
      "Testing at step=45, batch=35, test loss = 1.5609456300735474, test acc = 0.41999998688697815, time = 0.35213708877563477\n",
      "Testing at step=45, batch=40, test loss = 1.5042288303375244, test acc = 0.4350000023841858, time = 0.3578166961669922\n",
      "Testing at step=45, batch=45, test loss = 1.4729865789413452, test acc = 0.4650000035762787, time = 0.35019969940185547\n",
      "Step 45 finished in 263.7709918022156, Train loss = 1.4919160685539246, Test loss = 1.5586659264564515; Train Acc = 0.4720199997425079, Test Acc = 0.44370000004768373\n",
      "Training at step=46, batch=0, train loss = 1.4460872411727905, train acc = 0.47999998927116394, time = 0.9365782737731934\n",
      "Training at step=46, batch=25, train loss = 1.389955997467041, train acc = 0.5199999809265137, time = 0.9492838382720947\n",
      "Training at step=46, batch=50, train loss = 1.5027323961257935, train acc = 0.4699999988079071, time = 0.9400427341461182\n",
      "Training at step=46, batch=75, train loss = 1.4716078042984009, train acc = 0.5149999856948853, time = 0.9395489692687988\n",
      "Training at step=46, batch=100, train loss = 1.5059963464736938, train acc = 0.45500001311302185, time = 0.954139232635498\n",
      "Training at step=46, batch=125, train loss = 1.402989149093628, train acc = 0.5149999856948853, time = 0.9438438415527344\n",
      "Training at step=46, batch=150, train loss = 1.4820964336395264, train acc = 0.45500001311302185, time = 0.9570939540863037\n",
      "Training at step=46, batch=175, train loss = 1.4735335111618042, train acc = 0.4749999940395355, time = 0.9343066215515137\n",
      "Training at step=46, batch=200, train loss = 1.3258143663406372, train acc = 0.5299999713897705, time = 0.9435234069824219\n",
      "Training at step=46, batch=225, train loss = 1.2952839136123657, train acc = 0.5249999761581421, time = 0.9512381553649902\n",
      "Testing at step=46, batch=0, test loss = 1.504562258720398, test acc = 0.4950000047683716, time = 0.3682215213775635\n",
      "Testing at step=46, batch=5, test loss = 1.5934457778930664, test acc = 0.41999998688697815, time = 0.3599414825439453\n",
      "Testing at step=46, batch=10, test loss = 1.5624417066574097, test acc = 0.4350000023841858, time = 0.35242581367492676\n",
      "Testing at step=46, batch=15, test loss = 1.5253607034683228, test acc = 0.4699999988079071, time = 0.36655259132385254\n",
      "Testing at step=46, batch=20, test loss = 1.5641636848449707, test acc = 0.44999998807907104, time = 0.3489840030670166\n",
      "Testing at step=46, batch=25, test loss = 1.4154701232910156, test acc = 0.5049999952316284, time = 0.36586952209472656\n",
      "Testing at step=46, batch=30, test loss = 1.539521336555481, test acc = 0.4449999928474426, time = 0.3484635353088379\n",
      "Testing at step=46, batch=35, test loss = 1.5721831321716309, test acc = 0.44999998807907104, time = 0.3562798500061035\n",
      "Testing at step=46, batch=40, test loss = 1.5251350402832031, test acc = 0.4399999976158142, time = 0.36011195182800293\n",
      "Testing at step=46, batch=45, test loss = 1.5524673461914062, test acc = 0.46000000834465027, time = 0.3535940647125244\n",
      "Step 46 finished in 264.0956554412842, Train loss = 1.4904435396194458, Test loss = 1.5574835729599; Train Acc = 0.4727199996709824, Test Acc = 0.44719999849796294\n",
      "Training at step=47, batch=0, train loss = 1.4958343505859375, train acc = 0.4449999928474426, time = 0.9398932456970215\n",
      "Training at step=47, batch=25, train loss = 1.461540699005127, train acc = 0.5049999952316284, time = 1.1539506912231445\n",
      "Training at step=47, batch=50, train loss = 1.461809515953064, train acc = 0.46000000834465027, time = 0.9431371688842773\n",
      "Training at step=47, batch=75, train loss = 1.4710495471954346, train acc = 0.49000000953674316, time = 0.944709300994873\n",
      "Training at step=47, batch=100, train loss = 1.4945244789123535, train acc = 0.4350000023841858, time = 0.947765588760376\n",
      "Training at step=47, batch=125, train loss = 1.4826449155807495, train acc = 0.48500001430511475, time = 0.9989993572235107\n",
      "Training at step=47, batch=150, train loss = 1.507126808166504, train acc = 0.4399999976158142, time = 0.9346356391906738\n",
      "Training at step=47, batch=175, train loss = 1.6036075353622437, train acc = 0.47999998927116394, time = 0.9456145763397217\n",
      "Training at step=47, batch=200, train loss = 1.524594783782959, train acc = 0.4399999976158142, time = 0.9545929431915283\n",
      "Training at step=47, batch=225, train loss = 1.438480257987976, train acc = 0.5350000262260437, time = 0.9483528137207031\n",
      "Testing at step=47, batch=0, test loss = 1.4337981939315796, test acc = 0.5099999904632568, time = 0.3521151542663574\n",
      "Testing at step=47, batch=5, test loss = 1.5530118942260742, test acc = 0.4650000035762787, time = 0.3495633602142334\n",
      "Testing at step=47, batch=10, test loss = 1.514244556427002, test acc = 0.41499999165534973, time = 0.3592846393585205\n",
      "Testing at step=47, batch=15, test loss = 1.5013309717178345, test acc = 0.4950000047683716, time = 0.3509793281555176\n",
      "Testing at step=47, batch=20, test loss = 1.4458273649215698, test acc = 0.49000000953674316, time = 0.3597221374511719\n",
      "Testing at step=47, batch=25, test loss = 1.6286931037902832, test acc = 0.3700000047683716, time = 0.36730122566223145\n",
      "Testing at step=47, batch=30, test loss = 1.577734112739563, test acc = 0.4449999928474426, time = 0.3669137954711914\n",
      "Testing at step=47, batch=35, test loss = 1.5440946817398071, test acc = 0.5, time = 0.3543257713317871\n",
      "Testing at step=47, batch=40, test loss = 1.5826464891433716, test acc = 0.47999998927116394, time = 0.360806941986084\n",
      "Testing at step=47, batch=45, test loss = 1.5429109334945679, test acc = 0.4449999928474426, time = 0.3671741485595703\n",
      "Step 47 finished in 264.37153482437134, Train loss = 1.4868301601409912, Test loss = 1.551862177848816; Train Acc = 0.4728399994373321, Test Acc = 0.45199999928474427\n",
      "Training at step=48, batch=0, train loss = 1.4098130464553833, train acc = 0.4650000035762787, time = 0.9377856254577637\n",
      "Training at step=48, batch=25, train loss = 1.4027775526046753, train acc = 0.5149999856948853, time = 0.9613046646118164\n",
      "Training at step=48, batch=50, train loss = 1.3951780796051025, train acc = 0.5249999761581421, time = 0.9473426342010498\n",
      "Training at step=48, batch=75, train loss = 1.4982680082321167, train acc = 0.4399999976158142, time = 0.9748029708862305\n",
      "Training at step=48, batch=100, train loss = 1.4267609119415283, train acc = 0.5199999809265137, time = 0.9367456436157227\n",
      "Training at step=48, batch=125, train loss = 1.5734401941299438, train acc = 0.46000000834465027, time = 0.9482333660125732\n",
      "Training at step=48, batch=150, train loss = 1.5010621547698975, train acc = 0.4449999928474426, time = 0.9389004707336426\n",
      "Training at step=48, batch=175, train loss = 1.5953656435012817, train acc = 0.4300000071525574, time = 0.9346320629119873\n",
      "Training at step=48, batch=200, train loss = 1.3965450525283813, train acc = 0.5, time = 0.9446961879730225\n",
      "Training at step=48, batch=225, train loss = 1.4647417068481445, train acc = 0.49000000953674316, time = 0.9415042400360107\n",
      "Testing at step=48, batch=0, test loss = 1.5686993598937988, test acc = 0.46000000834465027, time = 0.3512897491455078\n",
      "Testing at step=48, batch=5, test loss = 1.6600192785263062, test acc = 0.4300000071525574, time = 0.3528456687927246\n",
      "Testing at step=48, batch=10, test loss = 1.587164044380188, test acc = 0.4350000023841858, time = 0.35416674613952637\n",
      "Testing at step=48, batch=15, test loss = 1.4706677198410034, test acc = 0.4650000035762787, time = 0.3490111827850342\n",
      "Testing at step=48, batch=20, test loss = 1.4737448692321777, test acc = 0.4699999988079071, time = 0.3577902317047119\n",
      "Testing at step=48, batch=25, test loss = 1.5389070510864258, test acc = 0.44999998807907104, time = 0.35399913787841797\n",
      "Testing at step=48, batch=30, test loss = 1.6195487976074219, test acc = 0.4000000059604645, time = 0.34781956672668457\n",
      "Testing at step=48, batch=35, test loss = 1.4665623903274536, test acc = 0.48500001430511475, time = 0.35464954376220703\n",
      "Testing at step=48, batch=40, test loss = 1.6274510622024536, test acc = 0.41999998688697815, time = 0.35477542877197266\n",
      "Testing at step=48, batch=45, test loss = 1.554438829421997, test acc = 0.4749999940395355, time = 0.3550872802734375\n",
      "Step 48 finished in 263.5537974834442, Train loss = 1.4810630784034728, Test loss = 1.548286247253418; Train Acc = 0.47329999899864195, Test Acc = 0.4507999980449677\n",
      "Training at step=49, batch=0, train loss = 1.3844736814498901, train acc = 0.5099999904632568, time = 0.941173791885376\n",
      "Training at step=49, batch=25, train loss = 1.4135299921035767, train acc = 0.45500001311302185, time = 0.9412140846252441\n",
      "Training at step=49, batch=50, train loss = 1.4886672496795654, train acc = 0.47999998927116394, time = 0.934412956237793\n",
      "Training at step=49, batch=75, train loss = 1.4770170450210571, train acc = 0.4350000023841858, time = 0.945601224899292\n",
      "Training at step=49, batch=100, train loss = 1.3906335830688477, train acc = 0.5400000214576721, time = 0.9434933662414551\n",
      "Training at step=49, batch=125, train loss = 1.4541505575180054, train acc = 0.47999998927116394, time = 0.944105863571167\n",
      "Training at step=49, batch=150, train loss = 1.498542070388794, train acc = 0.44999998807907104, time = 0.9626812934875488\n",
      "Training at step=49, batch=175, train loss = 1.5384947061538696, train acc = 0.4449999928474426, time = 0.9476139545440674\n",
      "Training at step=49, batch=200, train loss = 1.5777782201766968, train acc = 0.4099999964237213, time = 0.9502403736114502\n",
      "Training at step=49, batch=225, train loss = 1.5226243734359741, train acc = 0.4449999928474426, time = 0.9426805973052979\n",
      "Testing at step=49, batch=0, test loss = 1.5592008829116821, test acc = 0.38999998569488525, time = 0.35921812057495117\n",
      "Testing at step=49, batch=5, test loss = 1.432399868965149, test acc = 0.4950000047683716, time = 0.37288808822631836\n",
      "Testing at step=49, batch=10, test loss = 1.6167620420455933, test acc = 0.4050000011920929, time = 0.3611791133880615\n",
      "Testing at step=49, batch=15, test loss = 1.6262625455856323, test acc = 0.44999998807907104, time = 0.4927082061767578\n",
      "Testing at step=49, batch=20, test loss = 1.622655987739563, test acc = 0.46000000834465027, time = 0.3666105270385742\n",
      "Testing at step=49, batch=25, test loss = 1.6020435094833374, test acc = 0.4449999928474426, time = 0.352492094039917\n",
      "Testing at step=49, batch=30, test loss = 1.540852665901184, test acc = 0.41999998688697815, time = 0.3501553535461426\n",
      "Testing at step=49, batch=35, test loss = 1.466310977935791, test acc = 0.49000000953674316, time = 0.34862565994262695\n",
      "Testing at step=49, batch=40, test loss = 1.727911353111267, test acc = 0.38499999046325684, time = 0.3578050136566162\n",
      "Testing at step=49, batch=45, test loss = 1.6013398170471191, test acc = 0.4449999928474426, time = 0.35158658027648926\n",
      "Step 49 finished in 263.68879199028015, Train loss = 1.4751485362052918, Test loss = 1.5571036434173584; Train Acc = 0.4771399984359741, Test Acc = 0.4496999990940094\n",
      "Training at step=50, batch=0, train loss = 1.4947911500930786, train acc = 0.4699999988079071, time = 0.9447934627532959\n",
      "Training at step=50, batch=25, train loss = 1.5059133768081665, train acc = 0.4950000047683716, time = 0.9472119808197021\n",
      "Training at step=50, batch=50, train loss = 1.4769612550735474, train acc = 0.4749999940395355, time = 0.9491002559661865\n",
      "Training at step=50, batch=75, train loss = 1.4059154987335205, train acc = 0.4950000047683716, time = 0.9422807693481445\n",
      "Training at step=50, batch=100, train loss = 1.5943964719772339, train acc = 0.4399999976158142, time = 0.9497263431549072\n",
      "Training at step=50, batch=125, train loss = 1.4901772737503052, train acc = 0.48500001430511475, time = 0.9578695297241211\n",
      "Training at step=50, batch=150, train loss = 1.5406831502914429, train acc = 0.4749999940395355, time = 0.9431431293487549\n",
      "Training at step=50, batch=175, train loss = 1.5262031555175781, train acc = 0.44999998807907104, time = 0.9400439262390137\n",
      "Training at step=50, batch=200, train loss = 1.5703036785125732, train acc = 0.4699999988079071, time = 0.9445221424102783\n",
      "Training at step=50, batch=225, train loss = 1.507450819015503, train acc = 0.48500001430511475, time = 0.9457144737243652\n",
      "Testing at step=50, batch=0, test loss = 1.480838418006897, test acc = 0.44999998807907104, time = 0.3553652763366699\n",
      "Testing at step=50, batch=5, test loss = 1.4967682361602783, test acc = 0.48500001430511475, time = 0.35721731185913086\n",
      "Testing at step=50, batch=10, test loss = 1.550068974494934, test acc = 0.4449999928474426, time = 0.3541231155395508\n",
      "Testing at step=50, batch=15, test loss = 1.652488112449646, test acc = 0.4000000059604645, time = 0.34700679779052734\n",
      "Testing at step=50, batch=20, test loss = 1.6438004970550537, test acc = 0.41499999165534973, time = 0.3542804718017578\n",
      "Testing at step=50, batch=25, test loss = 1.5169885158538818, test acc = 0.49000000953674316, time = 0.3542978763580322\n",
      "Testing at step=50, batch=30, test loss = 1.6337456703186035, test acc = 0.4699999988079071, time = 0.3519740104675293\n",
      "Testing at step=50, batch=35, test loss = 1.5703638792037964, test acc = 0.4449999928474426, time = 0.3538994789123535\n",
      "Testing at step=50, batch=40, test loss = 1.5929216146469116, test acc = 0.4399999976158142, time = 0.36844396591186523\n",
      "Testing at step=50, batch=45, test loss = 1.5871108770370483, test acc = 0.46000000834465027, time = 0.36496472358703613\n",
      "Step 50 finished in 263.4389612674713, Train loss = 1.4741844639778137, Test loss = 1.560576376914978; Train Acc = 0.47812000024318696, Test Acc = 0.4494999998807907\n",
      "Training at step=51, batch=0, train loss = 1.3750627040863037, train acc = 0.5149999856948853, time = 0.9445536136627197\n",
      "Training at step=51, batch=25, train loss = 1.4244593381881714, train acc = 0.5099999904632568, time = 0.9442245960235596\n",
      "Training at step=51, batch=50, train loss = 1.4408382177352905, train acc = 0.5099999904632568, time = 0.9430680274963379\n",
      "Training at step=51, batch=75, train loss = 1.4653334617614746, train acc = 0.48500001430511475, time = 0.9504528045654297\n",
      "Training at step=51, batch=100, train loss = 1.5573036670684814, train acc = 0.4449999928474426, time = 0.9403760433197021\n",
      "Training at step=51, batch=125, train loss = 1.4497839212417603, train acc = 0.4699999988079071, time = 0.9466326236724854\n",
      "Training at step=51, batch=150, train loss = 1.5298433303833008, train acc = 0.41999998688697815, time = 0.9574942588806152\n",
      "Training at step=51, batch=175, train loss = 1.385941505432129, train acc = 0.5199999809265137, time = 0.9369840621948242\n",
      "Training at step=51, batch=200, train loss = 1.4403605461120605, train acc = 0.5149999856948853, time = 0.942678689956665\n",
      "Training at step=51, batch=225, train loss = 1.6406679153442383, train acc = 0.38999998569488525, time = 0.9599509239196777\n",
      "Testing at step=51, batch=0, test loss = 1.4950312376022339, test acc = 0.47999998927116394, time = 0.3590402603149414\n",
      "Testing at step=51, batch=5, test loss = 1.5315557718276978, test acc = 0.44999998807907104, time = 0.3574354648590088\n",
      "Testing at step=51, batch=10, test loss = 1.4575929641723633, test acc = 0.5299999713897705, time = 0.3589355945587158\n",
      "Testing at step=51, batch=15, test loss = 1.5328525304794312, test acc = 0.38999998569488525, time = 0.34944748878479004\n",
      "Testing at step=51, batch=20, test loss = 1.537571907043457, test acc = 0.4650000035762787, time = 0.35292601585388184\n",
      "Testing at step=51, batch=25, test loss = 1.648229718208313, test acc = 0.4350000023841858, time = 0.35412120819091797\n",
      "Testing at step=51, batch=30, test loss = 1.5243115425109863, test acc = 0.44999998807907104, time = 0.35373878479003906\n",
      "Testing at step=51, batch=35, test loss = 1.5653445720672607, test acc = 0.4050000011920929, time = 0.3513622283935547\n",
      "Testing at step=51, batch=40, test loss = 1.5600165128707886, test acc = 0.44999998807907104, time = 0.3492164611816406\n",
      "Testing at step=51, batch=45, test loss = 1.484345555305481, test acc = 0.47999998927116394, time = 0.34906601905822754\n",
      "Step 51 finished in 263.3453130722046, Train loss = 1.473085744857788, Test loss = 1.5454236388206481; Train Acc = 0.47641999888420106, Test Acc = 0.4490000003576279\n",
      "Training at step=52, batch=0, train loss = 1.4737008810043335, train acc = 0.47999998927116394, time = 0.9418854713439941\n",
      "Training at step=52, batch=25, train loss = 1.5080715417861938, train acc = 0.4099999964237213, time = 0.9361505508422852\n",
      "Training at step=52, batch=50, train loss = 1.5094952583312988, train acc = 0.4749999940395355, time = 0.957179069519043\n",
      "Training at step=52, batch=75, train loss = 1.3307675123214722, train acc = 0.5600000023841858, time = 0.941760778427124\n",
      "Training at step=52, batch=100, train loss = 1.3411376476287842, train acc = 0.5249999761581421, time = 0.9411790370941162\n",
      "Training at step=52, batch=125, train loss = 1.5529736280441284, train acc = 0.4350000023841858, time = 0.9377579689025879\n",
      "Training at step=52, batch=150, train loss = 1.6107957363128662, train acc = 0.41999998688697815, time = 0.9422683715820312\n",
      "Training at step=52, batch=175, train loss = 1.3649859428405762, train acc = 0.5149999856948853, time = 0.9794700145721436\n",
      "Training at step=52, batch=200, train loss = 1.3245477676391602, train acc = 0.5149999856948853, time = 0.9417703151702881\n",
      "Training at step=52, batch=225, train loss = 1.3917423486709595, train acc = 0.550000011920929, time = 0.9462084770202637\n",
      "Testing at step=52, batch=0, test loss = 1.5793280601501465, test acc = 0.4399999976158142, time = 0.3556840419769287\n",
      "Testing at step=52, batch=5, test loss = 1.5234538316726685, test acc = 0.41999998688697815, time = 0.48757100105285645\n",
      "Testing at step=52, batch=10, test loss = 1.480170726776123, test acc = 0.49000000953674316, time = 0.35927724838256836\n",
      "Testing at step=52, batch=15, test loss = 1.5772939920425415, test acc = 0.4300000071525574, time = 0.35236120223999023\n",
      "Testing at step=52, batch=20, test loss = 1.4980756044387817, test acc = 0.47999998927116394, time = 0.3964366912841797\n",
      "Testing at step=52, batch=25, test loss = 1.4913840293884277, test acc = 0.46000000834465027, time = 0.34985899925231934\n",
      "Testing at step=52, batch=30, test loss = 1.5439937114715576, test acc = 0.49000000953674316, time = 0.3581576347351074\n",
      "Testing at step=52, batch=35, test loss = 1.5094923973083496, test acc = 0.45500001311302185, time = 0.3582193851470947\n",
      "Testing at step=52, batch=40, test loss = 1.436740756034851, test acc = 0.5199999809265137, time = 0.3540670871734619\n",
      "Testing at step=52, batch=45, test loss = 1.6162947416305542, test acc = 0.44999998807907104, time = 0.34954118728637695\n",
      "Step 52 finished in 263.6609046459198, Train loss = 1.464710970878601, Test loss = 1.5372304725646972; Train Acc = 0.47971999788284303, Test Acc = 0.45480000019073485\n",
      "Training at step=53, batch=0, train loss = 1.4305486679077148, train acc = 0.47999998927116394, time = 0.9442732334136963\n",
      "Training at step=53, batch=25, train loss = 1.55049729347229, train acc = 0.46000000834465027, time = 0.9398365020751953\n",
      "Training at step=53, batch=50, train loss = 1.4472839832305908, train acc = 0.5049999952316284, time = 0.940833568572998\n",
      "Training at step=53, batch=75, train loss = 1.516634464263916, train acc = 0.44999998807907104, time = 0.9532661437988281\n",
      "Training at step=53, batch=100, train loss = 1.454299807548523, train acc = 0.5049999952316284, time = 0.9490597248077393\n",
      "Training at step=53, batch=125, train loss = 1.3759297132492065, train acc = 0.4699999988079071, time = 0.9448552131652832\n",
      "Training at step=53, batch=150, train loss = 1.3723276853561401, train acc = 0.5049999952316284, time = 0.9576847553253174\n",
      "Training at step=53, batch=175, train loss = 1.4631108045578003, train acc = 0.5049999952316284, time = 0.9622359275817871\n",
      "Training at step=53, batch=200, train loss = 1.4184598922729492, train acc = 0.4950000047683716, time = 0.9344527721405029\n",
      "Training at step=53, batch=225, train loss = 1.4628485441207886, train acc = 0.4749999940395355, time = 0.9423995018005371\n",
      "Testing at step=53, batch=0, test loss = 1.5692793130874634, test acc = 0.44999998807907104, time = 0.47246289253234863\n",
      "Testing at step=53, batch=5, test loss = 1.5121232271194458, test acc = 0.4650000035762787, time = 0.34984874725341797\n",
      "Testing at step=53, batch=10, test loss = 1.4984055757522583, test acc = 0.45500001311302185, time = 0.35601043701171875\n",
      "Testing at step=53, batch=15, test loss = 1.5683891773223877, test acc = 0.4650000035762787, time = 0.36345839500427246\n",
      "Testing at step=53, batch=20, test loss = 1.5291599035263062, test acc = 0.44999998807907104, time = 0.3479499816894531\n",
      "Testing at step=53, batch=25, test loss = 1.4945285320281982, test acc = 0.4699999988079071, time = 0.34822702407836914\n",
      "Testing at step=53, batch=30, test loss = 1.509497880935669, test acc = 0.44999998807907104, time = 0.3506941795349121\n",
      "Testing at step=53, batch=35, test loss = 1.5905972719192505, test acc = 0.45500001311302185, time = 0.351060152053833\n",
      "Testing at step=53, batch=40, test loss = 1.6033086776733398, test acc = 0.4449999928474426, time = 0.3556480407714844\n",
      "Testing at step=53, batch=45, test loss = 1.5539110898971558, test acc = 0.4650000035762787, time = 0.35628843307495117\n",
      "Step 53 finished in 264.0989181995392, Train loss = 1.4639605889320373, Test loss = 1.5613645172119142; Train Acc = 0.4813399987220764, Test Acc = 0.4514999997615814\n",
      "Training at step=54, batch=0, train loss = 1.402903437614441, train acc = 0.5, time = 0.9400389194488525\n",
      "Training at step=54, batch=25, train loss = 1.499961256980896, train acc = 0.4399999976158142, time = 0.9446244239807129\n",
      "Training at step=54, batch=50, train loss = 1.509128212928772, train acc = 0.47999998927116394, time = 0.936302900314331\n",
      "Training at step=54, batch=75, train loss = 1.4062031507492065, train acc = 0.5099999904632568, time = 0.9577105045318604\n",
      "Training at step=54, batch=100, train loss = 1.5414143800735474, train acc = 0.44999998807907104, time = 0.9485385417938232\n",
      "Training at step=54, batch=125, train loss = 1.5012714862823486, train acc = 0.5, time = 0.9521329402923584\n",
      "Training at step=54, batch=150, train loss = 1.4230008125305176, train acc = 0.4950000047683716, time = 0.9362246990203857\n",
      "Training at step=54, batch=175, train loss = 1.4965397119522095, train acc = 0.4699999988079071, time = 0.9443380832672119\n",
      "Training at step=54, batch=200, train loss = 1.530776858329773, train acc = 0.42500001192092896, time = 0.947512149810791\n",
      "Training at step=54, batch=225, train loss = 1.4773348569869995, train acc = 0.4749999940395355, time = 0.9551882743835449\n",
      "Testing at step=54, batch=0, test loss = 1.4734803438186646, test acc = 0.5099999904632568, time = 0.3498215675354004\n",
      "Testing at step=54, batch=5, test loss = 1.6620898246765137, test acc = 0.38999998569488525, time = 0.3518557548522949\n",
      "Testing at step=54, batch=10, test loss = 1.4760046005249023, test acc = 0.4749999940395355, time = 0.35517406463623047\n",
      "Testing at step=54, batch=15, test loss = 1.4811532497406006, test acc = 0.4699999988079071, time = 0.3588848114013672\n",
      "Testing at step=54, batch=20, test loss = 1.6176379919052124, test acc = 0.41499999165534973, time = 0.3520774841308594\n",
      "Testing at step=54, batch=25, test loss = 1.613463282585144, test acc = 0.4650000035762787, time = 0.35238027572631836\n",
      "Testing at step=54, batch=30, test loss = 1.5872199535369873, test acc = 0.4300000071525574, time = 0.3503429889678955\n",
      "Testing at step=54, batch=35, test loss = 1.5354018211364746, test acc = 0.4449999928474426, time = 0.3570547103881836\n",
      "Testing at step=54, batch=40, test loss = 1.5030999183654785, test acc = 0.47999998927116394, time = 0.36980128288269043\n",
      "Testing at step=54, batch=45, test loss = 1.6157838106155396, test acc = 0.4099999964237213, time = 0.35637331008911133\n",
      "Step 54 finished in 263.97164487838745, Train loss = 1.459412766456604, Test loss = 1.5428093671798706; Train Acc = 0.4840999993085861, Test Acc = 0.4559999984502792\n",
      "Training at step=55, batch=0, train loss = 1.5280561447143555, train acc = 0.47999998927116394, time = 0.9433853626251221\n",
      "Training at step=55, batch=25, train loss = 1.3566783666610718, train acc = 0.5049999952316284, time = 0.9487142562866211\n",
      "Training at step=55, batch=50, train loss = 1.4120869636535645, train acc = 0.5299999713897705, time = 0.933013916015625\n",
      "Training at step=55, batch=75, train loss = 1.6399649381637573, train acc = 0.41999998688697815, time = 0.9536333084106445\n",
      "Training at step=55, batch=100, train loss = 1.4666627645492554, train acc = 0.4650000035762787, time = 0.9379572868347168\n",
      "Training at step=55, batch=125, train loss = 1.4159365892410278, train acc = 0.5, time = 0.9401466846466064\n",
      "Training at step=55, batch=150, train loss = 1.4090783596038818, train acc = 0.4650000035762787, time = 0.9458615779876709\n",
      "Training at step=55, batch=175, train loss = 1.5170680284500122, train acc = 0.4399999976158142, time = 0.9369194507598877\n",
      "Training at step=55, batch=200, train loss = 1.4323973655700684, train acc = 0.4699999988079071, time = 0.9350399971008301\n",
      "Training at step=55, batch=225, train loss = 1.4168481826782227, train acc = 0.5, time = 0.9526598453521729\n",
      "Testing at step=55, batch=0, test loss = 1.5386487245559692, test acc = 0.4300000071525574, time = 0.3493936061859131\n",
      "Testing at step=55, batch=5, test loss = 1.759507179260254, test acc = 0.32499998807907104, time = 0.35251688957214355\n",
      "Testing at step=55, batch=10, test loss = 1.4954496622085571, test acc = 0.4699999988079071, time = 0.35447120666503906\n",
      "Testing at step=55, batch=15, test loss = 1.533113718032837, test acc = 0.41499999165534973, time = 0.34690022468566895\n",
      "Testing at step=55, batch=20, test loss = 1.5915964841842651, test acc = 0.4449999928474426, time = 0.3508274555206299\n",
      "Testing at step=55, batch=25, test loss = 1.5396629571914673, test acc = 0.4399999976158142, time = 0.3494455814361572\n",
      "Testing at step=55, batch=30, test loss = 1.5736292600631714, test acc = 0.42500001192092896, time = 0.3585638999938965\n",
      "Testing at step=55, batch=35, test loss = 1.5160506963729858, test acc = 0.47999998927116394, time = 0.3526322841644287\n",
      "Testing at step=55, batch=40, test loss = 1.5224632024765015, test acc = 0.42500001192092896, time = 0.3569521903991699\n",
      "Testing at step=55, batch=45, test loss = 1.4994194507598877, test acc = 0.49000000953674316, time = 0.36139416694641113\n",
      "Step 55 finished in 264.0595440864563, Train loss = 1.4587534856796265, Test loss = 1.5397112607955932; Train Acc = 0.48221999895572665, Test Acc = 0.4537000006437302\n",
      "Training at step=56, batch=0, train loss = 1.4811569452285767, train acc = 0.44999998807907104, time = 0.9561669826507568\n",
      "Training at step=56, batch=25, train loss = 1.3372082710266113, train acc = 0.48500001430511475, time = 0.9342563152313232\n",
      "Training at step=56, batch=50, train loss = 1.5112286806106567, train acc = 0.4650000035762787, time = 0.9453802108764648\n",
      "Training at step=56, batch=75, train loss = 1.4484080076217651, train acc = 0.5249999761581421, time = 0.9384257793426514\n",
      "Training at step=56, batch=100, train loss = 1.6080780029296875, train acc = 0.4350000023841858, time = 0.9451084136962891\n",
      "Training at step=56, batch=125, train loss = 1.3919545412063599, train acc = 0.48500001430511475, time = 0.9451098442077637\n",
      "Training at step=56, batch=150, train loss = 1.376686692237854, train acc = 0.5450000166893005, time = 0.9357333183288574\n",
      "Training at step=56, batch=175, train loss = 1.4239033460617065, train acc = 0.5099999904632568, time = 0.9503889083862305\n",
      "Training at step=56, batch=200, train loss = 1.4844187498092651, train acc = 0.4699999988079071, time = 0.9450716972351074\n",
      "Training at step=56, batch=225, train loss = 1.2697967290878296, train acc = 0.6100000143051147, time = 0.9378528594970703\n",
      "Testing at step=56, batch=0, test loss = 1.7373167276382446, test acc = 0.3799999952316284, time = 0.35610413551330566\n",
      "Testing at step=56, batch=5, test loss = 1.4736841917037964, test acc = 0.4950000047683716, time = 0.3624236583709717\n",
      "Testing at step=56, batch=10, test loss = 1.497552752494812, test acc = 0.4449999928474426, time = 0.36570096015930176\n",
      "Testing at step=56, batch=15, test loss = 1.4001975059509277, test acc = 0.49000000953674316, time = 0.36939525604248047\n",
      "Testing at step=56, batch=20, test loss = 1.600504755973816, test acc = 0.41499999165534973, time = 0.3600633144378662\n",
      "Testing at step=56, batch=25, test loss = 1.5623546838760376, test acc = 0.4300000071525574, time = 0.35880422592163086\n",
      "Testing at step=56, batch=30, test loss = 1.6606289148330688, test acc = 0.4099999964237213, time = 0.37293314933776855\n",
      "Testing at step=56, batch=35, test loss = 1.5231232643127441, test acc = 0.4749999940395355, time = 0.3646585941314697\n",
      "Testing at step=56, batch=40, test loss = 1.5886294841766357, test acc = 0.42500001192092896, time = 0.353548526763916\n",
      "Testing at step=56, batch=45, test loss = 1.5810999870300293, test acc = 0.42500001192092896, time = 0.34879612922668457\n",
      "Step 56 finished in 264.31053924560547, Train loss = 1.4509161586761474, Test loss = 1.5401814770698548; Train Acc = 0.486319998383522, Test Acc = 0.4569000005722046\n",
      "Training at step=57, batch=0, train loss = 1.444870114326477, train acc = 0.4950000047683716, time = 0.9494040012359619\n",
      "Training at step=57, batch=25, train loss = 1.4607044458389282, train acc = 0.4950000047683716, time = 0.9509265422821045\n",
      "Training at step=57, batch=50, train loss = 1.5178828239440918, train acc = 0.4749999940395355, time = 0.9471921920776367\n",
      "Training at step=57, batch=75, train loss = 1.4456089735031128, train acc = 0.47999998927116394, time = 0.9366898536682129\n",
      "Training at step=57, batch=100, train loss = 1.4551125764846802, train acc = 0.5, time = 0.9455609321594238\n",
      "Training at step=57, batch=125, train loss = 1.4328235387802124, train acc = 0.45500001311302185, time = 0.9437737464904785\n",
      "Training at step=57, batch=150, train loss = 1.4375355243682861, train acc = 0.5400000214576721, time = 0.9427356719970703\n",
      "Training at step=57, batch=175, train loss = 1.5432149171829224, train acc = 0.4399999976158142, time = 1.0432779788970947\n",
      "Training at step=57, batch=200, train loss = 1.386170506477356, train acc = 0.48500001430511475, time = 0.9426932334899902\n",
      "Training at step=57, batch=225, train loss = 1.4964936971664429, train acc = 0.5049999952316284, time = 0.9345321655273438\n",
      "Testing at step=57, batch=0, test loss = 1.4435691833496094, test acc = 0.49000000953674316, time = 0.3528897762298584\n",
      "Testing at step=57, batch=5, test loss = 1.6640336513519287, test acc = 0.39500001072883606, time = 0.35637903213500977\n",
      "Testing at step=57, batch=10, test loss = 1.511115550994873, test acc = 0.4350000023841858, time = 0.3526589870452881\n",
      "Testing at step=57, batch=15, test loss = 1.4995399713516235, test acc = 0.4650000035762787, time = 0.3589797019958496\n",
      "Testing at step=57, batch=20, test loss = 1.6394715309143066, test acc = 0.4099999964237213, time = 0.3576343059539795\n",
      "Testing at step=57, batch=25, test loss = 1.6657662391662598, test acc = 0.45500001311302185, time = 0.35054945945739746\n",
      "Testing at step=57, batch=30, test loss = 1.5865615606307983, test acc = 0.4350000023841858, time = 0.36161017417907715\n",
      "Testing at step=57, batch=35, test loss = 1.6265270709991455, test acc = 0.4300000071525574, time = 0.361954927444458\n",
      "Testing at step=57, batch=40, test loss = 1.4903287887573242, test acc = 0.47999998927116394, time = 0.3533446788787842\n",
      "Testing at step=57, batch=45, test loss = 1.5013396739959717, test acc = 0.5049999952316284, time = 0.3545684814453125\n",
      "Step 57 finished in 264.0228991508484, Train loss = 1.451199336528778, Test loss = 1.5402981758117675; Train Acc = 0.48662000000476835, Test Acc = 0.459899999499321\n",
      "Training at step=58, batch=0, train loss = 1.404679298400879, train acc = 0.5450000166893005, time = 0.9479677677154541\n",
      "Training at step=58, batch=25, train loss = 1.4632383584976196, train acc = 0.47999998927116394, time = 0.9461727142333984\n",
      "Training at step=58, batch=50, train loss = 1.414217233657837, train acc = 0.49000000953674316, time = 0.9526469707489014\n",
      "Training at step=58, batch=75, train loss = 1.5281327962875366, train acc = 0.4650000035762787, time = 0.9420692920684814\n",
      "Training at step=58, batch=100, train loss = 1.4747238159179688, train acc = 0.4699999988079071, time = 0.9466552734375\n",
      "Training at step=58, batch=125, train loss = 1.618962287902832, train acc = 0.42500001192092896, time = 0.9475803375244141\n",
      "Training at step=58, batch=150, train loss = 1.4392849206924438, train acc = 0.4650000035762787, time = 0.9496943950653076\n",
      "Training at step=58, batch=175, train loss = 1.3625186681747437, train acc = 0.5249999761581421, time = 0.94515061378479\n",
      "Training at step=58, batch=200, train loss = 1.3620244264602661, train acc = 0.5149999856948853, time = 0.9428448677062988\n",
      "Training at step=58, batch=225, train loss = 1.519813895225525, train acc = 0.5049999952316284, time = 0.9489579200744629\n",
      "Testing at step=58, batch=0, test loss = 1.5425629615783691, test acc = 0.48500001430511475, time = 0.3495004177093506\n",
      "Testing at step=58, batch=5, test loss = 1.4215065240859985, test acc = 0.5, time = 0.3643991947174072\n",
      "Testing at step=58, batch=10, test loss = 1.5305657386779785, test acc = 0.4950000047683716, time = 0.3563086986541748\n",
      "Testing at step=58, batch=15, test loss = 1.5529080629348755, test acc = 0.4699999988079071, time = 0.35156941413879395\n",
      "Testing at step=58, batch=20, test loss = 1.4399725198745728, test acc = 0.48500001430511475, time = 0.3522319793701172\n",
      "Testing at step=58, batch=25, test loss = 1.5100135803222656, test acc = 0.45500001311302185, time = 0.3499627113342285\n",
      "Testing at step=58, batch=30, test loss = 1.3998398780822754, test acc = 0.5049999952316284, time = 0.35916709899902344\n",
      "Testing at step=58, batch=35, test loss = 1.5300941467285156, test acc = 0.4399999976158142, time = 0.3534688949584961\n",
      "Testing at step=58, batch=40, test loss = 1.5733479261398315, test acc = 0.4399999976158142, time = 0.35579395294189453\n",
      "Testing at step=58, batch=45, test loss = 1.440332055091858, test acc = 0.4749999940395355, time = 0.35687875747680664\n",
      "Step 58 finished in 264.0160505771637, Train loss = 1.445921025276184, Test loss = 1.5350765466690064; Train Acc = 0.4896799994707108, Test Acc = 0.4582000011205673\n",
      "Training at step=59, batch=0, train loss = 1.3500816822052002, train acc = 0.5550000071525574, time = 0.9438254833221436\n",
      "Training at step=59, batch=25, train loss = 1.5302071571350098, train acc = 0.4699999988079071, time = 0.9481537342071533\n",
      "Training at step=59, batch=50, train loss = 1.3044437170028687, train acc = 0.5600000023841858, time = 0.9583172798156738\n",
      "Training at step=59, batch=75, train loss = 1.4725247621536255, train acc = 0.49000000953674316, time = 0.938591480255127\n",
      "Training at step=59, batch=100, train loss = 1.4671125411987305, train acc = 0.47999998927116394, time = 0.9656360149383545\n",
      "Training at step=59, batch=125, train loss = 1.5562858581542969, train acc = 0.44999998807907104, time = 0.9422376155853271\n",
      "Training at step=59, batch=150, train loss = 1.4044504165649414, train acc = 0.47999998927116394, time = 0.9438967704772949\n",
      "Training at step=59, batch=175, train loss = 1.4506020545959473, train acc = 0.4350000023841858, time = 0.9415581226348877\n",
      "Training at step=59, batch=200, train loss = 1.4475898742675781, train acc = 0.4749999940395355, time = 0.9497942924499512\n",
      "Training at step=59, batch=225, train loss = 1.4099066257476807, train acc = 0.5400000214576721, time = 0.941131591796875\n",
      "Testing at step=59, batch=0, test loss = 1.3708319664001465, test acc = 0.5350000262260437, time = 0.36107325553894043\n",
      "Testing at step=59, batch=5, test loss = 1.3927744626998901, test acc = 0.5, time = 0.370495080947876\n",
      "Testing at step=59, batch=10, test loss = 1.5680131912231445, test acc = 0.4300000071525574, time = 0.3604443073272705\n",
      "Testing at step=59, batch=15, test loss = 1.5546672344207764, test acc = 0.46000000834465027, time = 0.3564023971557617\n",
      "Testing at step=59, batch=20, test loss = 1.4547885656356812, test acc = 0.4699999988079071, time = 0.3633706569671631\n",
      "Testing at step=59, batch=25, test loss = 1.4395604133605957, test acc = 0.5, time = 0.35687875747680664\n",
      "Testing at step=59, batch=30, test loss = 1.657375454902649, test acc = 0.41499999165534973, time = 0.3555285930633545\n",
      "Testing at step=59, batch=35, test loss = 1.6251009702682495, test acc = 0.45500001311302185, time = 0.3576061725616455\n",
      "Testing at step=59, batch=40, test loss = 1.5444273948669434, test acc = 0.46000000834465027, time = 0.3679325580596924\n",
      "Testing at step=59, batch=45, test loss = 1.648221492767334, test acc = 0.42500001192092896, time = 0.35970473289489746\n",
      "Step 59 finished in 263.81050419807434, Train loss = 1.4436016440391541, Test loss = 1.536166672706604; Train Acc = 0.48978000020980833, Test Acc = 0.45700000286102294\n",
      "Training at step=60, batch=0, train loss = 1.4547936916351318, train acc = 0.49000000953674316, time = 0.9505274295806885\n",
      "Training at step=60, batch=25, train loss = 1.41068696975708, train acc = 0.47999998927116394, time = 0.9453508853912354\n",
      "Training at step=60, batch=50, train loss = 1.3215289115905762, train acc = 0.550000011920929, time = 0.9570260047912598\n",
      "Training at step=60, batch=75, train loss = 1.6331167221069336, train acc = 0.44999998807907104, time = 0.9385967254638672\n",
      "Training at step=60, batch=100, train loss = 1.4528239965438843, train acc = 0.49000000953674316, time = 0.9421167373657227\n",
      "Training at step=60, batch=125, train loss = 1.3057442903518677, train acc = 0.5199999809265137, time = 0.9384140968322754\n",
      "Training at step=60, batch=150, train loss = 1.539914608001709, train acc = 0.4699999988079071, time = 0.9583921432495117\n",
      "Training at step=60, batch=175, train loss = 1.3531016111373901, train acc = 0.5, time = 0.9395012855529785\n",
      "Training at step=60, batch=200, train loss = 1.4200220108032227, train acc = 0.48500001430511475, time = 0.938190221786499\n",
      "Training at step=60, batch=225, train loss = 1.3687554597854614, train acc = 0.5149999856948853, time = 0.9380056858062744\n",
      "Testing at step=60, batch=0, test loss = 1.5084807872772217, test acc = 0.47999998927116394, time = 0.3632791042327881\n",
      "Testing at step=60, batch=5, test loss = 1.6083813905715942, test acc = 0.49000000953674316, time = 0.35857534408569336\n",
      "Testing at step=60, batch=10, test loss = 1.6302833557128906, test acc = 0.42500001192092896, time = 0.3647041320800781\n",
      "Testing at step=60, batch=15, test loss = 1.4549987316131592, test acc = 0.5049999952316284, time = 0.3554816246032715\n",
      "Testing at step=60, batch=20, test loss = 1.4846690893173218, test acc = 0.44999998807907104, time = 0.3525419235229492\n",
      "Testing at step=60, batch=25, test loss = 1.4531009197235107, test acc = 0.5099999904632568, time = 0.38317441940307617\n",
      "Testing at step=60, batch=30, test loss = 1.4922257661819458, test acc = 0.4399999976158142, time = 0.35513734817504883\n",
      "Testing at step=60, batch=35, test loss = 1.6332025527954102, test acc = 0.38999998569488525, time = 0.368847131729126\n",
      "Testing at step=60, batch=40, test loss = 1.5321143865585327, test acc = 0.5, time = 0.37186479568481445\n",
      "Testing at step=60, batch=45, test loss = 1.5913739204406738, test acc = 0.4449999928474426, time = 0.3498249053955078\n",
      "Step 60 finished in 263.9558594226837, Train loss = 1.4402396717071533, Test loss = 1.5350118947029114; Train Acc = 0.49196000015735625, Test Acc = 0.45929999828338625\n",
      "Training at step=61, batch=0, train loss = 1.4592722654342651, train acc = 0.5, time = 1.0510985851287842\n",
      "Training at step=61, batch=25, train loss = 1.492432713508606, train acc = 0.44999998807907104, time = 0.9407238960266113\n",
      "Training at step=61, batch=50, train loss = 1.4874621629714966, train acc = 0.4699999988079071, time = 0.9437625408172607\n",
      "Training at step=61, batch=75, train loss = 1.4938148260116577, train acc = 0.4449999928474426, time = 0.9419515132904053\n",
      "Training at step=61, batch=100, train loss = 1.6464009284973145, train acc = 0.4300000071525574, time = 1.0588605403900146\n",
      "Training at step=61, batch=125, train loss = 1.4005992412567139, train acc = 0.5400000214576721, time = 0.9373435974121094\n",
      "Training at step=61, batch=150, train loss = 1.4626851081848145, train acc = 0.5049999952316284, time = 0.9483954906463623\n",
      "Training at step=61, batch=175, train loss = 1.4824104309082031, train acc = 0.48500001430511475, time = 0.9484579563140869\n",
      "Training at step=61, batch=200, train loss = 1.601465106010437, train acc = 0.4399999976158142, time = 0.9419548511505127\n",
      "Training at step=61, batch=225, train loss = 1.4859955310821533, train acc = 0.47999998927116394, time = 0.945033073425293\n",
      "Testing at step=61, batch=0, test loss = 1.5621750354766846, test acc = 0.44999998807907104, time = 0.36280155181884766\n",
      "Testing at step=61, batch=5, test loss = 1.4048411846160889, test acc = 0.5, time = 0.3594224452972412\n",
      "Testing at step=61, batch=10, test loss = 1.710384726524353, test acc = 0.4000000059604645, time = 0.364363431930542\n",
      "Testing at step=61, batch=15, test loss = 1.6246423721313477, test acc = 0.4000000059604645, time = 0.35281991958618164\n",
      "Testing at step=61, batch=20, test loss = 1.5210343599319458, test acc = 0.45500001311302185, time = 0.3532283306121826\n",
      "Testing at step=61, batch=25, test loss = 1.4971249103546143, test acc = 0.46000000834465027, time = 0.3534364700317383\n",
      "Testing at step=61, batch=30, test loss = 1.4555550813674927, test acc = 0.44999998807907104, time = 0.36374449729919434\n",
      "Testing at step=61, batch=35, test loss = 1.4969182014465332, test acc = 0.4650000035762787, time = 0.3483126163482666\n",
      "Testing at step=61, batch=40, test loss = 1.4604132175445557, test acc = 0.5049999952316284, time = 0.37378692626953125\n",
      "Testing at step=61, batch=45, test loss = 1.5654096603393555, test acc = 0.42500001192092896, time = 0.3635995388031006\n",
      "Step 61 finished in 264.1825876235962, Train loss = 1.4355505170822143, Test loss = 1.524021530151367; Train Acc = 0.4912799994945526, Test Acc = 0.4580999994277954\n",
      "Training at step=62, batch=0, train loss = 1.490420937538147, train acc = 0.48500001430511475, time = 0.9488718509674072\n",
      "Training at step=62, batch=25, train loss = 1.3568496704101562, train acc = 0.47999998927116394, time = 0.9419119358062744\n",
      "Training at step=62, batch=50, train loss = 1.3504242897033691, train acc = 0.550000011920929, time = 0.9471774101257324\n",
      "Training at step=62, batch=75, train loss = 1.4177430868148804, train acc = 0.4749999940395355, time = 0.946505069732666\n",
      "Training at step=62, batch=100, train loss = 1.4399996995925903, train acc = 0.47999998927116394, time = 0.9551589488983154\n",
      "Training at step=62, batch=125, train loss = 1.429407000541687, train acc = 0.4950000047683716, time = 0.9398126602172852\n",
      "Training at step=62, batch=150, train loss = 1.3931556940078735, train acc = 0.5149999856948853, time = 0.952521800994873\n",
      "Training at step=62, batch=175, train loss = 1.6145532131195068, train acc = 0.4449999928474426, time = 0.9439606666564941\n",
      "Training at step=62, batch=200, train loss = 1.503218412399292, train acc = 0.4749999940395355, time = 0.9468750953674316\n",
      "Training at step=62, batch=225, train loss = 1.269413709640503, train acc = 0.574999988079071, time = 0.9550225734710693\n",
      "Testing at step=62, batch=0, test loss = 1.5992982387542725, test acc = 0.45500001311302185, time = 0.35529136657714844\n",
      "Testing at step=62, batch=5, test loss = 1.4929001331329346, test acc = 0.47999998927116394, time = 0.363483190536499\n",
      "Testing at step=62, batch=10, test loss = 1.640079140663147, test acc = 0.4000000059604645, time = 0.35240697860717773\n",
      "Testing at step=62, batch=15, test loss = 1.38154935836792, test acc = 0.5, time = 0.34958362579345703\n",
      "Testing at step=62, batch=20, test loss = 1.5037751197814941, test acc = 0.45500001311302185, time = 0.3659703731536865\n",
      "Testing at step=62, batch=25, test loss = 1.4862489700317383, test acc = 0.4749999940395355, time = 0.3560159206390381\n",
      "Testing at step=62, batch=30, test loss = 1.6312083005905151, test acc = 0.46000000834465027, time = 0.35285186767578125\n",
      "Testing at step=62, batch=35, test loss = 1.4113457202911377, test acc = 0.5199999809265137, time = 0.35074400901794434\n",
      "Testing at step=62, batch=40, test loss = 1.5310914516448975, test acc = 0.4399999976158142, time = 0.5505592823028564\n",
      "Testing at step=62, batch=45, test loss = 1.513056993484497, test acc = 0.46000000834465027, time = 0.3495020866394043\n",
      "Step 62 finished in 264.21346521377563, Train loss = 1.4343064703941346, Test loss = 1.543603003025055; Train Acc = 0.49151999890804293, Test Acc = 0.45589999794960023\n",
      "Training at step=63, batch=0, train loss = 1.5012016296386719, train acc = 0.4399999976158142, time = 0.9348936080932617\n",
      "Training at step=63, batch=25, train loss = 1.4599201679229736, train acc = 0.4449999928474426, time = 0.959791898727417\n",
      "Training at step=63, batch=50, train loss = 1.4643746614456177, train acc = 0.4449999928474426, time = 0.9399516582489014\n",
      "Training at step=63, batch=75, train loss = 1.3279818296432495, train acc = 0.550000011920929, time = 0.9456446170806885\n",
      "Training at step=63, batch=100, train loss = 1.5671696662902832, train acc = 0.4050000011920929, time = 0.9552173614501953\n",
      "Training at step=63, batch=125, train loss = 1.4995079040527344, train acc = 0.45500001311302185, time = 0.9452857971191406\n",
      "Training at step=63, batch=150, train loss = 1.5361285209655762, train acc = 0.4650000035762787, time = 0.9395716190338135\n",
      "Training at step=63, batch=175, train loss = 1.5459786653518677, train acc = 0.4399999976158142, time = 0.9516274929046631\n",
      "Training at step=63, batch=200, train loss = 1.3440899848937988, train acc = 0.4950000047683716, time = 0.9458377361297607\n",
      "Training at step=63, batch=225, train loss = 1.4969825744628906, train acc = 0.5149999856948853, time = 0.9387834072113037\n",
      "Testing at step=63, batch=0, test loss = 1.4751319885253906, test acc = 0.5, time = 0.3497638702392578\n",
      "Testing at step=63, batch=5, test loss = 1.6330446004867554, test acc = 0.41499999165534973, time = 0.3682410717010498\n",
      "Testing at step=63, batch=10, test loss = 1.4185634851455688, test acc = 0.5, time = 0.361051082611084\n",
      "Testing at step=63, batch=15, test loss = 1.4210609197616577, test acc = 0.5149999856948853, time = 0.34887099266052246\n",
      "Testing at step=63, batch=20, test loss = 1.4413344860076904, test acc = 0.5099999904632568, time = 0.35314440727233887\n",
      "Testing at step=63, batch=25, test loss = 1.5572880506515503, test acc = 0.4350000023841858, time = 0.35928916931152344\n",
      "Testing at step=63, batch=30, test loss = 1.6608428955078125, test acc = 0.4000000059604645, time = 0.35571837425231934\n",
      "Testing at step=63, batch=35, test loss = 1.5904226303100586, test acc = 0.46000000834465027, time = 0.4255542755126953\n",
      "Testing at step=63, batch=40, test loss = 1.4937483072280884, test acc = 0.47999998927116394, time = 0.3609659671783447\n",
      "Testing at step=63, batch=45, test loss = 1.5240193605422974, test acc = 0.4300000071525574, time = 0.36507415771484375\n",
      "Step 63 finished in 264.09727668762207, Train loss = 1.4326273317337037, Test loss = 1.5176834487915039; Train Acc = 0.49083999967575076, Test Acc = 0.461700000166893\n",
      "Training at step=64, batch=0, train loss = 1.4627385139465332, train acc = 0.5199999809265137, time = 0.955254077911377\n",
      "Training at step=64, batch=25, train loss = 1.3045623302459717, train acc = 0.5799999833106995, time = 0.9375135898590088\n",
      "Training at step=64, batch=50, train loss = 1.3798986673355103, train acc = 0.5450000166893005, time = 0.9445371627807617\n",
      "Training at step=64, batch=75, train loss = 1.5382128953933716, train acc = 0.4350000023841858, time = 0.9511592388153076\n",
      "Training at step=64, batch=100, train loss = 1.5010932683944702, train acc = 0.47999998927116394, time = 0.9406871795654297\n",
      "Training at step=64, batch=125, train loss = 1.6671351194381714, train acc = 0.4350000023841858, time = 0.9403069019317627\n",
      "Training at step=64, batch=150, train loss = 1.5453312397003174, train acc = 0.4399999976158142, time = 0.9400355815887451\n",
      "Training at step=64, batch=175, train loss = 1.4871008396148682, train acc = 0.5049999952316284, time = 0.9374918937683105\n",
      "Training at step=64, batch=200, train loss = 1.4451324939727783, train acc = 0.5049999952316284, time = 0.9575364589691162\n",
      "Training at step=64, batch=225, train loss = 1.4348112344741821, train acc = 0.4749999940395355, time = 0.9444811344146729\n",
      "Testing at step=64, batch=0, test loss = 1.5514057874679565, test acc = 0.4399999976158142, time = 0.35229039192199707\n",
      "Testing at step=64, batch=5, test loss = 1.4542250633239746, test acc = 0.5600000023841858, time = 0.35005664825439453\n",
      "Testing at step=64, batch=10, test loss = 1.4327157735824585, test acc = 0.4699999988079071, time = 0.3541877269744873\n",
      "Testing at step=64, batch=15, test loss = 1.5825477838516235, test acc = 0.41999998688697815, time = 0.3484811782836914\n",
      "Testing at step=64, batch=20, test loss = 1.5266914367675781, test acc = 0.4950000047683716, time = 0.3521440029144287\n",
      "Testing at step=64, batch=25, test loss = 1.4894479513168335, test acc = 0.5049999952316284, time = 0.3498806953430176\n",
      "Testing at step=64, batch=30, test loss = 1.522547721862793, test acc = 0.42500001192092896, time = 0.3583793640136719\n",
      "Testing at step=64, batch=35, test loss = 1.4336775541305542, test acc = 0.5049999952316284, time = 0.3487510681152344\n",
      "Testing at step=64, batch=40, test loss = 1.5048104524612427, test acc = 0.5, time = 0.3670337200164795\n",
      "Testing at step=64, batch=45, test loss = 1.6868089437484741, test acc = 0.4050000011920929, time = 0.3527677059173584\n",
      "Step 64 finished in 263.75376319885254, Train loss = 1.4255012879371642, Test loss = 1.530298764705658; Train Acc = 0.49679999887943266, Test Acc = 0.46049999952316284\n",
      "Training at step=65, batch=0, train loss = 1.466148853302002, train acc = 0.47999998927116394, time = 0.9422633647918701\n",
      "Training at step=65, batch=25, train loss = 1.4120155572891235, train acc = 0.5099999904632568, time = 0.9491355419158936\n",
      "Training at step=65, batch=50, train loss = 1.3458956480026245, train acc = 0.5350000262260437, time = 0.9620456695556641\n",
      "Training at step=65, batch=75, train loss = 1.371057152748108, train acc = 0.5249999761581421, time = 0.9464197158813477\n",
      "Training at step=65, batch=100, train loss = 1.401440143585205, train acc = 0.48500001430511475, time = 0.9527137279510498\n",
      "Training at step=65, batch=125, train loss = 1.3445276021957397, train acc = 0.5149999856948853, time = 0.9524972438812256\n",
      "Training at step=65, batch=150, train loss = 1.2703839540481567, train acc = 0.6100000143051147, time = 0.9410672187805176\n",
      "Training at step=65, batch=175, train loss = 1.4252302646636963, train acc = 0.5, time = 0.9408845901489258\n",
      "Training at step=65, batch=200, train loss = 1.3437538146972656, train acc = 0.5350000262260437, time = 0.9436531066894531\n",
      "Training at step=65, batch=225, train loss = 1.4601783752441406, train acc = 0.46000000834465027, time = 0.9378924369812012\n",
      "Testing at step=65, batch=0, test loss = 1.4723321199417114, test acc = 0.48500001430511475, time = 0.34977030754089355\n",
      "Testing at step=65, batch=5, test loss = 1.5441566705703735, test acc = 0.4749999940395355, time = 0.348111629486084\n",
      "Testing at step=65, batch=10, test loss = 1.655011773109436, test acc = 0.44999998807907104, time = 0.35198974609375\n",
      "Testing at step=65, batch=15, test loss = 1.7062022686004639, test acc = 0.39500001072883606, time = 0.35296106338500977\n",
      "Testing at step=65, batch=20, test loss = 1.4494684934616089, test acc = 0.4650000035762787, time = 0.3532068729400635\n",
      "Testing at step=65, batch=25, test loss = 1.5584548711776733, test acc = 0.4399999976158142, time = 0.3543827533721924\n",
      "Testing at step=65, batch=30, test loss = 1.6473312377929688, test acc = 0.4050000011920929, time = 0.35410141944885254\n",
      "Testing at step=65, batch=35, test loss = 1.5178793668746948, test acc = 0.4449999928474426, time = 0.358872652053833\n",
      "Testing at step=65, batch=40, test loss = 1.4587103128433228, test acc = 0.4699999988079071, time = 0.3553805351257324\n",
      "Testing at step=65, batch=45, test loss = 1.6433722972869873, test acc = 0.41499999165534973, time = 0.3524434566497803\n",
      "Step 65 finished in 263.523001909256, Train loss = 1.4248503499031067, Test loss = 1.5601259541511536; Train Acc = 0.4953999989032745, Test Acc = 0.44820000052452086\n",
      "Training at step=66, batch=0, train loss = 1.4470399618148804, train acc = 0.4350000023841858, time = 0.9455933570861816\n",
      "Training at step=66, batch=25, train loss = 1.4191135168075562, train acc = 0.4950000047683716, time = 0.9497385025024414\n",
      "Training at step=66, batch=50, train loss = 1.5114774703979492, train acc = 0.4300000071525574, time = 0.9486582279205322\n",
      "Training at step=66, batch=75, train loss = 1.4080003499984741, train acc = 0.4749999940395355, time = 0.943070650100708\n",
      "Training at step=66, batch=100, train loss = 1.498100757598877, train acc = 0.4699999988079071, time = 0.9556052684783936\n",
      "Training at step=66, batch=125, train loss = 1.2704401016235352, train acc = 0.5600000023841858, time = 1.131547451019287\n",
      "Training at step=66, batch=150, train loss = 1.3883256912231445, train acc = 0.5400000214576721, time = 0.9462838172912598\n",
      "Training at step=66, batch=175, train loss = 1.374005913734436, train acc = 0.4650000035762787, time = 0.9392127990722656\n",
      "Training at step=66, batch=200, train loss = 1.5364868640899658, train acc = 0.5, time = 0.9388427734375\n",
      "Training at step=66, batch=225, train loss = 1.336847186088562, train acc = 0.5049999952316284, time = 1.0444424152374268\n",
      "Testing at step=66, batch=0, test loss = 1.5935245752334595, test acc = 0.4099999964237213, time = 0.3556492328643799\n",
      "Testing at step=66, batch=5, test loss = 1.5591132640838623, test acc = 0.4399999976158142, time = 0.3640458583831787\n",
      "Testing at step=66, batch=10, test loss = 1.5094027519226074, test acc = 0.4350000023841858, time = 0.35147833824157715\n",
      "Testing at step=66, batch=15, test loss = 1.5670363903045654, test acc = 0.4449999928474426, time = 0.36136603355407715\n",
      "Testing at step=66, batch=20, test loss = 1.6333093643188477, test acc = 0.375, time = 0.35858154296875\n",
      "Testing at step=66, batch=25, test loss = 1.4668461084365845, test acc = 0.5350000262260437, time = 0.36562609672546387\n",
      "Testing at step=66, batch=30, test loss = 1.5471338033676147, test acc = 0.4749999940395355, time = 0.36429786682128906\n",
      "Testing at step=66, batch=35, test loss = 1.5952155590057373, test acc = 0.42500001192092896, time = 0.3630788326263428\n",
      "Testing at step=66, batch=40, test loss = 1.5592796802520752, test acc = 0.4650000035762787, time = 0.3550400733947754\n",
      "Testing at step=66, batch=45, test loss = 1.5483484268188477, test acc = 0.4399999976158142, time = 0.35761284828186035\n",
      "Step 66 finished in 264.0803611278534, Train loss = 1.417871094226837, Test loss = 1.5297497820854187; Train Acc = 0.4989799998998642, Test Acc = 0.46349999904632566\n",
      "Training at step=67, batch=0, train loss = 1.5016075372695923, train acc = 0.5249999761581421, time = 0.941462516784668\n",
      "Training at step=67, batch=25, train loss = 1.3816026449203491, train acc = 0.5199999809265137, time = 0.9375488758087158\n",
      "Training at step=67, batch=50, train loss = 1.536057710647583, train acc = 0.4350000023841858, time = 0.9485383033752441\n",
      "Training at step=67, batch=75, train loss = 1.6153349876403809, train acc = 0.45500001311302185, time = 0.9535727500915527\n",
      "Training at step=67, batch=100, train loss = 1.4078255891799927, train acc = 0.4950000047683716, time = 0.9420304298400879\n",
      "Training at step=67, batch=125, train loss = 1.352230191230774, train acc = 0.5099999904632568, time = 0.9427917003631592\n",
      "Training at step=67, batch=150, train loss = 1.4806190729141235, train acc = 0.47999998927116394, time = 0.9407463073730469\n",
      "Training at step=67, batch=175, train loss = 1.5481213331222534, train acc = 0.44999998807907104, time = 0.9385931491851807\n",
      "Training at step=67, batch=200, train loss = 1.4147210121154785, train acc = 0.5350000262260437, time = 0.9438698291778564\n",
      "Training at step=67, batch=225, train loss = 1.469535231590271, train acc = 0.4699999988079071, time = 0.9389035701751709\n",
      "Testing at step=67, batch=0, test loss = 1.600069522857666, test acc = 0.42500001192092896, time = 0.35735201835632324\n",
      "Testing at step=67, batch=5, test loss = 1.4929306507110596, test acc = 0.4399999976158142, time = 0.3482668399810791\n",
      "Testing at step=67, batch=10, test loss = 1.5820852518081665, test acc = 0.45500001311302185, time = 0.3562788963317871\n",
      "Testing at step=67, batch=15, test loss = 1.501713752746582, test acc = 0.4449999928474426, time = 0.3537106513977051\n",
      "Testing at step=67, batch=20, test loss = 1.406947135925293, test acc = 0.5799999833106995, time = 0.35865283012390137\n",
      "Testing at step=67, batch=25, test loss = 1.6365512609481812, test acc = 0.4449999928474426, time = 0.3564612865447998\n",
      "Testing at step=67, batch=30, test loss = 1.5014734268188477, test acc = 0.4650000035762787, time = 0.3548755645751953\n",
      "Testing at step=67, batch=35, test loss = 1.7279669046401978, test acc = 0.41499999165534973, time = 0.3549365997314453\n",
      "Testing at step=67, batch=40, test loss = 1.5624841451644897, test acc = 0.4699999988079071, time = 0.36675357818603516\n",
      "Testing at step=67, batch=45, test loss = 1.5500717163085938, test acc = 0.4300000071525574, time = 0.3715386390686035\n",
      "Step 67 finished in 263.48801589012146, Train loss = 1.4187856917381287, Test loss = 1.515883526802063; Train Acc = 0.49733999752998354, Test Acc = 0.4615999984741211\n",
      "Training at step=68, batch=0, train loss = 1.3279218673706055, train acc = 0.5, time = 0.9416871070861816\n",
      "Training at step=68, batch=25, train loss = 1.5100511312484741, train acc = 0.4950000047683716, time = 0.9431698322296143\n",
      "Training at step=68, batch=50, train loss = 1.3308310508728027, train acc = 0.5450000166893005, time = 0.9498274326324463\n",
      "Training at step=68, batch=75, train loss = 1.449954628944397, train acc = 0.5149999856948853, time = 0.9460163116455078\n",
      "Training at step=68, batch=100, train loss = 1.4350825548171997, train acc = 0.5149999856948853, time = 0.9468400478363037\n",
      "Training at step=68, batch=125, train loss = 1.423370361328125, train acc = 0.44999998807907104, time = 0.9558138847351074\n",
      "Training at step=68, batch=150, train loss = 1.429405689239502, train acc = 0.5099999904632568, time = 0.9552247524261475\n",
      "Training at step=68, batch=175, train loss = 1.4720062017440796, train acc = 0.45500001311302185, time = 0.9473361968994141\n",
      "Training at step=68, batch=200, train loss = 1.476332664489746, train acc = 0.4749999940395355, time = 0.9419584274291992\n",
      "Training at step=68, batch=225, train loss = 1.3949813842773438, train acc = 0.5299999713897705, time = 0.9528200626373291\n",
      "Testing at step=68, batch=0, test loss = 1.6148383617401123, test acc = 0.41499999165534973, time = 0.3567824363708496\n",
      "Testing at step=68, batch=5, test loss = 1.4361810684204102, test acc = 0.48500001430511475, time = 0.3585216999053955\n",
      "Testing at step=68, batch=10, test loss = 1.5733436346054077, test acc = 0.45500001311302185, time = 0.36152124404907227\n",
      "Testing at step=68, batch=15, test loss = 1.3666998147964478, test acc = 0.4950000047683716, time = 0.352344274520874\n",
      "Testing at step=68, batch=20, test loss = 1.5830981731414795, test acc = 0.4300000071525574, time = 0.3484158515930176\n",
      "Testing at step=68, batch=25, test loss = 1.480615258216858, test acc = 0.4350000023841858, time = 0.34812307357788086\n",
      "Testing at step=68, batch=30, test loss = 1.4687190055847168, test acc = 0.4699999988079071, time = 0.35596323013305664\n",
      "Testing at step=68, batch=35, test loss = 1.6147246360778809, test acc = 0.4350000023841858, time = 0.3536248207092285\n",
      "Testing at step=68, batch=40, test loss = 1.6397349834442139, test acc = 0.45500001311302185, time = 0.3540472984313965\n",
      "Testing at step=68, batch=45, test loss = 1.557621955871582, test acc = 0.47999998927116394, time = 0.35153675079345703\n",
      "Step 68 finished in 264.0685474872589, Train loss = 1.418335159301758, Test loss = 1.5172810578346252; Train Acc = 0.498539999127388, Test Acc = 0.46459999918937683\n",
      "Training at step=69, batch=0, train loss = 1.4502896070480347, train acc = 0.4699999988079071, time = 0.9467048645019531\n",
      "Training at step=69, batch=25, train loss = 1.3996080160140991, train acc = 0.48500001430511475, time = 0.943469762802124\n",
      "Training at step=69, batch=50, train loss = 1.5496571063995361, train acc = 0.4650000035762787, time = 0.9385204315185547\n",
      "Training at step=69, batch=75, train loss = 1.3024628162384033, train acc = 0.5350000262260437, time = 0.9437501430511475\n",
      "Training at step=69, batch=100, train loss = 1.4978742599487305, train acc = 0.4699999988079071, time = 0.948371410369873\n",
      "Training at step=69, batch=125, train loss = 1.4077975749969482, train acc = 0.5649999976158142, time = 0.9533758163452148\n",
      "Training at step=69, batch=150, train loss = 1.4085726737976074, train acc = 0.5199999809265137, time = 0.9343166351318359\n",
      "Training at step=69, batch=175, train loss = 1.451911449432373, train acc = 0.47999998927116394, time = 0.9573266506195068\n",
      "Training at step=69, batch=200, train loss = 1.4664955139160156, train acc = 0.4749999940395355, time = 0.9534018039703369\n",
      "Training at step=69, batch=225, train loss = 1.4590455293655396, train acc = 0.48500001430511475, time = 0.946312665939331\n",
      "Testing at step=69, batch=0, test loss = 1.4788079261779785, test acc = 0.49000000953674316, time = 0.3538062572479248\n",
      "Testing at step=69, batch=5, test loss = 1.4327484369277954, test acc = 0.44999998807907104, time = 0.3464233875274658\n",
      "Testing at step=69, batch=10, test loss = 1.4575774669647217, test acc = 0.4449999928474426, time = 0.348156213760376\n",
      "Testing at step=69, batch=15, test loss = 1.508859395980835, test acc = 0.47999998927116394, time = 0.34624290466308594\n",
      "Testing at step=69, batch=20, test loss = 1.6410998106002808, test acc = 0.42500001192092896, time = 0.35408997535705566\n",
      "Testing at step=69, batch=25, test loss = 1.4040471315383911, test acc = 0.4699999988079071, time = 0.34938478469848633\n",
      "Testing at step=69, batch=30, test loss = 1.5317063331604004, test acc = 0.4749999940395355, time = 0.3484363555908203\n",
      "Testing at step=69, batch=35, test loss = 1.4898419380187988, test acc = 0.4950000047683716, time = 0.34856271743774414\n",
      "Testing at step=69, batch=40, test loss = 1.6803522109985352, test acc = 0.4350000023841858, time = 0.3563370704650879\n",
      "Testing at step=69, batch=45, test loss = 1.5640003681182861, test acc = 0.5049999952316284, time = 0.3461601734161377\n",
      "Step 69 finished in 263.7469720840454, Train loss = 1.4097750024795532, Test loss = 1.5250992488861084; Train Acc = 0.5007599966526032, Test Acc = 0.4639999997615814\n",
      "Training at step=70, batch=0, train loss = 1.4802099466323853, train acc = 0.47999998927116394, time = 0.9384188652038574\n",
      "Training at step=70, batch=25, train loss = 1.6133663654327393, train acc = 0.44999998807907104, time = 0.939373254776001\n",
      "Training at step=70, batch=50, train loss = 1.3641659021377563, train acc = 0.5400000214576721, time = 0.9427945613861084\n",
      "Training at step=70, batch=75, train loss = 1.4213489294052124, train acc = 0.5, time = 0.9577105045318604\n",
      "Training at step=70, batch=100, train loss = 1.3362019062042236, train acc = 0.5199999809265137, time = 0.936591386795044\n",
      "Training at step=70, batch=125, train loss = 1.3864848613739014, train acc = 0.4749999940395355, time = 0.941368818283081\n",
      "Training at step=70, batch=150, train loss = 1.3463548421859741, train acc = 0.5, time = 0.9555432796478271\n",
      "Training at step=70, batch=175, train loss = 1.4530601501464844, train acc = 0.4399999976158142, time = 0.9580540657043457\n",
      "Training at step=70, batch=200, train loss = 1.5635491609573364, train acc = 0.44999998807907104, time = 0.9489426612854004\n",
      "Training at step=70, batch=225, train loss = 1.5450870990753174, train acc = 0.4650000035762787, time = 0.9525105953216553\n",
      "Testing at step=70, batch=0, test loss = 1.5015679597854614, test acc = 0.4650000035762787, time = 0.35394954681396484\n",
      "Testing at step=70, batch=5, test loss = 1.4211801290512085, test acc = 0.4650000035762787, time = 0.46897268295288086\n",
      "Testing at step=70, batch=10, test loss = 1.6011918783187866, test acc = 0.45500001311302185, time = 0.35298633575439453\n",
      "Testing at step=70, batch=15, test loss = 1.5789467096328735, test acc = 0.4399999976158142, time = 0.3539271354675293\n",
      "Testing at step=70, batch=20, test loss = 1.4902639389038086, test acc = 0.47999998927116394, time = 0.3498532772064209\n",
      "Testing at step=70, batch=25, test loss = 1.529964566230774, test acc = 0.4650000035762787, time = 0.3528108596801758\n",
      "Testing at step=70, batch=30, test loss = 1.54819655418396, test acc = 0.42500001192092896, time = 0.3524959087371826\n",
      "Testing at step=70, batch=35, test loss = 1.5774905681610107, test acc = 0.4449999928474426, time = 0.34775710105895996\n",
      "Testing at step=70, batch=40, test loss = 1.3901711702346802, test acc = 0.5249999761581421, time = 0.3642556667327881\n",
      "Testing at step=70, batch=45, test loss = 1.656407117843628, test acc = 0.4350000023841858, time = 0.37326908111572266\n",
      "Step 70 finished in 263.8767237663269, Train loss = 1.4100631680488587, Test loss = 1.5241150736808777; Train Acc = 0.5026399970054627, Test Acc = 0.46350000202655794\n",
      "Training at step=71, batch=0, train loss = 1.4073996543884277, train acc = 0.5049999952316284, time = 0.9444613456726074\n",
      "Training at step=71, batch=25, train loss = 1.4191774129867554, train acc = 0.48500001430511475, time = 0.9482641220092773\n",
      "Training at step=71, batch=50, train loss = 1.2682266235351562, train acc = 0.5950000286102295, time = 0.9469833374023438\n",
      "Training at step=71, batch=75, train loss = 1.4515466690063477, train acc = 0.5249999761581421, time = 0.9421041011810303\n",
      "Training at step=71, batch=100, train loss = 1.3983808755874634, train acc = 0.5249999761581421, time = 0.9483222961425781\n",
      "Training at step=71, batch=125, train loss = 1.5615006685256958, train acc = 0.4650000035762787, time = 0.9376256465911865\n",
      "Training at step=71, batch=150, train loss = 1.3651237487792969, train acc = 0.45500001311302185, time = 0.9372191429138184\n",
      "Training at step=71, batch=175, train loss = 1.3707969188690186, train acc = 0.5299999713897705, time = 0.9499485492706299\n",
      "Training at step=71, batch=200, train loss = 1.4066078662872314, train acc = 0.4950000047683716, time = 0.9399783611297607\n",
      "Training at step=71, batch=225, train loss = 1.3593744039535522, train acc = 0.5, time = 0.9395685195922852\n",
      "Testing at step=71, batch=0, test loss = 1.5873842239379883, test acc = 0.45500001311302185, time = 0.5440402030944824\n",
      "Testing at step=71, batch=5, test loss = 1.5596153736114502, test acc = 0.46000000834465027, time = 0.3596522808074951\n",
      "Testing at step=71, batch=10, test loss = 1.520865797996521, test acc = 0.4699999988079071, time = 0.36420130729675293\n",
      "Testing at step=71, batch=15, test loss = 1.4626858234405518, test acc = 0.47999998927116394, time = 0.351306676864624\n",
      "Testing at step=71, batch=20, test loss = 1.4506385326385498, test acc = 0.4749999940395355, time = 0.36985111236572266\n",
      "Testing at step=71, batch=25, test loss = 1.6561219692230225, test acc = 0.42500001192092896, time = 0.36585092544555664\n",
      "Testing at step=71, batch=30, test loss = 1.4755301475524902, test acc = 0.4650000035762787, time = 0.34890198707580566\n",
      "Testing at step=71, batch=35, test loss = 1.525683879852295, test acc = 0.4449999928474426, time = 0.35189104080200195\n",
      "Testing at step=71, batch=40, test loss = 1.550215244293213, test acc = 0.44999998807907104, time = 0.35340404510498047\n",
      "Testing at step=71, batch=45, test loss = 1.343142032623291, test acc = 0.5299999713897705, time = 0.3568589687347412\n",
      "Step 71 finished in 264.163289308548, Train loss = 1.4049305768013, Test loss = 1.5140097546577453; Train Acc = 0.5038599992990493, Test Acc = 0.4649000006914139\n",
      "Training at step=72, batch=0, train loss = 1.323327660560608, train acc = 0.5299999713897705, time = 0.9422383308410645\n",
      "Training at step=72, batch=25, train loss = 1.4147204160690308, train acc = 0.49000000953674316, time = 0.9457507133483887\n",
      "Training at step=72, batch=50, train loss = 1.4174189567565918, train acc = 0.48500001430511475, time = 0.9382405281066895\n",
      "Training at step=72, batch=75, train loss = 1.2732133865356445, train acc = 0.5099999904632568, time = 0.9464111328125\n",
      "Training at step=72, batch=100, train loss = 1.432958960533142, train acc = 0.5149999856948853, time = 0.9384243488311768\n",
      "Training at step=72, batch=125, train loss = 1.3802554607391357, train acc = 0.5450000166893005, time = 0.952930212020874\n",
      "Training at step=72, batch=150, train loss = 1.2399855852127075, train acc = 0.574999988079071, time = 0.9453921318054199\n",
      "Training at step=72, batch=175, train loss = 1.340467095375061, train acc = 0.5899999737739563, time = 0.9461779594421387\n",
      "Training at step=72, batch=200, train loss = 1.362687110900879, train acc = 0.5299999713897705, time = 0.9393839836120605\n",
      "Training at step=72, batch=225, train loss = 1.5066946744918823, train acc = 0.5149999856948853, time = 0.9442243576049805\n",
      "Testing at step=72, batch=0, test loss = 1.5948066711425781, test acc = 0.44999998807907104, time = 0.36104822158813477\n",
      "Testing at step=72, batch=5, test loss = 1.5874210596084595, test acc = 0.42500001192092896, time = 0.36192750930786133\n",
      "Testing at step=72, batch=10, test loss = 1.4216197729110718, test acc = 0.550000011920929, time = 0.3560981750488281\n",
      "Testing at step=72, batch=15, test loss = 1.5648863315582275, test acc = 0.4699999988079071, time = 0.35573434829711914\n",
      "Testing at step=72, batch=20, test loss = 1.503551959991455, test acc = 0.4749999940395355, time = 0.3495197296142578\n",
      "Testing at step=72, batch=25, test loss = 1.7335426807403564, test acc = 0.38499999046325684, time = 0.3496870994567871\n",
      "Testing at step=72, batch=30, test loss = 1.6140295267105103, test acc = 0.4000000059604645, time = 0.3493654727935791\n",
      "Testing at step=72, batch=35, test loss = 1.4234880208969116, test acc = 0.5199999809265137, time = 0.3593473434448242\n",
      "Testing at step=72, batch=40, test loss = 1.5066189765930176, test acc = 0.4699999988079071, time = 0.35746073722839355\n",
      "Testing at step=72, batch=45, test loss = 1.513026475906372, test acc = 0.48500001430511475, time = 0.3526463508605957\n",
      "Step 72 finished in 263.72884345054626, Train loss = 1.4017302317619325, Test loss = 1.5247435808181762; Train Acc = 0.5041799972057343, Test Acc = 0.46270000040531156\n",
      "Training at step=73, batch=0, train loss = 1.3925776481628418, train acc = 0.5249999761581421, time = 0.9554526805877686\n",
      "Training at step=73, batch=25, train loss = 1.3671722412109375, train acc = 0.5400000214576721, time = 0.9444339275360107\n",
      "Training at step=73, batch=50, train loss = 1.4311450719833374, train acc = 0.4650000035762787, time = 0.9557974338531494\n",
      "Training at step=73, batch=75, train loss = 1.3797472715377808, train acc = 0.45500001311302185, time = 0.9463663101196289\n",
      "Training at step=73, batch=100, train loss = 1.5296741724014282, train acc = 0.48500001430511475, time = 0.9389517307281494\n",
      "Training at step=73, batch=125, train loss = 1.3378138542175293, train acc = 0.5400000214576721, time = 0.9506955146789551\n",
      "Training at step=73, batch=150, train loss = 1.261114239692688, train acc = 0.5699999928474426, time = 0.9519786834716797\n",
      "Training at step=73, batch=175, train loss = 1.3729796409606934, train acc = 0.5249999761581421, time = 0.9384474754333496\n",
      "Training at step=73, batch=200, train loss = 1.5636930465698242, train acc = 0.4449999928474426, time = 0.9455897808074951\n",
      "Training at step=73, batch=225, train loss = 1.3212881088256836, train acc = 0.4950000047683716, time = 0.9491117000579834\n",
      "Testing at step=73, batch=0, test loss = 1.5157191753387451, test acc = 0.4099999964237213, time = 0.3591649532318115\n",
      "Testing at step=73, batch=5, test loss = 1.5591562986373901, test acc = 0.48500001430511475, time = 0.35359764099121094\n",
      "Testing at step=73, batch=10, test loss = 1.4558302164077759, test acc = 0.48500001430511475, time = 0.3518218994140625\n",
      "Testing at step=73, batch=15, test loss = 1.3575459718704224, test acc = 0.5099999904632568, time = 0.36087894439697266\n",
      "Testing at step=73, batch=20, test loss = 1.5383572578430176, test acc = 0.47999998927116394, time = 0.3488118648529053\n",
      "Testing at step=73, batch=25, test loss = 1.4778016805648804, test acc = 0.4749999940395355, time = 0.3547940254211426\n",
      "Testing at step=73, batch=30, test loss = 1.4876941442489624, test acc = 0.4650000035762787, time = 0.3507862091064453\n",
      "Testing at step=73, batch=35, test loss = 1.5765057802200317, test acc = 0.5149999856948853, time = 0.3484525680541992\n",
      "Testing at step=73, batch=40, test loss = 1.5605069398880005, test acc = 0.42500001192092896, time = 0.34723615646362305\n",
      "Testing at step=73, batch=45, test loss = 1.491502046585083, test acc = 0.47999998927116394, time = 0.3509061336517334\n",
      "Step 73 finished in 263.7267439365387, Train loss = 1.4011469283103943, Test loss = 1.5124956274032593; Train Acc = 0.5042599991559983, Test Acc = 0.46840000092983247\n",
      "Training at step=74, batch=0, train loss = 1.3263964653015137, train acc = 0.5, time = 0.9492883682250977\n",
      "Training at step=74, batch=25, train loss = 1.2658300399780273, train acc = 0.5550000071525574, time = 0.9369149208068848\n",
      "Training at step=74, batch=50, train loss = 1.33113694190979, train acc = 0.5550000071525574, time = 0.9366633892059326\n",
      "Training at step=74, batch=75, train loss = 1.3615961074829102, train acc = 0.49000000953674316, time = 0.9399092197418213\n",
      "Training at step=74, batch=100, train loss = 1.3138281106948853, train acc = 0.5350000262260437, time = 0.9369432926177979\n",
      "Training at step=74, batch=125, train loss = 1.3679966926574707, train acc = 0.5400000214576721, time = 0.9416184425354004\n",
      "Training at step=74, batch=150, train loss = 1.3307276964187622, train acc = 0.5350000262260437, time = 0.9622900485992432\n",
      "Training at step=74, batch=175, train loss = 1.3533008098602295, train acc = 0.550000011920929, time = 0.9589102268218994\n",
      "Training at step=74, batch=200, train loss = 1.3384485244750977, train acc = 0.5550000071525574, time = 0.9582762718200684\n",
      "Training at step=74, batch=225, train loss = 1.430331826210022, train acc = 0.5649999976158142, time = 0.9452252388000488\n",
      "Testing at step=74, batch=0, test loss = 1.4809439182281494, test acc = 0.4650000035762787, time = 0.36115479469299316\n",
      "Testing at step=74, batch=5, test loss = 1.3965719938278198, test acc = 0.5049999952316284, time = 0.367931604385376\n",
      "Testing at step=74, batch=10, test loss = 1.4022774696350098, test acc = 0.5149999856948853, time = 0.35796284675598145\n",
      "Testing at step=74, batch=15, test loss = 1.5310150384902954, test acc = 0.4749999940395355, time = 0.3594019412994385\n",
      "Testing at step=74, batch=20, test loss = 1.5437116622924805, test acc = 0.47999998927116394, time = 0.36208677291870117\n",
      "Testing at step=74, batch=25, test loss = 1.530935525894165, test acc = 0.4650000035762787, time = 0.36087989807128906\n",
      "Testing at step=74, batch=30, test loss = 1.4539494514465332, test acc = 0.5099999904632568, time = 0.3573179244995117\n",
      "Testing at step=74, batch=35, test loss = 1.5560200214385986, test acc = 0.44999998807907104, time = 0.36313867568969727\n",
      "Testing at step=74, batch=40, test loss = 1.5034244060516357, test acc = 0.48500001430511475, time = 0.34731507301330566\n",
      "Testing at step=74, batch=45, test loss = 1.5819573402404785, test acc = 0.41999998688697815, time = 0.349473237991333\n",
      "Step 74 finished in 263.9881992340088, Train loss = 1.4020999727249146, Test loss = 1.5177390098571777; Train Acc = 0.5043799990415573, Test Acc = 0.46729999959468843\n",
      "Training at step=75, batch=0, train loss = 1.293226957321167, train acc = 0.5450000166893005, time = 0.9566457271575928\n",
      "Training at step=75, batch=25, train loss = 1.3431735038757324, train acc = 0.5099999904632568, time = 0.949791669845581\n",
      "Training at step=75, batch=50, train loss = 1.3103554248809814, train acc = 0.5299999713897705, time = 0.9418232440948486\n",
      "Training at step=75, batch=75, train loss = 1.4750882387161255, train acc = 0.49000000953674316, time = 0.9586210250854492\n",
      "Training at step=75, batch=100, train loss = 1.494676113128662, train acc = 0.47999998927116394, time = 0.9545688629150391\n",
      "Training at step=75, batch=125, train loss = 1.4476416110992432, train acc = 0.49000000953674316, time = 0.9444544315338135\n",
      "Training at step=75, batch=150, train loss = 1.2734313011169434, train acc = 0.5699999928474426, time = 0.9441444873809814\n",
      "Training at step=75, batch=175, train loss = 1.4301385879516602, train acc = 0.47999998927116394, time = 0.9393918514251709\n",
      "Training at step=75, batch=200, train loss = 1.4483140707015991, train acc = 0.5, time = 0.9479944705963135\n",
      "Training at step=75, batch=225, train loss = 1.433952808380127, train acc = 0.4699999988079071, time = 0.9358053207397461\n",
      "Testing at step=75, batch=0, test loss = 1.436417579650879, test acc = 0.47999998927116394, time = 0.36336517333984375\n",
      "Testing at step=75, batch=5, test loss = 1.4478306770324707, test acc = 0.49000000953674316, time = 0.3683788776397705\n",
      "Testing at step=75, batch=10, test loss = 1.416835069656372, test acc = 0.5049999952316284, time = 0.3559842109680176\n",
      "Testing at step=75, batch=15, test loss = 1.441246747970581, test acc = 0.5199999809265137, time = 0.37262868881225586\n",
      "Testing at step=75, batch=20, test loss = 1.3712072372436523, test acc = 0.4950000047683716, time = 0.36916542053222656\n",
      "Testing at step=75, batch=25, test loss = 1.6024272441864014, test acc = 0.4399999976158142, time = 0.35393762588500977\n",
      "Testing at step=75, batch=30, test loss = 1.4474924802780151, test acc = 0.5249999761581421, time = 0.3681631088256836\n",
      "Testing at step=75, batch=35, test loss = 1.4393330812454224, test acc = 0.47999998927116394, time = 0.3522462844848633\n",
      "Testing at step=75, batch=40, test loss = 1.5506696701049805, test acc = 0.4300000071525574, time = 0.3729429244995117\n",
      "Testing at step=75, batch=45, test loss = 1.5857954025268555, test acc = 0.4350000023841858, time = 0.3622145652770996\n",
      "Step 75 finished in 264.1274211406708, Train loss = 1.3974190602302552, Test loss = 1.5103525876998902; Train Acc = 0.5046399983167649, Test Acc = 0.4653999996185303\n",
      "Training at step=76, batch=0, train loss = 1.4348807334899902, train acc = 0.49000000953674316, time = 0.9404022693634033\n",
      "Training at step=76, batch=25, train loss = 1.3336747884750366, train acc = 0.5400000214576721, time = 0.9553911685943604\n",
      "Training at step=76, batch=50, train loss = 1.3849934339523315, train acc = 0.5249999761581421, time = 0.9410319328308105\n",
      "Training at step=76, batch=75, train loss = 1.5256929397583008, train acc = 0.44999998807907104, time = 0.9508435726165771\n",
      "Training at step=76, batch=100, train loss = 1.3887284994125366, train acc = 0.5199999809265137, time = 0.9480068683624268\n",
      "Training at step=76, batch=125, train loss = 1.450424313545227, train acc = 0.46000000834465027, time = 0.9396731853485107\n",
      "Training at step=76, batch=150, train loss = 1.4517736434936523, train acc = 0.4699999988079071, time = 0.9351356029510498\n",
      "Training at step=76, batch=175, train loss = 1.4033100605010986, train acc = 0.5299999713897705, time = 0.9561896324157715\n",
      "Training at step=76, batch=200, train loss = 1.27973473072052, train acc = 0.5600000023841858, time = 0.9438297748565674\n",
      "Training at step=76, batch=225, train loss = 1.378995418548584, train acc = 0.5350000262260437, time = 0.9515738487243652\n",
      "Testing at step=76, batch=0, test loss = 1.568537950515747, test acc = 0.46000000834465027, time = 0.35355401039123535\n",
      "Testing at step=76, batch=5, test loss = 1.4284073114395142, test acc = 0.4749999940395355, time = 0.35361146926879883\n",
      "Testing at step=76, batch=10, test loss = 1.4775893688201904, test acc = 0.46000000834465027, time = 0.3665757179260254\n",
      "Testing at step=76, batch=15, test loss = 1.4172024726867676, test acc = 0.5099999904632568, time = 0.3647584915161133\n",
      "Testing at step=76, batch=20, test loss = 1.4640746116638184, test acc = 0.49000000953674316, time = 0.3596055507659912\n",
      "Testing at step=76, batch=25, test loss = 1.5066319704055786, test acc = 0.46000000834465027, time = 0.3574542999267578\n",
      "Testing at step=76, batch=30, test loss = 1.4516407251358032, test acc = 0.47999998927116394, time = 0.3589773178100586\n",
      "Testing at step=76, batch=35, test loss = 1.6126512289047241, test acc = 0.4050000011920929, time = 0.3675367832183838\n",
      "Testing at step=76, batch=40, test loss = 1.4759132862091064, test acc = 0.4950000047683716, time = 0.3546595573425293\n",
      "Testing at step=76, batch=45, test loss = 1.4197343587875366, test acc = 0.4699999988079071, time = 0.3512096405029297\n",
      "Step 76 finished in 263.9066073894501, Train loss = 1.392149836540222, Test loss = 1.5264144372940063; Train Acc = 0.5081199986934661, Test Acc = 0.46190000116825103\n",
      "Training at step=77, batch=0, train loss = 1.3097522258758545, train acc = 0.5049999952316284, time = 0.950786828994751\n",
      "Training at step=77, batch=25, train loss = 1.3813910484313965, train acc = 0.550000011920929, time = 0.9488809108734131\n",
      "Training at step=77, batch=50, train loss = 1.4322521686553955, train acc = 0.5199999809265137, time = 0.9450657367706299\n",
      "Training at step=77, batch=75, train loss = 1.528517723083496, train acc = 0.44999998807907104, time = 0.9444031715393066\n",
      "Training at step=77, batch=100, train loss = 1.5245826244354248, train acc = 0.4449999928474426, time = 0.9705557823181152\n",
      "Training at step=77, batch=125, train loss = 1.490899682044983, train acc = 0.44999998807907104, time = 0.9440770149230957\n",
      "Training at step=77, batch=150, train loss = 1.4111295938491821, train acc = 0.47999998927116394, time = 0.9373986721038818\n",
      "Training at step=77, batch=175, train loss = 1.4409217834472656, train acc = 0.4749999940395355, time = 0.9419965744018555\n",
      "Training at step=77, batch=200, train loss = 1.3222496509552002, train acc = 0.5199999809265137, time = 0.9426519870758057\n",
      "Training at step=77, batch=225, train loss = 1.5327916145324707, train acc = 0.49000000953674316, time = 0.9553396701812744\n",
      "Testing at step=77, batch=0, test loss = 1.4729363918304443, test acc = 0.5049999952316284, time = 0.3695993423461914\n",
      "Testing at step=77, batch=5, test loss = 1.5213639736175537, test acc = 0.4650000035762787, time = 0.34583020210266113\n",
      "Testing at step=77, batch=10, test loss = 1.5340412855148315, test acc = 0.47999998927116394, time = 0.361865758895874\n",
      "Testing at step=77, batch=15, test loss = 1.3891509771347046, test acc = 0.5350000262260437, time = 0.362201452255249\n",
      "Testing at step=77, batch=20, test loss = 1.4218002557754517, test acc = 0.49000000953674316, time = 0.36835336685180664\n",
      "Testing at step=77, batch=25, test loss = 1.5794439315795898, test acc = 0.4650000035762787, time = 0.362642765045166\n",
      "Testing at step=77, batch=30, test loss = 1.3875257968902588, test acc = 0.4699999988079071, time = 0.3570108413696289\n",
      "Testing at step=77, batch=35, test loss = 1.5334160327911377, test acc = 0.5, time = 0.35191941261291504\n",
      "Testing at step=77, batch=40, test loss = 1.639215111732483, test acc = 0.39500001072883606, time = 0.36342906951904297\n",
      "Testing at step=77, batch=45, test loss = 1.4327304363250732, test acc = 0.47999998927116394, time = 0.35744404792785645\n",
      "Step 77 finished in 264.1543562412262, Train loss = 1.3915994200706483, Test loss = 1.5188998770713806; Train Acc = 0.5067199982404709, Test Acc = 0.4653000003099442\n",
      "Training at step=78, batch=0, train loss = 1.3619195222854614, train acc = 0.5249999761581421, time = 0.9468824863433838\n",
      "Training at step=78, batch=25, train loss = 1.2299226522445679, train acc = 0.5649999976158142, time = 0.9417667388916016\n",
      "Training at step=78, batch=50, train loss = 1.311069130897522, train acc = 0.5350000262260437, time = 0.942671537399292\n",
      "Training at step=78, batch=75, train loss = 1.2795182466506958, train acc = 0.574999988079071, time = 0.9414875507354736\n",
      "Training at step=78, batch=100, train loss = 1.4527374505996704, train acc = 0.5, time = 0.966947078704834\n",
      "Training at step=78, batch=125, train loss = 1.3202461004257202, train acc = 0.47999998927116394, time = 0.9460933208465576\n",
      "Training at step=78, batch=150, train loss = 1.3492628335952759, train acc = 0.5550000071525574, time = 0.9812240600585938\n",
      "Training at step=78, batch=175, train loss = 1.3440709114074707, train acc = 0.5099999904632568, time = 0.9471085071563721\n",
      "Training at step=78, batch=200, train loss = 1.4411625862121582, train acc = 0.46000000834465027, time = 0.9552335739135742\n",
      "Training at step=78, batch=225, train loss = 1.2838644981384277, train acc = 0.5299999713897705, time = 0.9482684135437012\n",
      "Testing at step=78, batch=0, test loss = 1.5724862813949585, test acc = 0.4449999928474426, time = 0.3628697395324707\n",
      "Testing at step=78, batch=5, test loss = 1.4956496953964233, test acc = 0.4699999988079071, time = 0.3582031726837158\n",
      "Testing at step=78, batch=10, test loss = 1.4785799980163574, test acc = 0.47999998927116394, time = 0.3547325134277344\n",
      "Testing at step=78, batch=15, test loss = 1.4577429294586182, test acc = 0.5, time = 0.34818243980407715\n",
      "Testing at step=78, batch=20, test loss = 1.574164628982544, test acc = 0.47999998927116394, time = 0.36057019233703613\n",
      "Testing at step=78, batch=25, test loss = 1.5581333637237549, test acc = 0.4449999928474426, time = 0.3701801300048828\n",
      "Testing at step=78, batch=30, test loss = 1.43007493019104, test acc = 0.49000000953674316, time = 0.37744832038879395\n",
      "Testing at step=78, batch=35, test loss = 1.493086814880371, test acc = 0.47999998927116394, time = 0.372882604598999\n",
      "Testing at step=78, batch=40, test loss = 1.54367995262146, test acc = 0.45500001311302185, time = 0.36058545112609863\n",
      "Testing at step=78, batch=45, test loss = 1.506166934967041, test acc = 0.5249999761581421, time = 0.35492730140686035\n",
      "Step 78 finished in 264.151109457016, Train loss = 1.3890401864051818, Test loss = 1.52411789894104; Train Acc = 0.5119999988079071, Test Acc = 0.4622000002861023\n",
      "Training at step=79, batch=0, train loss = 1.2870336771011353, train acc = 0.5149999856948853, time = 1.0283527374267578\n",
      "Training at step=79, batch=25, train loss = 1.332992672920227, train acc = 0.5149999856948853, time = 0.9400336742401123\n",
      "Training at step=79, batch=50, train loss = 1.415942668914795, train acc = 0.49000000953674316, time = 0.9503114223480225\n",
      "Training at step=79, batch=75, train loss = 1.3507757186889648, train acc = 0.4950000047683716, time = 0.9427354335784912\n",
      "Training at step=79, batch=100, train loss = 1.4977247714996338, train acc = 0.46000000834465027, time = 0.9466948509216309\n",
      "Training at step=79, batch=125, train loss = 1.3235344886779785, train acc = 0.5699999928474426, time = 0.9445407390594482\n",
      "Training at step=79, batch=150, train loss = 1.3871465921401978, train acc = 0.4749999940395355, time = 0.9458498954772949\n",
      "Training at step=79, batch=175, train loss = 1.481541633605957, train acc = 0.5049999952316284, time = 0.95499587059021\n",
      "Training at step=79, batch=200, train loss = 1.4497566223144531, train acc = 0.5099999904632568, time = 0.9507849216461182\n",
      "Training at step=79, batch=225, train loss = 1.263337254524231, train acc = 0.5649999976158142, time = 0.9366064071655273\n",
      "Testing at step=79, batch=0, test loss = 1.4478791952133179, test acc = 0.5149999856948853, time = 0.3499412536621094\n",
      "Testing at step=79, batch=5, test loss = 1.438848853111267, test acc = 0.5149999856948853, time = 0.3668990135192871\n",
      "Testing at step=79, batch=10, test loss = 1.4219790697097778, test acc = 0.5550000071525574, time = 0.35744500160217285\n",
      "Testing at step=79, batch=15, test loss = 1.5001344680786133, test acc = 0.45500001311302185, time = 0.3589744567871094\n",
      "Testing at step=79, batch=20, test loss = 1.4750394821166992, test acc = 0.4950000047683716, time = 0.34972715377807617\n",
      "Testing at step=79, batch=25, test loss = 1.5987277030944824, test acc = 0.45500001311302185, time = 0.35030102729797363\n",
      "Testing at step=79, batch=30, test loss = 1.553131103515625, test acc = 0.42500001192092896, time = 0.3686554431915283\n",
      "Testing at step=79, batch=35, test loss = 1.5928585529327393, test acc = 0.4449999928474426, time = 0.36448025703430176\n",
      "Testing at step=79, batch=40, test loss = 1.5525627136230469, test acc = 0.5, time = 0.35650038719177246\n",
      "Testing at step=79, batch=45, test loss = 1.422920823097229, test acc = 0.550000011920929, time = 0.3617706298828125\n",
      "Step 79 finished in 264.21934747695923, Train loss = 1.3846580772399901, Test loss = 1.5248522758483887; Train Acc = 0.5117399981021881, Test Acc = 0.46520000040531156\n",
      "Training at step=80, batch=0, train loss = 1.3852484226226807, train acc = 0.5049999952316284, time = 0.9503707885742188\n",
      "Training at step=80, batch=25, train loss = 1.4319825172424316, train acc = 0.4950000047683716, time = 0.9573426246643066\n",
      "Training at step=80, batch=50, train loss = 1.5179492235183716, train acc = 0.48500001430511475, time = 0.9412522315979004\n",
      "Training at step=80, batch=75, train loss = 1.3206614255905151, train acc = 0.5249999761581421, time = 0.9419007301330566\n",
      "Training at step=80, batch=100, train loss = 1.4232958555221558, train acc = 0.5049999952316284, time = 0.9431898593902588\n",
      "Training at step=80, batch=125, train loss = 1.475940227508545, train acc = 0.45500001311302185, time = 0.9560928344726562\n",
      "Training at step=80, batch=150, train loss = 1.3829950094223022, train acc = 0.5, time = 0.9393384456634521\n",
      "Training at step=80, batch=175, train loss = 1.4775986671447754, train acc = 0.4699999988079071, time = 0.9429268836975098\n",
      "Training at step=80, batch=200, train loss = 1.392953634262085, train acc = 0.5350000262260437, time = 0.9398946762084961\n",
      "Training at step=80, batch=225, train loss = 1.4651434421539307, train acc = 0.4650000035762787, time = 0.9490125179290771\n",
      "Testing at step=80, batch=0, test loss = 1.4560866355895996, test acc = 0.49000000953674316, time = 0.37152957916259766\n",
      "Testing at step=80, batch=5, test loss = 1.5273361206054688, test acc = 0.4749999940395355, time = 0.3643779754638672\n",
      "Testing at step=80, batch=10, test loss = 1.567396879196167, test acc = 0.4399999976158142, time = 0.35379457473754883\n",
      "Testing at step=80, batch=15, test loss = 1.6017199754714966, test acc = 0.44999998807907104, time = 0.3514120578765869\n",
      "Testing at step=80, batch=20, test loss = 1.6523061990737915, test acc = 0.41999998688697815, time = 0.36416006088256836\n",
      "Testing at step=80, batch=25, test loss = 1.627686619758606, test acc = 0.41999998688697815, time = 0.35675549507141113\n",
      "Testing at step=80, batch=30, test loss = 1.6185916662216187, test acc = 0.47999998927116394, time = 0.3583486080169678\n",
      "Testing at step=80, batch=35, test loss = 1.5850352048873901, test acc = 0.41999998688697815, time = 0.36079955101013184\n",
      "Testing at step=80, batch=40, test loss = 1.6655911207199097, test acc = 0.41499999165534973, time = 0.3575592041015625\n",
      "Testing at step=80, batch=45, test loss = 1.651254415512085, test acc = 0.42500001192092896, time = 0.35425257682800293\n",
      "Step 80 finished in 264.1469247341156, Train loss = 1.3842608981132507, Test loss = 1.5184107208251953; Train Acc = 0.510579998254776, Test Acc = 0.46649999618530275\n",
      "Training at step=81, batch=0, train loss = 1.318864345550537, train acc = 0.5350000262260437, time = 0.9409105777740479\n",
      "Training at step=81, batch=25, train loss = 1.3534975051879883, train acc = 0.5149999856948853, time = 0.9950556755065918\n",
      "Training at step=81, batch=50, train loss = 1.3928484916687012, train acc = 0.44999998807907104, time = 0.9711084365844727\n",
      "Training at step=81, batch=75, train loss = 1.2577835321426392, train acc = 0.5699999928474426, time = 0.9553871154785156\n",
      "Training at step=81, batch=100, train loss = 1.3030247688293457, train acc = 0.5400000214576721, time = 0.9398443698883057\n",
      "Training at step=81, batch=125, train loss = 1.3570054769515991, train acc = 0.5400000214576721, time = 0.9367125034332275\n",
      "Training at step=81, batch=150, train loss = 1.2810499668121338, train acc = 0.5049999952316284, time = 0.9453792572021484\n",
      "Training at step=81, batch=175, train loss = 1.3806939125061035, train acc = 0.4950000047683716, time = 0.9472484588623047\n",
      "Training at step=81, batch=200, train loss = 1.3476887941360474, train acc = 0.550000011920929, time = 0.9705312252044678\n",
      "Training at step=81, batch=225, train loss = 1.4906277656555176, train acc = 0.4650000035762787, time = 0.9483106136322021\n",
      "Testing at step=81, batch=0, test loss = 1.425463080406189, test acc = 0.47999998927116394, time = 0.35111546516418457\n",
      "Testing at step=81, batch=5, test loss = 1.4391299486160278, test acc = 0.4950000047683716, time = 0.3881399631500244\n",
      "Testing at step=81, batch=10, test loss = 1.421379566192627, test acc = 0.5249999761581421, time = 0.3556489944458008\n",
      "Testing at step=81, batch=15, test loss = 1.5021597146987915, test acc = 0.45500001311302185, time = 0.35520124435424805\n",
      "Testing at step=81, batch=20, test loss = 1.530297040939331, test acc = 0.4099999964237213, time = 0.3550450801849365\n",
      "Testing at step=81, batch=25, test loss = 1.493839144706726, test acc = 0.4699999988079071, time = 0.35622477531433105\n",
      "Testing at step=81, batch=30, test loss = 1.516432285308838, test acc = 0.4950000047683716, time = 0.35550928115844727\n",
      "Testing at step=81, batch=35, test loss = 1.445532202720642, test acc = 0.44999998807907104, time = 0.3559904098510742\n",
      "Testing at step=81, batch=40, test loss = 1.4416515827178955, test acc = 0.4950000047683716, time = 0.347243070602417\n",
      "Testing at step=81, batch=45, test loss = 1.5124424695968628, test acc = 0.49000000953674316, time = 0.3516397476196289\n",
      "Step 81 finished in 264.0158896446228, Train loss = 1.380168839931488, Test loss = 1.5274054169654847; Train Acc = 0.5128199961185456, Test Acc = 0.46000000059604645\n",
      "Training at step=82, batch=0, train loss = 1.4318938255310059, train acc = 0.5299999713897705, time = 0.944936990737915\n",
      "Training at step=82, batch=25, train loss = 1.3254410028457642, train acc = 0.5450000166893005, time = 0.9363405704498291\n",
      "Training at step=82, batch=50, train loss = 1.3387482166290283, train acc = 0.5, time = 0.9517557621002197\n",
      "Training at step=82, batch=75, train loss = 1.416121482849121, train acc = 0.4699999988079071, time = 0.94594407081604\n",
      "Training at step=82, batch=100, train loss = 1.26507568359375, train acc = 0.5600000023841858, time = 0.9430141448974609\n",
      "Training at step=82, batch=125, train loss = 1.3527655601501465, train acc = 0.5099999904632568, time = 0.9458661079406738\n",
      "Training at step=82, batch=150, train loss = 1.397132158279419, train acc = 0.5099999904632568, time = 0.947167158126831\n",
      "Training at step=82, batch=175, train loss = 1.363175392150879, train acc = 0.5049999952316284, time = 0.9554429054260254\n",
      "Training at step=82, batch=200, train loss = 1.4274051189422607, train acc = 0.5, time = 0.9485869407653809\n",
      "Training at step=82, batch=225, train loss = 1.3232084512710571, train acc = 0.550000011920929, time = 0.9671907424926758\n",
      "Testing at step=82, batch=0, test loss = 1.4986320734024048, test acc = 0.45500001311302185, time = 0.3519918918609619\n",
      "Testing at step=82, batch=5, test loss = 1.4661554098129272, test acc = 0.5149999856948853, time = 0.3509337902069092\n",
      "Testing at step=82, batch=10, test loss = 1.4927676916122437, test acc = 0.44999998807907104, time = 0.3527367115020752\n",
      "Testing at step=82, batch=15, test loss = 1.5112532377243042, test acc = 0.47999998927116394, time = 0.35205578804016113\n",
      "Testing at step=82, batch=20, test loss = 1.4623924493789673, test acc = 0.5049999952316284, time = 0.3475167751312256\n",
      "Testing at step=82, batch=25, test loss = 1.3849148750305176, test acc = 0.5099999904632568, time = 0.34978818893432617\n",
      "Testing at step=82, batch=30, test loss = 1.374900460243225, test acc = 0.5299999713897705, time = 0.34618139266967773\n",
      "Testing at step=82, batch=35, test loss = 1.4942293167114258, test acc = 0.49000000953674316, time = 0.35280489921569824\n",
      "Testing at step=82, batch=40, test loss = 1.428017258644104, test acc = 0.5099999904632568, time = 0.3521912097930908\n",
      "Testing at step=82, batch=45, test loss = 1.580519199371338, test acc = 0.41499999165534973, time = 0.35147833824157715\n",
      "Step 82 finished in 263.20699191093445, Train loss = 1.376111448287964, Test loss = 1.51339341878891; Train Acc = 0.5136799982786179, Test Acc = 0.4703999996185303\n",
      "Training at step=83, batch=0, train loss = 1.4126996994018555, train acc = 0.5450000166893005, time = 0.9395864009857178\n",
      "Training at step=83, batch=25, train loss = 1.2833225727081299, train acc = 0.5400000214576721, time = 0.9492604732513428\n",
      "Training at step=83, batch=50, train loss = 1.3541635274887085, train acc = 0.5299999713897705, time = 0.9426922798156738\n",
      "Training at step=83, batch=75, train loss = 1.1857916116714478, train acc = 0.6499999761581421, time = 0.9396941661834717\n",
      "Training at step=83, batch=100, train loss = 1.3381445407867432, train acc = 0.5400000214576721, time = 0.9513778686523438\n",
      "Training at step=83, batch=125, train loss = 1.4006129503250122, train acc = 0.47999998927116394, time = 0.9439210891723633\n",
      "Training at step=83, batch=150, train loss = 1.3286998271942139, train acc = 0.5149999856948853, time = 0.947439432144165\n",
      "Training at step=83, batch=175, train loss = 1.443721055984497, train acc = 0.5149999856948853, time = 0.9527475833892822\n",
      "Training at step=83, batch=200, train loss = 1.219553828239441, train acc = 0.5600000023841858, time = 0.9436960220336914\n",
      "Training at step=83, batch=225, train loss = 1.4053398370742798, train acc = 0.44999998807907104, time = 0.9382383823394775\n",
      "Testing at step=83, batch=0, test loss = 1.501298189163208, test acc = 0.4399999976158142, time = 0.3507370948791504\n",
      "Testing at step=83, batch=5, test loss = 1.6123015880584717, test acc = 0.4399999976158142, time = 0.3536252975463867\n",
      "Testing at step=83, batch=10, test loss = 1.466841459274292, test acc = 0.4449999928474426, time = 0.35310959815979004\n",
      "Testing at step=83, batch=15, test loss = 1.561430811882019, test acc = 0.48500001430511475, time = 0.35382771492004395\n",
      "Testing at step=83, batch=20, test loss = 1.644838809967041, test acc = 0.4000000059604645, time = 0.34751224517822266\n",
      "Testing at step=83, batch=25, test loss = 1.5727952718734741, test acc = 0.4449999928474426, time = 0.3506588935852051\n",
      "Testing at step=83, batch=30, test loss = 1.4784307479858398, test acc = 0.4350000023841858, time = 0.35031938552856445\n",
      "Testing at step=83, batch=35, test loss = 1.6464042663574219, test acc = 0.4749999940395355, time = 0.3580648899078369\n",
      "Testing at step=83, batch=40, test loss = 1.599204659461975, test acc = 0.4350000023841858, time = 0.35788440704345703\n",
      "Testing at step=83, batch=45, test loss = 1.5971359014511108, test acc = 0.4399999976158142, time = 0.3614165782928467\n",
      "Step 83 finished in 263.93333315849304, Train loss = 1.3784575219154358, Test loss = 1.5182482957839967; Train Acc = 0.5136799993515014, Test Acc = 0.4663999992609024\n",
      "Training at step=84, batch=0, train loss = 1.4205001592636108, train acc = 0.5, time = 0.9401111602783203\n",
      "Training at step=84, batch=25, train loss = 1.3770742416381836, train acc = 0.48500001430511475, time = 1.1503348350524902\n",
      "Training at step=84, batch=50, train loss = 1.3287104368209839, train acc = 0.5450000166893005, time = 0.9387514591217041\n",
      "Training at step=84, batch=75, train loss = 1.4001384973526, train acc = 0.5099999904632568, time = 0.9605622291564941\n",
      "Training at step=84, batch=100, train loss = 1.3334656953811646, train acc = 0.5099999904632568, time = 0.9427425861358643\n",
      "Training at step=84, batch=125, train loss = 1.4066029787063599, train acc = 0.4749999940395355, time = 1.004610538482666\n",
      "Training at step=84, batch=150, train loss = 1.5567501783370972, train acc = 0.4449999928474426, time = 0.9470157623291016\n",
      "Training at step=84, batch=175, train loss = 1.4616646766662598, train acc = 0.48500001430511475, time = 0.9612419605255127\n",
      "Training at step=84, batch=200, train loss = 1.2463223934173584, train acc = 0.5849999785423279, time = 0.9588196277618408\n",
      "Training at step=84, batch=225, train loss = 1.3676154613494873, train acc = 0.550000011920929, time = 0.9608795642852783\n",
      "Testing at step=84, batch=0, test loss = 1.6243685483932495, test acc = 0.4399999976158142, time = 0.35341548919677734\n",
      "Testing at step=84, batch=5, test loss = 1.5319457054138184, test acc = 0.4399999976158142, time = 0.35854315757751465\n",
      "Testing at step=84, batch=10, test loss = 1.575437068939209, test acc = 0.46000000834465027, time = 0.3532090187072754\n",
      "Testing at step=84, batch=15, test loss = 1.392887830734253, test acc = 0.48500001430511475, time = 0.3704707622528076\n",
      "Testing at step=84, batch=20, test loss = 1.5994114875793457, test acc = 0.4050000011920929, time = 0.35298824310302734\n",
      "Testing at step=84, batch=25, test loss = 1.5146691799163818, test acc = 0.4350000023841858, time = 0.36807751655578613\n",
      "Testing at step=84, batch=30, test loss = 1.5292763710021973, test acc = 0.42500001192092896, time = 0.36699461936950684\n",
      "Testing at step=84, batch=35, test loss = 1.4418972730636597, test acc = 0.47999998927116394, time = 0.36635279655456543\n",
      "Testing at step=84, batch=40, test loss = 1.509576439857483, test acc = 0.45500001311302185, time = 0.35404014587402344\n",
      "Testing at step=84, batch=45, test loss = 1.601670742034912, test acc = 0.4050000011920929, time = 0.3666400909423828\n",
      "Step 84 finished in 264.56379318237305, Train loss = 1.3775016841888428, Test loss = 1.5351392650604248; Train Acc = 0.5136599994897842, Test Acc = 0.4556000006198883\n",
      "Training at step=85, batch=0, train loss = 1.2740073204040527, train acc = 0.5849999785423279, time = 0.9486327171325684\n",
      "Training at step=85, batch=25, train loss = 1.3842922449111938, train acc = 0.4699999988079071, time = 0.9485213756561279\n",
      "Training at step=85, batch=50, train loss = 1.3022645711898804, train acc = 0.5249999761581421, time = 0.9365112781524658\n",
      "Training at step=85, batch=75, train loss = 1.3673702478408813, train acc = 0.5, time = 0.9474983215332031\n",
      "Training at step=85, batch=100, train loss = 1.3605782985687256, train acc = 0.5099999904632568, time = 0.9351911544799805\n",
      "Training at step=85, batch=125, train loss = 1.2874970436096191, train acc = 0.5600000023841858, time = 0.9495251178741455\n",
      "Training at step=85, batch=150, train loss = 1.362369179725647, train acc = 0.5299999713897705, time = 0.9420490264892578\n",
      "Training at step=85, batch=175, train loss = 1.4168128967285156, train acc = 0.47999998927116394, time = 0.9478452205657959\n",
      "Training at step=85, batch=200, train loss = 1.253021240234375, train acc = 0.5649999976158142, time = 0.9393434524536133\n",
      "Training at step=85, batch=225, train loss = 1.4854122400283813, train acc = 0.4300000071525574, time = 0.9429218769073486\n",
      "Testing at step=85, batch=0, test loss = 1.5792181491851807, test acc = 0.4399999976158142, time = 0.36493587493896484\n",
      "Testing at step=85, batch=5, test loss = 1.4203370809555054, test acc = 0.5099999904632568, time = 0.35358190536499023\n",
      "Testing at step=85, batch=10, test loss = 1.5271629095077515, test acc = 0.4399999976158142, time = 0.36226701736450195\n",
      "Testing at step=85, batch=15, test loss = 1.5870983600616455, test acc = 0.45500001311302185, time = 0.35448694229125977\n",
      "Testing at step=85, batch=20, test loss = 1.460951566696167, test acc = 0.48500001430511475, time = 0.36060476303100586\n",
      "Testing at step=85, batch=25, test loss = 1.5937108993530273, test acc = 0.4300000071525574, time = 0.35866355895996094\n",
      "Testing at step=85, batch=30, test loss = 1.4613795280456543, test acc = 0.4699999988079071, time = 0.35654735565185547\n",
      "Testing at step=85, batch=35, test loss = 1.4295824766159058, test acc = 0.49000000953674316, time = 0.35559630393981934\n",
      "Testing at step=85, batch=40, test loss = 1.4531687498092651, test acc = 0.4950000047683716, time = 0.3537564277648926\n",
      "Testing at step=85, batch=45, test loss = 1.554405927658081, test acc = 0.45500001311302185, time = 0.3502342700958252\n",
      "Step 85 finished in 263.88219380378723, Train loss = 1.3727460398674012, Test loss = 1.513221070766449; Train Acc = 0.5150999987125396, Test Acc = 0.46909999907016753\n",
      "Training at step=86, batch=0, train loss = 1.2520347833633423, train acc = 0.5550000071525574, time = 0.9496276378631592\n",
      "Training at step=86, batch=25, train loss = 1.492591381072998, train acc = 0.4650000035762787, time = 0.9483141899108887\n",
      "Training at step=86, batch=50, train loss = 1.4472320079803467, train acc = 0.5099999904632568, time = 0.9437987804412842\n",
      "Training at step=86, batch=75, train loss = 1.3185348510742188, train acc = 0.4950000047683716, time = 0.9482460021972656\n",
      "Training at step=86, batch=100, train loss = 1.5395208597183228, train acc = 0.47999998927116394, time = 0.9418666362762451\n",
      "Training at step=86, batch=125, train loss = 1.2613670825958252, train acc = 0.5400000214576721, time = 0.9624500274658203\n",
      "Training at step=86, batch=150, train loss = 1.5427311658859253, train acc = 0.4749999940395355, time = 0.9493992328643799\n",
      "Training at step=86, batch=175, train loss = 1.4355114698410034, train acc = 0.5299999713897705, time = 0.9469249248504639\n",
      "Training at step=86, batch=200, train loss = 1.3382991552352905, train acc = 0.5550000071525574, time = 0.9446883201599121\n",
      "Training at step=86, batch=225, train loss = 1.3332643508911133, train acc = 0.5550000071525574, time = 0.9416251182556152\n",
      "Testing at step=86, batch=0, test loss = 1.5612720251083374, test acc = 0.4399999976158142, time = 0.3528318405151367\n",
      "Testing at step=86, batch=5, test loss = 1.5965957641601562, test acc = 0.4050000011920929, time = 0.3503134250640869\n",
      "Testing at step=86, batch=10, test loss = 1.5604259967803955, test acc = 0.46000000834465027, time = 0.36603593826293945\n",
      "Testing at step=86, batch=15, test loss = 1.515202283859253, test acc = 0.4699999988079071, time = 0.34920263290405273\n",
      "Testing at step=86, batch=20, test loss = 1.5447965860366821, test acc = 0.4099999964237213, time = 0.355593204498291\n",
      "Testing at step=86, batch=25, test loss = 1.7016632556915283, test acc = 0.4000000059604645, time = 0.3558948040008545\n",
      "Testing at step=86, batch=30, test loss = 1.5200517177581787, test acc = 0.41499999165534973, time = 0.3653404712677002\n",
      "Testing at step=86, batch=35, test loss = 1.4533398151397705, test acc = 0.4950000047683716, time = 0.36521315574645996\n",
      "Testing at step=86, batch=40, test loss = 1.528298020362854, test acc = 0.4749999940395355, time = 0.347597599029541\n",
      "Testing at step=86, batch=45, test loss = 1.6723674535751343, test acc = 0.4099999964237213, time = 0.3527863025665283\n",
      "Step 86 finished in 264.2905628681183, Train loss = 1.3693879914283753, Test loss = 1.5231971907615662; Train Acc = 0.5161199991703034, Test Acc = 0.46779999911785125\n",
      "Training at step=87, batch=0, train loss = 1.3715109825134277, train acc = 0.5049999952316284, time = 0.9467263221740723\n",
      "Training at step=87, batch=25, train loss = 1.3452645540237427, train acc = 0.5350000262260437, time = 0.945685625076294\n",
      "Training at step=87, batch=50, train loss = 1.256547451019287, train acc = 0.5450000166893005, time = 0.94846510887146\n",
      "Training at step=87, batch=75, train loss = 1.370882272720337, train acc = 0.4950000047683716, time = 0.9449479579925537\n",
      "Training at step=87, batch=100, train loss = 1.333980679512024, train acc = 0.5350000262260437, time = 0.9644021987915039\n",
      "Training at step=87, batch=125, train loss = 1.309982419013977, train acc = 0.5149999856948853, time = 0.9398961067199707\n",
      "Training at step=87, batch=150, train loss = 1.484910488128662, train acc = 0.45500001311302185, time = 0.9448239803314209\n",
      "Training at step=87, batch=175, train loss = 1.4093666076660156, train acc = 0.5, time = 0.9515316486358643\n",
      "Training at step=87, batch=200, train loss = 1.413169264793396, train acc = 0.48500001430511475, time = 0.947274923324585\n",
      "Training at step=87, batch=225, train loss = 1.3684778213500977, train acc = 0.5149999856948853, time = 0.9398601055145264\n",
      "Testing at step=87, batch=0, test loss = 1.473077416419983, test acc = 0.5049999952316284, time = 0.3493466377258301\n",
      "Testing at step=87, batch=5, test loss = 1.4398598670959473, test acc = 0.5, time = 0.3528761863708496\n",
      "Testing at step=87, batch=10, test loss = 1.473297119140625, test acc = 0.4950000047683716, time = 0.3498406410217285\n",
      "Testing at step=87, batch=15, test loss = 1.4297579526901245, test acc = 0.5, time = 0.3461620807647705\n",
      "Testing at step=87, batch=20, test loss = 1.5820379257202148, test acc = 0.46000000834465027, time = 0.35001587867736816\n",
      "Testing at step=87, batch=25, test loss = 1.548904538154602, test acc = 0.44999998807907104, time = 0.3515172004699707\n",
      "Testing at step=87, batch=30, test loss = 1.5974098443984985, test acc = 0.48500001430511475, time = 0.35034608840942383\n",
      "Testing at step=87, batch=35, test loss = 1.454762578010559, test acc = 0.4950000047683716, time = 0.3525247573852539\n",
      "Testing at step=87, batch=40, test loss = 1.4427026510238647, test acc = 0.5, time = 0.35276150703430176\n",
      "Testing at step=87, batch=45, test loss = 1.535465955734253, test acc = 0.49000000953674316, time = 0.354860782623291\n",
      "Step 87 finished in 263.94815325737, Train loss = 1.3677850580215454, Test loss = 1.5024690961837768; Train Acc = 0.5165999990701675, Test Acc = 0.4717999988794327\n",
      "Training at step=88, batch=0, train loss = 1.3948899507522583, train acc = 0.4950000047683716, time = 0.9399781227111816\n",
      "Training at step=88, batch=25, train loss = 1.322655200958252, train acc = 0.5, time = 0.9544801712036133\n",
      "Training at step=88, batch=50, train loss = 1.2809600830078125, train acc = 0.5600000023841858, time = 1.0052926540374756\n",
      "Training at step=88, batch=75, train loss = 1.2363309860229492, train acc = 0.5950000286102295, time = 0.9480807781219482\n",
      "Training at step=88, batch=100, train loss = 1.3396989107131958, train acc = 0.5149999856948853, time = 0.9543776512145996\n",
      "Training at step=88, batch=125, train loss = 1.4481866359710693, train acc = 0.47999998927116394, time = 0.9589896202087402\n",
      "Training at step=88, batch=150, train loss = 1.569948434829712, train acc = 0.47999998927116394, time = 0.9967546463012695\n",
      "Training at step=88, batch=175, train loss = 1.3838534355163574, train acc = 0.4650000035762787, time = 0.958745002746582\n",
      "Training at step=88, batch=200, train loss = 1.3149523735046387, train acc = 0.5400000214576721, time = 0.9472954273223877\n",
      "Training at step=88, batch=225, train loss = 1.5255998373031616, train acc = 0.46000000834465027, time = 0.9484283924102783\n",
      "Testing at step=88, batch=0, test loss = 1.4027894735336304, test acc = 0.5350000262260437, time = 0.3557589054107666\n",
      "Testing at step=88, batch=5, test loss = 1.5982290506362915, test acc = 0.4399999976158142, time = 0.36333394050598145\n",
      "Testing at step=88, batch=10, test loss = 1.4898252487182617, test acc = 0.4449999928474426, time = 0.36783432960510254\n",
      "Testing at step=88, batch=15, test loss = 1.536537766456604, test acc = 0.46000000834465027, time = 0.3643836975097656\n",
      "Testing at step=88, batch=20, test loss = 1.4901341199874878, test acc = 0.4699999988079071, time = 0.3501873016357422\n",
      "Testing at step=88, batch=25, test loss = 1.5178645849227905, test acc = 0.44999998807907104, time = 0.34972357749938965\n",
      "Testing at step=88, batch=30, test loss = 1.5516825914382935, test acc = 0.4650000035762787, time = 0.3522958755493164\n",
      "Testing at step=88, batch=35, test loss = 1.4728868007659912, test acc = 0.48500001430511475, time = 0.3496429920196533\n",
      "Testing at step=88, batch=40, test loss = 1.463838815689087, test acc = 0.5, time = 0.34784913063049316\n",
      "Testing at step=88, batch=45, test loss = 1.5195764303207397, test acc = 0.4300000071525574, time = 0.3507859706878662\n",
      "Step 88 finished in 264.2313549518585, Train loss = 1.3609827747344971, Test loss = 1.520969605445862; Train Acc = 0.5187799980640412, Test Acc = 0.46630000114440917\n",
      "Training at step=89, batch=0, train loss = 1.3252537250518799, train acc = 0.550000011920929, time = 0.938758134841919\n",
      "Training at step=89, batch=25, train loss = 1.3394343852996826, train acc = 0.49000000953674316, time = 0.9410600662231445\n",
      "Training at step=89, batch=50, train loss = 1.3905234336853027, train acc = 0.49000000953674316, time = 0.9436440467834473\n",
      "Training at step=89, batch=75, train loss = 1.4161088466644287, train acc = 0.5, time = 0.9389700889587402\n",
      "Training at step=89, batch=100, train loss = 1.5091300010681152, train acc = 0.44999998807907104, time = 0.9519526958465576\n",
      "Training at step=89, batch=125, train loss = 1.419745683670044, train acc = 0.5099999904632568, time = 0.9470021724700928\n",
      "Training at step=89, batch=150, train loss = 1.4069546461105347, train acc = 0.4950000047683716, time = 0.936974048614502\n",
      "Training at step=89, batch=175, train loss = 1.4233577251434326, train acc = 0.47999998927116394, time = 0.9589605331420898\n",
      "Training at step=89, batch=200, train loss = 1.3727792501449585, train acc = 0.4699999988079071, time = 0.9316627979278564\n",
      "Training at step=89, batch=225, train loss = 1.354945421218872, train acc = 0.5350000262260437, time = 0.9459755420684814\n",
      "Testing at step=89, batch=0, test loss = 1.5389775037765503, test acc = 0.45500001311302185, time = 0.3551452159881592\n",
      "Testing at step=89, batch=5, test loss = 1.5465143918991089, test acc = 0.4099999964237213, time = 0.3588404655456543\n",
      "Testing at step=89, batch=10, test loss = 1.4424753189086914, test acc = 0.5149999856948853, time = 0.36283040046691895\n",
      "Testing at step=89, batch=15, test loss = 1.473699927330017, test acc = 0.49000000953674316, time = 0.35921192169189453\n",
      "Testing at step=89, batch=20, test loss = 1.522892951965332, test acc = 0.4399999976158142, time = 0.3625607490539551\n",
      "Testing at step=89, batch=25, test loss = 1.5512157678604126, test acc = 0.45500001311302185, time = 0.3491330146789551\n",
      "Testing at step=89, batch=30, test loss = 1.6092511415481567, test acc = 0.4350000023841858, time = 0.3542170524597168\n",
      "Testing at step=89, batch=35, test loss = 1.7007970809936523, test acc = 0.4099999964237213, time = 0.3613107204437256\n",
      "Testing at step=89, batch=40, test loss = 1.5758676528930664, test acc = 0.46000000834465027, time = 0.363156795501709\n",
      "Testing at step=89, batch=45, test loss = 1.5045008659362793, test acc = 0.4449999928474426, time = 0.35829997062683105\n",
      "Step 89 finished in 263.62627243995667, Train loss = 1.36292072057724, Test loss = 1.5129279923439025; Train Acc = 0.519339997291565, Test Acc = 0.4687999993562698\n",
      "Training at step=90, batch=0, train loss = 1.2207534313201904, train acc = 0.550000011920929, time = 0.9396297931671143\n",
      "Training at step=90, batch=25, train loss = 1.312296748161316, train acc = 0.5450000166893005, time = 0.9440605640411377\n",
      "Training at step=90, batch=50, train loss = 1.3224568367004395, train acc = 0.5199999809265137, time = 0.9402670860290527\n",
      "Training at step=90, batch=75, train loss = 1.4639041423797607, train acc = 0.4749999940395355, time = 0.9444656372070312\n",
      "Training at step=90, batch=100, train loss = 1.308463454246521, train acc = 0.5199999809265137, time = 0.9525809288024902\n",
      "Training at step=90, batch=125, train loss = 1.4167500734329224, train acc = 0.49000000953674316, time = 0.9422922134399414\n",
      "Training at step=90, batch=150, train loss = 1.367130160331726, train acc = 0.5249999761581421, time = 0.9424068927764893\n",
      "Training at step=90, batch=175, train loss = 1.5075654983520508, train acc = 0.4300000071525574, time = 0.9473080635070801\n",
      "Training at step=90, batch=200, train loss = 1.4151678085327148, train acc = 0.5149999856948853, time = 0.9448592662811279\n",
      "Training at step=90, batch=225, train loss = 1.437171459197998, train acc = 0.5099999904632568, time = 0.9419798851013184\n",
      "Testing at step=90, batch=0, test loss = 1.409213662147522, test acc = 0.5400000214576721, time = 0.35888051986694336\n",
      "Testing at step=90, batch=5, test loss = 1.508935570716858, test acc = 0.4749999940395355, time = 0.35122156143188477\n",
      "Testing at step=90, batch=10, test loss = 1.6045435667037964, test acc = 0.4449999928474426, time = 0.3497335910797119\n",
      "Testing at step=90, batch=15, test loss = 1.4455227851867676, test acc = 0.5350000262260437, time = 0.35877442359924316\n",
      "Testing at step=90, batch=20, test loss = 1.4532997608184814, test acc = 0.4699999988079071, time = 0.3533468246459961\n",
      "Testing at step=90, batch=25, test loss = 1.7496774196624756, test acc = 0.41499999165534973, time = 0.3581058979034424\n",
      "Testing at step=90, batch=30, test loss = 1.4433658123016357, test acc = 0.49000000953674316, time = 0.35447144508361816\n",
      "Testing at step=90, batch=35, test loss = 1.5045183897018433, test acc = 0.45500001311302185, time = 0.35053563117980957\n",
      "Testing at step=90, batch=40, test loss = 1.4829899072647095, test acc = 0.49000000953674316, time = 0.35857295989990234\n",
      "Testing at step=90, batch=45, test loss = 1.5734783411026, test acc = 0.4300000071525574, time = 0.36254262924194336\n",
      "Step 90 finished in 263.8674199581146, Train loss = 1.3606662673950196, Test loss = 1.5067341208457947; Train Acc = 0.5174799993038177, Test Acc = 0.4729000020027161\n",
      "Training at step=91, batch=0, train loss = 1.165944218635559, train acc = 0.5799999833106995, time = 0.9482169151306152\n",
      "Training at step=91, batch=25, train loss = 1.4286171197891235, train acc = 0.4950000047683716, time = 0.9332001209259033\n",
      "Training at step=91, batch=50, train loss = 1.3623429536819458, train acc = 0.5299999713897705, time = 0.9397382736206055\n",
      "Training at step=91, batch=75, train loss = 1.3881524801254272, train acc = 0.550000011920929, time = 0.936305046081543\n",
      "Training at step=91, batch=100, train loss = 1.378232479095459, train acc = 0.5099999904632568, time = 1.1000661849975586\n",
      "Training at step=91, batch=125, train loss = 1.3538283109664917, train acc = 0.5400000214576721, time = 0.939328670501709\n",
      "Training at step=91, batch=150, train loss = 1.493046522140503, train acc = 0.49000000953674316, time = 0.9480233192443848\n",
      "Training at step=91, batch=175, train loss = 1.3123685121536255, train acc = 0.5299999713897705, time = 0.9518961906433105\n",
      "Training at step=91, batch=200, train loss = 1.4491829872131348, train acc = 0.4650000035762787, time = 0.9355287551879883\n",
      "Training at step=91, batch=225, train loss = 1.3909019231796265, train acc = 0.5199999809265137, time = 0.9401264190673828\n",
      "Testing at step=91, batch=0, test loss = 1.5642772912979126, test acc = 0.48500001430511475, time = 0.35686588287353516\n",
      "Testing at step=91, batch=5, test loss = 1.3331104516983032, test acc = 0.5199999809265137, time = 0.35425567626953125\n",
      "Testing at step=91, batch=10, test loss = 1.4429559707641602, test acc = 0.46000000834465027, time = 0.35497546195983887\n",
      "Testing at step=91, batch=15, test loss = 1.5153077840805054, test acc = 0.4699999988079071, time = 0.34863948822021484\n",
      "Testing at step=91, batch=20, test loss = 1.4471615552902222, test acc = 0.5049999952316284, time = 0.3598198890686035\n",
      "Testing at step=91, batch=25, test loss = 1.4932665824890137, test acc = 0.5149999856948853, time = 0.35408854484558105\n",
      "Testing at step=91, batch=30, test loss = 1.3658488988876343, test acc = 0.5400000214576721, time = 0.3480691909790039\n",
      "Testing at step=91, batch=35, test loss = 1.5619007349014282, test acc = 0.5099999904632568, time = 0.3571312427520752\n",
      "Testing at step=91, batch=40, test loss = 1.4776180982589722, test acc = 0.49000000953674316, time = 0.35382509231567383\n",
      "Testing at step=91, batch=45, test loss = 1.497584342956543, test acc = 0.4350000023841858, time = 0.3592381477355957\n",
      "Step 91 finished in 263.10458111763, Train loss = 1.3591109857559205, Test loss = 1.5055278968811034; Train Acc = 0.521039998292923, Test Acc = 0.4721000003814697\n",
      "Training at step=92, batch=0, train loss = 1.2074998617172241, train acc = 0.5849999785423279, time = 0.9365065097808838\n",
      "Training at step=92, batch=25, train loss = 1.309458613395691, train acc = 0.5699999928474426, time = 0.9589159488677979\n",
      "Training at step=92, batch=50, train loss = 1.472158670425415, train acc = 0.4650000035762787, time = 0.9414939880371094\n",
      "Training at step=92, batch=75, train loss = 1.3720908164978027, train acc = 0.550000011920929, time = 0.9403438568115234\n",
      "Training at step=92, batch=100, train loss = 1.4449959993362427, train acc = 0.49000000953674316, time = 0.9418466091156006\n",
      "Training at step=92, batch=125, train loss = 1.4261785745620728, train acc = 0.5149999856948853, time = 0.9406993389129639\n",
      "Training at step=92, batch=150, train loss = 1.312716007232666, train acc = 0.550000011920929, time = 0.9582886695861816\n",
      "Training at step=92, batch=175, train loss = 1.350102186203003, train acc = 0.5550000071525574, time = 0.9600729942321777\n",
      "Training at step=92, batch=200, train loss = 1.312554955482483, train acc = 0.5450000166893005, time = 0.939943790435791\n",
      "Training at step=92, batch=225, train loss = 1.319454550743103, train acc = 0.5350000262260437, time = 0.9360129833221436\n",
      "Testing at step=92, batch=0, test loss = 1.4605129957199097, test acc = 0.5249999761581421, time = 0.36176276206970215\n",
      "Testing at step=92, batch=5, test loss = 1.5361138582229614, test acc = 0.4650000035762787, time = 0.3550395965576172\n",
      "Testing at step=92, batch=10, test loss = 1.4143974781036377, test acc = 0.5299999713897705, time = 0.36661195755004883\n",
      "Testing at step=92, batch=15, test loss = 1.5461256504058838, test acc = 0.4449999928474426, time = 0.35294270515441895\n",
      "Testing at step=92, batch=20, test loss = 1.4394267797470093, test acc = 0.5299999713897705, time = 0.35358166694641113\n",
      "Testing at step=92, batch=25, test loss = 1.4647819995880127, test acc = 0.4749999940395355, time = 0.35449957847595215\n",
      "Testing at step=92, batch=30, test loss = 1.4759691953659058, test acc = 0.49000000953674316, time = 0.3557734489440918\n",
      "Testing at step=92, batch=35, test loss = 1.495022177696228, test acc = 0.5149999856948853, time = 0.3559107780456543\n",
      "Testing at step=92, batch=40, test loss = 1.4805833101272583, test acc = 0.4950000047683716, time = 0.3629114627838135\n",
      "Testing at step=92, batch=45, test loss = 1.3584986925125122, test acc = 0.4950000047683716, time = 0.3522517681121826\n",
      "Step 92 finished in 264.0175244808197, Train loss = 1.3524448614120483, Test loss = 1.5044049072265624; Train Acc = 0.5223399991989136, Test Acc = 0.4685999971628189\n",
      "Training at step=93, batch=0, train loss = 1.4599703550338745, train acc = 0.49000000953674316, time = 0.9424417018890381\n",
      "Training at step=93, batch=25, train loss = 1.2759346961975098, train acc = 0.5550000071525574, time = 0.9474272727966309\n",
      "Training at step=93, batch=50, train loss = 1.3741756677627563, train acc = 0.5099999904632568, time = 0.945223331451416\n",
      "Training at step=93, batch=75, train loss = 1.457262396812439, train acc = 0.48500001430511475, time = 1.0096609592437744\n",
      "Training at step=93, batch=100, train loss = 1.3682143688201904, train acc = 0.5199999809265137, time = 0.9459161758422852\n",
      "Training at step=93, batch=125, train loss = 1.4100897312164307, train acc = 0.4749999940395355, time = 0.9403002262115479\n",
      "Training at step=93, batch=150, train loss = 1.3567876815795898, train acc = 0.5199999809265137, time = 0.935476541519165\n",
      "Training at step=93, batch=175, train loss = 1.3511862754821777, train acc = 0.5299999713897705, time = 0.9540627002716064\n",
      "Training at step=93, batch=200, train loss = 1.4452403783798218, train acc = 0.4399999976158142, time = 0.9434380531311035\n",
      "Training at step=93, batch=225, train loss = 1.3851635456085205, train acc = 0.5249999761581421, time = 0.9445497989654541\n",
      "Testing at step=93, batch=0, test loss = 1.4972777366638184, test acc = 0.49000000953674316, time = 0.35371828079223633\n",
      "Testing at step=93, batch=5, test loss = 1.313075065612793, test acc = 0.4950000047683716, time = 0.35232996940612793\n",
      "Testing at step=93, batch=10, test loss = 1.5391950607299805, test acc = 0.44999998807907104, time = 0.3528141975402832\n",
      "Testing at step=93, batch=15, test loss = 1.5705569982528687, test acc = 0.45500001311302185, time = 0.3462560176849365\n",
      "Testing at step=93, batch=20, test loss = 1.4362140893936157, test acc = 0.5099999904632568, time = 0.3581552505493164\n",
      "Testing at step=93, batch=25, test loss = 1.4798903465270996, test acc = 0.4699999988079071, time = 0.35146522521972656\n",
      "Testing at step=93, batch=30, test loss = 1.3343545198440552, test acc = 0.5099999904632568, time = 0.3556380271911621\n",
      "Testing at step=93, batch=35, test loss = 1.3923710584640503, test acc = 0.4749999940395355, time = 0.3530898094177246\n",
      "Testing at step=93, batch=40, test loss = 1.6018424034118652, test acc = 0.46000000834465027, time = 0.35170412063598633\n",
      "Testing at step=93, batch=45, test loss = 1.408987283706665, test acc = 0.45500001311302185, time = 0.35237836837768555\n",
      "Step 93 finished in 263.51113176345825, Train loss = 1.3531331815719605, Test loss = 1.5003560185432434; Train Acc = 0.5215999997854233, Test Acc = 0.471000000834465\n",
      "Training at step=94, batch=0, train loss = 1.274781584739685, train acc = 0.5350000262260437, time = 0.9384877681732178\n",
      "Training at step=94, batch=25, train loss = 1.4400197267532349, train acc = 0.5, time = 0.9359753131866455\n",
      "Training at step=94, batch=50, train loss = 1.2776236534118652, train acc = 0.550000011920929, time = 0.9407427310943604\n",
      "Training at step=94, batch=75, train loss = 1.3492944240570068, train acc = 0.5299999713897705, time = 0.9483656883239746\n",
      "Training at step=94, batch=100, train loss = 1.4072372913360596, train acc = 0.5049999952316284, time = 0.9573531150817871\n",
      "Training at step=94, batch=125, train loss = 1.269208550453186, train acc = 0.550000011920929, time = 0.9349260330200195\n",
      "Training at step=94, batch=150, train loss = 1.2660529613494873, train acc = 0.5199999809265137, time = 0.938561201095581\n",
      "Training at step=94, batch=175, train loss = 1.4186989068984985, train acc = 0.48500001430511475, time = 0.934699296951294\n",
      "Training at step=94, batch=200, train loss = 1.406787395477295, train acc = 0.5049999952316284, time = 0.9426453113555908\n",
      "Training at step=94, batch=225, train loss = 1.3027304410934448, train acc = 0.5699999928474426, time = 0.9511430263519287\n",
      "Testing at step=94, batch=0, test loss = 1.6540679931640625, test acc = 0.4350000023841858, time = 0.35758352279663086\n",
      "Testing at step=94, batch=5, test loss = 1.4940667152404785, test acc = 0.49000000953674316, time = 0.3503077030181885\n",
      "Testing at step=94, batch=10, test loss = 1.5583126544952393, test acc = 0.49000000953674316, time = 0.35095691680908203\n",
      "Testing at step=94, batch=15, test loss = 1.431764841079712, test acc = 0.5149999856948853, time = 0.3490011692047119\n",
      "Testing at step=94, batch=20, test loss = 1.597135066986084, test acc = 0.4300000071525574, time = 0.3566117286682129\n",
      "Testing at step=94, batch=25, test loss = 1.3942548036575317, test acc = 0.45500001311302185, time = 0.3528730869293213\n",
      "Testing at step=94, batch=30, test loss = 1.5436245203018188, test acc = 0.47999998927116394, time = 0.34897685050964355\n",
      "Testing at step=94, batch=35, test loss = 1.4863073825836182, test acc = 0.4300000071525574, time = 0.35782313346862793\n",
      "Testing at step=94, batch=40, test loss = 1.6423710584640503, test acc = 0.4050000011920929, time = 0.34903907775878906\n",
      "Testing at step=94, batch=45, test loss = 1.6173402070999146, test acc = 0.4000000059604645, time = 0.3584916591644287\n",
      "Step 94 finished in 263.4549939632416, Train loss = 1.350853563785553, Test loss = 1.51749990940094; Train Acc = 0.5259599993228913, Test Acc = 0.4705000001192093\n",
      "Training at step=95, batch=0, train loss = 1.3501641750335693, train acc = 0.4749999940395355, time = 0.940664529800415\n",
      "Training at step=95, batch=25, train loss = 1.198275089263916, train acc = 0.574999988079071, time = 0.9437780380249023\n",
      "Training at step=95, batch=50, train loss = 1.3813977241516113, train acc = 0.5450000166893005, time = 0.943840503692627\n",
      "Training at step=95, batch=75, train loss = 1.1779148578643799, train acc = 0.5649999976158142, time = 0.9427793025970459\n",
      "Training at step=95, batch=100, train loss = 1.4061466455459595, train acc = 0.4950000047683716, time = 0.9634344577789307\n",
      "Training at step=95, batch=125, train loss = 1.3025391101837158, train acc = 0.5600000023841858, time = 0.9459762573242188\n",
      "Training at step=95, batch=150, train loss = 1.460601806640625, train acc = 0.4749999940395355, time = 0.9365763664245605\n",
      "Training at step=95, batch=175, train loss = 1.308417558670044, train acc = 0.5699999928474426, time = 0.9431662559509277\n",
      "Training at step=95, batch=200, train loss = 1.324741005897522, train acc = 0.5350000262260437, time = 0.9436681270599365\n",
      "Training at step=95, batch=225, train loss = 1.287513017654419, train acc = 0.5550000071525574, time = 0.957371711730957\n",
      "Testing at step=95, batch=0, test loss = 1.5313568115234375, test acc = 0.4300000071525574, time = 0.35519838333129883\n",
      "Testing at step=95, batch=5, test loss = 1.527001976966858, test acc = 0.45500001311302185, time = 0.35437774658203125\n",
      "Testing at step=95, batch=10, test loss = 1.5358105897903442, test acc = 0.49000000953674316, time = 0.3487727642059326\n",
      "Testing at step=95, batch=15, test loss = 1.6704832315444946, test acc = 0.4399999976158142, time = 0.3531930446624756\n",
      "Testing at step=95, batch=20, test loss = 1.4526828527450562, test acc = 0.4449999928474426, time = 0.3508872985839844\n",
      "Testing at step=95, batch=25, test loss = 1.5297497510910034, test acc = 0.41499999165534973, time = 0.35498929023742676\n",
      "Testing at step=95, batch=30, test loss = 1.5269726514816284, test acc = 0.46000000834465027, time = 0.35159969329833984\n",
      "Testing at step=95, batch=35, test loss = 1.548500418663025, test acc = 0.4000000059604645, time = 0.35399961471557617\n",
      "Testing at step=95, batch=40, test loss = 1.515537977218628, test acc = 0.44999998807907104, time = 0.3530275821685791\n",
      "Testing at step=95, batch=45, test loss = 1.4464797973632812, test acc = 0.4950000047683716, time = 0.3555185794830322\n",
      "Step 95 finished in 263.4687488079071, Train loss = 1.348207929611206, Test loss = 1.5144107723236084; Train Acc = 0.5251399992704392, Test Acc = 0.46460000216960906\n",
      "Training at step=96, batch=0, train loss = 1.1839653253555298, train acc = 0.5550000071525574, time = 0.9675967693328857\n",
      "Training at step=96, batch=25, train loss = 1.322385311126709, train acc = 0.5550000071525574, time = 0.9452731609344482\n",
      "Training at step=96, batch=50, train loss = 1.295404314994812, train acc = 0.5450000166893005, time = 0.9441666603088379\n",
      "Training at step=96, batch=75, train loss = 1.3247753381729126, train acc = 0.5, time = 0.9468624591827393\n",
      "Training at step=96, batch=100, train loss = 1.3654954433441162, train acc = 0.49000000953674316, time = 0.9420852661132812\n",
      "Training at step=96, batch=125, train loss = 1.349063515663147, train acc = 0.5350000262260437, time = 0.9380991458892822\n",
      "Training at step=96, batch=150, train loss = 1.3631341457366943, train acc = 0.5299999713897705, time = 0.9436001777648926\n",
      "Training at step=96, batch=175, train loss = 1.336521863937378, train acc = 0.5350000262260437, time = 0.9369983673095703\n",
      "Training at step=96, batch=200, train loss = 1.3811213970184326, train acc = 0.5049999952316284, time = 0.9525356292724609\n",
      "Training at step=96, batch=225, train loss = 1.3970417976379395, train acc = 0.4950000047683716, time = 0.9430263042449951\n",
      "Testing at step=96, batch=0, test loss = 1.4185289144515991, test acc = 0.4650000035762787, time = 0.34992098808288574\n",
      "Testing at step=96, batch=5, test loss = 1.6042675971984863, test acc = 0.4399999976158142, time = 0.34819841384887695\n",
      "Testing at step=96, batch=10, test loss = 1.5093138217926025, test acc = 0.45500001311302185, time = 0.345592737197876\n",
      "Testing at step=96, batch=15, test loss = 1.521885871887207, test acc = 0.45500001311302185, time = 0.3563723564147949\n",
      "Testing at step=96, batch=20, test loss = 1.6017680168151855, test acc = 0.46000000834465027, time = 0.35054588317871094\n",
      "Testing at step=96, batch=25, test loss = 1.5232746601104736, test acc = 0.4650000035762787, time = 0.3570115566253662\n",
      "Testing at step=96, batch=30, test loss = 1.5658036470413208, test acc = 0.4699999988079071, time = 0.3496863842010498\n",
      "Testing at step=96, batch=35, test loss = 1.526613712310791, test acc = 0.47999998927116394, time = 0.3528106212615967\n",
      "Testing at step=96, batch=40, test loss = 1.5578019618988037, test acc = 0.4749999940395355, time = 0.3537571430206299\n",
      "Testing at step=96, batch=45, test loss = 1.528197169303894, test acc = 0.46000000834465027, time = 0.3509547710418701\n",
      "Step 96 finished in 263.61540269851685, Train loss = 1.3462816882133484, Test loss = 1.501783673763275; Train Acc = 0.5244599976539612, Test Acc = 0.47489999771118163\n",
      "Training at step=97, batch=0, train loss = 1.1711432933807373, train acc = 0.5799999833106995, time = 0.9556818008422852\n",
      "Training at step=97, batch=25, train loss = 1.2978692054748535, train acc = 0.5400000214576721, time = 0.94985032081604\n",
      "Training at step=97, batch=50, train loss = 1.3526667356491089, train acc = 0.5, time = 0.9479935169219971\n",
      "Training at step=97, batch=75, train loss = 1.4203234910964966, train acc = 0.4950000047683716, time = 0.9629518985748291\n",
      "Training at step=97, batch=100, train loss = 1.3113058805465698, train acc = 0.550000011920929, time = 0.9356944561004639\n",
      "Training at step=97, batch=125, train loss = 1.3682371377944946, train acc = 0.5299999713897705, time = 0.9437909126281738\n",
      "Training at step=97, batch=150, train loss = 1.405005693435669, train acc = 0.5049999952316284, time = 0.9407444000244141\n",
      "Training at step=97, batch=175, train loss = 1.3213653564453125, train acc = 0.5199999809265137, time = 0.955188512802124\n",
      "Training at step=97, batch=200, train loss = 1.4183422327041626, train acc = 0.47999998927116394, time = 0.94637131690979\n",
      "Training at step=97, batch=225, train loss = 1.3903368711471558, train acc = 0.5199999809265137, time = 0.9527623653411865\n",
      "Testing at step=97, batch=0, test loss = 1.623335599899292, test acc = 0.46000000834465027, time = 0.35063624382019043\n",
      "Testing at step=97, batch=5, test loss = 1.51171875, test acc = 0.4449999928474426, time = 0.35164570808410645\n",
      "Testing at step=97, batch=10, test loss = 1.5985455513000488, test acc = 0.5, time = 0.356797456741333\n",
      "Testing at step=97, batch=15, test loss = 1.587022066116333, test acc = 0.4749999940395355, time = 0.35447168350219727\n",
      "Testing at step=97, batch=20, test loss = 1.4287936687469482, test acc = 0.48500001430511475, time = 0.3493771553039551\n",
      "Testing at step=97, batch=25, test loss = 1.5119225978851318, test acc = 0.4350000023841858, time = 0.3549222946166992\n",
      "Testing at step=97, batch=30, test loss = 1.5273170471191406, test acc = 0.45500001311302185, time = 0.3610086441040039\n",
      "Testing at step=97, batch=35, test loss = 1.5605424642562866, test acc = 0.44999998807907104, time = 0.36165809631347656\n",
      "Testing at step=97, batch=40, test loss = 1.5555083751678467, test acc = 0.46000000834465027, time = 0.3522365093231201\n",
      "Testing at step=97, batch=45, test loss = 1.6366815567016602, test acc = 0.38499999046325684, time = 0.3550276756286621\n",
      "Step 97 finished in 263.60745453834534, Train loss = 1.3427261896133422, Test loss = 1.5188668704032897; Train Acc = 0.5266999967098236, Test Acc = 0.46860000014305114\n",
      "Training at step=98, batch=0, train loss = 1.357243299484253, train acc = 0.5350000262260437, time = 0.9418246746063232\n",
      "Training at step=98, batch=25, train loss = 1.2849757671356201, train acc = 0.550000011920929, time = 0.9379177093505859\n",
      "Training at step=98, batch=50, train loss = 1.4341075420379639, train acc = 0.49000000953674316, time = 0.9347991943359375\n",
      "Training at step=98, batch=75, train loss = 1.290884256362915, train acc = 0.5550000071525574, time = 0.9564712047576904\n",
      "Training at step=98, batch=100, train loss = 1.3450044393539429, train acc = 0.5099999904632568, time = 0.936091423034668\n",
      "Training at step=98, batch=125, train loss = 1.3540441989898682, train acc = 0.550000011920929, time = 0.9530980587005615\n",
      "Training at step=98, batch=150, train loss = 1.3644922971725464, train acc = 0.5249999761581421, time = 0.9359056949615479\n",
      "Training at step=98, batch=175, train loss = 1.2950819730758667, train acc = 0.574999988079071, time = 0.9463109970092773\n",
      "Training at step=98, batch=200, train loss = 1.330082654953003, train acc = 0.5149999856948853, time = 0.9452099800109863\n",
      "Training at step=98, batch=225, train loss = 1.2214187383651733, train acc = 0.550000011920929, time = 0.9510211944580078\n",
      "Testing at step=98, batch=0, test loss = 1.5158839225769043, test acc = 0.5149999856948853, time = 0.3492004871368408\n",
      "Testing at step=98, batch=5, test loss = 1.61118745803833, test acc = 0.4449999928474426, time = 0.3508169651031494\n",
      "Testing at step=98, batch=10, test loss = 1.4968996047973633, test acc = 0.5149999856948853, time = 0.3512904644012451\n",
      "Testing at step=98, batch=15, test loss = 1.5574390888214111, test acc = 0.4449999928474426, time = 0.36286449432373047\n",
      "Testing at step=98, batch=20, test loss = 1.5029387474060059, test acc = 0.5, time = 0.365966796875\n",
      "Testing at step=98, batch=25, test loss = 1.5761198997497559, test acc = 0.4449999928474426, time = 0.36423778533935547\n",
      "Testing at step=98, batch=30, test loss = 1.4238842725753784, test acc = 0.5, time = 0.35279083251953125\n",
      "Testing at step=98, batch=35, test loss = 1.578299880027771, test acc = 0.41999998688697815, time = 0.3617689609527588\n",
      "Testing at step=98, batch=40, test loss = 1.6140427589416504, test acc = 0.45500001311302185, time = 0.3686642646789551\n",
      "Testing at step=98, batch=45, test loss = 1.3871417045593262, test acc = 0.5, time = 0.3642289638519287\n",
      "Step 98 finished in 264.09972190856934, Train loss = 1.341140030860901, Test loss = 1.5032377791404725; Train Acc = 0.5268399993181229, Test Acc = 0.47459999680519105\n",
      "Training at step=99, batch=0, train loss = 1.4570693969726562, train acc = 0.4650000035762787, time = 0.9564113616943359\n",
      "Training at step=99, batch=25, train loss = 1.3956272602081299, train acc = 0.44999998807907104, time = 0.9486994743347168\n",
      "Training at step=99, batch=50, train loss = 1.3506722450256348, train acc = 0.5249999761581421, time = 0.9643988609313965\n",
      "Training at step=99, batch=75, train loss = 1.3279428482055664, train acc = 0.5450000166893005, time = 0.9419207572937012\n",
      "Training at step=99, batch=100, train loss = 1.3359215259552002, train acc = 0.5550000071525574, time = 1.125560998916626\n",
      "Training at step=99, batch=125, train loss = 1.2759283781051636, train acc = 0.5099999904632568, time = 0.9361393451690674\n",
      "Training at step=99, batch=150, train loss = 1.325445294380188, train acc = 0.4950000047683716, time = 0.9532325267791748\n",
      "Training at step=99, batch=175, train loss = 1.3756664991378784, train acc = 0.5350000262260437, time = 0.9381659030914307\n",
      "Training at step=99, batch=200, train loss = 1.2996069192886353, train acc = 0.5400000214576721, time = 1.10194730758667\n",
      "Training at step=99, batch=225, train loss = 1.3162879943847656, train acc = 0.5299999713897705, time = 0.9385666847229004\n",
      "Testing at step=99, batch=0, test loss = 1.4100444316864014, test acc = 0.47999998927116394, time = 0.3540019989013672\n",
      "Testing at step=99, batch=5, test loss = 1.5495272874832153, test acc = 0.4350000023841858, time = 0.3573615550994873\n",
      "Testing at step=99, batch=10, test loss = 1.6621791124343872, test acc = 0.4050000011920929, time = 0.3494572639465332\n",
      "Testing at step=99, batch=15, test loss = 1.543434500694275, test acc = 0.46000000834465027, time = 0.3496830463409424\n",
      "Testing at step=99, batch=20, test loss = 1.4537456035614014, test acc = 0.5199999809265137, time = 0.3536338806152344\n",
      "Testing at step=99, batch=25, test loss = 1.4071704149246216, test acc = 0.47999998927116394, time = 0.3512556552886963\n",
      "Testing at step=99, batch=30, test loss = 1.5130670070648193, test acc = 0.44999998807907104, time = 0.35166263580322266\n",
      "Testing at step=99, batch=35, test loss = 1.5533825159072876, test acc = 0.47999998927116394, time = 0.34767818450927734\n",
      "Testing at step=99, batch=40, test loss = 1.367138385772705, test acc = 0.5550000071525574, time = 0.3487131595611572\n",
      "Testing at step=99, batch=45, test loss = 1.5751922130584717, test acc = 0.4399999976158142, time = 0.3533504009246826\n",
      "Step 99 finished in 264.261438369751, Train loss = 1.3398631038665771, Test loss = 1.5041958260536195; Train Acc = 0.5268999974727631, Test Acc = 0.46999999701976775\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.1\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T17:06:02.381493Z",
     "iopub.status.busy": "2024-04-03T17:06:02.381278Z",
     "iopub.status.idle": "2024-04-03T17:06:02.758413Z",
     "shell.execute_reply": "2024-04-03T17:06:02.757799Z",
     "shell.execute_reply.started": "2024-04-03T17:06:02.381473Z"
    },
    "id": "U0Q0vFm7B6cg"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2-cifar10_full_repacement.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"2-cifar10_full_replacement.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
