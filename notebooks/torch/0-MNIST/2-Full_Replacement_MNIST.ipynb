{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:36.882723Z",
     "start_time": "2024-04-07T14:02:47.064346Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:37.033986Z",
     "iopub.status.busy": "2024-04-02T16:10:37.033383Z",
     "iopub.status.idle": "2024-04-02T16:10:43.549089Z",
     "shell.execute_reply": "2024-04-02T16:10:43.548169Z",
     "shell.execute_reply.started": "2024-04-02T16:10:37.033958Z"
    },
    "id": "5ebnFsErY0EM",
    "outputId": "e8932b66-715c-467f-912a-8b449c518214",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.1+cu116)\r\n",
      "Collecting torch\r\n",
      "  Downloading torch-2.2.2-cp39-cp39-manylinux1_x86_64.whl (755.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\r\n",
      "Collecting torchvision\r\n",
      "  Downloading torchvision-0.17.2-cp39-cp39-manylinux1_x86_64.whl (6.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.12.1+cu116)\r\n",
      "Collecting torchaudio\r\n",
      "  Downloading torchaudio-2.2.2-cp39-cp39-manylinux1_x86_64.whl (3.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\r\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\r\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\r\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\r\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\r\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\r\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting triton==2.2.0\r\n",
      "  Downloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\r\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\r\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting sympy\r\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3\r\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\r\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\r\n",
      "Collecting typing-extensions>=4.8.0\r\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\r\n",
      "Collecting nvidia-nvjitlink-cu12\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\r\n",
      "Collecting mpmath>=0.19\r\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.4.0\r\n",
      "    Uninstalling typing_extensions-4.4.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.4.0\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.12.1+cu116\r\n",
      "    Uninstalling torch-1.12.1+cu116:\r\n",
      "      Successfully uninstalled torch-1.12.1+cu116\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.13.1+cu116\r\n",
      "    Uninstalling torchvision-0.13.1+cu116:\r\n",
      "      Successfully uninstalled torchvision-0.13.1+cu116\r\n",
      "  Attempting uninstall: torchaudio\r\n",
      "    Found existing installation: torchaudio 0.12.1+cu116\r\n",
      "    Uninstalling torchaudio-0.12.1+cu116:\r\n",
      "      Successfully uninstalled torchaudio-0.12.1+cu116\r\n",
      "Successfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sympy-1.12 torch-2.2.2 torchaudio-2.2.2 torchvision-0.17.2 triton-2.2.0 typing-extensions-4.11.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting pennylane\r\n",
      "  Downloading PennyLane-0.35.1-py3-none-any.whl (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cotengra\r\n",
      "  Downloading cotengra-0.5.6-py3-none-any.whl (148 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.0/148.0 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting quimb\r\n",
      "  Downloading quimb-1.7.3-py3-none-any.whl (500 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.7/500.7 kB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torchmetrics\r\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting toml\r\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\r\n",
      "Collecting appdirs\r\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Collecting semantic-version>=2.7\r\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (3.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.23.4)\r\n",
      "Collecting autoray>=0.6.1\r\n",
      "  Downloading autoray-0.6.9-py3-none-any.whl (49 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pennylane-lightning>=0.35\r\n",
      "  Downloading PennyLane_Lightning-0.35.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting rustworkx\r\n",
      "  Downloading rustworkx-0.14.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pennylane) (4.11.0)\r\n",
      "Collecting autograd\r\n",
      "  Downloading autograd-1.6.2-py3-none-any.whl (49 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\r\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\r\n",
      "Collecting numba>=0.39\r\n",
      "  Downloading numba-0.59.1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\r\n",
      "\u001b[?25hCollecting cytoolz>=0.8.0\r\n",
      "  Downloading cytoolz-0.12.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.2.2)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\r\n",
      "Collecting lightning-utilities>=0.8.0\r\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\r\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0\r\n",
      "  Downloading llvmlite-0.42.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2023.1.0)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (3.9.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.127)\r\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\r\n",
      "Installing collected packages: appdirs, toml, semantic-version, rustworkx, llvmlite, lightning-utilities, cytoolz, autoray, autograd, numba, cotengra, quimb, torchmetrics, pennylane-lightning, pennylane\r\n",
      "Successfully installed appdirs-1.4.4 autograd-1.6.2 autoray-0.6.9 cotengra-0.5.6 cytoolz-0.12.3 lightning-utilities-0.11.2 llvmlite-0.42.0 numba-0.59.1 pennylane-0.35.1 pennylane-lightning-0.35.1 quimb-1.7.3 rustworkx-0.14.2 semantic-version-2.10.0 toml-0.10.2 torchmetrics-1.3.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!pip install torch torchvision torchaudio --upgrade\n",
    "!pip install pennylane cotengra quimb torchmetrics --upgrade\n",
    "#!pip install -U \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:40.242061Z",
     "start_time": "2024-04-07T14:04:36.885586Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:43.550874Z",
     "iopub.status.busy": "2024-04-02T16:10:43.550642Z",
     "iopub.status.idle": "2024-04-02T16:10:47.679639Z",
     "shell.execute_reply": "2024-04-02T16:10:47.678406Z",
     "shell.execute_reply.started": "2024-04-02T16:10:43.550852Z"
    },
    "id": "VN2wD-U7bGse",
    "outputId": "efc73948-0832-402c-eafc-b0e6adce4a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch \n",
    "import torchvision  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WmQPQuLbSFw"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:40.634551Z",
     "start_time": "2024-04-07T14:04:40.243349Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:47.680869Z",
     "iopub.status.busy": "2024-04-02T16:10:47.680416Z",
     "iopub.status.idle": "2024-04-02T16:10:47.809475Z",
     "shell.execute_reply": "2024-04-02T16:10:47.808255Z",
     "shell.execute_reply.started": "2024-04-02T16:10:47.680826Z"
    },
    "id": "rUhQUP2dbTja",
    "outputId": "0d3641db-a2ff-4689-8b7e-32e0e8b313d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 32, 32])\n",
      "torch.Size([32])\n",
      "tensor([7, 7, 5, 7, 4, 7, 7, 7, 5, 4, 7, 2, 7, 8, 1, 1, 1, 6, 4, 1, 5, 1, 2, 7,\n",
      "        8, 0, 8, 6, 1, 8, 8, 7])\n",
      "tensor([-1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j,  0.2863+0.j,  0.7098+0.j, -0.7333+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j])\n"
     ]
    }
   ],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQI8WWY6byQq"
   },
   "source": [
    "# Some Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:42.329623Z",
     "start_time": "2024-04-07T14:04:40.635748Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:47.812079Z",
     "iopub.status.busy": "2024-04-02T16:10:47.811853Z",
     "iopub.status.idle": "2024-04-02T16:10:49.208648Z",
     "shell.execute_reply": "2024-04-02T16:10:49.207311Z",
     "shell.execute_reply.started": "2024-04-02T16:10:47.812056Z"
    },
    "id": "7B4DTncWbwQl",
    "outputId": "38975b7c-66c4-49f5-e5fd-158e55028cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-0.j, 0.+0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j,  0.+0.j, -1.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-0.j, 0.+0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j,  0.+0.j, -1.+0.j],\n",
      "        [ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.-0.j, 0.-1.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  0.-0.j,  0.-0.j, -1.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j,  1.-0.j,  0.-0.j],\n",
      "        [ 0.+0.j,  1.-0.j,  0.+0.j,  0.-0.j],\n",
      "        [-1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, -0.+0.j, 0.-0.j, 0.+1.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, -0.-1.j, 0.+0.j, -0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -0.+0.j, -1.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -0.+0.j, -1.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-0.j, -0.+0.j, 0.+1.j],\n",
      "        [0.+0.j, 0.+0.j, -0.-1.j, -0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j, -0.+0.j,  1.-0.j]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "ket = {\n",
    "    '0':torch.tensor([1.,0.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '1':torch.tensor([0.,1.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '+':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) + torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device)),\n",
    "    '-':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) - torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device))\n",
    "}\n",
    "\n",
    "pauli = {\n",
    "    'I':torch.tensor([[1.,0.],[0.,1.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'X':torch.tensor([[0.,1.],[1.,0.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Y':torch.tensor([[0., -1.j],[1.j, 0]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Z':torch.tensor([[1.,0.],[0.,-1.]], dtype = COMPLEX_DTYPE, device=device)\n",
    "}\n",
    "\n",
    "def tensor_product(*args):\n",
    "  input_list = [a for a in args]\n",
    "  return functools.reduce(torch.kron, input_list)\n",
    "\n",
    "def multi_qubit_identity(n_qubits:int):\n",
    "  assert n_qubits>0\n",
    "  if n_qubits == 1:\n",
    "    return pauli['I']\n",
    "  else:\n",
    "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
    "\n",
    "pauli_words_su4 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
    "      #print(key1, key2)\n",
    "      #print(pauli[key1])\n",
    "      #print(pauli[key2])\n",
    "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
    "      #print(pauli_words_su4[key1+key2])\n",
    "\n",
    "pauli_words_su8 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      if not key1+key2+key3 == 'III':\n",
    "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
    "\n",
    "pauli_words_su16 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        if not key1+key2+key3+key4 == 'IIII':\n",
    "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
    "              pauli[key1],\n",
    "              pauli[key2],\n",
    "              pauli[key3],\n",
    "              pauli[key4]\n",
    "          )\n",
    "\n",
    "pauli_words_su32 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        for key5 in pauli.keys():\n",
    "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
    "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
    "                pauli[key1],\n",
    "                pauli[key2],\n",
    "                pauli[key3],\n",
    "                pauli[key4],\n",
    "                pauli[key5]\n",
    "            )\n",
    "\n",
    "def su32_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = list(pauli_words_su32.values())\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, torch.stack(dict_values))\n",
    "\n",
    "  return torch.matrix_exp(1j*generator)\n",
    "\n",
    "def su4_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = torch.stack(list(pauli_words_su4.values()))\n",
    "\n",
    "  generator = torch.einsum(\"i, ijk -> jk\", params, dict_values)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "def pauli_dict_func(key):\n",
    "    return pauli[key]\n",
    "\n",
    "def pauli_dict_func_multiple_keys(keys):\n",
    "    return list(map(pauli_dict_func, keys))\n",
    "\n",
    "def pauli_string_tensor_prod(pauli_string:str):\n",
    "    paulis_char = list(pauli_string)\n",
    "    paulis_mat = pauli_dict_func_multiple_keys(paulis_char)\n",
    "    return tensor_product(*paulis_mat)\n",
    "\n",
    "def generate_nqubit_pauli_strings(n_qubits:int):\n",
    "    assert n_qubits>0\n",
    "    pauli_labels = ['I', 'X', 'Y', 'Z']\n",
    "    pauli_strings = []\n",
    "    for labels in itertools.product(pauli_labels, repeat=n_qubits):\n",
    "        pauli_str = \"\".join(labels)\n",
    "        if pauli_str != 'I'*n_qubits:\n",
    "            pauli_strings.append(pauli_str)\n",
    "    return pauli_strings\n",
    "\n",
    "def generate_pauli_tensor_list(pauli_strings:list):\n",
    "    return list(map(pauli_string_tensor_prod, pauli_strings))\n",
    "\n",
    "\n",
    "print(\n",
    "    generate_pauli_tensor_list(generate_nqubit_pauli_strings(2))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:42.336581Z",
     "start_time": "2024-04-07T14:04:42.331263Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:49.210092Z",
     "iopub.status.busy": "2024-04-02T16:10:49.209878Z",
     "iopub.status.idle": "2024-04-02T16:10:49.219820Z",
     "shell.execute_reply": "2024-04-02T16:10:49.219262Z",
     "shell.execute_reply.started": "2024-04-02T16:10:49.210072Z"
    },
    "id": "Xs0c2F1eBnGc"
   },
   "outputs": [],
   "source": [
    "def measure_sv(\n",
    "    state,\n",
    "    observable\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Measure a statevector with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not\n",
    "  \"\"\"\n",
    "  expectation_value = torch.conj(state)@observable@state\n",
    "  return torch.real(expectation_value)\n",
    "\n",
    "def measure_dm(\n",
    "    rho,\n",
    "    observable\n",
    "):\n",
    "  \"\"\"\n",
    "  Measure a density matrix with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not.\n",
    "  \"\"\"\n",
    "  product = torch.matmul(rho, observable)\n",
    "\n",
    "  # Calculate the trace, which is the sum of diagonal elements\n",
    "  trace = torch.trace(product)\n",
    "\n",
    "  # The expectation value should be real for physical observables\n",
    "  return torch.real(trace)\n",
    "\n",
    "vmap_measure_sv = torch.vmap(measure_sv, in_dims=(None, 0), out_dims=0)\n",
    "vmap_measure_dm = torch.vmap(measure_dm, in_dims=(None, 0), out_dims=0)\n",
    "\n",
    "# assuming the input patch observables (hermitianized) has shape (batchsize,n_patches, c, h, w)\n",
    "# assuming the input set statevectors has shape (c, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_sv_batched_ob = torch.vmap(measure_sv, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "# assuming the input set desnity matrices has shape (batchsize, n_patches, c, 2**n, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_dm_batched_ob = torch.vmap(measure_dm, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "def bitstring_to_state(bitstring:str):\n",
    "  \"\"\"\n",
    "  Convert a bit string, like '0101001' or '+-+-101'\n",
    "  to a statevector. Each character in the bitstring must be among\n",
    "  0, 1, + and -\n",
    "  \"\"\"\n",
    "  assert len(bitstring)>0\n",
    "  for c in bitstring:\n",
    "    assert c in ['0', '1', '+', '-']\n",
    "  single_qubit_states = [ket[c] for c in bitstring]\n",
    "  return tensor_product(*single_qubit_states)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFZqT6bpElaL"
   },
   "source": [
    "# Flipped Quanvolution Kernel\n",
    "\n",
    "First, let's define the function to extract patches for a single kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:42.387023Z",
     "start_time": "2024-04-07T14:04:42.337635Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:49.221071Z",
     "iopub.status.busy": "2024-04-02T16:10:49.220885Z",
     "iopub.status.idle": "2024-04-02T16:10:49.226832Z",
     "shell.execute_reply": "2024-04-02T16:10:49.226163Z",
     "shell.execute_reply.started": "2024-04-02T16:10:49.221053Z"
    },
    "id": "He4HdMRHC7T6"
   },
   "outputs": [],
   "source": [
    "def extract_patches(image, patch_size, stride, padding=None):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image with multiple input channels and optional custom padding.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (in_channels, height, width).\n",
    "        patch_size (int): Size of the square patches to extract.\n",
    "        stride (int): Stride between patches.\n",
    "        padding (tuple): Padding value(s) for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n",
    "    pad_l, pad_r, pad_t, pad_b = padding if padding is not None else (0,0,0,0)\n",
    "\n",
    "    if padding is not None:\n",
    "        image = torch.nn.functional.pad(image, (pad_l, pad_r, pad_t, pad_b), mode='constant')\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    height, width = image.shape[-2],  image.shape[-1]\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = image[..., i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    patches = torch.stack(patches, dim=-4)\n",
    "    return patches # has shape (batchsize, n_patches, channel, h, w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRqflooEqKN"
   },
   "source": [
    "## A Three-by-Three Quanv Kernel Function\n",
    "\n",
    "For 3 by 3 patches, we only need a 2-qubit parameterised state, generated by the SU4 gate applied to the $|00\\rangle$ state. The first thing to do is to pad the 3 by 3 patches to 4 by 4. However, for input with size\n",
    "\n",
    "$$\n",
    "D_1\\times H_1 \\times W_1\n",
    "$$\n",
    "\n",
    "where $D_1$ is the number of input channels, we will need $D_1$ different two-qubit states, generated by $D_1$ different parameterised circuits. These circuits will produce $D_1$ expectation values. Suppose each circuit is parameterised by $\\theta_i, i\\in \\{1,2,\\cdots, D_1\\}$, the parameterised (pure) state can be written as\n",
    "\n",
    "$$\n",
    "|\\psi (\\theta_i)⟩ = SU4(\\theta_i)|00⟩\n",
    "$$\n",
    "\n",
    "For a channel $M_i$ of patch $M$ with shape $4\\times 4$, the expectation value is calculated by\n",
    "\n",
    "$$\n",
    "⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$\n",
    "\n",
    "Then, a single pixel value of the output channel corresponding this single kernel is\n",
    "\n",
    "$$\n",
    "\\frac{1}{D_1}\\sum_{i=1}^{D_1}⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:42.437834Z",
     "start_time": "2024-04-07T14:04:42.389346Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:49.227850Z",
     "iopub.status.busy": "2024-04-02T16:10:49.227670Z",
     "iopub.status.idle": "2024-04-02T16:10:49.233341Z",
     "shell.execute_reply": "2024-04-02T16:10:49.232708Z",
     "shell.execute_reply.started": "2024-04-02T16:10:49.227837Z"
    },
    "id": "Yzn4KEt5ErG7"
   },
   "outputs": [],
   "source": [
    "def generate_2q_param_state(theta):\n",
    "  state = bitstring_to_state('00')\n",
    "  state = torch.matmul(su4_op(theta), state)\n",
    "  return state\n",
    "\n",
    "vmap_generate_2q_param_state = torch.vmap(generate_2q_param_state, in_dims=0, out_dims = 0)\n",
    "\n",
    "def single_kernel_op_over_batched_patches(thetas, patch):\n",
    "  # patch has shape (c_in, h, w)\n",
    "  # thetas has shape (c_in, 4^2-1) for SU4 gates\n",
    "  n_theta = thetas.shape[-2]\n",
    "  n_channel = patch.shape[-3]\n",
    "  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n",
    "  states = vmap_generate_2q_param_state(thetas)\n",
    "  #print(\"States shape\", states.shape)\n",
    "  patch = torch.nn.functional.pad(patch, (0, 1, 0, 1), mode='constant')\n",
    "  patch_t = torch.einsum(\"...jk->...kj\", patch)\n",
    "  herm_patch = (patch_t+patch)/2 # has dim (batchsize, num_patches, c, h, w)\n",
    "  #print(\"Herm patch shape\", herm_patch.shape)\n",
    "  channel_out = vmap_measure_channel_sv_batched_ob(states, herm_patch) # has dim (batchsize,n_patches, c)\n",
    "  return torch.sum(channel_out, axis = -1)/n_theta # has dim (batchszie, n_patches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4BXEKd8I67_"
   },
   "source": [
    "## Multiple Output Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:42.477923Z",
     "start_time": "2024-04-07T14:04:42.438981Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:49.234207Z",
     "iopub.status.busy": "2024-04-02T16:10:49.234022Z",
     "iopub.status.idle": "2024-04-02T16:10:49.237565Z",
     "shell.execute_reply": "2024-04-02T16:10:49.237013Z",
     "shell.execute_reply.started": "2024-04-02T16:10:49.234191Z"
    },
    "id": "72vkHV_BI80l"
   },
   "outputs": [],
   "source": [
    "# For multiple channel output\n",
    "# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n",
    "vmap_vmap_single_kernel_op_through_extracted_patches = torch.vmap(single_kernel_op_over_batched_patches, in_dims=(0, None), out_dims=-2) # output has dim (batchsize, c_out, n_patches)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuZ6G5-sKK_J"
   },
   "source": [
    "## Pytorch Module for Three-by-Three Flipped Quanv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:42.524052Z",
     "start_time": "2024-04-07T14:04:42.479076Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:49.238726Z",
     "iopub.status.busy": "2024-04-02T16:10:49.238539Z",
     "iopub.status.idle": "2024-04-02T16:10:49.246127Z",
     "shell.execute_reply": "2024-04-02T16:10:49.245545Z",
     "shell.execute_reply.started": "2024-04-02T16:10:49.238711Z"
    },
    "id": "Gww_XdJ5KPJt"
   },
   "outputs": [],
   "source": [
    "class FlippedQuanv3x3(torch.nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, padding):\n",
    "    super(FlippedQuanv3x3, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.pad_l, self.pad_r, self.pad_t, self.pad_b = padding if padding is not None else (0,0,0,0)\n",
    "    self.pad = (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n",
    "    self.weight= torch.nn.Parameter(torch.randn((out_channels, in_channels, 4**2-1)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn((out_channels, 1)).type(COMPLEX_DTYPE))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batchsize ,c_in, h, w)\n",
    "    # weight has shape (c_out, c_in, 15)\n",
    "    # bias has shape (c_out, 1)\n",
    "    x = x.type(COMPLEX_DTYPE)\n",
    "    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n",
    "    h_out = (h_in-3+(self.pad_t+self.pad_b))//self.stride +1\n",
    "    w_out = (w_in-3+(self.pad_l+self.pad_r))//self.stride +1\n",
    "\n",
    "    #print(self.weight.shape)\n",
    "    #print(patches.shape)\n",
    "\n",
    "    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n",
    "\n",
    "\n",
    "    out = out + self.bias\n",
    "    return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_channels={self.in_channels}, out_channels={self.out_channels}, stride={self.stride}, padding={self.padding}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jY9sQZ7Ynla"
   },
   "source": [
    "# Linear Layer\n",
    "\n",
    "For input dimension $D$, the quantum linear layer is a $n = \\lceil \\log_4(D+1)⌉$-qubit quantum circuit. Both the data encoding and parameterised circuit is achieved via the $SU(2^n)$ unitary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:44.126106Z",
     "start_time": "2024-04-07T14:04:42.525306Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:49.248151Z",
     "iopub.status.busy": "2024-04-02T16:10:49.247958Z",
     "iopub.status.idle": "2024-04-02T16:10:50.659602Z",
     "shell.execute_reply": "2024-04-02T16:10:50.658944Z",
     "shell.execute_reply.started": "2024-04-02T16:10:49.248134Z"
    },
    "id": "AXxNIObFYnPW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0847, 0.6649, 0.0675, 0.1829], device='cuda:0')\n",
      "tensor(1.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def data_encode_unitary(padded_data, t):\n",
    "  original_dim = padded_data.shape[-1]\n",
    "  new_dim = torch.sqrt(torch.tensor(original_dim)).type(torch.int)\n",
    "  data = torch.reshape(padded_data, (new_dim, new_dim))\n",
    "  generator = (data + torch.einsum(\"...jk->...kj\", data))/2\n",
    "  return torch.linalg.matrix_exp(1.0j*generator*t)\n",
    "\n",
    "def su_n(params, pauli_string_tensor_list):\n",
    "  # params has dim 4**n-1\n",
    "  paulis = torch.stack(pauli_string_tensor_list)\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, paulis)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "def linear_layer_func(padded_data, params, pauli_string_tensor_list, observables, n_qubits):\n",
    "  n_rep = params.shape[0]\n",
    "  state = bitstring_to_state(\"+\"*n_qubits)\n",
    "  for i in range(n_rep):\n",
    "    data_unitary = data_encode_unitary(padded_data, 1/n_rep)\n",
    "    #print(data_unitary.shape)\n",
    "    #print(state.shape)\n",
    "    state = torch.matmul(\n",
    "      data_unitary,\n",
    "      state\n",
    "    )\n",
    "    sun_gate = su_n(params[i], pauli_string_tensor_list)\n",
    "    #print(sun_gate.shape)\n",
    "    state = torch.matmul(\n",
    "      sun_gate,\n",
    "      state\n",
    "    )\n",
    "  return vmap_measure_sv(state, observables)\n",
    "\n",
    "test_params = torch.randn((3, 4**2-1),device=device).type(COMPLEX_DTYPE)\n",
    "test_data = torch.randn(4**2, device=device).type(COMPLEX_DTYPE)\n",
    "test_obs = torch.stack([torch.outer(bitstring_to_state('00'), bitstring_to_state('00')),\n",
    "                        torch.outer(bitstring_to_state('01'), bitstring_to_state('01')),\n",
    "                        torch.outer(bitstring_to_state('10'), bitstring_to_state('10')),\n",
    "                         torch.outer(bitstring_to_state('11'), bitstring_to_state('11'))])\n",
    "test_pauli_string_tensor_list = generate_pauli_tensor_list(generate_nqubit_pauli_strings(2))\n",
    "\n",
    "\n",
    "test_out = linear_layer_func(test_data, test_params, test_pauli_string_tensor_list, test_obs, 2)\n",
    "print(test_out)\n",
    "print(torch.sum(test_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:44.148477Z",
     "start_time": "2024-04-07T14:04:44.128663Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:50.660727Z",
     "iopub.status.busy": "2024-04-02T16:10:50.660497Z",
     "iopub.status.idle": "2024-04-02T16:10:50.673338Z",
     "shell.execute_reply": "2024-04-02T16:10:50.672720Z",
     "shell.execute_reply.started": "2024-04-02T16:10:50.660708Z"
    },
    "id": "2F4_SBgIYnMv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3414, 0.4328, 0.0295, 0.1964],\n",
      "        [0.4175, 0.1313, 0.2182, 0.2330],\n",
      "        [0.1889, 0.5187, 0.1613, 0.1311]], device='cuda:0')\n",
      "tensor(3.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "vmap_batch_linear_layer_func = torch.vmap(linear_layer_func, in_dims=(0, None, None, None, None), out_dims=0)\n",
    "\n",
    "test_data = torch.randn((3, 4**2), device=device).type(COMPLEX_DTYPE)\n",
    "test_out = vmap_batch_linear_layer_func(test_data, test_params, test_pauli_string_tensor_list, test_obs, 2)\n",
    "print(test_out)\n",
    "print(torch.sum(test_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryROGtnC_3po"
   },
   "source": [
    "## PyTorch Module for the Quantum Version of Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:44.214024Z",
     "start_time": "2024-04-07T14:04:44.150350Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:50.674334Z",
     "iopub.status.busy": "2024-04-02T16:10:50.674133Z",
     "iopub.status.idle": "2024-04-02T16:10:50.704785Z",
     "shell.execute_reply": "2024-04-02T16:10:50.704215Z",
     "shell.execute_reply.started": "2024-04-02T16:10:50.674316Z"
    },
    "id": "RlTC952w_8VW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 45])\n",
      "torch.Size([16, 6])\n",
      "DataReUploadingLinear(in_dim=45, out_dim=6, n_qubits=3, n_reps=10)\n"
     ]
    }
   ],
   "source": [
    "class DataReUploadingLinear(torch.nn.Module):\n",
    "  def __init__(self, in_dim, out_dim, n_qubits, n_reps):\n",
    "    super(DataReUploadingLinear, self).__init__()\n",
    "    assert 2**n_qubits >= out_dim\n",
    "    assert 4**n_qubits >= in_dim\n",
    "    self.in_dim = in_dim\n",
    "    self.out_dim = out_dim\n",
    "    self.n_qubits = n_qubits\n",
    "    self.n_reps = n_reps\n",
    "    self.pauli_string_tensor_list = generate_pauli_tensor_list(generate_nqubit_pauli_strings(n_qubits))\n",
    "    self.param_dim = 4**n_qubits-1\n",
    "    self.params = torch.nn.Parameter(torch.randn((self.n_reps,self.param_dim)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn(out_dim).type(REAL_DTYPE))\n",
    "    self.observables = self.generate_observables()\n",
    "    self.pad_size = 4**n_qubits-self.in_dim\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has size (batchsize, in_dim)\n",
    "    # pad x\n",
    "    x = torch.nn.functional.pad(x, (0, self.pad_size)).type(COMPLEX_DTYPE)\n",
    "    out = vmap_batch_linear_layer_func(x, self.params, self.pauli_string_tensor_list, self.observables, self.n_qubits)\n",
    "    out = out.type(REAL_DTYPE) + self.bias\n",
    "    return out\n",
    "\n",
    "  def generate_observables(self):\n",
    "    observables = []\n",
    "    for i in range(self.out_dim):\n",
    "      temp_bitstring = '{0:b}'.format(i).zfill(self.n_qubits)\n",
    "      ob = torch.outer(bitstring_to_state(temp_bitstring), bitstring_to_state(temp_bitstring))\n",
    "      observables.append(ob)\n",
    "    return torch.stack(observables)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_dim={self.in_dim}, out_dim={self.out_dim}, n_qubits={self.n_qubits}, n_reps={self.n_reps}'\n",
    "\n",
    "test_linear_module = DataReUploadingLinear(in_dim=45, out_dim=6, n_qubits=3, n_reps = 10).to(device)\n",
    "test_data = torch.randn((16, 45), device=device).type(COMPLEX_DTYPE)\n",
    "print(test_data.shape)\n",
    "test_out = test_linear_module(test_data.to(device))\n",
    "print(test_out.shape)\n",
    "print(test_linear_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yumZTjDVu63"
   },
   "source": [
    "# Replacing Classical Layers with Quantum Layers\n",
    "\n",
    "Let's replace `Conv2d` with `FlippedQuanv3x3`, and `Linear` with `QuantLinear`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T14:04:46.507149Z",
     "start_time": "2024-04-07T14:04:44.217336Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:50.705862Z",
     "iopub.status.busy": "2024-04-02T16:10:50.705674Z",
     "iopub.status.idle": "2024-04-02T16:10:52.137407Z",
     "shell.execute_reply": "2024-04-02T16:10:52.136754Z",
     "shell.execute_reply.started": "2024-04-02T16:10:50.705842Z"
    },
    "id": "W4RC5ou-VzbE",
    "outputId": "9eb0b35d-ed18-431b-8a53-ba5216d3fd06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 32, 32])\n",
      "HybridNet(\n",
      "  (layers): Sequential(\n",
      "    (0): FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None)\n",
      "    (1): FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): DataReUploadingLinear(in_dim=12544, out_dim=10, n_qubits=7, n_reps=10)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/1953195815.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n",
      "/tmp/ipykernel_36/300606786.py:30: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:301.)\n",
      "  return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "class HybridNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HybridNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None), # 32->30\n",
    "        FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None), # 30->28\n",
    "        torch.nn.Flatten(),\n",
    "        DataReUploadingLinear(16*28*28, 10, 7, 10)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "\n",
    "net = HybridNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T22:46:29.304322Z",
     "start_time": "2024-04-07T14:04:46.508838Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "execution": {
     "iopub.execute_input": "2024-04-02T16:10:52.140051Z",
     "iopub.status.busy": "2024-04-02T16:10:52.139814Z",
     "iopub.status.idle": "2024-04-03T00:55:24.432378Z",
     "shell.execute_reply": "2024-04-03T00:55:24.431260Z",
     "shell.execute_reply.started": "2024-04-02T16:10:52.140031Z"
    },
    "id": "x4ZY59q1WS4x",
    "outputId": "6532aea1-47a3-49f3-fe36-b774aee1905f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 300, Number of test batches = 50\n",
      "Print every train batch = 30, Print every test batch = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/1953195815.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at step=0, batch=0, train loss = 2.6262035369873047, train acc = 0.10000000149011612, time = 0.9974138736724854\n",
      "Training at step=0, batch=30, train loss = 2.647953510284424, train acc = 0.10999999940395355, time = 0.9526901245117188\n",
      "Training at step=0, batch=60, train loss = 2.695133686065674, train acc = 0.07999999821186066, time = 0.9520604610443115\n",
      "Training at step=0, batch=90, train loss = 2.60817813873291, train acc = 0.10499999672174454, time = 0.9561350345611572\n",
      "Training at step=0, batch=120, train loss = 2.7170846462249756, train acc = 0.07999999821186066, time = 0.9380104541778564\n",
      "Training at step=0, batch=150, train loss = 2.7266244888305664, train acc = 0.08500000089406967, time = 0.9516506195068359\n",
      "Training at step=0, batch=180, train loss = 2.524245023727417, train acc = 0.13500000536441803, time = 0.9519257545471191\n",
      "Training at step=0, batch=210, train loss = 2.539255380630493, train acc = 0.10000000149011612, time = 0.9430215358734131\n",
      "Training at step=0, batch=240, train loss = 2.48237681388855, train acc = 0.07999999821186066, time = 0.9465610980987549\n",
      "Training at step=0, batch=270, train loss = 2.2584633827209473, train acc = 0.1599999964237213, time = 0.9445474147796631\n",
      "Testing at step=0, batch=0, test loss = 2.1013617515563965, test acc = 0.33000001311302185, time = 0.34519195556640625\n",
      "Testing at step=0, batch=5, test loss = 2.1060094833374023, test acc = 0.30000001192092896, time = 0.3433799743652344\n",
      "Testing at step=0, batch=10, test loss = 2.0407419204711914, test acc = 0.3400000035762787, time = 0.3389885425567627\n",
      "Testing at step=0, batch=15, test loss = 2.059937000274658, test acc = 0.3400000035762787, time = 0.3717036247253418\n",
      "Testing at step=0, batch=20, test loss = 2.0375983715057373, test acc = 0.36000001430511475, time = 0.37191271781921387\n",
      "Testing at step=0, batch=25, test loss = 2.0582046508789062, test acc = 0.3400000035762787, time = 0.33964967727661133\n",
      "Testing at step=0, batch=30, test loss = 2.0109472274780273, test acc = 0.3700000047683716, time = 0.36374616622924805\n",
      "Testing at step=0, batch=35, test loss = 2.067034959793091, test acc = 0.3400000035762787, time = 0.33731508255004883\n",
      "Testing at step=0, batch=40, test loss = 2.0901384353637695, test acc = 0.3100000023841858, time = 0.34500885009765625\n",
      "Testing at step=0, batch=45, test loss = 2.0128960609436035, test acc = 0.3400000035762787, time = 0.3420877456665039\n",
      "Step 0 finished in 311.7183074951172, Train loss = 2.5492526284853616, Test loss = 2.06086642742157; Train Acc = 0.11693333347638447, Test Acc = 0.34190000116825103\n",
      "Training at step=1, batch=0, train loss = 2.074751377105713, train acc = 0.32499998807907104, time = 0.9442861080169678\n",
      "Training at step=1, batch=30, train loss = 1.7602241039276123, train acc = 0.5649999976158142, time = 0.9403722286224365\n",
      "Training at step=1, batch=60, train loss = 0.8862211108207703, train acc = 0.7149999737739563, time = 0.9453096389770508\n",
      "Training at step=1, batch=90, train loss = 0.6942176818847656, train acc = 0.800000011920929, time = 0.9592766761779785\n",
      "Training at step=1, batch=120, train loss = 0.5613515377044678, train acc = 0.8050000071525574, time = 0.9411981105804443\n",
      "Training at step=1, batch=150, train loss = 0.5006600022315979, train acc = 0.8399999737739563, time = 0.9398219585418701\n",
      "Training at step=1, batch=180, train loss = 0.6407527923583984, train acc = 0.7850000262260437, time = 0.9587156772613525\n",
      "Training at step=1, batch=210, train loss = 0.5253095626831055, train acc = 0.8500000238418579, time = 0.941448450088501\n",
      "Training at step=1, batch=240, train loss = 0.4481050968170166, train acc = 0.8550000190734863, time = 0.9445092678070068\n",
      "Training at step=1, batch=270, train loss = 0.44765013456344604, train acc = 0.8500000238418579, time = 0.9377884864807129\n",
      "Testing at step=1, batch=0, test loss = 0.39059412479400635, test acc = 0.8700000047683716, time = 0.3372361660003662\n",
      "Testing at step=1, batch=5, test loss = 0.31877046823501587, test acc = 0.9200000166893005, time = 0.35370659828186035\n",
      "Testing at step=1, batch=10, test loss = 0.2718200981616974, test acc = 0.925000011920929, time = 0.33870553970336914\n",
      "Testing at step=1, batch=15, test loss = 0.29844072461128235, test acc = 0.9300000071525574, time = 0.3385307788848877\n",
      "Testing at step=1, batch=20, test loss = 0.4207988977432251, test acc = 0.875, time = 0.33655714988708496\n",
      "Testing at step=1, batch=25, test loss = 0.3506962060928345, test acc = 0.8849999904632568, time = 0.33890414237976074\n",
      "Testing at step=1, batch=30, test loss = 0.22314617037773132, test acc = 0.9350000023841858, time = 0.3362998962402344\n",
      "Testing at step=1, batch=35, test loss = 0.3061339259147644, test acc = 0.9150000214576721, time = 0.37207937240600586\n",
      "Testing at step=1, batch=40, test loss = 0.5184500813484192, test acc = 0.8399999737739563, time = 0.34029245376586914\n",
      "Testing at step=1, batch=45, test loss = 0.37950053811073303, test acc = 0.875, time = 0.3769645690917969\n",
      "Step 1 finished in 311.3220155239105, Train loss = 0.7414793339371681, Test loss = 0.3618190085887909; Train Acc = 0.7736999989549319, Test Acc = 0.8912999987602234\n",
      "Training at step=2, batch=0, train loss = 0.42325305938720703, train acc = 0.8849999904632568, time = 0.9432947635650635\n",
      "Training at step=2, batch=30, train loss = 0.3867584466934204, train acc = 0.8849999904632568, time = 0.9411807060241699\n",
      "Training at step=2, batch=60, train loss = 0.3377794921398163, train acc = 0.8949999809265137, time = 0.9373307228088379\n",
      "Training at step=2, batch=90, train loss = 0.37891632318496704, train acc = 0.8500000238418579, time = 0.9373745918273926\n",
      "Training at step=2, batch=120, train loss = 0.3784426748752594, train acc = 0.8650000095367432, time = 0.9609780311584473\n",
      "Training at step=2, batch=150, train loss = 0.3845149576663971, train acc = 0.875, time = 0.9402351379394531\n",
      "Training at step=2, batch=180, train loss = 0.2608403265476227, train acc = 0.9200000166893005, time = 0.9421648979187012\n",
      "Training at step=2, batch=210, train loss = 0.4643225073814392, train acc = 0.8600000143051147, time = 0.9394025802612305\n",
      "Training at step=2, batch=240, train loss = 0.2262866199016571, train acc = 0.9300000071525574, time = 0.9480483531951904\n",
      "Training at step=2, batch=270, train loss = 0.40468552708625793, train acc = 0.8650000095367432, time = 0.9394278526306152\n",
      "Testing at step=2, batch=0, test loss = 0.34447190165519714, test acc = 0.925000011920929, time = 0.3394291400909424\n",
      "Testing at step=2, batch=5, test loss = 0.20782466232776642, test acc = 0.9649999737739563, time = 0.338550329208374\n",
      "Testing at step=2, batch=10, test loss = 0.34945279359817505, test acc = 0.9100000262260437, time = 0.3365650177001953\n",
      "Testing at step=2, batch=15, test loss = 0.2470441460609436, test acc = 0.925000011920929, time = 0.33739781379699707\n",
      "Testing at step=2, batch=20, test loss = 0.30444303154945374, test acc = 0.8999999761581421, time = 0.3354465961456299\n",
      "Testing at step=2, batch=25, test loss = 0.22841964662075043, test acc = 0.9350000023841858, time = 0.3380885124206543\n",
      "Testing at step=2, batch=30, test loss = 0.2762007415294647, test acc = 0.9200000166893005, time = 0.33595919609069824\n",
      "Testing at step=2, batch=35, test loss = 0.3158486485481262, test acc = 0.9150000214576721, time = 0.333942174911499\n",
      "Testing at step=2, batch=40, test loss = 0.37857550382614136, test acc = 0.875, time = 0.3608229160308838\n",
      "Testing at step=2, batch=45, test loss = 0.2811392545700073, test acc = 0.9300000071525574, time = 0.33560824394226074\n",
      "Step 2 finished in 310.7506220340729, Train loss = 0.34626099015275635, Test loss = 0.2850732463598251; Train Acc = 0.894066665371259, Test Acc = 0.9137999975681305\n",
      "Training at step=3, batch=0, train loss = 0.3089476525783539, train acc = 0.9150000214576721, time = 0.9373857975006104\n",
      "Training at step=3, batch=30, train loss = 0.2943645715713501, train acc = 0.9200000166893005, time = 0.9491775035858154\n",
      "Training at step=3, batch=60, train loss = 0.4432311952114105, train acc = 0.8849999904632568, time = 0.9455087184906006\n",
      "Training at step=3, batch=90, train loss = 0.34929144382476807, train acc = 0.8949999809265137, time = 0.9542801380157471\n",
      "Training at step=3, batch=120, train loss = 0.23549692332744598, train acc = 0.9399999976158142, time = 0.9647932052612305\n",
      "Training at step=3, batch=150, train loss = 0.2373361587524414, train acc = 0.9350000023841858, time = 0.9480686187744141\n",
      "Training at step=3, batch=180, train loss = 0.2646532356739044, train acc = 0.8949999809265137, time = 0.942349910736084\n",
      "Training at step=3, batch=210, train loss = 0.3175707757472992, train acc = 0.8899999856948853, time = 0.9471578598022461\n",
      "Training at step=3, batch=240, train loss = 0.18486928939819336, train acc = 0.949999988079071, time = 0.9410519599914551\n",
      "Training at step=3, batch=270, train loss = 0.17396599054336548, train acc = 0.9599999785423279, time = 0.9936404228210449\n",
      "Testing at step=3, batch=0, test loss = 0.3330296277999878, test acc = 0.9399999976158142, time = 0.34056639671325684\n",
      "Testing at step=3, batch=5, test loss = 0.22602713108062744, test acc = 0.9200000166893005, time = 0.33806300163269043\n",
      "Testing at step=3, batch=10, test loss = 0.179409459233284, test acc = 0.925000011920929, time = 0.33544468879699707\n",
      "Testing at step=3, batch=15, test loss = 0.2615317404270172, test acc = 0.9399999976158142, time = 0.3379695415496826\n",
      "Testing at step=3, batch=20, test loss = 0.15801650285720825, test acc = 0.9350000023841858, time = 0.34903931617736816\n",
      "Testing at step=3, batch=25, test loss = 0.23846542835235596, test acc = 0.9399999976158142, time = 0.3351588249206543\n",
      "Testing at step=3, batch=30, test loss = 0.26138344407081604, test acc = 0.9300000071525574, time = 0.3350963592529297\n",
      "Testing at step=3, batch=35, test loss = 0.19188612699508667, test acc = 0.9599999785423279, time = 0.34244251251220703\n",
      "Testing at step=3, batch=40, test loss = 0.1671605259180069, test acc = 0.9350000023841858, time = 0.36893582344055176\n",
      "Testing at step=3, batch=45, test loss = 0.2933027446269989, test acc = 0.9200000166893005, time = 0.33751773834228516\n",
      "Step 3 finished in 310.40025997161865, Train loss = 0.2857857917745908, Test loss = 0.23534125298261643; Train Acc = 0.9146833348274231, Test Acc = 0.9309999990463257\n",
      "Training at step=4, batch=0, train loss = 0.2516617178916931, train acc = 0.9449999928474426, time = 0.9451801776885986\n",
      "Training at step=4, batch=30, train loss = 0.2598750591278076, train acc = 0.9200000166893005, time = 0.9358248710632324\n",
      "Training at step=4, batch=60, train loss = 0.24656742811203003, train acc = 0.925000011920929, time = 0.9623234272003174\n",
      "Training at step=4, batch=90, train loss = 0.19185741245746613, train acc = 0.9649999737739563, time = 0.9445052146911621\n",
      "Training at step=4, batch=120, train loss = 0.3275562524795532, train acc = 0.9200000166893005, time = 0.9509096145629883\n",
      "Training at step=4, batch=150, train loss = 0.2560991644859314, train acc = 0.9150000214576721, time = 0.9449727535247803\n",
      "Training at step=4, batch=180, train loss = 0.22587017714977264, train acc = 0.9399999976158142, time = 0.9403185844421387\n",
      "Training at step=4, batch=210, train loss = 0.2453368753194809, train acc = 0.9100000262260437, time = 0.9438052177429199\n",
      "Training at step=4, batch=240, train loss = 0.28852972388267517, train acc = 0.9200000166893005, time = 0.9429211616516113\n",
      "Training at step=4, batch=270, train loss = 0.1568741500377655, train acc = 0.9399999976158142, time = 0.9402928352355957\n",
      "Testing at step=4, batch=0, test loss = 0.19993790984153748, test acc = 0.9350000023841858, time = 0.34213924407958984\n",
      "Testing at step=4, batch=5, test loss = 0.2190309315919876, test acc = 0.9399999976158142, time = 0.3402442932128906\n",
      "Testing at step=4, batch=10, test loss = 0.13685786724090576, test acc = 0.9549999833106995, time = 0.3412282466888428\n",
      "Testing at step=4, batch=15, test loss = 0.15322710573673248, test acc = 0.949999988079071, time = 0.3626875877380371\n",
      "Testing at step=4, batch=20, test loss = 0.18034975230693817, test acc = 0.949999988079071, time = 0.36437296867370605\n",
      "Testing at step=4, batch=25, test loss = 0.22104261815547943, test acc = 0.9549999833106995, time = 0.3611023426055908\n",
      "Testing at step=4, batch=30, test loss = 0.18719236552715302, test acc = 0.925000011920929, time = 0.3634817600250244\n",
      "Testing at step=4, batch=35, test loss = 0.2752297520637512, test acc = 0.925000011920929, time = 0.3603787422180176\n",
      "Testing at step=4, batch=40, test loss = 0.15717363357543945, test acc = 0.9350000023841858, time = 0.36261725425720215\n",
      "Testing at step=4, batch=45, test loss = 0.2518872916698456, test acc = 0.8999999761581421, time = 0.36518096923828125\n",
      "Step 4 finished in 312.34608602523804, Train loss = 0.24641475550830363, Test loss = 0.20655474707484245; Train Acc = 0.9264666680494944, Test Acc = 0.9372999954223633\n",
      "Training at step=5, batch=0, train loss = 0.1579340100288391, train acc = 0.9549999833106995, time = 0.9450640678405762\n",
      "Training at step=5, batch=30, train loss = 0.30154722929000854, train acc = 0.9399999976158142, time = 0.9425902366638184\n",
      "Training at step=5, batch=60, train loss = 0.23209594190120697, train acc = 0.9300000071525574, time = 0.9373948574066162\n",
      "Training at step=5, batch=90, train loss = 0.23387236893177032, train acc = 0.925000011920929, time = 0.9419982433319092\n",
      "Training at step=5, batch=120, train loss = 0.20440804958343506, train acc = 0.9300000071525574, time = 0.9344229698181152\n",
      "Training at step=5, batch=150, train loss = 0.16386456787586212, train acc = 0.9200000166893005, time = 1.0077927112579346\n",
      "Training at step=5, batch=180, train loss = 0.18201813101768494, train acc = 0.9399999976158142, time = 0.9380097389221191\n",
      "Training at step=5, batch=210, train loss = 0.22982697188854218, train acc = 0.9200000166893005, time = 0.9669630527496338\n",
      "Training at step=5, batch=240, train loss = 0.1676415055990219, train acc = 0.9449999928474426, time = 0.9553661346435547\n",
      "Training at step=5, batch=270, train loss = 0.21748442947864532, train acc = 0.9449999928474426, time = 0.9390997886657715\n",
      "Testing at step=5, batch=0, test loss = 0.16636791825294495, test acc = 0.9549999833106995, time = 0.33745884895324707\n",
      "Testing at step=5, batch=5, test loss = 0.21387772262096405, test acc = 0.9350000023841858, time = 0.35913944244384766\n",
      "Testing at step=5, batch=10, test loss = 0.16777735948562622, test acc = 0.9449999928474426, time = 0.3392951488494873\n",
      "Testing at step=5, batch=15, test loss = 0.1925976574420929, test acc = 0.9350000023841858, time = 0.3368055820465088\n",
      "Testing at step=5, batch=20, test loss = 0.159044548869133, test acc = 0.9599999785423279, time = 0.3352494239807129\n",
      "Testing at step=5, batch=25, test loss = 0.10149938613176346, test acc = 0.9599999785423279, time = 0.3705563545227051\n",
      "Testing at step=5, batch=30, test loss = 0.25143006443977356, test acc = 0.9150000214576721, time = 0.33508729934692383\n",
      "Testing at step=5, batch=35, test loss = 0.14495186507701874, test acc = 0.9549999833106995, time = 0.3350253105163574\n",
      "Testing at step=5, batch=40, test loss = 0.13935193419456482, test acc = 0.9700000286102295, time = 0.3376460075378418\n",
      "Testing at step=5, batch=45, test loss = 0.2105211317539215, test acc = 0.9449999928474426, time = 0.3776674270629883\n",
      "Step 5 finished in 311.7653081417084, Train loss = 0.22186608696977297, Test loss = 0.1884727005660534; Train Acc = 0.9342000003655752, Test Acc = 0.9444999945163727\n",
      "Training at step=6, batch=0, train loss = 0.28917717933654785, train acc = 0.9300000071525574, time = 0.9591383934020996\n",
      "Training at step=6, batch=30, train loss = 0.27084270119667053, train acc = 0.9100000262260437, time = 0.9471454620361328\n",
      "Training at step=6, batch=60, train loss = 0.2878173887729645, train acc = 0.8999999761581421, time = 0.9436304569244385\n",
      "Training at step=6, batch=90, train loss = 0.2824704647064209, train acc = 0.9399999976158142, time = 0.956251859664917\n",
      "Training at step=6, batch=120, train loss = 0.22651754319667816, train acc = 0.9200000166893005, time = 0.9380340576171875\n",
      "Training at step=6, batch=150, train loss = 0.14912518858909607, train acc = 0.9549999833106995, time = 0.955437421798706\n",
      "Training at step=6, batch=180, train loss = 0.24106402695178986, train acc = 0.9399999976158142, time = 0.9441487789154053\n",
      "Training at step=6, batch=210, train loss = 0.25527307391166687, train acc = 0.9100000262260437, time = 0.9447450637817383\n",
      "Training at step=6, batch=240, train loss = 0.20716440677642822, train acc = 0.9399999976158142, time = 0.9671480655670166\n",
      "Training at step=6, batch=270, train loss = 0.13772372901439667, train acc = 0.9599999785423279, time = 0.9378278255462646\n",
      "Testing at step=6, batch=0, test loss = 0.2861660420894623, test acc = 0.9150000214576721, time = 0.33788537979125977\n",
      "Testing at step=6, batch=5, test loss = 0.13574029505252838, test acc = 0.9549999833106995, time = 0.33865952491760254\n",
      "Testing at step=6, batch=10, test loss = 0.206853985786438, test acc = 0.9350000023841858, time = 0.3417065143585205\n",
      "Testing at step=6, batch=15, test loss = 0.1120365783572197, test acc = 0.9700000286102295, time = 0.3386383056640625\n",
      "Testing at step=6, batch=20, test loss = 0.1250137835741043, test acc = 0.949999988079071, time = 0.34082818031311035\n",
      "Testing at step=6, batch=25, test loss = 0.21362191438674927, test acc = 0.9350000023841858, time = 0.33748602867126465\n",
      "Testing at step=6, batch=30, test loss = 0.22283300757408142, test acc = 0.9399999976158142, time = 0.3400442600250244\n",
      "Testing at step=6, batch=35, test loss = 0.1388615220785141, test acc = 0.949999988079071, time = 0.3430445194244385\n",
      "Testing at step=6, batch=40, test loss = 0.15426544845104218, test acc = 0.9549999833106995, time = 0.35916566848754883\n",
      "Testing at step=6, batch=45, test loss = 0.13583055138587952, test acc = 0.9599999785423279, time = 0.35901737213134766\n",
      "Step 6 finished in 311.1164767742157, Train loss = 0.20008125628034273, Test loss = 0.195435122102499; Train Acc = 0.9386999978621801, Test Acc = 0.9404999983310699\n",
      "Training at step=7, batch=0, train loss = 0.09603603184223175, train acc = 0.9800000190734863, time = 0.961524248123169\n",
      "Training at step=7, batch=30, train loss = 0.23355531692504883, train acc = 0.949999988079071, time = 0.9453024864196777\n",
      "Training at step=7, batch=60, train loss = 0.16578169167041779, train acc = 0.9449999928474426, time = 0.9694271087646484\n",
      "Training at step=7, batch=90, train loss = 0.23800896108150482, train acc = 0.9150000214576721, time = 0.9617817401885986\n",
      "Training at step=7, batch=120, train loss = 0.27669379115104675, train acc = 0.9350000023841858, time = 0.944941520690918\n",
      "Training at step=7, batch=150, train loss = 0.16352783143520355, train acc = 0.9399999976158142, time = 0.9423446655273438\n",
      "Training at step=7, batch=180, train loss = 0.13299454748630524, train acc = 0.9649999737739563, time = 0.9367947578430176\n",
      "Training at step=7, batch=210, train loss = 0.1697351336479187, train acc = 0.9399999976158142, time = 0.9439191818237305\n",
      "Training at step=7, batch=240, train loss = 0.15926621854305267, train acc = 0.9599999785423279, time = 0.9465255737304688\n",
      "Training at step=7, batch=270, train loss = 0.22828106582164764, train acc = 0.9350000023841858, time = 0.9396419525146484\n",
      "Testing at step=7, batch=0, test loss = 0.1958228498697281, test acc = 0.9300000071525574, time = 0.36145925521850586\n",
      "Testing at step=7, batch=5, test loss = 0.10281293094158173, test acc = 0.9649999737739563, time = 0.36426448822021484\n",
      "Testing at step=7, batch=10, test loss = 0.28564217686653137, test acc = 0.9300000071525574, time = 0.36322498321533203\n",
      "Testing at step=7, batch=15, test loss = 0.1775284707546234, test acc = 0.9449999928474426, time = 0.36354947090148926\n",
      "Testing at step=7, batch=20, test loss = 0.2016054391860962, test acc = 0.9549999833106995, time = 0.36205244064331055\n",
      "Testing at step=7, batch=25, test loss = 0.1228569969534874, test acc = 0.9599999785423279, time = 0.36196422576904297\n",
      "Testing at step=7, batch=30, test loss = 0.2661038339138031, test acc = 0.925000011920929, time = 0.3737034797668457\n",
      "Testing at step=7, batch=35, test loss = 0.15446248650550842, test acc = 0.9599999785423279, time = 0.3616013526916504\n",
      "Testing at step=7, batch=40, test loss = 0.14917992055416107, test acc = 0.9649999737739563, time = 0.34090590476989746\n",
      "Testing at step=7, batch=45, test loss = 0.11320409178733826, test acc = 0.9599999785423279, time = 0.35379648208618164\n",
      "Step 7 finished in 312.8489439487457, Train loss = 0.1804075198372205, Test loss = 0.17035206273198128; Train Acc = 0.9453166627883911, Test Acc = 0.9477999925613403\n",
      "Training at step=8, batch=0, train loss = 0.17035724222660065, train acc = 0.9449999928474426, time = 0.9345674514770508\n",
      "Training at step=8, batch=30, train loss = 0.21002238988876343, train acc = 0.949999988079071, time = 0.9420332908630371\n",
      "Training at step=8, batch=60, train loss = 0.1991485059261322, train acc = 0.9549999833106995, time = 0.9391958713531494\n",
      "Training at step=8, batch=90, train loss = 0.23641495406627655, train acc = 0.9399999976158142, time = 0.9400248527526855\n",
      "Training at step=8, batch=120, train loss = 0.20035409927368164, train acc = 0.9300000071525574, time = 0.9777607917785645\n",
      "Training at step=8, batch=150, train loss = 0.18494397401809692, train acc = 0.9549999833106995, time = 0.9631273746490479\n",
      "Training at step=8, batch=180, train loss = 0.1432051956653595, train acc = 0.9599999785423279, time = 0.9424116611480713\n",
      "Training at step=8, batch=210, train loss = 0.292572021484375, train acc = 0.9150000214576721, time = 0.9499561786651611\n",
      "Training at step=8, batch=240, train loss = 0.23029060661792755, train acc = 0.9449999928474426, time = 0.9427907466888428\n",
      "Training at step=8, batch=270, train loss = 0.13226187229156494, train acc = 0.9449999928474426, time = 0.9384965896606445\n",
      "Testing at step=8, batch=0, test loss = 0.16772569715976715, test acc = 0.9399999976158142, time = 0.3504164218902588\n",
      "Testing at step=8, batch=5, test loss = 0.16715465486049652, test acc = 0.9300000071525574, time = 0.3360590934753418\n",
      "Testing at step=8, batch=10, test loss = 0.08629205822944641, test acc = 0.9700000286102295, time = 0.35364794731140137\n",
      "Testing at step=8, batch=15, test loss = 0.24229218065738678, test acc = 0.9150000214576721, time = 0.3375880718231201\n",
      "Testing at step=8, batch=20, test loss = 0.17396485805511475, test acc = 0.949999988079071, time = 0.3352348804473877\n",
      "Testing at step=8, batch=25, test loss = 0.147267147898674, test acc = 0.9599999785423279, time = 0.33875322341918945\n",
      "Testing at step=8, batch=30, test loss = 0.17380553483963013, test acc = 0.949999988079071, time = 0.33560895919799805\n",
      "Testing at step=8, batch=35, test loss = 0.21107667684555054, test acc = 0.925000011920929, time = 0.3356447219848633\n",
      "Testing at step=8, batch=40, test loss = 0.0944935530424118, test acc = 0.9700000286102295, time = 0.3388066291809082\n",
      "Testing at step=8, batch=45, test loss = 0.10185948014259338, test acc = 0.9700000286102295, time = 0.33715224266052246\n",
      "Step 8 finished in 310.8004710674286, Train loss = 0.17220996563633284, Test loss = 0.1620102322101593; Train Acc = 0.9482833282152812, Test Acc = 0.9508000004291535\n",
      "Training at step=9, batch=0, train loss = 0.17799554765224457, train acc = 0.9300000071525574, time = 0.9384968280792236\n",
      "Training at step=9, batch=30, train loss = 0.17260178923606873, train acc = 0.9599999785423279, time = 0.9443154335021973\n",
      "Training at step=9, batch=60, train loss = 0.17569543421268463, train acc = 0.9449999928474426, time = 0.9478926658630371\n",
      "Training at step=9, batch=90, train loss = 0.22541701793670654, train acc = 0.9350000023841858, time = 0.9360249042510986\n",
      "Training at step=9, batch=120, train loss = 0.08373315632343292, train acc = 0.9750000238418579, time = 0.9391353130340576\n",
      "Training at step=9, batch=150, train loss = 0.08487457036972046, train acc = 0.9850000143051147, time = 0.9388678073883057\n",
      "Training at step=9, batch=180, train loss = 0.13366514444351196, train acc = 0.9599999785423279, time = 0.9462623596191406\n",
      "Training at step=9, batch=210, train loss = 0.20947274565696716, train acc = 0.949999988079071, time = 0.9566588401794434\n",
      "Training at step=9, batch=240, train loss = 0.1324516236782074, train acc = 0.9549999833106995, time = 0.9393718242645264\n",
      "Training at step=9, batch=270, train loss = 0.14071248471736908, train acc = 0.9599999785423279, time = 0.9357452392578125\n",
      "Testing at step=9, batch=0, test loss = 0.18010137975215912, test acc = 0.9449999928474426, time = 0.340390682220459\n",
      "Testing at step=9, batch=5, test loss = 0.10188040882349014, test acc = 0.9599999785423279, time = 0.3396773338317871\n",
      "Testing at step=9, batch=10, test loss = 0.16013474762439728, test acc = 0.9399999976158142, time = 0.37049221992492676\n",
      "Testing at step=9, batch=15, test loss = 0.15708118677139282, test acc = 0.9700000286102295, time = 0.3416297435760498\n",
      "Testing at step=9, batch=20, test loss = 0.21773283183574677, test acc = 0.9399999976158142, time = 0.33976078033447266\n",
      "Testing at step=9, batch=25, test loss = 0.1850002408027649, test acc = 0.9449999928474426, time = 0.33779454231262207\n",
      "Testing at step=9, batch=30, test loss = 0.23162022233009338, test acc = 0.925000011920929, time = 0.3385190963745117\n",
      "Testing at step=9, batch=35, test loss = 0.1597834676504135, test acc = 0.9649999737739563, time = 0.35910892486572266\n",
      "Testing at step=9, batch=40, test loss = 0.2103947401046753, test acc = 0.925000011920929, time = 0.3440895080566406\n",
      "Testing at step=9, batch=45, test loss = 0.12534309923648834, test acc = 0.9649999737739563, time = 0.3787667751312256\n",
      "Step 9 finished in 310.4894516468048, Train loss = 0.15819930605590343, Test loss = 0.15548401638865472; Train Acc = 0.9528166613976161, Test Acc = 0.9530999994277954\n",
      "Training at step=10, batch=0, train loss = 0.16184672713279724, train acc = 0.9399999976158142, time = 0.9508700370788574\n",
      "Training at step=10, batch=30, train loss = 0.14919660985469818, train acc = 0.9599999785423279, time = 0.9593188762664795\n",
      "Training at step=10, batch=60, train loss = 0.1556711494922638, train acc = 0.9599999785423279, time = 0.940673828125\n",
      "Training at step=10, batch=90, train loss = 0.1460307538509369, train acc = 0.9449999928474426, time = 0.9413580894470215\n",
      "Training at step=10, batch=120, train loss = 0.14551810920238495, train acc = 0.9599999785423279, time = 0.934429407119751\n",
      "Training at step=10, batch=150, train loss = 0.12465353310108185, train acc = 0.9549999833106995, time = 0.9396336078643799\n",
      "Training at step=10, batch=180, train loss = 0.09346681833267212, train acc = 0.9700000286102295, time = 0.9385831356048584\n",
      "Training at step=10, batch=210, train loss = 0.09359573572874069, train acc = 0.9599999785423279, time = 0.9438912868499756\n",
      "Training at step=10, batch=240, train loss = 0.09526943415403366, train acc = 0.9549999833106995, time = 0.9645400047302246\n",
      "Training at step=10, batch=270, train loss = 0.1269395649433136, train acc = 0.9449999928474426, time = 0.9465687274932861\n",
      "Testing at step=10, batch=0, test loss = 0.14738325774669647, test acc = 0.9599999785423279, time = 0.3375711441040039\n",
      "Testing at step=10, batch=5, test loss = 0.10935533791780472, test acc = 0.9549999833106995, time = 0.3370828628540039\n",
      "Testing at step=10, batch=10, test loss = 0.09439995884895325, test acc = 0.9649999737739563, time = 0.33852529525756836\n",
      "Testing at step=10, batch=15, test loss = 0.17781640589237213, test acc = 0.949999988079071, time = 0.33687567710876465\n",
      "Testing at step=10, batch=20, test loss = 0.07879070937633514, test acc = 0.9750000238418579, time = 0.33689451217651367\n",
      "Testing at step=10, batch=25, test loss = 0.1469288468360901, test acc = 0.949999988079071, time = 0.3393065929412842\n",
      "Testing at step=10, batch=30, test loss = 0.08789388835430145, test acc = 0.9850000143051147, time = 0.3433253765106201\n",
      "Testing at step=10, batch=35, test loss = 0.1409708708524704, test acc = 0.9649999737739563, time = 0.33977556228637695\n",
      "Testing at step=10, batch=40, test loss = 0.14247453212738037, test acc = 0.9549999833106995, time = 0.33744120597839355\n",
      "Testing at step=10, batch=45, test loss = 0.10140668600797653, test acc = 0.9549999833106995, time = 0.33821892738342285\n",
      "Step 10 finished in 310.83754682540894, Train loss = 0.14815355802575747, Test loss = 0.13828004285693168; Train Acc = 0.9544833296537399, Test Acc = 0.9571999943256378\n",
      "Training at step=11, batch=0, train loss = 0.13864949345588684, train acc = 0.9449999928474426, time = 0.9397022724151611\n",
      "Training at step=11, batch=30, train loss = 0.14688676595687866, train acc = 0.9549999833106995, time = 0.9473304748535156\n",
      "Training at step=11, batch=60, train loss = 0.09843002259731293, train acc = 0.9599999785423279, time = 0.9386618137359619\n",
      "Training at step=11, batch=90, train loss = 0.14036916196346283, train acc = 0.9599999785423279, time = 0.9509181976318359\n",
      "Training at step=11, batch=120, train loss = 0.11904501169919968, train acc = 0.9700000286102295, time = 0.9412577152252197\n",
      "Training at step=11, batch=150, train loss = 0.13603860139846802, train acc = 0.9449999928474426, time = 0.9485387802124023\n",
      "Training at step=11, batch=180, train loss = 0.14328768849372864, train acc = 0.9599999785423279, time = 0.9445524215698242\n",
      "Training at step=11, batch=210, train loss = 0.05723433196544647, train acc = 0.9750000238418579, time = 0.9422764778137207\n",
      "Training at step=11, batch=240, train loss = 0.21507690846920013, train acc = 0.925000011920929, time = 0.9398369789123535\n",
      "Training at step=11, batch=270, train loss = 0.16533346474170685, train acc = 0.9700000286102295, time = 0.9542489051818848\n",
      "Testing at step=11, batch=0, test loss = 0.20235611498355865, test acc = 0.9549999833106995, time = 0.3377687931060791\n",
      "Testing at step=11, batch=5, test loss = 0.14357033371925354, test acc = 0.9549999833106995, time = 0.3340907096862793\n",
      "Testing at step=11, batch=10, test loss = 0.13508754968643188, test acc = 0.949999988079071, time = 0.3892676830291748\n",
      "Testing at step=11, batch=15, test loss = 0.15274539589881897, test acc = 0.9549999833106995, time = 0.3343985080718994\n",
      "Testing at step=11, batch=20, test loss = 0.10240394622087479, test acc = 0.9649999737739563, time = 0.3359649181365967\n",
      "Testing at step=11, batch=25, test loss = 0.05431415140628815, test acc = 0.9900000095367432, time = 0.33379650115966797\n",
      "Testing at step=11, batch=30, test loss = 0.1324254870414734, test acc = 0.9599999785423279, time = 0.33823466300964355\n",
      "Testing at step=11, batch=35, test loss = 0.10321566462516785, test acc = 0.9750000238418579, time = 0.34960365295410156\n",
      "Testing at step=11, batch=40, test loss = 0.13923263549804688, test acc = 0.9549999833106995, time = 0.3363485336303711\n",
      "Testing at step=11, batch=45, test loss = 0.12133242934942245, test acc = 0.9449999928474426, time = 0.33580493927001953\n",
      "Step 11 finished in 310.2606110572815, Train loss = 0.1415729829793175, Test loss = 0.12760586909949778; Train Acc = 0.956416662534078, Test Acc = 0.9602999985218048\n",
      "Training at step=12, batch=0, train loss = 0.08352203667163849, train acc = 0.9750000238418579, time = 0.9443278312683105\n",
      "Training at step=12, batch=30, train loss = 0.13334476947784424, train acc = 0.9549999833106995, time = 0.9638833999633789\n",
      "Training at step=12, batch=60, train loss = 0.12637385725975037, train acc = 0.9649999737739563, time = 0.9361348152160645\n",
      "Training at step=12, batch=90, train loss = 0.15103359520435333, train acc = 0.949999988079071, time = 0.9676058292388916\n",
      "Training at step=12, batch=120, train loss = 0.11175810545682907, train acc = 0.9700000286102295, time = 0.9380810260772705\n",
      "Training at step=12, batch=150, train loss = 0.12234949320554733, train acc = 0.9549999833106995, time = 0.938910722732544\n",
      "Training at step=12, batch=180, train loss = 0.16846628487110138, train acc = 0.9449999928474426, time = 0.9446682929992676\n",
      "Training at step=12, batch=210, train loss = 0.15730920433998108, train acc = 0.949999988079071, time = 0.9419898986816406\n",
      "Training at step=12, batch=240, train loss = 0.13577310740947723, train acc = 0.9599999785423279, time = 0.9429857730865479\n",
      "Training at step=12, batch=270, train loss = 0.1420803815126419, train acc = 0.9549999833106995, time = 0.964242696762085\n",
      "Testing at step=12, batch=0, test loss = 0.16498824954032898, test acc = 0.9449999928474426, time = 0.34283447265625\n",
      "Testing at step=12, batch=5, test loss = 0.11452272534370422, test acc = 0.9649999737739563, time = 0.3394756317138672\n",
      "Testing at step=12, batch=10, test loss = 0.14128029346466064, test acc = 0.9449999928474426, time = 0.3419229984283447\n",
      "Testing at step=12, batch=15, test loss = 0.09033588320016861, test acc = 0.9700000286102295, time = 0.34725260734558105\n",
      "Testing at step=12, batch=20, test loss = 0.07832686603069305, test acc = 0.9599999785423279, time = 0.3339400291442871\n",
      "Testing at step=12, batch=25, test loss = 0.12008717656135559, test acc = 0.9649999737739563, time = 0.3346424102783203\n",
      "Testing at step=12, batch=30, test loss = 0.11381245404481888, test acc = 0.9649999737739563, time = 0.33532071113586426\n",
      "Testing at step=12, batch=35, test loss = 0.09993413090705872, test acc = 0.9649999737739563, time = 0.34538865089416504\n",
      "Testing at step=12, batch=40, test loss = 0.1476227045059204, test acc = 0.9549999833106995, time = 0.34996581077575684\n",
      "Testing at step=12, batch=45, test loss = 0.12577250599861145, test acc = 0.9449999928474426, time = 0.334977388381958\n",
      "Step 12 finished in 312.0238149166107, Train loss = 0.1335452144717177, Test loss = 0.1306190375983715; Train Acc = 0.959766664703687, Test Acc = 0.9602999949455261\n",
      "Training at step=13, batch=0, train loss = 0.1525873988866806, train acc = 0.949999988079071, time = 0.9445395469665527\n",
      "Training at step=13, batch=30, train loss = 0.21046026051044464, train acc = 0.9449999928474426, time = 0.940140962600708\n",
      "Training at step=13, batch=60, train loss = 0.09781122207641602, train acc = 0.9649999737739563, time = 0.9392142295837402\n",
      "Training at step=13, batch=90, train loss = 0.13675396144390106, train acc = 0.9599999785423279, time = 0.943988561630249\n",
      "Training at step=13, batch=120, train loss = 0.10049722343683243, train acc = 0.9800000190734863, time = 0.9404590129852295\n",
      "Training at step=13, batch=150, train loss = 0.1305256485939026, train acc = 0.9649999737739563, time = 0.9683575630187988\n",
      "Training at step=13, batch=180, train loss = 0.0815311074256897, train acc = 0.9599999785423279, time = 0.9432222843170166\n",
      "Training at step=13, batch=210, train loss = 0.09291461855173111, train acc = 0.9649999737739563, time = 0.9395267963409424\n",
      "Training at step=13, batch=240, train loss = 0.179081991314888, train acc = 0.9300000071525574, time = 0.9521589279174805\n",
      "Training at step=13, batch=270, train loss = 0.13423942029476166, train acc = 0.9700000286102295, time = 0.9404909610748291\n",
      "Testing at step=13, batch=0, test loss = 0.06386739760637283, test acc = 0.9700000286102295, time = 0.34661388397216797\n",
      "Testing at step=13, batch=5, test loss = 0.11995008587837219, test acc = 0.9750000238418579, time = 0.3390347957611084\n",
      "Testing at step=13, batch=10, test loss = 0.11414336413145065, test acc = 0.9700000286102295, time = 0.3402121067047119\n",
      "Testing at step=13, batch=15, test loss = 0.10779394209384918, test acc = 0.9599999785423279, time = 0.33478665351867676\n",
      "Testing at step=13, batch=20, test loss = 0.05773167684674263, test acc = 0.9800000190734863, time = 0.33646726608276367\n",
      "Testing at step=13, batch=25, test loss = 0.18064697086811066, test acc = 0.9449999928474426, time = 0.3549818992614746\n",
      "Testing at step=13, batch=30, test loss = 0.1411568969488144, test acc = 0.9549999833106995, time = 0.3360908031463623\n",
      "Testing at step=13, batch=35, test loss = 0.13746756315231323, test acc = 0.9599999785423279, time = 0.33829569816589355\n",
      "Testing at step=13, batch=40, test loss = 0.11896573752164841, test acc = 0.9649999737739563, time = 0.3364222049713135\n",
      "Testing at step=13, batch=45, test loss = 0.19939059019088745, test acc = 0.9300000071525574, time = 0.3520321846008301\n",
      "Step 13 finished in 310.27362036705017, Train loss = 0.1267562053973476, Test loss = 0.12416076466441155; Train Acc = 0.9619166644414266, Test Acc = 0.9608999991416931\n",
      "Training at step=14, batch=0, train loss = 0.11581219732761383, train acc = 0.9700000286102295, time = 0.9649832248687744\n",
      "Training at step=14, batch=30, train loss = 0.14740429818630219, train acc = 0.949999988079071, time = 0.9434719085693359\n",
      "Training at step=14, batch=60, train loss = 0.1105119064450264, train acc = 0.9549999833106995, time = 0.941608190536499\n",
      "Training at step=14, batch=90, train loss = 0.09489353001117706, train acc = 0.9700000286102295, time = 0.937861442565918\n",
      "Training at step=14, batch=120, train loss = 0.12542419135570526, train acc = 0.9549999833106995, time = 0.9404275417327881\n",
      "Training at step=14, batch=150, train loss = 0.1301993876695633, train acc = 0.949999988079071, time = 0.9401702880859375\n",
      "Training at step=14, batch=180, train loss = 0.13935397565364838, train acc = 0.9599999785423279, time = 0.935288667678833\n",
      "Training at step=14, batch=210, train loss = 0.13035979866981506, train acc = 0.949999988079071, time = 0.9518923759460449\n",
      "Training at step=14, batch=240, train loss = 0.11444132775068283, train acc = 0.9599999785423279, time = 0.9384849071502686\n",
      "Training at step=14, batch=270, train loss = 0.1121501624584198, train acc = 0.9599999785423279, time = 0.9665548801422119\n",
      "Testing at step=14, batch=0, test loss = 0.1315910667181015, test acc = 0.9649999737739563, time = 0.36535120010375977\n",
      "Testing at step=14, batch=5, test loss = 0.12341424822807312, test acc = 0.9549999833106995, time = 0.3612344264984131\n",
      "Testing at step=14, batch=10, test loss = 0.123757503926754, test acc = 0.949999988079071, time = 0.36023378372192383\n",
      "Testing at step=14, batch=15, test loss = 0.07950542122125626, test acc = 0.9800000190734863, time = 0.3610234260559082\n",
      "Testing at step=14, batch=20, test loss = 0.10787905007600784, test acc = 0.949999988079071, time = 0.35999536514282227\n",
      "Testing at step=14, batch=25, test loss = 0.12397654354572296, test acc = 0.9700000286102295, time = 0.3397049903869629\n",
      "Testing at step=14, batch=30, test loss = 0.13864028453826904, test acc = 0.9649999737739563, time = 0.342756986618042\n",
      "Testing at step=14, batch=35, test loss = 0.11166384816169739, test acc = 0.9599999785423279, time = 0.3382890224456787\n",
      "Testing at step=14, batch=40, test loss = 0.12433239072561264, test acc = 0.949999988079071, time = 0.3555769920349121\n",
      "Testing at step=14, batch=45, test loss = 0.1347176879644394, test acc = 0.9599999785423279, time = 0.33638811111450195\n",
      "Step 14 finished in 310.30024123191833, Train loss = 0.1220159654940168, Test loss = 0.14106071278452872; Train Acc = 0.9625833334525427, Test Acc = 0.9548999905586243\n",
      "Training at step=15, batch=0, train loss = 0.11612687259912491, train acc = 0.949999988079071, time = 0.9425864219665527\n",
      "Training at step=15, batch=30, train loss = 0.11641205102205276, train acc = 0.949999988079071, time = 0.9484357833862305\n",
      "Training at step=15, batch=60, train loss = 0.08406001329421997, train acc = 0.9700000286102295, time = 0.9412796497344971\n",
      "Training at step=15, batch=90, train loss = 0.0720188096165657, train acc = 0.9750000238418579, time = 0.9380440711975098\n",
      "Training at step=15, batch=120, train loss = 0.05858824774622917, train acc = 0.9850000143051147, time = 0.9418623447418213\n",
      "Training at step=15, batch=150, train loss = 0.060412079095840454, train acc = 0.9850000143051147, time = 0.9415895938873291\n",
      "Training at step=15, batch=180, train loss = 0.10940153151750565, train acc = 0.9599999785423279, time = 0.9405307769775391\n",
      "Training at step=15, batch=210, train loss = 0.11555549502372742, train acc = 0.9750000238418579, time = 0.9415688514709473\n",
      "Training at step=15, batch=240, train loss = 0.09720791131258011, train acc = 0.9649999737739563, time = 0.9369196891784668\n",
      "Training at step=15, batch=270, train loss = 0.09552856534719467, train acc = 0.9649999737739563, time = 0.947371244430542\n",
      "Testing at step=15, batch=0, test loss = 0.07321908324956894, test acc = 0.9700000286102295, time = 0.3737223148345947\n",
      "Testing at step=15, batch=5, test loss = 0.03967774286866188, test acc = 0.9850000143051147, time = 0.3399693965911865\n",
      "Testing at step=15, batch=10, test loss = 0.09334851056337357, test acc = 0.9850000143051147, time = 0.3406081199645996\n",
      "Testing at step=15, batch=15, test loss = 0.07619526237249374, test acc = 0.9750000238418579, time = 0.33686017990112305\n",
      "Testing at step=15, batch=20, test loss = 0.06876517087221146, test acc = 0.9850000143051147, time = 0.3370389938354492\n",
      "Testing at step=15, batch=25, test loss = 0.25703418254852295, test acc = 0.9649999737739563, time = 0.33876848220825195\n",
      "Testing at step=15, batch=30, test loss = 0.16650082170963287, test acc = 0.9449999928474426, time = 0.33677244186401367\n",
      "Testing at step=15, batch=35, test loss = 0.09154820442199707, test acc = 0.9599999785423279, time = 0.3379085063934326\n",
      "Testing at step=15, batch=40, test loss = 0.14816510677337646, test acc = 0.9549999833106995, time = 0.34114670753479004\n",
      "Testing at step=15, batch=45, test loss = 0.06052123010158539, test acc = 0.9750000238418579, time = 0.3396029472351074\n",
      "Step 15 finished in 309.86494994163513, Train loss = 0.11544457340613007, Test loss = 0.12150770872831344; Train Acc = 0.9653500000635783, Test Acc = 0.9646999967098236\n",
      "Training at step=16, batch=0, train loss = 0.1302403062582016, train acc = 0.949999988079071, time = 0.9409191608428955\n",
      "Training at step=16, batch=30, train loss = 0.07212162762880325, train acc = 0.9850000143051147, time = 0.9364688396453857\n",
      "Training at step=16, batch=60, train loss = 0.06622402369976044, train acc = 0.9750000238418579, time = 0.9407479763031006\n",
      "Training at step=16, batch=90, train loss = 0.13718196749687195, train acc = 0.9549999833106995, time = 0.937042236328125\n",
      "Training at step=16, batch=120, train loss = 0.16019372642040253, train acc = 0.9549999833106995, time = 0.9381494522094727\n",
      "Training at step=16, batch=150, train loss = 0.1461893767118454, train acc = 0.9449999928474426, time = 0.9383945465087891\n",
      "Training at step=16, batch=180, train loss = 0.06298325955867767, train acc = 0.9800000190734863, time = 0.9417626857757568\n",
      "Training at step=16, batch=210, train loss = 0.0658838152885437, train acc = 0.9800000190734863, time = 0.9375607967376709\n",
      "Training at step=16, batch=240, train loss = 0.09386830031871796, train acc = 0.9649999737739563, time = 0.963106632232666\n",
      "Training at step=16, batch=270, train loss = 0.08019180595874786, train acc = 0.9700000286102295, time = 0.9695484638214111\n",
      "Testing at step=16, batch=0, test loss = 0.10039720684289932, test acc = 0.9800000190734863, time = 0.35475826263427734\n",
      "Testing at step=16, batch=5, test loss = 0.10876481980085373, test acc = 0.9599999785423279, time = 0.34603214263916016\n",
      "Testing at step=16, batch=10, test loss = 0.09227306395769119, test acc = 0.9750000238418579, time = 0.33958983421325684\n",
      "Testing at step=16, batch=15, test loss = 0.1705469936132431, test acc = 0.9449999928474426, time = 0.34314751625061035\n",
      "Testing at step=16, batch=20, test loss = 0.15294507145881653, test acc = 0.9549999833106995, time = 0.34203338623046875\n",
      "Testing at step=16, batch=25, test loss = 0.07436371594667435, test acc = 0.9750000238418579, time = 0.3390529155731201\n",
      "Testing at step=16, batch=30, test loss = 0.15647754073143005, test acc = 0.9649999737739563, time = 0.33949828147888184\n",
      "Testing at step=16, batch=35, test loss = 0.05958665907382965, test acc = 0.9850000143051147, time = 0.3404698371887207\n",
      "Testing at step=16, batch=40, test loss = 0.11453771591186523, test acc = 0.9549999833106995, time = 0.336101770401001\n",
      "Testing at step=16, batch=45, test loss = 0.10358967632055283, test acc = 0.9700000286102295, time = 0.35639452934265137\n",
      "Step 16 finished in 312.1581826210022, Train loss = 0.11024573463946581, Test loss = 0.11161871038377286; Train Acc = 0.966350000500679, Test Acc = 0.966300002336502\n",
      "Training at step=17, batch=0, train loss = 0.08844296634197235, train acc = 0.9750000238418579, time = 0.944094181060791\n",
      "Training at step=17, batch=30, train loss = 0.08553517609834671, train acc = 0.9700000286102295, time = 0.9918923377990723\n",
      "Training at step=17, batch=60, train loss = 0.09091595560312271, train acc = 0.9549999833106995, time = 0.9485011100769043\n",
      "Training at step=17, batch=90, train loss = 0.12920840084552765, train acc = 0.9700000286102295, time = 0.940974235534668\n",
      "Training at step=17, batch=120, train loss = 0.10943184792995453, train acc = 0.9700000286102295, time = 0.9448978900909424\n",
      "Training at step=17, batch=150, train loss = 0.12446537613868713, train acc = 0.9649999737739563, time = 0.9393181800842285\n",
      "Training at step=17, batch=180, train loss = 0.12634140253067017, train acc = 0.9549999833106995, time = 0.9378993511199951\n",
      "Training at step=17, batch=210, train loss = 0.14785976707935333, train acc = 0.9649999737739563, time = 0.9441030025482178\n",
      "Training at step=17, batch=240, train loss = 0.11282134801149368, train acc = 0.9649999737739563, time = 0.9393668174743652\n",
      "Training at step=17, batch=270, train loss = 0.13081392645835876, train acc = 0.9549999833106995, time = 0.9396336078643799\n",
      "Testing at step=17, batch=0, test loss = 0.08044054359197617, test acc = 0.9700000286102295, time = 0.33767104148864746\n",
      "Testing at step=17, batch=5, test loss = 0.05244915187358856, test acc = 0.9750000238418579, time = 0.33785319328308105\n",
      "Testing at step=17, batch=10, test loss = 0.11852161586284637, test acc = 0.9599999785423279, time = 0.3367471694946289\n",
      "Testing at step=17, batch=15, test loss = 0.06434869021177292, test acc = 0.9800000190734863, time = 0.3429129123687744\n",
      "Testing at step=17, batch=20, test loss = 0.08008331060409546, test acc = 0.9700000286102295, time = 0.34275269508361816\n",
      "Testing at step=17, batch=25, test loss = 0.1738581508398056, test acc = 0.9649999737739563, time = 0.33910346031188965\n",
      "Testing at step=17, batch=30, test loss = 0.1685018539428711, test acc = 0.9449999928474426, time = 0.33664631843566895\n",
      "Testing at step=17, batch=35, test loss = 0.03542354702949524, test acc = 0.9850000143051147, time = 0.33846259117126465\n",
      "Testing at step=17, batch=40, test loss = 0.08678688108921051, test acc = 0.9700000286102295, time = 0.33711719512939453\n",
      "Testing at step=17, batch=45, test loss = 0.11414292454719543, test acc = 0.9700000286102295, time = 0.3391566276550293\n",
      "Step 17 finished in 310.22448205947876, Train loss = 0.10679733715330561, Test loss = 0.10857868395745754; Train Acc = 0.9674333357810974, Test Acc = 0.9692000031471253\n",
      "Training at step=18, batch=0, train loss = 0.049425192177295685, train acc = 0.9800000190734863, time = 0.9413321018218994\n",
      "Training at step=18, batch=30, train loss = 0.12482953816652298, train acc = 0.9750000238418579, time = 0.9462180137634277\n",
      "Training at step=18, batch=60, train loss = 0.0836799368262291, train acc = 0.9800000190734863, time = 0.9637985229492188\n",
      "Training at step=18, batch=90, train loss = 0.15746377408504486, train acc = 0.9449999928474426, time = 0.9454336166381836\n",
      "Training at step=18, batch=120, train loss = 0.07911224663257599, train acc = 0.9800000190734863, time = 0.9390294551849365\n",
      "Training at step=18, batch=150, train loss = 0.12939277291297913, train acc = 0.9649999737739563, time = 0.9611754417419434\n",
      "Training at step=18, batch=180, train loss = 0.09180878847837448, train acc = 0.9700000286102295, time = 0.9615895748138428\n",
      "Training at step=18, batch=210, train loss = 0.10801166296005249, train acc = 0.9750000238418579, time = 0.9379639625549316\n",
      "Training at step=18, batch=240, train loss = 0.0757586881518364, train acc = 0.9900000095367432, time = 0.9372541904449463\n",
      "Training at step=18, batch=270, train loss = 0.1702793836593628, train acc = 0.9399999976158142, time = 0.9411451816558838\n",
      "Testing at step=18, batch=0, test loss = 0.06979327648878098, test acc = 0.9700000286102295, time = 0.3829643726348877\n",
      "Testing at step=18, batch=5, test loss = 0.10207807272672653, test acc = 0.9649999737739563, time = 0.33983349800109863\n",
      "Testing at step=18, batch=10, test loss = 0.10210072249174118, test acc = 0.9700000286102295, time = 0.345914363861084\n",
      "Testing at step=18, batch=15, test loss = 0.12828978896141052, test acc = 0.9649999737739563, time = 0.3393580913543701\n",
      "Testing at step=18, batch=20, test loss = 0.08257089555263519, test acc = 0.9750000238418579, time = 0.337352991104126\n",
      "Testing at step=18, batch=25, test loss = 0.10322573035955429, test acc = 0.9700000286102295, time = 0.33687472343444824\n",
      "Testing at step=18, batch=30, test loss = 0.08426567167043686, test acc = 0.9750000238418579, time = 0.3381469249725342\n",
      "Testing at step=18, batch=35, test loss = 0.0990683063864708, test acc = 0.9750000238418579, time = 0.3354957103729248\n",
      "Testing at step=18, batch=40, test loss = 0.1428515613079071, test acc = 0.949999988079071, time = 0.3410484790802002\n",
      "Testing at step=18, batch=45, test loss = 0.15931107103824615, test acc = 0.9549999833106995, time = 0.33773088455200195\n",
      "Step 18 finished in 311.47017002105713, Train loss = 0.10269585471600294, Test loss = 0.11148929215967655; Train Acc = 0.9685500023762385, Test Acc = 0.9657000041007996\n",
      "Training at step=19, batch=0, train loss = 0.10106651484966278, train acc = 0.9700000286102295, time = 0.9406261444091797\n",
      "Training at step=19, batch=30, train loss = 0.06340405344963074, train acc = 0.9800000190734863, time = 0.9408783912658691\n",
      "Training at step=19, batch=60, train loss = 0.08860785514116287, train acc = 0.9700000286102295, time = 0.9416704177856445\n",
      "Training at step=19, batch=90, train loss = 0.10768333077430725, train acc = 0.9700000286102295, time = 0.9431624412536621\n",
      "Training at step=19, batch=120, train loss = 0.04546258598566055, train acc = 0.9850000143051147, time = 0.9421517848968506\n",
      "Training at step=19, batch=150, train loss = 0.14901046454906464, train acc = 0.9449999928474426, time = 0.9387490749359131\n",
      "Training at step=19, batch=180, train loss = 0.1141899898648262, train acc = 0.9649999737739563, time = 0.9382944107055664\n",
      "Training at step=19, batch=210, train loss = 0.07866348326206207, train acc = 0.9599999785423279, time = 0.9398128986358643\n",
      "Training at step=19, batch=240, train loss = 0.15909916162490845, train acc = 0.9350000023841858, time = 0.9492142200469971\n",
      "Training at step=19, batch=270, train loss = 0.15193289518356323, train acc = 0.9599999785423279, time = 0.9408895969390869\n",
      "Testing at step=19, batch=0, test loss = 0.1395205706357956, test acc = 0.949999988079071, time = 0.3403630256652832\n",
      "Testing at step=19, batch=5, test loss = 0.07740369439125061, test acc = 0.9800000190734863, time = 0.3384525775909424\n",
      "Testing at step=19, batch=10, test loss = 0.15140344202518463, test acc = 0.9599999785423279, time = 0.34448933601379395\n",
      "Testing at step=19, batch=15, test loss = 0.1553061604499817, test acc = 0.9399999976158142, time = 0.3374323844909668\n",
      "Testing at step=19, batch=20, test loss = 0.08722179383039474, test acc = 0.9850000143051147, time = 0.35300445556640625\n",
      "Testing at step=19, batch=25, test loss = 0.09856986999511719, test acc = 0.9700000286102295, time = 0.3342916965484619\n",
      "Testing at step=19, batch=30, test loss = 0.0681072473526001, test acc = 0.9800000190734863, time = 0.33545494079589844\n",
      "Testing at step=19, batch=35, test loss = 0.0806419774889946, test acc = 0.9649999737739563, time = 0.33740711212158203\n",
      "Testing at step=19, batch=40, test loss = 0.08013136684894562, test acc = 0.9700000286102295, time = 0.3388357162475586\n",
      "Testing at step=19, batch=45, test loss = 0.1535613238811493, test acc = 0.9549999833106995, time = 0.3339719772338867\n",
      "Step 19 finished in 310.3479998111725, Train loss = 0.09887810116633773, Test loss = 0.10756517745554448; Train Acc = 0.9687833368778229, Test Acc = 0.9665000021457673\n",
      "Training at step=20, batch=0, train loss = 0.06576976180076599, train acc = 0.9800000190734863, time = 0.937751293182373\n",
      "Training at step=20, batch=30, train loss = 0.10461612045764923, train acc = 0.9649999737739563, time = 0.9369957447052002\n",
      "Training at step=20, batch=60, train loss = 0.07258061319589615, train acc = 0.9750000238418579, time = 0.9410703182220459\n",
      "Training at step=20, batch=90, train loss = 0.14389343559741974, train acc = 0.9549999833106995, time = 0.9409284591674805\n",
      "Training at step=20, batch=120, train loss = 0.11129740625619888, train acc = 0.9750000238418579, time = 0.9361839294433594\n",
      "Training at step=20, batch=150, train loss = 0.04689789190888405, train acc = 0.9800000190734863, time = 0.9372200965881348\n",
      "Training at step=20, batch=180, train loss = 0.16118766367435455, train acc = 0.9549999833106995, time = 0.9385731220245361\n",
      "Training at step=20, batch=210, train loss = 0.07426213473081589, train acc = 0.9750000238418579, time = 0.9423832893371582\n",
      "Training at step=20, batch=240, train loss = 0.10224583745002747, train acc = 0.9549999833106995, time = 0.9725387096405029\n",
      "Training at step=20, batch=270, train loss = 0.11670958250761032, train acc = 0.9750000238418579, time = 0.942974328994751\n",
      "Testing at step=20, batch=0, test loss = 0.045381974428892136, test acc = 0.9950000047683716, time = 0.3424558639526367\n",
      "Testing at step=20, batch=5, test loss = 0.11685841530561447, test acc = 0.9449999928474426, time = 0.34119701385498047\n",
      "Testing at step=20, batch=10, test loss = 0.07243449240922928, test acc = 0.9750000238418579, time = 0.33980464935302734\n",
      "Testing at step=20, batch=15, test loss = 0.046904824674129486, test acc = 0.9950000047683716, time = 0.3397986888885498\n",
      "Testing at step=20, batch=20, test loss = 0.13755826652050018, test acc = 0.9599999785423279, time = 0.33837056159973145\n",
      "Testing at step=20, batch=25, test loss = 0.1969112902879715, test acc = 0.9449999928474426, time = 0.34121131896972656\n",
      "Testing at step=20, batch=30, test loss = 0.053174156695604324, test acc = 0.9800000190734863, time = 0.33647823333740234\n",
      "Testing at step=20, batch=35, test loss = 0.16136816143989563, test acc = 0.9399999976158142, time = 0.33527421951293945\n",
      "Testing at step=20, batch=40, test loss = 0.06843379139900208, test acc = 0.9800000190734863, time = 0.337526798248291\n",
      "Testing at step=20, batch=45, test loss = 0.12516796588897705, test acc = 0.9649999737739563, time = 0.3397698402404785\n",
      "Step 20 finished in 309.4737391471863, Train loss = 0.09389244050408403, Test loss = 0.09827900409698487; Train Acc = 0.9717666727304458, Test Acc = 0.9699000060558319\n",
      "Training at step=21, batch=0, train loss = 0.11534957587718964, train acc = 0.9800000190734863, time = 0.946768045425415\n",
      "Training at step=21, batch=30, train loss = 0.0740090161561966, train acc = 0.9700000286102295, time = 0.935283899307251\n",
      "Training at step=21, batch=60, train loss = 0.08395829796791077, train acc = 0.9750000238418579, time = 0.9401559829711914\n",
      "Training at step=21, batch=90, train loss = 0.09011644124984741, train acc = 0.9649999737739563, time = 0.9615423679351807\n",
      "Training at step=21, batch=120, train loss = 0.09643769264221191, train acc = 0.949999988079071, time = 0.9485952854156494\n",
      "Training at step=21, batch=150, train loss = 0.07891248911619186, train acc = 0.9599999785423279, time = 0.9432039260864258\n",
      "Training at step=21, batch=180, train loss = 0.13110540807247162, train acc = 0.9599999785423279, time = 0.9636626243591309\n",
      "Training at step=21, batch=210, train loss = 0.10661637037992477, train acc = 0.949999988079071, time = 0.9516298770904541\n",
      "Training at step=21, batch=240, train loss = 0.06370387971401215, train acc = 0.9800000190734863, time = 0.9417498111724854\n",
      "Training at step=21, batch=270, train loss = 0.04860273376107216, train acc = 0.9850000143051147, time = 1.680506706237793\n",
      "Testing at step=21, batch=0, test loss = 0.08801599591970444, test acc = 0.9800000190734863, time = 0.35463976860046387\n",
      "Testing at step=21, batch=5, test loss = 0.13937263190746307, test acc = 0.9649999737739563, time = 0.35333895683288574\n",
      "Testing at step=21, batch=10, test loss = 0.07774534076452255, test acc = 0.9750000238418579, time = 0.3567321300506592\n",
      "Testing at step=21, batch=15, test loss = 0.10173559933900833, test acc = 0.9649999737739563, time = 0.3499910831451416\n",
      "Testing at step=21, batch=20, test loss = 0.07529334723949432, test acc = 0.9750000238418579, time = 0.34687185287475586\n",
      "Testing at step=21, batch=25, test loss = 0.12840406596660614, test acc = 0.9649999737739563, time = 0.3522310256958008\n",
      "Testing at step=21, batch=30, test loss = 0.10278521478176117, test acc = 0.9800000190734863, time = 0.3553955554962158\n",
      "Testing at step=21, batch=35, test loss = 0.06961134076118469, test acc = 0.9850000143051147, time = 0.35929107666015625\n",
      "Testing at step=21, batch=40, test loss = 0.12176910042762756, test acc = 0.9599999785423279, time = 0.3582732677459717\n",
      "Testing at step=21, batch=45, test loss = 0.035019420087337494, test acc = 0.9900000095367432, time = 0.761052131652832\n",
      "Step 21 finished in 326.80657958984375, Train loss = 0.0920340688712895, Test loss = 0.100390207991004; Train Acc = 0.9711333398024241, Test Acc = 0.970600004196167\n",
      "Training at step=22, batch=0, train loss = 0.04032078757882118, train acc = 0.9900000095367432, time = 1.6995997428894043\n",
      "Training at step=22, batch=30, train loss = 0.08720096945762634, train acc = 0.9700000286102295, time = 1.788900375366211\n",
      "Training at step=22, batch=60, train loss = 0.16796556115150452, train acc = 0.9449999928474426, time = 1.6750991344451904\n",
      "Training at step=22, batch=90, train loss = 0.051954761147499084, train acc = 0.9850000143051147, time = 1.8614718914031982\n",
      "Training at step=22, batch=120, train loss = 0.08323650062084198, train acc = 0.9850000143051147, time = 0.9611153602600098\n",
      "Training at step=22, batch=150, train loss = 0.0722413957118988, train acc = 0.9649999737739563, time = 0.9543056488037109\n",
      "Training at step=22, batch=180, train loss = 0.07657879590988159, train acc = 0.9900000095367432, time = 0.9428346157073975\n",
      "Training at step=22, batch=210, train loss = 0.13165183365345, train acc = 0.9599999785423279, time = 0.9686388969421387\n",
      "Training at step=22, batch=240, train loss = 0.10667095333337784, train acc = 0.9750000238418579, time = 0.9462158679962158\n",
      "Training at step=22, batch=270, train loss = 0.05847137048840523, train acc = 0.9850000143051147, time = 0.985234260559082\n",
      "Testing at step=22, batch=0, test loss = 0.13794678449630737, test acc = 0.9800000190734863, time = 0.36759114265441895\n",
      "Testing at step=22, batch=5, test loss = 0.0867416113615036, test acc = 0.9599999785423279, time = 0.3474140167236328\n",
      "Testing at step=22, batch=10, test loss = 0.04571732506155968, test acc = 0.9850000143051147, time = 0.34192609786987305\n",
      "Testing at step=22, batch=15, test loss = 0.14151063561439514, test acc = 0.9599999785423279, time = 0.34652233123779297\n",
      "Testing at step=22, batch=20, test loss = 0.21385696530342102, test acc = 0.9300000071525574, time = 0.34705376625061035\n",
      "Testing at step=22, batch=25, test loss = 0.03054811805486679, test acc = 0.9900000095367432, time = 0.3403146266937256\n",
      "Testing at step=22, batch=30, test loss = 0.0857231542468071, test acc = 0.9800000190734863, time = 0.3568406105041504\n",
      "Testing at step=22, batch=35, test loss = 0.06758729368448257, test acc = 0.9700000286102295, time = 0.34737610816955566\n",
      "Testing at step=22, batch=40, test loss = 0.11028008162975311, test acc = 0.9599999785423279, time = 0.3470773696899414\n",
      "Testing at step=22, batch=45, test loss = 0.13973642885684967, test acc = 0.9549999833106995, time = 0.34760189056396484\n",
      "Step 22 finished in 414.5126836299896, Train loss = 0.08862410519272089, Test loss = 0.0988839902728796; Train Acc = 0.9721166737874349, Test Acc = 0.9686000049114227\n",
      "Training at step=23, batch=0, train loss = 0.12334997206926346, train acc = 0.9700000286102295, time = 0.9778375625610352\n",
      "Training at step=23, batch=30, train loss = 0.07273305207490921, train acc = 0.9750000238418579, time = 0.941234827041626\n",
      "Training at step=23, batch=60, train loss = 0.11410669982433319, train acc = 0.9700000286102295, time = 0.940406322479248\n",
      "Training at step=23, batch=90, train loss = 0.07606902718544006, train acc = 0.9700000286102295, time = 0.9398186206817627\n",
      "Training at step=23, batch=120, train loss = 0.06337536126375198, train acc = 0.9700000286102295, time = 0.9418067932128906\n",
      "Training at step=23, batch=150, train loss = 0.09527923911809921, train acc = 0.9750000238418579, time = 0.9474573135375977\n",
      "Training at step=23, batch=180, train loss = 0.106513112783432, train acc = 0.9700000286102295, time = 0.9462978839874268\n",
      "Training at step=23, batch=210, train loss = 0.09637109935283661, train acc = 0.9700000286102295, time = 0.968921422958374\n",
      "Training at step=23, batch=240, train loss = 0.09579259902238846, train acc = 0.9700000286102295, time = 0.9407591819763184\n",
      "Training at step=23, batch=270, train loss = 0.08769029378890991, train acc = 0.9850000143051147, time = 0.9399998188018799\n",
      "Testing at step=23, batch=0, test loss = 0.08591489493846893, test acc = 0.9800000190734863, time = 0.339357852935791\n",
      "Testing at step=23, batch=5, test loss = 0.11146590113639832, test acc = 0.9599999785423279, time = 0.3350820541381836\n",
      "Testing at step=23, batch=10, test loss = 0.0882784053683281, test acc = 0.9700000286102295, time = 0.3354480266571045\n",
      "Testing at step=23, batch=15, test loss = 0.09990180283784866, test acc = 0.9800000190734863, time = 0.3368799686431885\n",
      "Testing at step=23, batch=20, test loss = 0.07134097069501877, test acc = 0.9700000286102295, time = 0.34236574172973633\n",
      "Testing at step=23, batch=25, test loss = 0.08877429366111755, test acc = 0.9700000286102295, time = 0.3636460304260254\n",
      "Testing at step=23, batch=30, test loss = 0.12411108613014221, test acc = 0.949999988079071, time = 0.3427011966705322\n",
      "Testing at step=23, batch=35, test loss = 0.11525961011648178, test acc = 0.9649999737739563, time = 0.33681678771972656\n",
      "Testing at step=23, batch=40, test loss = 0.13434460759162903, test acc = 0.9700000286102295, time = 0.3354518413543701\n",
      "Testing at step=23, batch=45, test loss = 0.1201171875, test acc = 0.9700000286102295, time = 0.33814215660095215\n",
      "Step 23 finished in 311.60716485977173, Train loss = 0.08556849020843704, Test loss = 0.10058357656002044; Train Acc = 0.974183341662089, Test Acc = 0.9694000113010407\n",
      "Training at step=24, batch=0, train loss = 0.16206146776676178, train acc = 0.949999988079071, time = 0.9367015361785889\n",
      "Training at step=24, batch=30, train loss = 0.13282270729541779, train acc = 0.9649999737739563, time = 0.9404544830322266\n",
      "Training at step=24, batch=60, train loss = 0.10866869986057281, train acc = 0.9599999785423279, time = 0.9459445476531982\n",
      "Training at step=24, batch=90, train loss = 0.08151085674762726, train acc = 0.9649999737739563, time = 0.9894683361053467\n",
      "Training at step=24, batch=120, train loss = 0.03869306296110153, train acc = 0.9950000047683716, time = 0.9445078372955322\n",
      "Training at step=24, batch=150, train loss = 0.03638685494661331, train acc = 0.9900000095367432, time = 0.942803144454956\n",
      "Training at step=24, batch=180, train loss = 0.08185861259698868, train acc = 0.9800000190734863, time = 0.9437918663024902\n",
      "Training at step=24, batch=210, train loss = 0.08521667122840881, train acc = 0.9850000143051147, time = 0.9344136714935303\n",
      "Training at step=24, batch=240, train loss = 0.10372205823659897, train acc = 0.9750000238418579, time = 0.9777200222015381\n",
      "Training at step=24, batch=270, train loss = 0.12489034980535507, train acc = 0.9649999737739563, time = 0.9441330432891846\n",
      "Testing at step=24, batch=0, test loss = 0.14672599732875824, test acc = 0.9599999785423279, time = 0.35713624954223633\n",
      "Testing at step=24, batch=5, test loss = 0.026914717629551888, test acc = 0.9950000047683716, time = 0.33678483963012695\n",
      "Testing at step=24, batch=10, test loss = 0.13672155141830444, test acc = 0.9599999785423279, time = 0.34037065505981445\n",
      "Testing at step=24, batch=15, test loss = 0.05915728956460953, test acc = 0.9800000190734863, time = 0.33932042121887207\n",
      "Testing at step=24, batch=20, test loss = 0.04186234623193741, test acc = 0.9900000095367432, time = 0.34087252616882324\n",
      "Testing at step=24, batch=25, test loss = 0.10657354444265366, test acc = 0.9750000238418579, time = 0.3367011547088623\n",
      "Testing at step=24, batch=30, test loss = 0.0932827740907669, test acc = 0.9700000286102295, time = 0.3382120132446289\n",
      "Testing at step=24, batch=35, test loss = 0.12920799851417542, test acc = 0.949999988079071, time = 0.33602356910705566\n",
      "Testing at step=24, batch=40, test loss = 0.05076614394783974, test acc = 0.9850000143051147, time = 0.35091447830200195\n",
      "Testing at step=24, batch=45, test loss = 0.10851683467626572, test acc = 0.9750000238418579, time = 0.33749842643737793\n",
      "Step 24 finished in 311.3132038116455, Train loss = 0.08763366834570964, Test loss = 0.09875640448182821; Train Acc = 0.973083340326945, Test Acc = 0.9692000007629394\n",
      "Training at step=25, batch=0, train loss = 0.05594920739531517, train acc = 0.9900000095367432, time = 0.9843020439147949\n",
      "Training at step=25, batch=30, train loss = 0.04183584824204445, train acc = 0.9850000143051147, time = 0.9399235248565674\n",
      "Training at step=25, batch=60, train loss = 0.06398998945951462, train acc = 0.9700000286102295, time = 0.93808913230896\n",
      "Training at step=25, batch=90, train loss = 0.1730099469423294, train acc = 0.9649999737739563, time = 0.9457757472991943\n",
      "Training at step=25, batch=120, train loss = 0.11061504483222961, train acc = 0.9750000238418579, time = 0.9387569427490234\n",
      "Training at step=25, batch=150, train loss = 0.07661041617393494, train acc = 0.9850000143051147, time = 0.9368162155151367\n",
      "Training at step=25, batch=180, train loss = 0.07866299152374268, train acc = 0.9750000238418579, time = 0.944310188293457\n",
      "Training at step=25, batch=210, train loss = 0.1414368897676468, train acc = 0.9649999737739563, time = 0.9541018009185791\n",
      "Training at step=25, batch=240, train loss = 0.12720461189746857, train acc = 0.9649999737739563, time = 0.9427609443664551\n",
      "Training at step=25, batch=270, train loss = 0.09808503836393356, train acc = 0.9599999785423279, time = 0.9487016201019287\n",
      "Testing at step=25, batch=0, test loss = 0.16906848549842834, test acc = 0.9599999785423279, time = 0.3379530906677246\n",
      "Testing at step=25, batch=5, test loss = 0.10800351947546005, test acc = 0.9850000143051147, time = 0.3381214141845703\n",
      "Testing at step=25, batch=10, test loss = 0.13405251502990723, test acc = 0.9649999737739563, time = 0.3406200408935547\n",
      "Testing at step=25, batch=15, test loss = 0.07909899950027466, test acc = 0.9800000190734863, time = 0.3409423828125\n",
      "Testing at step=25, batch=20, test loss = 0.08519013226032257, test acc = 0.9750000238418579, time = 0.33588576316833496\n",
      "Testing at step=25, batch=25, test loss = 0.11766072362661362, test acc = 0.9700000286102295, time = 0.33509016036987305\n",
      "Testing at step=25, batch=30, test loss = 0.06805600970983505, test acc = 0.9700000286102295, time = 0.33886051177978516\n",
      "Testing at step=25, batch=35, test loss = 0.04970903322100639, test acc = 0.9750000238418579, time = 0.3418710231781006\n",
      "Testing at step=25, batch=40, test loss = 0.06211079657077789, test acc = 0.9850000143051147, time = 0.3384208679199219\n",
      "Testing at step=25, batch=45, test loss = 0.061157673597335815, test acc = 0.9850000143051147, time = 0.36110877990722656\n",
      "Step 25 finished in 310.3439452648163, Train loss = 0.08290062608197331, Test loss = 0.09616243008524179; Train Acc = 0.9745666750272115, Test Acc = 0.9697000026702881\n",
      "Training at step=26, batch=0, train loss = 0.12136353552341461, train acc = 0.9750000238418579, time = 0.9378623962402344\n",
      "Training at step=26, batch=30, train loss = 0.1433827430009842, train acc = 0.9549999833106995, time = 0.9358878135681152\n",
      "Training at step=26, batch=60, train loss = 0.11709244549274445, train acc = 0.9649999737739563, time = 0.9409542083740234\n",
      "Training at step=26, batch=90, train loss = 0.08372310549020767, train acc = 0.9800000190734863, time = 0.9569535255432129\n",
      "Training at step=26, batch=120, train loss = 0.06108323484659195, train acc = 0.9800000190734863, time = 0.9786202907562256\n",
      "Training at step=26, batch=150, train loss = 0.05731848627328873, train acc = 0.9750000238418579, time = 0.9400544166564941\n",
      "Training at step=26, batch=180, train loss = 0.10506543517112732, train acc = 0.9750000238418579, time = 0.9666967391967773\n",
      "Training at step=26, batch=210, train loss = 0.06708335131406784, train acc = 0.9750000238418579, time = 0.9443023204803467\n",
      "Training at step=26, batch=240, train loss = 0.12736809253692627, train acc = 0.9599999785423279, time = 0.9579458236694336\n",
      "Training at step=26, batch=270, train loss = 0.07488084584474564, train acc = 0.9700000286102295, time = 0.9404580593109131\n",
      "Testing at step=26, batch=0, test loss = 0.19022002816200256, test acc = 0.949999988079071, time = 0.34058451652526855\n",
      "Testing at step=26, batch=5, test loss = 0.04607919231057167, test acc = 0.9800000190734863, time = 0.33672380447387695\n",
      "Testing at step=26, batch=10, test loss = 0.03665603697299957, test acc = 0.9900000095367432, time = 0.3344564437866211\n",
      "Testing at step=26, batch=15, test loss = 0.10761890560388565, test acc = 0.9750000238418579, time = 0.34851717948913574\n",
      "Testing at step=26, batch=20, test loss = 0.10257986187934875, test acc = 0.9649999737739563, time = 0.33705782890319824\n",
      "Testing at step=26, batch=25, test loss = 0.07628509402275085, test acc = 0.9649999737739563, time = 0.3370668888092041\n",
      "Testing at step=26, batch=30, test loss = 0.10497179627418518, test acc = 0.9700000286102295, time = 0.33629775047302246\n",
      "Testing at step=26, batch=35, test loss = 0.06519798934459686, test acc = 0.9800000190734863, time = 0.33838844299316406\n",
      "Testing at step=26, batch=40, test loss = 0.04528379812836647, test acc = 0.9850000143051147, time = 0.339141845703125\n",
      "Testing at step=26, batch=45, test loss = 0.047805801033973694, test acc = 0.9850000143051147, time = 0.3406383991241455\n",
      "Step 26 finished in 310.0166087150574, Train loss = 0.08021754244963328, Test loss = 0.09673744790256024; Train Acc = 0.9743500103553137, Test Acc = 0.9702000021934509\n",
      "Training at step=27, batch=0, train loss = 0.04717046394944191, train acc = 0.9850000143051147, time = 0.9533333778381348\n",
      "Training at step=27, batch=30, train loss = 0.11274436861276627, train acc = 0.9649999737739563, time = 0.9436790943145752\n",
      "Training at step=27, batch=60, train loss = 0.04760768264532089, train acc = 0.9850000143051147, time = 0.9484522342681885\n",
      "Training at step=27, batch=90, train loss = 0.0470006950199604, train acc = 0.9800000190734863, time = 0.9449312686920166\n",
      "Training at step=27, batch=120, train loss = 0.04691338539123535, train acc = 0.9850000143051147, time = 0.9388570785522461\n",
      "Training at step=27, batch=150, train loss = 0.03292106091976166, train acc = 0.9900000095367432, time = 0.9467251300811768\n",
      "Training at step=27, batch=180, train loss = 0.19449199736118317, train acc = 0.9599999785423279, time = 0.9373314380645752\n",
      "Training at step=27, batch=210, train loss = 0.04694916680455208, train acc = 0.9850000143051147, time = 0.9444410800933838\n",
      "Training at step=27, batch=240, train loss = 0.0891990214586258, train acc = 0.9700000286102295, time = 0.9414305686950684\n",
      "Training at step=27, batch=270, train loss = 0.07605041563510895, train acc = 0.9750000238418579, time = 0.9378294944763184\n",
      "Testing at step=27, batch=0, test loss = 0.15537817776203156, test acc = 0.949999988079071, time = 0.33760499954223633\n",
      "Testing at step=27, batch=5, test loss = 0.0919356718659401, test acc = 0.9750000238418579, time = 0.3381485939025879\n",
      "Testing at step=27, batch=10, test loss = 0.048921164125204086, test acc = 0.9900000095367432, time = 0.3402237892150879\n",
      "Testing at step=27, batch=15, test loss = 0.14852456748485565, test acc = 0.9449999928474426, time = 0.3396151065826416\n",
      "Testing at step=27, batch=20, test loss = 0.09267517924308777, test acc = 0.9649999737739563, time = 0.3398282527923584\n",
      "Testing at step=27, batch=25, test loss = 0.07613382488489151, test acc = 0.9850000143051147, time = 0.3416872024536133\n",
      "Testing at step=27, batch=30, test loss = 0.12950830161571503, test acc = 0.9599999785423279, time = 0.36736583709716797\n",
      "Testing at step=27, batch=35, test loss = 0.1548479050397873, test acc = 0.9700000286102295, time = 0.3602476119995117\n",
      "Testing at step=27, batch=40, test loss = 0.03525545820593834, test acc = 0.9800000190734863, time = 0.378359317779541\n",
      "Testing at step=27, batch=45, test loss = 0.07117626070976257, test acc = 0.9850000143051147, time = 0.35831522941589355\n",
      "Step 27 finished in 311.0265452861786, Train loss = 0.07867304858441154, Test loss = 0.08786565531045198; Train Acc = 0.9758833432197571, Test Acc = 0.9739000082015992\n",
      "Training at step=28, batch=0, train loss = 0.12185406684875488, train acc = 0.9700000286102295, time = 0.9395334720611572\n",
      "Training at step=28, batch=30, train loss = 0.05493055284023285, train acc = 0.9800000190734863, time = 0.9481141567230225\n",
      "Training at step=28, batch=60, train loss = 0.024758651852607727, train acc = 0.9900000095367432, time = 0.9431567192077637\n",
      "Training at step=28, batch=90, train loss = 0.056112971156835556, train acc = 0.9750000238418579, time = 0.9411029815673828\n",
      "Training at step=28, batch=120, train loss = 0.0602748766541481, train acc = 0.9800000190734863, time = 0.94171142578125\n",
      "Training at step=28, batch=150, train loss = 0.07414703071117401, train acc = 0.9750000238418579, time = 0.9471402168273926\n",
      "Training at step=28, batch=180, train loss = 0.0970255583524704, train acc = 0.9750000238418579, time = 0.9380919933319092\n",
      "Training at step=28, batch=210, train loss = 0.07870958000421524, train acc = 0.9649999737739563, time = 0.9642014503479004\n",
      "Training at step=28, batch=240, train loss = 0.05796985328197479, train acc = 0.9700000286102295, time = 0.9493460655212402\n",
      "Training at step=28, batch=270, train loss = 0.0664864107966423, train acc = 0.9750000238418579, time = 0.9402432441711426\n",
      "Testing at step=28, batch=0, test loss = 0.12065467238426208, test acc = 0.9599999785423279, time = 0.33841586112976074\n",
      "Testing at step=28, batch=5, test loss = 0.10389258712530136, test acc = 0.9549999833106995, time = 0.3359966278076172\n",
      "Testing at step=28, batch=10, test loss = 0.05887806788086891, test acc = 0.9800000190734863, time = 0.3379526138305664\n",
      "Testing at step=28, batch=15, test loss = 0.07256920635700226, test acc = 0.9750000238418579, time = 0.3389134407043457\n",
      "Testing at step=28, batch=20, test loss = 0.11000032722949982, test acc = 0.9750000238418579, time = 0.36055922508239746\n",
      "Testing at step=28, batch=25, test loss = 0.08438777178525925, test acc = 0.9700000286102295, time = 0.3628840446472168\n",
      "Testing at step=28, batch=30, test loss = 0.10212388634681702, test acc = 0.9599999785423279, time = 0.3376438617706299\n",
      "Testing at step=28, batch=35, test loss = 0.10263130068778992, test acc = 0.9599999785423279, time = 0.3398916721343994\n",
      "Testing at step=28, batch=40, test loss = 0.06781145930290222, test acc = 0.9850000143051147, time = 0.3352212905883789\n",
      "Testing at step=28, batch=45, test loss = 0.09164203703403473, test acc = 0.9800000190734863, time = 0.34116315841674805\n",
      "Step 28 finished in 310.63300585746765, Train loss = 0.07586300576105715, Test loss = 0.08523845966905355; Train Acc = 0.9759166753292083, Test Acc = 0.9749000072479248\n",
      "Training at step=29, batch=0, train loss = 0.04548069089651108, train acc = 0.9750000238418579, time = 0.9585905075073242\n",
      "Training at step=29, batch=30, train loss = 0.10543237626552582, train acc = 0.9750000238418579, time = 0.9409010410308838\n",
      "Training at step=29, batch=60, train loss = 0.02898240089416504, train acc = 0.9850000143051147, time = 0.9414045810699463\n",
      "Training at step=29, batch=90, train loss = 0.10303786396980286, train acc = 0.9599999785423279, time = 0.9370150566101074\n",
      "Training at step=29, batch=120, train loss = 0.1387987732887268, train acc = 0.9649999737739563, time = 0.9594089984893799\n",
      "Training at step=29, batch=150, train loss = 0.02153485268354416, train acc = 0.9900000095367432, time = 0.9392061233520508\n",
      "Training at step=29, batch=180, train loss = 0.053316425532102585, train acc = 0.9850000143051147, time = 0.9389283657073975\n",
      "Training at step=29, batch=210, train loss = 0.05818842723965645, train acc = 0.9800000190734863, time = 0.9522905349731445\n",
      "Training at step=29, batch=240, train loss = 0.10206517577171326, train acc = 0.949999988079071, time = 0.9433321952819824\n",
      "Training at step=29, batch=270, train loss = 0.07913967221975327, train acc = 0.9750000238418579, time = 0.9386448860168457\n",
      "Testing at step=29, batch=0, test loss = 0.08375756442546844, test acc = 0.9750000238418579, time = 0.33633852005004883\n",
      "Testing at step=29, batch=5, test loss = 0.10663451999425888, test acc = 0.9649999737739563, time = 0.3360283374786377\n",
      "Testing at step=29, batch=10, test loss = 0.12623654305934906, test acc = 0.9549999833106995, time = 0.3396720886230469\n",
      "Testing at step=29, batch=15, test loss = 0.0730845183134079, test acc = 0.9700000286102295, time = 0.3435084819793701\n",
      "Testing at step=29, batch=20, test loss = 0.05648445710539818, test acc = 0.9800000190734863, time = 0.34059715270996094\n",
      "Testing at step=29, batch=25, test loss = 0.13749776780605316, test acc = 0.9599999785423279, time = 0.3733844757080078\n",
      "Testing at step=29, batch=30, test loss = 0.10595783591270447, test acc = 0.9649999737739563, time = 0.3362865447998047\n",
      "Testing at step=29, batch=35, test loss = 0.0174301415681839, test acc = 1.0, time = 0.3394150733947754\n",
      "Testing at step=29, batch=40, test loss = 0.07067424058914185, test acc = 0.9700000286102295, time = 0.3400547504425049\n",
      "Testing at step=29, batch=45, test loss = 0.061784982681274414, test acc = 0.9750000238418579, time = 0.3396944999694824\n",
      "Step 29 finished in 311.59740829467773, Train loss = 0.07237513623200358, Test loss = 0.09423061542212963; Train Acc = 0.9769500106573105, Test Acc = 0.9703000044822693\n",
      "Training at step=30, batch=0, train loss = 0.06815016269683838, train acc = 0.9850000143051147, time = 0.9435598850250244\n",
      "Training at step=30, batch=30, train loss = 0.05006507784128189, train acc = 0.9800000190734863, time = 0.9443614482879639\n",
      "Training at step=30, batch=60, train loss = 0.021716222167015076, train acc = 0.9950000047683716, time = 0.9362757205963135\n",
      "Training at step=30, batch=90, train loss = 0.08188359439373016, train acc = 0.9649999737739563, time = 0.9407269954681396\n",
      "Training at step=30, batch=120, train loss = 0.03279750421643257, train acc = 0.9850000143051147, time = 0.9419491291046143\n",
      "Training at step=30, batch=150, train loss = 0.03855636343359947, train acc = 0.9850000143051147, time = 0.9422023296356201\n",
      "Training at step=30, batch=180, train loss = 0.060675088316202164, train acc = 0.9800000190734863, time = 0.9458253383636475\n",
      "Training at step=30, batch=210, train loss = 0.10117878019809723, train acc = 0.9800000190734863, time = 0.9410500526428223\n",
      "Training at step=30, batch=240, train loss = 0.0829358696937561, train acc = 0.9599999785423279, time = 0.9469854831695557\n",
      "Training at step=30, batch=270, train loss = 0.03726436197757721, train acc = 0.9950000047683716, time = 0.9422900676727295\n",
      "Testing at step=30, batch=0, test loss = 0.045762743800878525, test acc = 0.9850000143051147, time = 0.3533463478088379\n",
      "Testing at step=30, batch=5, test loss = 0.04538634419441223, test acc = 0.9900000095367432, time = 0.33974170684814453\n",
      "Testing at step=30, batch=10, test loss = 0.1473776400089264, test acc = 0.9549999833106995, time = 0.33859777450561523\n",
      "Testing at step=30, batch=15, test loss = 0.09413085132837296, test acc = 0.9700000286102295, time = 0.34920692443847656\n",
      "Testing at step=30, batch=20, test loss = 0.06269144266843796, test acc = 0.9900000095367432, time = 0.34107136726379395\n",
      "Testing at step=30, batch=25, test loss = 0.1179303377866745, test acc = 0.9599999785423279, time = 0.339402437210083\n",
      "Testing at step=30, batch=30, test loss = 0.16483788192272186, test acc = 0.949999988079071, time = 0.33780479431152344\n",
      "Testing at step=30, batch=35, test loss = 0.06183983013033867, test acc = 0.9750000238418579, time = 0.33657288551330566\n",
      "Testing at step=30, batch=40, test loss = 0.04082835093140602, test acc = 0.9750000238418579, time = 0.339907169342041\n",
      "Testing at step=30, batch=45, test loss = 0.0601494126021862, test acc = 0.9700000286102295, time = 0.3373992443084717\n",
      "Step 30 finished in 309.8245635032654, Train loss = 0.07116474566360315, Test loss = 0.09925032813102007; Train Acc = 0.9772833446661632, Test Acc = 0.9695000064373016\n",
      "Training at step=31, batch=0, train loss = 0.04310526326298714, train acc = 0.9850000143051147, time = 0.9501197338104248\n",
      "Training at step=31, batch=30, train loss = 0.07439020276069641, train acc = 0.9750000238418579, time = 0.9403815269470215\n",
      "Training at step=31, batch=60, train loss = 0.06370509415864944, train acc = 0.9700000286102295, time = 0.9397428035736084\n",
      "Training at step=31, batch=90, train loss = 0.04709404334425926, train acc = 0.9750000238418579, time = 0.9392714500427246\n",
      "Training at step=31, batch=120, train loss = 0.0889410674571991, train acc = 0.9750000238418579, time = 0.9454536437988281\n",
      "Training at step=31, batch=150, train loss = 0.1698942333459854, train acc = 0.9700000286102295, time = 0.9374399185180664\n",
      "Training at step=31, batch=180, train loss = 0.03526782989501953, train acc = 0.9850000143051147, time = 0.937248706817627\n",
      "Training at step=31, batch=210, train loss = 0.07309023290872574, train acc = 0.9800000190734863, time = 0.9476873874664307\n",
      "Training at step=31, batch=240, train loss = 0.09327918291091919, train acc = 0.9649999737739563, time = 1.0030767917633057\n",
      "Training at step=31, batch=270, train loss = 0.08497840911149979, train acc = 0.9649999737739563, time = 0.95963454246521\n",
      "Testing at step=31, batch=0, test loss = 0.06890098750591278, test acc = 0.9850000143051147, time = 0.3436288833618164\n",
      "Testing at step=31, batch=5, test loss = 0.07258381694555283, test acc = 0.9750000238418579, time = 0.33994269371032715\n",
      "Testing at step=31, batch=10, test loss = 0.14202529191970825, test acc = 0.9649999737739563, time = 0.3420555591583252\n",
      "Testing at step=31, batch=15, test loss = 0.017778150737285614, test acc = 0.9950000047683716, time = 0.3373239040374756\n",
      "Testing at step=31, batch=20, test loss = 0.07587610930204391, test acc = 0.9750000238418579, time = 0.3427917957305908\n",
      "Testing at step=31, batch=25, test loss = 0.03699096292257309, test acc = 0.9850000143051147, time = 0.3587965965270996\n",
      "Testing at step=31, batch=30, test loss = 0.05170057341456413, test acc = 0.9850000143051147, time = 0.3424229621887207\n",
      "Testing at step=31, batch=35, test loss = 0.07169292867183685, test acc = 0.9800000190734863, time = 0.3602597713470459\n",
      "Testing at step=31, batch=40, test loss = 0.07910633832216263, test acc = 0.9700000286102295, time = 0.3513314723968506\n",
      "Testing at step=31, batch=45, test loss = 0.10427412390708923, test acc = 0.9750000238418579, time = 0.3367178440093994\n",
      "Step 31 finished in 312.1542537212372, Train loss = 0.06811543559965988, Test loss = 0.08843913957476617; Train Acc = 0.9777333456277847, Test Acc = 0.973200011253357\n",
      "Training at step=32, batch=0, train loss = 0.05746643990278244, train acc = 0.9850000143051147, time = 0.9457788467407227\n",
      "Training at step=32, batch=30, train loss = 0.055304158478975296, train acc = 0.9700000286102295, time = 0.9770138263702393\n",
      "Training at step=32, batch=60, train loss = 0.05462387576699257, train acc = 0.9750000238418579, time = 0.9390795230865479\n",
      "Training at step=32, batch=90, train loss = 0.049951910972595215, train acc = 0.9800000190734863, time = 0.9409487247467041\n",
      "Training at step=32, batch=120, train loss = 0.08774672448635101, train acc = 0.9750000238418579, time = 0.947016716003418\n",
      "Training at step=32, batch=150, train loss = 0.06842736899852753, train acc = 0.9750000238418579, time = 0.944774866104126\n",
      "Training at step=32, batch=180, train loss = 0.05106250196695328, train acc = 0.9900000095367432, time = 0.9480187892913818\n",
      "Training at step=32, batch=210, train loss = 0.047041840851306915, train acc = 0.9950000047683716, time = 0.9450967311859131\n",
      "Training at step=32, batch=240, train loss = 0.07158099859952927, train acc = 0.9850000143051147, time = 0.9648263454437256\n",
      "Training at step=32, batch=270, train loss = 0.037110358476638794, train acc = 0.9850000143051147, time = 0.9401876926422119\n",
      "Testing at step=32, batch=0, test loss = 0.09129448235034943, test acc = 0.9700000286102295, time = 0.3614497184753418\n",
      "Testing at step=32, batch=5, test loss = 0.0977807268500328, test acc = 0.9649999737739563, time = 0.35616540908813477\n",
      "Testing at step=32, batch=10, test loss = 0.07732709497213364, test acc = 0.9700000286102295, time = 0.33808350563049316\n",
      "Testing at step=32, batch=15, test loss = 0.06365979462862015, test acc = 0.9850000143051147, time = 0.33820605278015137\n",
      "Testing at step=32, batch=20, test loss = 0.08905825763940811, test acc = 0.9750000238418579, time = 0.33919215202331543\n",
      "Testing at step=32, batch=25, test loss = 0.06497640162706375, test acc = 0.9850000143051147, time = 0.34000492095947266\n",
      "Testing at step=32, batch=30, test loss = 0.12018416076898575, test acc = 0.9750000238418579, time = 0.33904194831848145\n",
      "Testing at step=32, batch=35, test loss = 0.15649671852588654, test acc = 0.9549999833106995, time = 0.33851051330566406\n",
      "Testing at step=32, batch=40, test loss = 0.047455042600631714, test acc = 0.9850000143051147, time = 0.3430171012878418\n",
      "Testing at step=32, batch=45, test loss = 0.07548023015260696, test acc = 0.9700000286102295, time = 0.341017484664917\n",
      "Step 32 finished in 310.4731676578522, Train loss = 0.06920122971137364, Test loss = 0.08725658409297467; Train Acc = 0.9783500121037165, Test Acc = 0.9744000089168549\n",
      "Training at step=33, batch=0, train loss = 0.054896339774131775, train acc = 0.9800000190734863, time = 0.9404609203338623\n",
      "Training at step=33, batch=30, train loss = 0.05083567649126053, train acc = 0.9900000095367432, time = 0.9423415660858154\n",
      "Training at step=33, batch=60, train loss = 0.07674732059240341, train acc = 0.9800000190734863, time = 0.9445281028747559\n",
      "Training at step=33, batch=90, train loss = 0.0595121756196022, train acc = 0.9850000143051147, time = 0.9423074722290039\n",
      "Training at step=33, batch=120, train loss = 0.08662217110395432, train acc = 0.9649999737739563, time = 0.9574308395385742\n",
      "Training at step=33, batch=150, train loss = 0.06425845623016357, train acc = 0.9800000190734863, time = 0.9395618438720703\n",
      "Training at step=33, batch=180, train loss = 0.06668650358915329, train acc = 0.9850000143051147, time = 0.9485442638397217\n",
      "Training at step=33, batch=210, train loss = 0.06703737378120422, train acc = 0.9700000286102295, time = 0.9521627426147461\n",
      "Training at step=33, batch=240, train loss = 0.05052247643470764, train acc = 0.9850000143051147, time = 0.9516429901123047\n",
      "Training at step=33, batch=270, train loss = 0.04572629928588867, train acc = 0.9850000143051147, time = 0.9428009986877441\n",
      "Testing at step=33, batch=0, test loss = 0.05766182020306587, test acc = 0.9750000238418579, time = 0.3494257926940918\n",
      "Testing at step=33, batch=5, test loss = 0.057964224368333817, test acc = 0.9800000190734863, time = 0.3383617401123047\n",
      "Testing at step=33, batch=10, test loss = 0.18633070588111877, test acc = 0.949999988079071, time = 0.3335843086242676\n",
      "Testing at step=33, batch=15, test loss = 0.082033671438694, test acc = 0.9750000238418579, time = 0.33605360984802246\n",
      "Testing at step=33, batch=20, test loss = 0.10349540412425995, test acc = 0.9750000238418579, time = 0.3662259578704834\n",
      "Testing at step=33, batch=25, test loss = 0.08986620604991913, test acc = 0.9750000238418579, time = 0.3403604030609131\n",
      "Testing at step=33, batch=30, test loss = 0.07228410989046097, test acc = 0.9900000095367432, time = 0.3386874198913574\n",
      "Testing at step=33, batch=35, test loss = 0.06574691087007523, test acc = 0.9800000190734863, time = 0.3374202251434326\n",
      "Testing at step=33, batch=40, test loss = 0.11603657901287079, test acc = 0.9599999785423279, time = 0.3364372253417969\n",
      "Testing at step=33, batch=45, test loss = 0.021438295021653175, test acc = 0.9900000095367432, time = 0.3339715003967285\n",
      "Step 33 finished in 310.9325485229492, Train loss = 0.06532058997390171, Test loss = 0.08907259423285722; Train Acc = 0.9795500121514003, Test Acc = 0.9736000061035156\n",
      "Training at step=34, batch=0, train loss = 0.08455800265073776, train acc = 0.9750000238418579, time = 0.9377686977386475\n",
      "Training at step=34, batch=30, train loss = 0.039562828838825226, train acc = 0.9850000143051147, time = 0.9398922920227051\n",
      "Training at step=34, batch=60, train loss = 0.05436863377690315, train acc = 0.9750000238418579, time = 0.9439601898193359\n",
      "Training at step=34, batch=90, train loss = 0.06195877864956856, train acc = 0.9750000238418579, time = 0.9577944278717041\n",
      "Training at step=34, batch=120, train loss = 0.08947116136550903, train acc = 0.9700000286102295, time = 0.9612305164337158\n",
      "Training at step=34, batch=150, train loss = 0.08387947082519531, train acc = 0.9700000286102295, time = 0.9629273414611816\n",
      "Training at step=34, batch=180, train loss = 0.07505256682634354, train acc = 0.9800000190734863, time = 0.9377031326293945\n",
      "Training at step=34, batch=210, train loss = 0.030337966978549957, train acc = 0.9850000143051147, time = 0.9359636306762695\n",
      "Training at step=34, batch=240, train loss = 0.03207904472947121, train acc = 0.9900000095367432, time = 0.9350817203521729\n",
      "Training at step=34, batch=270, train loss = 0.04859136417508125, train acc = 0.9800000190734863, time = 0.9367644786834717\n",
      "Testing at step=34, batch=0, test loss = 0.027998482808470726, test acc = 0.9850000143051147, time = 0.34975337982177734\n",
      "Testing at step=34, batch=5, test loss = 0.048292942345142365, test acc = 0.9900000095367432, time = 0.3389394283294678\n",
      "Testing at step=34, batch=10, test loss = 0.06494473665952682, test acc = 0.9850000143051147, time = 0.35869765281677246\n",
      "Testing at step=34, batch=15, test loss = 0.169066920876503, test acc = 0.9750000238418579, time = 0.34711217880249023\n",
      "Testing at step=34, batch=20, test loss = 0.07108102738857269, test acc = 0.9700000286102295, time = 0.3377249240875244\n",
      "Testing at step=34, batch=25, test loss = 0.10583789646625519, test acc = 0.9800000190734863, time = 0.3378870487213135\n",
      "Testing at step=34, batch=30, test loss = 0.10364213585853577, test acc = 0.9599999785423279, time = 0.34006309509277344\n",
      "Testing at step=34, batch=35, test loss = 0.04679323360323906, test acc = 0.9800000190734863, time = 0.33711671829223633\n",
      "Testing at step=34, batch=40, test loss = 0.10155686736106873, test acc = 0.9700000286102295, time = 0.34166455268859863\n",
      "Testing at step=34, batch=45, test loss = 0.0842997282743454, test acc = 0.9850000143051147, time = 0.3403456211090088\n",
      "Step 34 finished in 311.73150992393494, Train loss = 0.06260822881789257, Test loss = 0.08645881626754999; Train Acc = 0.9797666794061661, Test Acc = 0.9736000084877015\n",
      "Training at step=35, batch=0, train loss = 0.12636040151119232, train acc = 0.9750000238418579, time = 0.9426479339599609\n",
      "Training at step=35, batch=30, train loss = 0.045824743807315826, train acc = 0.9700000286102295, time = 0.9735195636749268\n",
      "Training at step=35, batch=60, train loss = 0.08125827461481094, train acc = 0.9700000286102295, time = 0.9507291316986084\n",
      "Training at step=35, batch=90, train loss = 0.04499804601073265, train acc = 0.9850000143051147, time = 0.9452226161956787\n",
      "Training at step=35, batch=120, train loss = 0.02353566698729992, train acc = 1.0, time = 0.9373378753662109\n",
      "Training at step=35, batch=150, train loss = 0.05198968946933746, train acc = 0.9800000190734863, time = 0.9564917087554932\n",
      "Training at step=35, batch=180, train loss = 0.02582368068397045, train acc = 0.9900000095367432, time = 0.9443151950836182\n",
      "Training at step=35, batch=210, train loss = 0.03655974939465523, train acc = 0.9950000047683716, time = 0.9480018615722656\n",
      "Training at step=35, batch=240, train loss = 0.05162361264228821, train acc = 0.9850000143051147, time = 1.3833529949188232\n",
      "Training at step=35, batch=270, train loss = 0.07112430781126022, train acc = 0.9750000238418579, time = 1.3415961265563965\n",
      "Testing at step=35, batch=0, test loss = 0.0714036300778389, test acc = 0.9850000143051147, time = 0.3449556827545166\n",
      "Testing at step=35, batch=5, test loss = 0.12350047379732132, test acc = 0.9599999785423279, time = 0.33738136291503906\n",
      "Testing at step=35, batch=10, test loss = 0.04002392664551735, test acc = 0.9850000143051147, time = 0.34153032302856445\n",
      "Testing at step=35, batch=15, test loss = 0.16852939128875732, test acc = 0.9700000286102295, time = 0.34202051162719727\n",
      "Testing at step=35, batch=20, test loss = 0.13992062211036682, test acc = 0.949999988079071, time = 0.34311485290527344\n",
      "Testing at step=35, batch=25, test loss = 0.08513599634170532, test acc = 0.9900000095367432, time = 0.34370970726013184\n",
      "Testing at step=35, batch=30, test loss = 0.10299201309680939, test acc = 0.9800000190734863, time = 0.334855318069458\n",
      "Testing at step=35, batch=35, test loss = 0.15224240720272064, test acc = 0.9700000286102295, time = 0.3383667469024658\n",
      "Testing at step=35, batch=40, test loss = 0.13447920978069305, test acc = 0.9549999833106995, time = 0.35539984703063965\n",
      "Testing at step=35, batch=45, test loss = 0.08256220072507858, test acc = 0.9649999737739563, time = 0.33835816383361816\n",
      "Step 35 finished in 362.80381965637207, Train loss = 0.06347945230081678, Test loss = 0.09272587433457374; Train Acc = 0.9793666785955429, Test Acc = 0.9726000118255616\n",
      "Training at step=36, batch=0, train loss = 0.03944692015647888, train acc = 0.9900000095367432, time = 0.9463956356048584\n",
      "Training at step=36, batch=30, train loss = 0.06355535984039307, train acc = 0.9800000190734863, time = 0.9387609958648682\n",
      "Training at step=36, batch=60, train loss = 0.07273588329553604, train acc = 0.9950000047683716, time = 0.9449529647827148\n",
      "Training at step=36, batch=90, train loss = 0.015959450975060463, train acc = 0.9950000047683716, time = 0.9429779052734375\n",
      "Training at step=36, batch=120, train loss = 0.07112985849380493, train acc = 0.9750000238418579, time = 0.9442875385284424\n",
      "Training at step=36, batch=150, train loss = 0.09464320540428162, train acc = 0.9750000238418579, time = 0.9406263828277588\n",
      "Training at step=36, batch=180, train loss = 0.06626174598932266, train acc = 0.9800000190734863, time = 0.9488387107849121\n",
      "Training at step=36, batch=210, train loss = 0.06448996812105179, train acc = 0.9750000238418579, time = 0.9428625106811523\n",
      "Training at step=36, batch=240, train loss = 0.02907780185341835, train acc = 0.9950000047683716, time = 0.9390347003936768\n",
      "Training at step=36, batch=270, train loss = 0.026595773175358772, train acc = 0.9900000095367432, time = 0.9414339065551758\n",
      "Testing at step=36, batch=0, test loss = 0.06765702366828918, test acc = 0.9850000143051147, time = 0.3357203006744385\n",
      "Testing at step=36, batch=5, test loss = 0.03855622932314873, test acc = 0.9900000095367432, time = 0.3351149559020996\n",
      "Testing at step=36, batch=10, test loss = 0.1288793683052063, test acc = 0.9649999737739563, time = 0.3359375\n",
      "Testing at step=36, batch=15, test loss = 0.11412449181079865, test acc = 0.9750000238418579, time = 0.33435821533203125\n",
      "Testing at step=36, batch=20, test loss = 0.06499072909355164, test acc = 0.9850000143051147, time = 0.3373711109161377\n",
      "Testing at step=36, batch=25, test loss = 0.0983562171459198, test acc = 0.9649999737739563, time = 0.3356797695159912\n",
      "Testing at step=36, batch=30, test loss = 0.07157078385353088, test acc = 0.9850000143051147, time = 0.3344416618347168\n",
      "Testing at step=36, batch=35, test loss = 0.11228125542402267, test acc = 0.9649999737739563, time = 0.36244702339172363\n",
      "Testing at step=36, batch=40, test loss = 0.04680800810456276, test acc = 0.9900000095367432, time = 0.33822107315063477\n",
      "Testing at step=36, batch=45, test loss = 0.14458082616329193, test acc = 0.9649999737739563, time = 0.33740735054016113\n",
      "Step 36 finished in 310.02768063545227, Train loss = 0.06092489382252097, Test loss = 0.09045427853241562; Train Acc = 0.9802166791756948, Test Acc = 0.9725000023841858\n",
      "Training at step=37, batch=0, train loss = 0.03579721599817276, train acc = 0.9950000047683716, time = 0.9481608867645264\n",
      "Training at step=37, batch=30, train loss = 0.09549371898174286, train acc = 0.9649999737739563, time = 0.9385018348693848\n",
      "Training at step=37, batch=60, train loss = 0.03955744579434395, train acc = 0.9800000190734863, time = 0.9393723011016846\n",
      "Training at step=37, batch=90, train loss = 0.029309283941984177, train acc = 0.9950000047683716, time = 0.9479200839996338\n",
      "Training at step=37, batch=120, train loss = 0.023907141759991646, train acc = 0.9900000095367432, time = 0.9437410831451416\n",
      "Training at step=37, batch=150, train loss = 0.04850050061941147, train acc = 0.9850000143051147, time = 0.939244270324707\n",
      "Training at step=37, batch=180, train loss = 0.03603051230311394, train acc = 0.9800000190734863, time = 0.9402282238006592\n",
      "Training at step=37, batch=210, train loss = 0.055556539446115494, train acc = 0.9850000143051147, time = 0.9377672672271729\n",
      "Training at step=37, batch=240, train loss = 0.09804065525531769, train acc = 0.9549999833106995, time = 0.9401028156280518\n",
      "Training at step=37, batch=270, train loss = 0.08021508902311325, train acc = 0.9700000286102295, time = 0.9447972774505615\n",
      "Testing at step=37, batch=0, test loss = 0.08299435675144196, test acc = 0.9750000238418579, time = 0.3407430648803711\n",
      "Testing at step=37, batch=5, test loss = 0.0682104304432869, test acc = 0.9800000190734863, time = 0.34241604804992676\n",
      "Testing at step=37, batch=10, test loss = 0.0677335187792778, test acc = 0.9800000190734863, time = 0.3392820358276367\n",
      "Testing at step=37, batch=15, test loss = 0.11043208837509155, test acc = 0.9750000238418579, time = 0.33849000930786133\n",
      "Testing at step=37, batch=20, test loss = 0.07267114520072937, test acc = 0.9850000143051147, time = 0.3536062240600586\n",
      "Testing at step=37, batch=25, test loss = 0.09990793466567993, test acc = 0.9700000286102295, time = 0.340090274810791\n",
      "Testing at step=37, batch=30, test loss = 0.09912142157554626, test acc = 0.9700000286102295, time = 0.33942127227783203\n",
      "Testing at step=37, batch=35, test loss = 0.13590598106384277, test acc = 0.9549999833106995, time = 0.3541419506072998\n",
      "Testing at step=37, batch=40, test loss = 0.0713852196931839, test acc = 0.9800000190734863, time = 0.33916616439819336\n",
      "Testing at step=37, batch=45, test loss = 0.05892385169863701, test acc = 0.9850000143051147, time = 0.36620354652404785\n",
      "Step 37 finished in 310.59811878204346, Train loss = 0.06090085062819223, Test loss = 0.08724132806062698; Train Acc = 0.9802833453814188, Test Acc = 0.973400012254715\n",
      "Training at step=38, batch=0, train loss = 0.05674907565116882, train acc = 0.9750000238418579, time = 0.9412112236022949\n",
      "Training at step=38, batch=30, train loss = 0.019628887996077538, train acc = 0.9950000047683716, time = 0.9451022148132324\n",
      "Training at step=38, batch=60, train loss = 0.0647914931178093, train acc = 0.9800000190734863, time = 0.97756028175354\n",
      "Training at step=38, batch=90, train loss = 0.07347504049539566, train acc = 0.9800000190734863, time = 0.9430646896362305\n",
      "Training at step=38, batch=120, train loss = 0.04227721691131592, train acc = 0.9850000143051147, time = 0.941115140914917\n",
      "Training at step=38, batch=150, train loss = 0.06556344777345657, train acc = 0.9750000238418579, time = 0.9395463466644287\n",
      "Training at step=38, batch=180, train loss = 0.04874182119965553, train acc = 0.9850000143051147, time = 0.93853759765625\n",
      "Training at step=38, batch=210, train loss = 0.03466786444187164, train acc = 0.9900000095367432, time = 0.9443392753601074\n",
      "Training at step=38, batch=240, train loss = 0.09140574187040329, train acc = 0.9700000286102295, time = 0.9383060932159424\n",
      "Training at step=38, batch=270, train loss = 0.050802115350961685, train acc = 0.9700000286102295, time = 0.9619033336639404\n",
      "Testing at step=38, batch=0, test loss = 0.027070648968219757, test acc = 0.9850000143051147, time = 0.3378875255584717\n",
      "Testing at step=38, batch=5, test loss = 0.07991530001163483, test acc = 0.9649999737739563, time = 0.33859968185424805\n",
      "Testing at step=38, batch=10, test loss = 0.13877679407596588, test acc = 0.9599999785423279, time = 0.342693567276001\n",
      "Testing at step=38, batch=15, test loss = 0.052646707743406296, test acc = 0.9800000190734863, time = 0.36053466796875\n",
      "Testing at step=38, batch=20, test loss = 0.07719434052705765, test acc = 0.9750000238418579, time = 0.4213676452636719\n",
      "Testing at step=38, batch=25, test loss = 0.06805668026208878, test acc = 0.9750000238418579, time = 0.3642146587371826\n",
      "Testing at step=38, batch=30, test loss = 0.05904038995504379, test acc = 0.9800000190734863, time = 0.339754581451416\n",
      "Testing at step=38, batch=35, test loss = 0.12696141004562378, test acc = 0.949999988079071, time = 0.3423326015472412\n",
      "Testing at step=38, batch=40, test loss = 0.07197120785713196, test acc = 0.9750000238418579, time = 0.3360788822174072\n",
      "Testing at step=38, batch=45, test loss = 0.11687371879816055, test acc = 0.9800000190734863, time = 0.337721586227417\n",
      "Step 38 finished in 310.8593761920929, Train loss = 0.057924752387528616, Test loss = 0.08671195454895496; Train Acc = 0.9813000138600667, Test Acc = 0.9729000091552734\n",
      "Training at step=39, batch=0, train loss = 0.047785282135009766, train acc = 0.9850000143051147, time = 0.9400894641876221\n",
      "Training at step=39, batch=30, train loss = 0.04501956328749657, train acc = 0.9800000190734863, time = 0.9431493282318115\n",
      "Training at step=39, batch=60, train loss = 0.1268710494041443, train acc = 0.9649999737739563, time = 0.9434046745300293\n",
      "Training at step=39, batch=90, train loss = 0.09471369534730911, train acc = 0.9599999785423279, time = 0.9408969879150391\n",
      "Training at step=39, batch=120, train loss = 0.034128252416849136, train acc = 0.9850000143051147, time = 0.9415988922119141\n",
      "Training at step=39, batch=150, train loss = 0.06607576459646225, train acc = 0.9800000190734863, time = 0.9419302940368652\n",
      "Training at step=39, batch=180, train loss = 0.05321233719587326, train acc = 0.9800000190734863, time = 0.9432079792022705\n",
      "Training at step=39, batch=210, train loss = 0.07352200150489807, train acc = 0.9800000190734863, time = 0.9449150562286377\n",
      "Training at step=39, batch=240, train loss = 0.049047891050577164, train acc = 0.9800000190734863, time = 0.9397132396697998\n",
      "Training at step=39, batch=270, train loss = 0.06041315943002701, train acc = 0.9900000095367432, time = 0.9393532276153564\n",
      "Testing at step=39, batch=0, test loss = 0.03854899853467941, test acc = 0.9850000143051147, time = 0.33730483055114746\n",
      "Testing at step=39, batch=5, test loss = 0.0537833534181118, test acc = 0.9850000143051147, time = 0.3455021381378174\n",
      "Testing at step=39, batch=10, test loss = 0.08328680694103241, test acc = 0.9750000238418579, time = 0.34097719192504883\n",
      "Testing at step=39, batch=15, test loss = 0.049433384090662, test acc = 0.9900000095367432, time = 0.33544039726257324\n",
      "Testing at step=39, batch=20, test loss = 0.09103474020957947, test acc = 0.9750000238418579, time = 0.3338429927825928\n",
      "Testing at step=39, batch=25, test loss = 0.10866246372461319, test acc = 0.9599999785423279, time = 0.3377840518951416\n",
      "Testing at step=39, batch=30, test loss = 0.07455351948738098, test acc = 0.9850000143051147, time = 0.3382840156555176\n",
      "Testing at step=39, batch=35, test loss = 0.06372905522584915, test acc = 0.9649999737739563, time = 0.3359262943267822\n",
      "Testing at step=39, batch=40, test loss = 0.10628757625818253, test acc = 0.9750000238418579, time = 0.3349127769470215\n",
      "Testing at step=39, batch=45, test loss = 0.11335211992263794, test acc = 0.9700000286102295, time = 0.3346095085144043\n",
      "Step 39 finished in 310.37246012687683, Train loss = 0.05554218132824947, Test loss = 0.08875229075551033; Train Acc = 0.9822500133514405, Test Acc = 0.9760000133514404\n",
      "Training at step=40, batch=0, train loss = 0.038852520287036896, train acc = 0.9800000190734863, time = 0.952883243560791\n",
      "Training at step=40, batch=30, train loss = 0.06182081624865532, train acc = 0.9800000190734863, time = 0.961716890335083\n",
      "Training at step=40, batch=60, train loss = 0.030601579695940018, train acc = 0.9900000095367432, time = 0.9386601448059082\n",
      "Training at step=40, batch=90, train loss = 0.10535482317209244, train acc = 0.9800000190734863, time = 0.9602198600769043\n",
      "Training at step=40, batch=120, train loss = 0.04672228917479515, train acc = 0.9900000095367432, time = 0.9339132308959961\n",
      "Training at step=40, batch=150, train loss = 0.04581998661160469, train acc = 0.9850000143051147, time = 0.939265251159668\n",
      "Training at step=40, batch=180, train loss = 0.13790491223335266, train acc = 0.9700000286102295, time = 0.9381515979766846\n",
      "Training at step=40, batch=210, train loss = 0.07624604552984238, train acc = 0.9700000286102295, time = 0.9379277229309082\n",
      "Training at step=40, batch=240, train loss = 0.07030855119228363, train acc = 0.9900000095367432, time = 0.9356105327606201\n",
      "Training at step=40, batch=270, train loss = 0.0781514048576355, train acc = 0.9649999737739563, time = 0.9399447441101074\n",
      "Testing at step=40, batch=0, test loss = 0.06073667109012604, test acc = 0.9850000143051147, time = 0.33504605293273926\n",
      "Testing at step=40, batch=5, test loss = 0.09209860116243362, test acc = 0.9800000190734863, time = 0.335216760635376\n",
      "Testing at step=40, batch=10, test loss = 0.06227881461381912, test acc = 0.9850000143051147, time = 0.33760929107666016\n",
      "Testing at step=40, batch=15, test loss = 0.11502302438020706, test acc = 0.9599999785423279, time = 0.3327329158782959\n",
      "Testing at step=40, batch=20, test loss = 0.13021999597549438, test acc = 0.9649999737739563, time = 0.3348078727722168\n",
      "Testing at step=40, batch=25, test loss = 0.04950784146785736, test acc = 0.9800000190734863, time = 0.3407566547393799\n",
      "Testing at step=40, batch=30, test loss = 0.04983130469918251, test acc = 0.9750000238418579, time = 0.34267210960388184\n",
      "Testing at step=40, batch=35, test loss = 0.06779888272285461, test acc = 0.9800000190734863, time = 0.33280181884765625\n",
      "Testing at step=40, batch=40, test loss = 0.05184103548526764, test acc = 0.9850000143051147, time = 0.33454060554504395\n",
      "Testing at step=40, batch=45, test loss = 0.06951569765806198, test acc = 0.9800000190734863, time = 0.4203379154205322\n",
      "Step 40 finished in 310.54098105430603, Train loss = 0.05577064415129523, Test loss = 0.08317836306989193; Train Acc = 0.9818166806300481, Test Acc = 0.9758000123500824\n",
      "Training at step=41, batch=0, train loss = 0.02414591796696186, train acc = 0.9950000047683716, time = 0.9351105690002441\n",
      "Training at step=41, batch=30, train loss = 0.05526987090706825, train acc = 0.9850000143051147, time = 0.9378771781921387\n",
      "Training at step=41, batch=60, train loss = 0.021405158564448357, train acc = 0.9900000095367432, time = 0.9412639141082764\n",
      "Training at step=41, batch=90, train loss = 0.057891640812158585, train acc = 0.9750000238418579, time = 0.9362492561340332\n",
      "Training at step=41, batch=120, train loss = 0.05940411984920502, train acc = 0.9850000143051147, time = 0.9450256824493408\n",
      "Training at step=41, batch=150, train loss = 0.07615458220243454, train acc = 0.9800000190734863, time = 0.9364175796508789\n",
      "Training at step=41, batch=180, train loss = 0.06372731178998947, train acc = 0.9700000286102295, time = 0.9365277290344238\n",
      "Training at step=41, batch=210, train loss = 0.07334718108177185, train acc = 0.9750000238418579, time = 0.9353320598602295\n",
      "Training at step=41, batch=240, train loss = 0.04918818548321724, train acc = 0.9900000095367432, time = 0.9343266487121582\n",
      "Training at step=41, batch=270, train loss = 0.04820440709590912, train acc = 0.9900000095367432, time = 0.9358031749725342\n",
      "Testing at step=41, batch=0, test loss = 0.056805502623319626, test acc = 0.9800000190734863, time = 0.3355989456176758\n",
      "Testing at step=41, batch=5, test loss = 0.01960272714495659, test acc = 0.9900000095367432, time = 0.33835721015930176\n",
      "Testing at step=41, batch=10, test loss = 0.13731703162193298, test acc = 0.9900000095367432, time = 0.3372924327850342\n",
      "Testing at step=41, batch=15, test loss = 0.10222624987363815, test acc = 0.9750000238418579, time = 0.33851075172424316\n",
      "Testing at step=41, batch=20, test loss = 0.07488571852445602, test acc = 0.9750000238418579, time = 0.3355731964111328\n",
      "Testing at step=41, batch=25, test loss = 0.1433568149805069, test acc = 0.9549999833106995, time = 0.34618353843688965\n",
      "Testing at step=41, batch=30, test loss = 0.11191944032907486, test acc = 0.9750000238418579, time = 0.3365483283996582\n",
      "Testing at step=41, batch=35, test loss = 0.07255169749259949, test acc = 0.9800000190734863, time = 0.33612060546875\n",
      "Testing at step=41, batch=40, test loss = 0.03809330239892006, test acc = 0.9750000238418579, time = 0.3372623920440674\n",
      "Testing at step=41, batch=45, test loss = 0.10554510354995728, test acc = 0.9649999737739563, time = 0.34023499488830566\n",
      "Step 41 finished in 309.771653175354, Train loss = 0.05371105669376751, Test loss = 0.08788834773004055; Train Acc = 0.9831166803836823, Test Acc = 0.9749000108242035\n",
      "Training at step=42, batch=0, train loss = 0.06688797473907471, train acc = 0.9700000286102295, time = 0.9377949237823486\n",
      "Training at step=42, batch=30, train loss = 0.04202548414468765, train acc = 0.9900000095367432, time = 0.9399621486663818\n",
      "Training at step=42, batch=60, train loss = 0.037078607827425, train acc = 0.9800000190734863, time = 0.9436793327331543\n",
      "Training at step=42, batch=90, train loss = 0.08083439618349075, train acc = 0.9850000143051147, time = 0.9472157955169678\n",
      "Training at step=42, batch=120, train loss = 0.06269438564777374, train acc = 0.9800000190734863, time = 0.9624052047729492\n",
      "Training at step=42, batch=150, train loss = 0.0715869590640068, train acc = 0.9800000190734863, time = 0.9500439167022705\n",
      "Training at step=42, batch=180, train loss = 0.10614006966352463, train acc = 0.949999988079071, time = 1.382552146911621\n",
      "Training at step=42, batch=210, train loss = 0.056525178253650665, train acc = 0.9750000238418579, time = 1.443774938583374\n",
      "Training at step=42, batch=240, train loss = 0.029584085568785667, train acc = 0.9900000095367432, time = 0.9614152908325195\n",
      "Training at step=42, batch=270, train loss = 0.0841032862663269, train acc = 0.9700000286102295, time = 0.9640882015228271\n",
      "Testing at step=42, batch=0, test loss = 0.02648903615772724, test acc = 0.9900000095367432, time = 0.3382742404937744\n",
      "Testing at step=42, batch=5, test loss = 0.11405280977487564, test acc = 0.9800000190734863, time = 0.33800363540649414\n",
      "Testing at step=42, batch=10, test loss = 0.058308571577072144, test acc = 0.9800000190734863, time = 0.3471369743347168\n",
      "Testing at step=42, batch=15, test loss = 0.05399756878614426, test acc = 0.9750000238418579, time = 0.3378565311431885\n",
      "Testing at step=42, batch=20, test loss = 0.06227605417370796, test acc = 0.9900000095367432, time = 0.3381764888763428\n",
      "Testing at step=42, batch=25, test loss = 0.13993340730667114, test acc = 0.9750000238418579, time = 0.33764028549194336\n",
      "Testing at step=42, batch=30, test loss = 0.10435601323843002, test acc = 0.9700000286102295, time = 0.33814239501953125\n",
      "Testing at step=42, batch=35, test loss = 0.12795689702033997, test acc = 0.9750000238418579, time = 0.3489525318145752\n",
      "Testing at step=42, batch=40, test loss = 0.07549739629030228, test acc = 0.9750000238418579, time = 0.3447844982147217\n",
      "Testing at step=42, batch=45, test loss = 0.0508650504052639, test acc = 0.9850000143051147, time = 0.33532285690307617\n",
      "Step 42 finished in 361.56066250801086, Train loss = 0.05134502121247351, Test loss = 0.08657433526590466; Train Acc = 0.9834666794538498, Test Acc = 0.9756000137329102\n",
      "Training at step=43, batch=0, train loss = 0.030788030475378036, train acc = 0.9850000143051147, time = 0.9424135684967041\n",
      "Training at step=43, batch=30, train loss = 0.024450959637761116, train acc = 1.0, time = 0.9418580532073975\n",
      "Training at step=43, batch=60, train loss = 0.015532681718468666, train acc = 1.0, time = 0.9385817050933838\n",
      "Training at step=43, batch=90, train loss = 0.07555004209280014, train acc = 0.9750000238418579, time = 0.937201738357544\n",
      "Training at step=43, batch=120, train loss = 0.025486096739768982, train acc = 0.9900000095367432, time = 0.9422774314880371\n",
      "Training at step=43, batch=150, train loss = 0.06091148778796196, train acc = 0.9800000190734863, time = 0.9352831840515137\n",
      "Training at step=43, batch=180, train loss = 0.06408649682998657, train acc = 0.9700000286102295, time = 0.9754726886749268\n",
      "Training at step=43, batch=210, train loss = 0.039984531700611115, train acc = 0.9950000047683716, time = 0.9393703937530518\n",
      "Training at step=43, batch=240, train loss = 0.013510633260011673, train acc = 0.9950000047683716, time = 0.9374592304229736\n",
      "Training at step=43, batch=270, train loss = 0.04772251471877098, train acc = 0.9850000143051147, time = 0.9444286823272705\n",
      "Testing at step=43, batch=0, test loss = 0.037519097328186035, test acc = 0.9900000095367432, time = 0.33780503273010254\n",
      "Testing at step=43, batch=5, test loss = 0.07754236459732056, test acc = 0.9700000286102295, time = 0.3440372943878174\n",
      "Testing at step=43, batch=10, test loss = 0.0775868147611618, test acc = 0.9900000095367432, time = 0.33676815032958984\n",
      "Testing at step=43, batch=15, test loss = 0.05644575506448746, test acc = 0.9850000143051147, time = 0.3412652015686035\n",
      "Testing at step=43, batch=20, test loss = 0.10512826591730118, test acc = 0.9700000286102295, time = 0.3597724437713623\n",
      "Testing at step=43, batch=25, test loss = 0.053568027913570404, test acc = 0.9900000095367432, time = 0.340193510055542\n",
      "Testing at step=43, batch=30, test loss = 0.11157925426959991, test acc = 0.9750000238418579, time = 0.34106016159057617\n",
      "Testing at step=43, batch=35, test loss = 0.06324628740549088, test acc = 0.9750000238418579, time = 0.337723970413208\n",
      "Testing at step=43, batch=40, test loss = 0.03899528831243515, test acc = 0.9800000190734863, time = 0.3377413749694824\n",
      "Testing at step=43, batch=45, test loss = 0.11404750496149063, test acc = 0.9700000286102295, time = 0.33953356742858887\n",
      "Step 43 finished in 309.9787724018097, Train loss = 0.053639072158063454, Test loss = 0.08558194264769554; Train Acc = 0.9829333464304606, Test Acc = 0.974400007724762\n",
      "Training at step=44, batch=0, train loss = 0.021019194275140762, train acc = 0.9950000047683716, time = 0.9468636512756348\n",
      "Training at step=44, batch=30, train loss = 0.05836664140224457, train acc = 0.9800000190734863, time = 0.9522180557250977\n",
      "Training at step=44, batch=60, train loss = 0.024155788123607635, train acc = 0.9900000095367432, time = 0.9531190395355225\n",
      "Training at step=44, batch=90, train loss = 0.03197534382343292, train acc = 0.9900000095367432, time = 0.9400041103363037\n",
      "Training at step=44, batch=120, train loss = 0.05274806171655655, train acc = 0.9950000047683716, time = 0.9413807392120361\n",
      "Training at step=44, batch=150, train loss = 0.0833645835518837, train acc = 0.9850000143051147, time = 0.941199779510498\n",
      "Training at step=44, batch=180, train loss = 0.03677494078874588, train acc = 0.9850000143051147, time = 0.9502110481262207\n",
      "Training at step=44, batch=210, train loss = 0.025057075545191765, train acc = 0.9950000047683716, time = 0.9381849765777588\n",
      "Training at step=44, batch=240, train loss = 0.05050448328256607, train acc = 0.9750000238418579, time = 0.9499726295471191\n",
      "Training at step=44, batch=270, train loss = 0.033252984285354614, train acc = 0.9850000143051147, time = 0.9483842849731445\n",
      "Testing at step=44, batch=0, test loss = 0.09354012459516525, test acc = 0.9649999737739563, time = 0.33898353576660156\n",
      "Testing at step=44, batch=5, test loss = 0.03123020939528942, test acc = 0.9950000047683716, time = 0.33871006965637207\n",
      "Testing at step=44, batch=10, test loss = 0.08372890204191208, test acc = 0.9700000286102295, time = 0.3357088565826416\n",
      "Testing at step=44, batch=15, test loss = 0.12224683910608292, test acc = 0.9800000190734863, time = 0.3475666046142578\n",
      "Testing at step=44, batch=20, test loss = 0.04251592606306076, test acc = 0.9800000190734863, time = 0.3408191204071045\n",
      "Testing at step=44, batch=25, test loss = 0.07086582481861115, test acc = 0.9850000143051147, time = 0.3418090343475342\n",
      "Testing at step=44, batch=30, test loss = 0.04379923641681671, test acc = 0.9800000190734863, time = 0.34296202659606934\n",
      "Testing at step=44, batch=35, test loss = 0.11651530861854553, test acc = 0.9700000286102295, time = 0.337679386138916\n",
      "Testing at step=44, batch=40, test loss = 0.04316820576786995, test acc = 0.9900000095367432, time = 0.33904218673706055\n",
      "Testing at step=44, batch=45, test loss = 0.07193739712238312, test acc = 0.9800000190734863, time = 0.33701181411743164\n",
      "Step 44 finished in 311.30049443244934, Train loss = 0.05048119990931203, Test loss = 0.09362917587161064; Train Acc = 0.9833000129461289, Test Acc = 0.9732000088691711\n",
      "Training at step=45, batch=0, train loss = 0.023207325488328934, train acc = 0.9950000047683716, time = 0.9455008506774902\n",
      "Training at step=45, batch=30, train loss = 0.02540154941380024, train acc = 1.0, time = 0.938300371170044\n",
      "Training at step=45, batch=60, train loss = 0.0879192054271698, train acc = 0.9549999833106995, time = 0.9404568672180176\n",
      "Training at step=45, batch=90, train loss = 0.05116437375545502, train acc = 0.9850000143051147, time = 0.9455418586730957\n",
      "Training at step=45, batch=120, train loss = 0.08910924941301346, train acc = 0.9750000238418579, time = 0.9453387260437012\n",
      "Training at step=45, batch=150, train loss = 0.09035921096801758, train acc = 0.9750000238418579, time = 0.9412126541137695\n",
      "Training at step=45, batch=180, train loss = 0.05664035677909851, train acc = 0.9850000143051147, time = 0.9609127044677734\n",
      "Training at step=45, batch=210, train loss = 0.12091013044118881, train acc = 0.9700000286102295, time = 0.950613260269165\n",
      "Training at step=45, batch=240, train loss = 0.05556361377239227, train acc = 0.9750000238418579, time = 0.9381735324859619\n",
      "Training at step=45, batch=270, train loss = 0.01751675270497799, train acc = 1.0, time = 0.9369347095489502\n",
      "Testing at step=45, batch=0, test loss = 0.18705914914608002, test acc = 0.9549999833106995, time = 0.360304594039917\n",
      "Testing at step=45, batch=5, test loss = 0.147239089012146, test acc = 0.9549999833106995, time = 0.3692348003387451\n",
      "Testing at step=45, batch=10, test loss = 0.06068478152155876, test acc = 0.9750000238418579, time = 0.3609347343444824\n",
      "Testing at step=45, batch=15, test loss = 0.037421613931655884, test acc = 0.9800000190734863, time = 0.34024953842163086\n",
      "Testing at step=45, batch=20, test loss = 0.1207960769534111, test acc = 0.949999988079071, time = 0.3480095863342285\n",
      "Testing at step=45, batch=25, test loss = 0.12266354262828827, test acc = 0.9649999737739563, time = 0.34036993980407715\n",
      "Testing at step=45, batch=30, test loss = 0.14785709977149963, test acc = 0.9549999833106995, time = 0.3391532897949219\n",
      "Testing at step=45, batch=35, test loss = 0.1255808025598526, test acc = 0.9800000190734863, time = 0.3388988971710205\n",
      "Testing at step=45, batch=40, test loss = 0.09996344894170761, test acc = 0.9700000286102295, time = 0.33960986137390137\n",
      "Testing at step=45, batch=45, test loss = 0.05751381069421768, test acc = 0.9800000190734863, time = 0.3430447578430176\n",
      "Step 45 finished in 311.9068057537079, Train loss = 0.049952761985672015, Test loss = 0.09276331804692745; Train Acc = 0.9838500130176544, Test Acc = 0.9718000078201294\n",
      "Training at step=46, batch=0, train loss = 0.03471214696764946, train acc = 0.9900000095367432, time = 0.9504232406616211\n",
      "Training at step=46, batch=30, train loss = 0.01670885644853115, train acc = 1.0, time = 0.9474020004272461\n",
      "Training at step=46, batch=60, train loss = 0.04387297481298447, train acc = 0.9750000238418579, time = 0.9442391395568848\n",
      "Training at step=46, batch=90, train loss = 0.0769684687256813, train acc = 0.9800000190734863, time = 0.940643310546875\n",
      "Training at step=46, batch=120, train loss = 0.05576663091778755, train acc = 0.9800000190734863, time = 0.9371354579925537\n",
      "Training at step=46, batch=150, train loss = 0.15375782549381256, train acc = 0.9700000286102295, time = 0.9426701068878174\n",
      "Training at step=46, batch=180, train loss = 0.05797995999455452, train acc = 0.9800000190734863, time = 0.9408869743347168\n",
      "Training at step=46, batch=210, train loss = 0.06776750832796097, train acc = 0.9900000095367432, time = 0.9387247562408447\n",
      "Training at step=46, batch=240, train loss = 0.06319525092840195, train acc = 0.9800000190734863, time = 0.9545011520385742\n",
      "Training at step=46, batch=270, train loss = 0.053311705589294434, train acc = 0.9850000143051147, time = 0.9413955211639404\n",
      "Testing at step=46, batch=0, test loss = 0.08846081048250198, test acc = 0.9800000190734863, time = 0.3444254398345947\n",
      "Testing at step=46, batch=5, test loss = 0.1502823382616043, test acc = 0.9700000286102295, time = 0.3387022018432617\n",
      "Testing at step=46, batch=10, test loss = 0.09682794660329819, test acc = 0.9750000238418579, time = 0.33618783950805664\n",
      "Testing at step=46, batch=15, test loss = 0.050963349640369415, test acc = 0.9850000143051147, time = 0.3376913070678711\n",
      "Testing at step=46, batch=20, test loss = 0.12899130582809448, test acc = 0.9700000286102295, time = 0.3365194797515869\n",
      "Testing at step=46, batch=25, test loss = 0.10001661628484726, test acc = 0.9649999737739563, time = 0.34282875061035156\n",
      "Testing at step=46, batch=30, test loss = 0.1457507461309433, test acc = 0.9599999785423279, time = 0.34805822372436523\n",
      "Testing at step=46, batch=35, test loss = 0.10759688168764114, test acc = 0.9649999737739563, time = 0.3395524024963379\n",
      "Testing at step=46, batch=40, test loss = 0.04970557242631912, test acc = 0.9900000095367432, time = 0.33616018295288086\n",
      "Testing at step=46, batch=45, test loss = 0.0810549333691597, test acc = 0.9700000286102295, time = 0.336045503616333\n",
      "Step 46 finished in 311.24965929985046, Train loss = 0.048508006039385994, Test loss = 0.0928046334721148; Train Acc = 0.9840500140190125, Test Acc = 0.9741000080108643\n",
      "Training at step=47, batch=0, train loss = 0.03306770697236061, train acc = 0.9900000095367432, time = 0.9456667900085449\n",
      "Training at step=47, batch=30, train loss = 0.045685455203056335, train acc = 0.9850000143051147, time = 0.9457755088806152\n",
      "Training at step=47, batch=60, train loss = 0.0619225800037384, train acc = 0.9800000190734863, time = 0.9432094097137451\n",
      "Training at step=47, batch=90, train loss = 0.060085516422986984, train acc = 0.9750000238418579, time = 0.9387593269348145\n",
      "Training at step=47, batch=120, train loss = 0.04493299499154091, train acc = 0.9700000286102295, time = 0.9633564949035645\n",
      "Training at step=47, batch=150, train loss = 0.05405249446630478, train acc = 0.9800000190734863, time = 0.9363210201263428\n",
      "Training at step=47, batch=180, train loss = 0.034962788224220276, train acc = 0.9900000095367432, time = 0.9452273845672607\n",
      "Training at step=47, batch=210, train loss = 0.038268495351076126, train acc = 0.9900000095367432, time = 0.9405269622802734\n",
      "Training at step=47, batch=240, train loss = 0.03136153891682625, train acc = 0.9900000095367432, time = 0.9462149143218994\n",
      "Training at step=47, batch=270, train loss = 0.036510828882455826, train acc = 0.9850000143051147, time = 0.9456076622009277\n",
      "Testing at step=47, batch=0, test loss = 0.1135459616780281, test acc = 0.9800000190734863, time = 0.33822154998779297\n",
      "Testing at step=47, batch=5, test loss = 0.07269714772701263, test acc = 0.9750000238418579, time = 0.34374117851257324\n",
      "Testing at step=47, batch=10, test loss = 0.12766996026039124, test acc = 0.9649999737739563, time = 0.33845090866088867\n",
      "Testing at step=47, batch=15, test loss = 0.05554526299238205, test acc = 0.9900000095367432, time = 0.37749505043029785\n",
      "Testing at step=47, batch=20, test loss = 0.06624370813369751, test acc = 0.9750000238418579, time = 0.338123083114624\n",
      "Testing at step=47, batch=25, test loss = 0.05793873220682144, test acc = 0.9900000095367432, time = 0.3723714351654053\n",
      "Testing at step=47, batch=30, test loss = 0.12589889764785767, test acc = 0.9649999737739563, time = 0.3394460678100586\n",
      "Testing at step=47, batch=35, test loss = 0.03245196491479874, test acc = 0.9900000095367432, time = 0.345944881439209\n",
      "Testing at step=47, batch=40, test loss = 0.03153834491968155, test acc = 0.9850000143051147, time = 0.3376646041870117\n",
      "Testing at step=47, batch=45, test loss = 0.06568624824285507, test acc = 0.9700000286102295, time = 0.3393995761871338\n",
      "Step 47 finished in 312.020387172699, Train loss = 0.04683069579924146, Test loss = 0.09083348862826825; Train Acc = 0.9846500124533971, Test Acc = 0.9748000073432922\n",
      "Training at step=48, batch=0, train loss = 0.04252740368247032, train acc = 0.9900000095367432, time = 0.947941780090332\n",
      "Training at step=48, batch=30, train loss = 0.044687896966934204, train acc = 0.9850000143051147, time = 0.9509367942810059\n",
      "Training at step=48, batch=60, train loss = 0.020953362807631493, train acc = 0.9950000047683716, time = 0.9435253143310547\n",
      "Training at step=48, batch=90, train loss = 0.011582832783460617, train acc = 1.0, time = 0.9514334201812744\n",
      "Training at step=48, batch=120, train loss = 0.07661418616771698, train acc = 0.9649999737739563, time = 0.9479706287384033\n",
      "Training at step=48, batch=150, train loss = 0.02906004711985588, train acc = 0.9900000095367432, time = 0.9453465938568115\n",
      "Training at step=48, batch=180, train loss = 0.12676255404949188, train acc = 0.9750000238418579, time = 0.9409866333007812\n",
      "Training at step=48, batch=210, train loss = 0.04886389151215553, train acc = 0.9700000286102295, time = 0.943037748336792\n",
      "Training at step=48, batch=240, train loss = 0.021818533539772034, train acc = 0.9900000095367432, time = 0.9989883899688721\n",
      "Training at step=48, batch=270, train loss = 0.029620375484228134, train acc = 0.9900000095367432, time = 0.9480152130126953\n",
      "Testing at step=48, batch=0, test loss = 0.10648517310619354, test acc = 0.9750000238418579, time = 0.33829259872436523\n",
      "Testing at step=48, batch=5, test loss = 0.029535209760069847, test acc = 0.9950000047683716, time = 0.34264564514160156\n",
      "Testing at step=48, batch=10, test loss = 0.05674533545970917, test acc = 0.9750000238418579, time = 0.3471696376800537\n",
      "Testing at step=48, batch=15, test loss = 0.1430477648973465, test acc = 0.9750000238418579, time = 0.34087443351745605\n",
      "Testing at step=48, batch=20, test loss = 0.07777352631092072, test acc = 0.9800000190734863, time = 0.3364591598510742\n",
      "Testing at step=48, batch=25, test loss = 0.053367406129837036, test acc = 0.9850000143051147, time = 0.33472394943237305\n",
      "Testing at step=48, batch=30, test loss = 0.143811896443367, test acc = 0.9649999737739563, time = 0.33687543869018555\n",
      "Testing at step=48, batch=35, test loss = 0.17844100296497345, test acc = 0.949999988079071, time = 0.33838629722595215\n",
      "Testing at step=48, batch=40, test loss = 0.07758333534002304, test acc = 0.9800000190734863, time = 0.33643293380737305\n",
      "Testing at step=48, batch=45, test loss = 0.12494461238384247, test acc = 0.9649999737739563, time = 0.3369109630584717\n",
      "Step 48 finished in 312.1957986354828, Train loss = 0.047572416610394914, Test loss = 0.0944121154025197; Train Acc = 0.9844500132401784, Test Acc = 0.975000011920929\n",
      "Training at step=49, batch=0, train loss = 0.028466852381825447, train acc = 0.9900000095367432, time = 0.9395620822906494\n",
      "Training at step=49, batch=30, train loss = 0.06954073160886765, train acc = 0.9800000190734863, time = 0.9422690868377686\n",
      "Training at step=49, batch=60, train loss = 0.01425420492887497, train acc = 0.9950000047683716, time = 0.9619972705841064\n",
      "Training at step=49, batch=90, train loss = 0.09489277750253677, train acc = 0.9750000238418579, time = 0.9575119018554688\n",
      "Training at step=49, batch=120, train loss = 0.024718843400478363, train acc = 0.9900000095367432, time = 0.9500064849853516\n",
      "Training at step=49, batch=150, train loss = 0.04243950545787811, train acc = 0.9900000095367432, time = 0.9572770595550537\n",
      "Training at step=49, batch=180, train loss = 0.08217473328113556, train acc = 0.9649999737739563, time = 0.9384219646453857\n",
      "Training at step=49, batch=210, train loss = 0.06029864773154259, train acc = 0.9850000143051147, time = 0.9410653114318848\n",
      "Training at step=49, batch=240, train loss = 0.029749274253845215, train acc = 0.9850000143051147, time = 0.9473035335540771\n",
      "Training at step=49, batch=270, train loss = 0.06930001080036163, train acc = 0.9700000286102295, time = 0.9383707046508789\n",
      "Testing at step=49, batch=0, test loss = 0.14455996453762054, test acc = 0.9449999928474426, time = 0.339052677154541\n",
      "Testing at step=49, batch=5, test loss = 0.03370391204953194, test acc = 0.9950000047683716, time = 0.3412754535675049\n",
      "Testing at step=49, batch=10, test loss = 0.054157596081495285, test acc = 0.9950000047683716, time = 0.3442800045013428\n",
      "Testing at step=49, batch=15, test loss = 0.16164201498031616, test acc = 0.9800000190734863, time = 0.3387882709503174\n",
      "Testing at step=49, batch=20, test loss = 0.16056926548480988, test acc = 0.9599999785423279, time = 0.3411107063293457\n",
      "Testing at step=49, batch=25, test loss = 0.12026326358318329, test acc = 0.9700000286102295, time = 0.34041285514831543\n",
      "Testing at step=49, batch=30, test loss = 0.03400963917374611, test acc = 0.9900000095367432, time = 0.33879637718200684\n",
      "Testing at step=49, batch=35, test loss = 0.1325404793024063, test acc = 0.9549999833106995, time = 0.33811402320861816\n",
      "Testing at step=49, batch=40, test loss = 0.03580879420042038, test acc = 0.9800000190734863, time = 0.3373377323150635\n",
      "Testing at step=49, batch=45, test loss = 0.12282443791627884, test acc = 0.9599999785423279, time = 0.3406202793121338\n",
      "Step 49 finished in 310.52911901474, Train loss = 0.04647124992528309, Test loss = 0.08721257165074349; Train Acc = 0.9848000113169352, Test Acc = 0.9747000074386597\n",
      "Training at step=50, batch=0, train loss = 0.046682704240083694, train acc = 0.9800000190734863, time = 0.9601538181304932\n",
      "Training at step=50, batch=30, train loss = 0.03817012906074524, train acc = 0.9800000190734863, time = 0.9611091613769531\n",
      "Training at step=50, batch=60, train loss = 0.04913894087076187, train acc = 0.9800000190734863, time = 0.9453277587890625\n",
      "Training at step=50, batch=90, train loss = 0.0235401913523674, train acc = 0.9950000047683716, time = 0.9596881866455078\n",
      "Training at step=50, batch=120, train loss = 0.05533149838447571, train acc = 0.9800000190734863, time = 0.9419131278991699\n",
      "Training at step=50, batch=150, train loss = 0.08794450759887695, train acc = 0.9649999737739563, time = 0.939755916595459\n",
      "Training at step=50, batch=180, train loss = 0.054979875683784485, train acc = 0.9700000286102295, time = 0.9386491775512695\n",
      "Training at step=50, batch=210, train loss = 0.02567426487803459, train acc = 0.9900000095367432, time = 0.961578369140625\n",
      "Training at step=50, batch=240, train loss = 0.06252069771289825, train acc = 0.9800000190734863, time = 0.9375662803649902\n",
      "Training at step=50, batch=270, train loss = 0.05768529325723648, train acc = 0.9850000143051147, time = 0.9416766166687012\n",
      "Testing at step=50, batch=0, test loss = 0.1208537369966507, test acc = 0.9700000286102295, time = 0.3420288562774658\n",
      "Testing at step=50, batch=5, test loss = 0.05964858829975128, test acc = 0.9850000143051147, time = 0.3395078182220459\n",
      "Testing at step=50, batch=10, test loss = 0.04211295023560524, test acc = 0.9700000286102295, time = 0.3387115001678467\n",
      "Testing at step=50, batch=15, test loss = 0.09036885201931, test acc = 0.9800000190734863, time = 0.33661532402038574\n",
      "Testing at step=50, batch=20, test loss = 0.11397943645715714, test acc = 0.9649999737739563, time = 0.3404688835144043\n",
      "Testing at step=50, batch=25, test loss = 0.09550844132900238, test acc = 0.9850000143051147, time = 0.34878063201904297\n",
      "Testing at step=50, batch=30, test loss = 0.10267173498868942, test acc = 0.9599999785423279, time = 0.33902645111083984\n",
      "Testing at step=50, batch=35, test loss = 0.14579959213733673, test acc = 0.9549999833106995, time = 0.33620476722717285\n",
      "Testing at step=50, batch=40, test loss = 0.12895098328590393, test acc = 0.9700000286102295, time = 0.3485109806060791\n",
      "Testing at step=50, batch=45, test loss = 0.0488542877137661, test acc = 0.9800000190734863, time = 0.334120512008667\n",
      "Step 50 finished in 310.5304741859436, Train loss = 0.044862717548385264, Test loss = 0.08740614388138056; Train Acc = 0.9858333458503087, Test Acc = 0.9757000124454498\n",
      "Training at step=51, batch=0, train loss = 0.0835728570818901, train acc = 0.9850000143051147, time = 0.9519197940826416\n",
      "Training at step=51, batch=30, train loss = 0.017752444371581078, train acc = 0.9950000047683716, time = 0.9424593448638916\n",
      "Training at step=51, batch=60, train loss = 0.07450542598962784, train acc = 0.9750000238418579, time = 0.9600191116333008\n",
      "Training at step=51, batch=90, train loss = 0.0450487844645977, train acc = 0.9750000238418579, time = 0.9467988014221191\n",
      "Training at step=51, batch=120, train loss = 0.08511757105588913, train acc = 0.9800000190734863, time = 0.9476914405822754\n",
      "Training at step=51, batch=150, train loss = 0.03149062767624855, train acc = 0.9850000143051147, time = 0.941922664642334\n",
      "Training at step=51, batch=180, train loss = 0.041230928152799606, train acc = 0.9900000095367432, time = 0.9428040981292725\n",
      "Training at step=51, batch=210, train loss = 0.03173507750034332, train acc = 0.9950000047683716, time = 0.9514780044555664\n",
      "Training at step=51, batch=240, train loss = 0.025431470945477486, train acc = 0.9900000095367432, time = 0.9405472278594971\n",
      "Training at step=51, batch=270, train loss = 0.07370701432228088, train acc = 0.9900000095367432, time = 0.943638801574707\n",
      "Testing at step=51, batch=0, test loss = 0.15909001231193542, test acc = 0.9649999737739563, time = 0.3464651107788086\n",
      "Testing at step=51, batch=5, test loss = 0.06025051698088646, test acc = 0.9850000143051147, time = 0.3385453224182129\n",
      "Testing at step=51, batch=10, test loss = 0.12298346310853958, test acc = 0.9800000190734863, time = 0.3372478485107422\n",
      "Testing at step=51, batch=15, test loss = 0.027809614315629005, test acc = 0.9850000143051147, time = 0.33761096000671387\n",
      "Testing at step=51, batch=20, test loss = 0.06336300075054169, test acc = 0.9800000190734863, time = 0.3396334648132324\n",
      "Testing at step=51, batch=25, test loss = 0.08805976808071136, test acc = 0.9700000286102295, time = 0.34021592140197754\n",
      "Testing at step=51, batch=30, test loss = 0.07415857911109924, test acc = 0.9700000286102295, time = 0.3370833396911621\n",
      "Testing at step=51, batch=35, test loss = 0.10781785845756531, test acc = 0.9700000286102295, time = 0.33931660652160645\n",
      "Testing at step=51, batch=40, test loss = 0.10905903577804565, test acc = 0.9700000286102295, time = 0.3397197723388672\n",
      "Testing at step=51, batch=45, test loss = 0.12112089991569519, test acc = 0.9800000190734863, time = 0.3399214744567871\n",
      "Step 51 finished in 309.87556195259094, Train loss = 0.04283558370700727, Test loss = 0.0916150515154004; Train Acc = 0.985716679294904, Test Acc = 0.9740000128746032\n",
      "Training at step=52, batch=0, train loss = 0.062000516802072525, train acc = 0.9850000143051147, time = 0.9437098503112793\n",
      "Training at step=52, batch=30, train loss = 0.03534086048603058, train acc = 0.9800000190734863, time = 0.944608211517334\n",
      "Training at step=52, batch=60, train loss = 0.014277376234531403, train acc = 0.9950000047683716, time = 0.9597747325897217\n",
      "Training at step=52, batch=90, train loss = 0.05832701176404953, train acc = 0.9900000095367432, time = 0.9359867572784424\n",
      "Training at step=52, batch=120, train loss = 0.024915141984820366, train acc = 0.9900000095367432, time = 0.9387493133544922\n",
      "Training at step=52, batch=150, train loss = 0.04336689040064812, train acc = 0.9900000095367432, time = 0.9428622722625732\n",
      "Training at step=52, batch=180, train loss = 0.045284949243068695, train acc = 0.9800000190734863, time = 0.9456799030303955\n",
      "Training at step=52, batch=210, train loss = 0.08903674781322479, train acc = 0.9800000190734863, time = 0.9397635459899902\n",
      "Training at step=52, batch=240, train loss = 0.03641191124916077, train acc = 0.9900000095367432, time = 0.939786434173584\n",
      "Training at step=52, batch=270, train loss = 0.027979129925370216, train acc = 0.9900000095367432, time = 0.935488224029541\n",
      "Testing at step=52, batch=0, test loss = 0.12830017507076263, test acc = 0.9700000286102295, time = 0.35985851287841797\n",
      "Testing at step=52, batch=5, test loss = 0.13838329911231995, test acc = 0.9700000286102295, time = 0.3611109256744385\n",
      "Testing at step=52, batch=10, test loss = 0.09857887029647827, test acc = 0.9700000286102295, time = 0.3619801998138428\n",
      "Testing at step=52, batch=15, test loss = 0.18050043284893036, test acc = 0.949999988079071, time = 0.35565853118896484\n",
      "Testing at step=52, batch=20, test loss = 0.1394186168909073, test acc = 0.9700000286102295, time = 0.33757638931274414\n",
      "Testing at step=52, batch=25, test loss = 0.08190365135669708, test acc = 0.9750000238418579, time = 0.3394479751586914\n",
      "Testing at step=52, batch=30, test loss = 0.06604503095149994, test acc = 0.9800000190734863, time = 0.3402726650238037\n",
      "Testing at step=52, batch=35, test loss = 0.11855635046958923, test acc = 0.9649999737739563, time = 0.34250593185424805\n",
      "Testing at step=52, batch=40, test loss = 0.17289236187934875, test acc = 0.9549999833106995, time = 0.3420424461364746\n",
      "Testing at step=52, batch=45, test loss = 0.1129726767539978, test acc = 0.9649999737739563, time = 0.3368093967437744\n",
      "Step 52 finished in 311.15728306770325, Train loss = 0.04274481062001238, Test loss = 0.0931179877370596; Train Acc = 0.9859166787068049, Test Acc = 0.9756000101566314\n",
      "Training at step=53, batch=0, train loss = 0.08811582624912262, train acc = 0.9750000238418579, time = 0.9375932216644287\n",
      "Training at step=53, batch=30, train loss = 0.018900273367762566, train acc = 0.9950000047683716, time = 0.9398324489593506\n",
      "Training at step=53, batch=60, train loss = 0.017974751070141792, train acc = 0.9900000095367432, time = 0.9447011947631836\n",
      "Training at step=53, batch=90, train loss = 0.03671044111251831, train acc = 0.9900000095367432, time = 0.9516832828521729\n",
      "Training at step=53, batch=120, train loss = 0.018935203552246094, train acc = 0.9950000047683716, time = 0.93747878074646\n",
      "Training at step=53, batch=150, train loss = 0.025043204426765442, train acc = 0.9950000047683716, time = 0.9473617076873779\n",
      "Training at step=53, batch=180, train loss = 0.04913066327571869, train acc = 0.9900000095367432, time = 0.9441053867340088\n",
      "Training at step=53, batch=210, train loss = 0.039012063294649124, train acc = 0.9900000095367432, time = 0.9483399391174316\n",
      "Training at step=53, batch=240, train loss = 0.0627303496003151, train acc = 0.9750000238418579, time = 0.9385983943939209\n",
      "Training at step=53, batch=270, train loss = 0.02311338484287262, train acc = 1.0, time = 0.9358105659484863\n",
      "Testing at step=53, batch=0, test loss = 0.03969753533601761, test acc = 0.9850000143051147, time = 0.3366405963897705\n",
      "Testing at step=53, batch=5, test loss = 0.04365457594394684, test acc = 0.9850000143051147, time = 0.33864450454711914\n",
      "Testing at step=53, batch=10, test loss = 0.0286320298910141, test acc = 0.9950000047683716, time = 0.33796072006225586\n",
      "Testing at step=53, batch=15, test loss = 0.13212572038173676, test acc = 0.9750000238418579, time = 0.3792405128479004\n",
      "Testing at step=53, batch=20, test loss = 0.08157002180814743, test acc = 0.9800000190734863, time = 0.3350822925567627\n",
      "Testing at step=53, batch=25, test loss = 0.12033601105213165, test acc = 0.9750000238418579, time = 0.33608174324035645\n",
      "Testing at step=53, batch=30, test loss = 0.021077096462249756, test acc = 0.9900000095367432, time = 0.3352961540222168\n",
      "Testing at step=53, batch=35, test loss = 0.16055679321289062, test acc = 0.9750000238418579, time = 0.33541297912597656\n",
      "Testing at step=53, batch=40, test loss = 0.061920490115880966, test acc = 0.9850000143051147, time = 0.33574557304382324\n",
      "Testing at step=53, batch=45, test loss = 0.0787670835852623, test acc = 0.9800000190734863, time = 0.3387322425842285\n",
      "Step 53 finished in 310.47873854637146, Train loss = 0.04222655996369819, Test loss = 0.09148193091154098; Train Acc = 0.9868000115950902, Test Acc = 0.9749000084400177\n",
      "Training at step=54, batch=0, train loss = 0.029789790511131287, train acc = 0.9950000047683716, time = 0.9393601417541504\n",
      "Training at step=54, batch=30, train loss = 0.028761988505721092, train acc = 0.9950000047683716, time = 0.9381649494171143\n",
      "Training at step=54, batch=60, train loss = 0.027775457128882408, train acc = 0.9900000095367432, time = 0.9616882801055908\n",
      "Training at step=54, batch=90, train loss = 0.01986854523420334, train acc = 0.9950000047683716, time = 0.9733872413635254\n",
      "Training at step=54, batch=120, train loss = 0.024527037516236305, train acc = 0.9900000095367432, time = 0.9389550685882568\n",
      "Training at step=54, batch=150, train loss = 0.04812515899538994, train acc = 0.9900000095367432, time = 0.9367244243621826\n",
      "Training at step=54, batch=180, train loss = 0.054153647273778915, train acc = 0.9800000190734863, time = 0.94394850730896\n",
      "Training at step=54, batch=210, train loss = 0.07681406289339066, train acc = 0.9750000238418579, time = 0.9394924640655518\n",
      "Training at step=54, batch=240, train loss = 0.02387113869190216, train acc = 0.9900000095367432, time = 0.9407784938812256\n",
      "Training at step=54, batch=270, train loss = 0.031065061688423157, train acc = 0.9950000047683716, time = 0.9428715705871582\n",
      "Testing at step=54, batch=0, test loss = 0.12941457331180573, test acc = 0.9599999785423279, time = 0.36641550064086914\n",
      "Testing at step=54, batch=5, test loss = 0.037155549973249435, test acc = 0.9850000143051147, time = 0.34494924545288086\n",
      "Testing at step=54, batch=10, test loss = 0.04445566609501839, test acc = 0.9800000190734863, time = 0.3366575241088867\n",
      "Testing at step=54, batch=15, test loss = 0.08143016695976257, test acc = 0.9900000095367432, time = 0.3360300064086914\n",
      "Testing at step=54, batch=20, test loss = 0.10143003612756729, test acc = 0.9750000238418579, time = 0.39312195777893066\n",
      "Testing at step=54, batch=25, test loss = 0.21304625272750854, test acc = 0.9599999785423279, time = 0.33882737159729004\n",
      "Testing at step=54, batch=30, test loss = 0.14739970862865448, test acc = 0.9599999785423279, time = 0.34031152725219727\n",
      "Testing at step=54, batch=35, test loss = 0.13255414366722107, test acc = 0.9599999785423279, time = 0.33685994148254395\n",
      "Testing at step=54, batch=40, test loss = 0.057522937655448914, test acc = 0.9850000143051147, time = 0.342242956161499\n",
      "Testing at step=54, batch=45, test loss = 0.06454159319400787, test acc = 0.9649999737739563, time = 0.33844828605651855\n",
      "Step 54 finished in 310.6345100402832, Train loss = 0.04075788286398165, Test loss = 0.09706120751798153; Train Acc = 0.9870500119527181, Test Acc = 0.972400004863739\n",
      "Training at step=55, batch=0, train loss = 0.054596949368715286, train acc = 0.9900000095367432, time = 0.9408245086669922\n",
      "Training at step=55, batch=30, train loss = 0.0517440140247345, train acc = 0.9900000095367432, time = 0.9407444000244141\n",
      "Training at step=55, batch=60, train loss = 0.03354299068450928, train acc = 0.9800000190734863, time = 0.9626893997192383\n",
      "Training at step=55, batch=90, train loss = 0.04249907657504082, train acc = 0.9850000143051147, time = 0.9415566921234131\n",
      "Training at step=55, batch=120, train loss = 0.017741719260811806, train acc = 0.9950000047683716, time = 0.9458589553833008\n",
      "Training at step=55, batch=150, train loss = 0.040933553129434586, train acc = 0.9900000095367432, time = 0.9445648193359375\n",
      "Training at step=55, batch=180, train loss = 0.04106583446264267, train acc = 0.9950000047683716, time = 0.9440226554870605\n",
      "Training at step=55, batch=210, train loss = 0.015404539182782173, train acc = 1.0, time = 0.9509708881378174\n",
      "Training at step=55, batch=240, train loss = 0.07647905498743057, train acc = 0.9800000190734863, time = 0.9414300918579102\n",
      "Training at step=55, batch=270, train loss = 0.04055780544877052, train acc = 0.9900000095367432, time = 0.9392201900482178\n",
      "Testing at step=55, batch=0, test loss = 0.05390729755163193, test acc = 0.9850000143051147, time = 0.33739137649536133\n",
      "Testing at step=55, batch=5, test loss = 0.07460790872573853, test acc = 0.9700000286102295, time = 0.33863401412963867\n",
      "Testing at step=55, batch=10, test loss = 0.0707380548119545, test acc = 0.9900000095367432, time = 0.3370780944824219\n",
      "Testing at step=55, batch=15, test loss = 0.007522940635681152, test acc = 1.0, time = 0.33570194244384766\n",
      "Testing at step=55, batch=20, test loss = 0.10459139198064804, test acc = 0.9700000286102295, time = 0.3359348773956299\n",
      "Testing at step=55, batch=25, test loss = 0.11188021302223206, test acc = 0.9599999785423279, time = 0.3388345241546631\n",
      "Testing at step=55, batch=30, test loss = 0.04619087651371956, test acc = 0.9850000143051147, time = 0.3596384525299072\n",
      "Testing at step=55, batch=35, test loss = 0.1054382249712944, test acc = 0.9700000286102295, time = 0.3362393379211426\n",
      "Testing at step=55, batch=40, test loss = 0.051011573523283005, test acc = 0.9900000095367432, time = 0.33650779724121094\n",
      "Testing at step=55, batch=45, test loss = 0.1262999325990677, test acc = 0.9700000286102295, time = 0.33528780937194824\n",
      "Step 55 finished in 310.0040912628174, Train loss = 0.04261737642344087, Test loss = 0.09983454436063767; Train Acc = 0.9857166783014933, Test Acc = 0.9723000061511994\n",
      "Training at step=56, batch=0, train loss = 0.05050893872976303, train acc = 0.9800000190734863, time = 0.936105489730835\n",
      "Training at step=56, batch=30, train loss = 0.057016801089048386, train acc = 0.9900000095367432, time = 0.9353008270263672\n",
      "Training at step=56, batch=60, train loss = 0.05142321437597275, train acc = 0.9900000095367432, time = 0.9391887187957764\n",
      "Training at step=56, batch=90, train loss = 0.016668502241373062, train acc = 0.9950000047683716, time = 0.9561021327972412\n",
      "Training at step=56, batch=120, train loss = 0.1293783038854599, train acc = 0.9549999833106995, time = 0.9401664733886719\n",
      "Training at step=56, batch=150, train loss = 0.03140006959438324, train acc = 0.9900000095367432, time = 0.9446756839752197\n",
      "Training at step=56, batch=180, train loss = 0.019194910302758217, train acc = 0.9950000047683716, time = 0.9431877136230469\n",
      "Training at step=56, batch=210, train loss = 0.015239696949720383, train acc = 0.9950000047683716, time = 0.9420082569122314\n",
      "Training at step=56, batch=240, train loss = 0.028100525960326195, train acc = 0.9850000143051147, time = 0.9638047218322754\n",
      "Training at step=56, batch=270, train loss = 0.057246651500463486, train acc = 0.9800000190734863, time = 0.9389772415161133\n",
      "Testing at step=56, batch=0, test loss = 0.10704073309898376, test acc = 0.9599999785423279, time = 0.3414418697357178\n",
      "Testing at step=56, batch=5, test loss = 0.07454858720302582, test acc = 0.9800000190734863, time = 0.34841156005859375\n",
      "Testing at step=56, batch=10, test loss = 0.08771178871393204, test acc = 0.9750000238418579, time = 0.33805036544799805\n",
      "Testing at step=56, batch=15, test loss = 0.10417895019054413, test acc = 0.9800000190734863, time = 0.33815622329711914\n",
      "Testing at step=56, batch=20, test loss = 0.16158099472522736, test acc = 0.9649999737739563, time = 0.3408074378967285\n",
      "Testing at step=56, batch=25, test loss = 0.06650665402412415, test acc = 0.9700000286102295, time = 0.3401169776916504\n",
      "Testing at step=56, batch=30, test loss = 0.14407789707183838, test acc = 0.9599999785423279, time = 0.3385300636291504\n",
      "Testing at step=56, batch=35, test loss = 0.05196181312203407, test acc = 0.9850000143051147, time = 0.39491796493530273\n",
      "Testing at step=56, batch=40, test loss = 0.11442145705223083, test acc = 0.9850000143051147, time = 0.3390190601348877\n",
      "Testing at step=56, batch=45, test loss = 0.07862087339162827, test acc = 0.9750000238418579, time = 0.3375117778778076\n",
      "Step 56 finished in 310.2892167568207, Train loss = 0.03810976088202248, Test loss = 0.10357915449887514; Train Acc = 0.9879000105460485, Test Acc = 0.9745000100135803\n",
      "Training at step=57, batch=0, train loss = 0.02517872117459774, train acc = 0.9950000047683716, time = 0.9386327266693115\n",
      "Training at step=57, batch=30, train loss = 0.04760146141052246, train acc = 0.9850000143051147, time = 0.9372363090515137\n",
      "Training at step=57, batch=60, train loss = 0.03669861704111099, train acc = 0.9900000095367432, time = 0.9372096061706543\n",
      "Training at step=57, batch=90, train loss = 0.03379075229167938, train acc = 0.9850000143051147, time = 0.9386591911315918\n",
      "Training at step=57, batch=120, train loss = 0.02811763808131218, train acc = 0.9900000095367432, time = 0.9449121952056885\n",
      "Training at step=57, batch=150, train loss = 0.05330834910273552, train acc = 0.9800000190734863, time = 0.9384934902191162\n",
      "Training at step=57, batch=180, train loss = 0.034938737750053406, train acc = 0.9950000047683716, time = 0.9961695671081543\n",
      "Training at step=57, batch=210, train loss = 0.04097817465662956, train acc = 0.9850000143051147, time = 0.9448873996734619\n",
      "Training at step=57, batch=240, train loss = 0.05437588319182396, train acc = 0.9750000238418579, time = 0.939298152923584\n",
      "Training at step=57, batch=270, train loss = 0.013723213225603104, train acc = 0.9950000047683716, time = 0.9479396343231201\n",
      "Testing at step=57, batch=0, test loss = 0.09930498898029327, test acc = 0.9750000238418579, time = 0.33823180198669434\n",
      "Testing at step=57, batch=5, test loss = 0.2098204791545868, test acc = 0.9599999785423279, time = 0.3379096984863281\n",
      "Testing at step=57, batch=10, test loss = 0.07651302963495255, test acc = 0.9700000286102295, time = 0.3386414051055908\n",
      "Testing at step=57, batch=15, test loss = 0.05344586446881294, test acc = 0.9800000190734863, time = 0.3379216194152832\n",
      "Testing at step=57, batch=20, test loss = 0.12315309792757034, test acc = 0.9549999833106995, time = 0.3386662006378174\n",
      "Testing at step=57, batch=25, test loss = 0.12698093056678772, test acc = 0.9649999737739563, time = 0.34773874282836914\n",
      "Testing at step=57, batch=30, test loss = 0.08322349190711975, test acc = 0.9750000238418579, time = 0.345076322555542\n",
      "Testing at step=57, batch=35, test loss = 0.06407596915960312, test acc = 0.9750000238418579, time = 0.33870434761047363\n",
      "Testing at step=57, batch=40, test loss = 0.0889592245221138, test acc = 0.9750000238418579, time = 0.3435842990875244\n",
      "Testing at step=57, batch=45, test loss = 0.07444141805171967, test acc = 0.9800000190734863, time = 0.3566160202026367\n",
      "Step 57 finished in 309.5275902748108, Train loss = 0.03962018519407138, Test loss = 0.09689698033034802; Train Acc = 0.9868666785955429, Test Acc = 0.9731000089645385\n",
      "Training at step=58, batch=0, train loss = 0.0455147884786129, train acc = 0.9800000190734863, time = 0.9444856643676758\n",
      "Training at step=58, batch=30, train loss = 0.060397449880838394, train acc = 0.9800000190734863, time = 0.9393618106842041\n",
      "Training at step=58, batch=60, train loss = 0.012885508127510548, train acc = 0.9950000047683716, time = 0.961550235748291\n",
      "Training at step=58, batch=90, train loss = 0.012765472754836082, train acc = 1.0, time = 0.9425663948059082\n",
      "Training at step=58, batch=120, train loss = 0.039979249238967896, train acc = 0.9850000143051147, time = 0.9851970672607422\n",
      "Training at step=58, batch=150, train loss = 0.04665157198905945, train acc = 0.9750000238418579, time = 0.9417593479156494\n",
      "Training at step=58, batch=180, train loss = 0.03161603957414627, train acc = 0.9850000143051147, time = 0.9449491500854492\n",
      "Training at step=58, batch=210, train loss = 0.01206471212208271, train acc = 0.9950000047683716, time = 0.9391825199127197\n",
      "Training at step=58, batch=240, train loss = 0.05503109097480774, train acc = 0.9800000190734863, time = 0.9411599636077881\n",
      "Training at step=58, batch=270, train loss = 0.018656639382243156, train acc = 0.9900000095367432, time = 0.9468417167663574\n",
      "Testing at step=58, batch=0, test loss = 0.04167145863175392, test acc = 0.9800000190734863, time = 0.33976221084594727\n",
      "Testing at step=58, batch=5, test loss = 0.1254773885011673, test acc = 0.9700000286102295, time = 0.3373410701751709\n",
      "Testing at step=58, batch=10, test loss = 0.013894901610910892, test acc = 0.9950000047683716, time = 0.34018635749816895\n",
      "Testing at step=58, batch=15, test loss = 0.04468857869505882, test acc = 0.9800000190734863, time = 0.36298561096191406\n",
      "Testing at step=58, batch=20, test loss = 0.19269829988479614, test acc = 0.9350000023841858, time = 0.3596160411834717\n",
      "Testing at step=58, batch=25, test loss = 0.11370182782411575, test acc = 0.9800000190734863, time = 0.3380610942840576\n",
      "Testing at step=58, batch=30, test loss = 0.1292484700679779, test acc = 0.9599999785423279, time = 0.3432445526123047\n",
      "Testing at step=58, batch=35, test loss = 0.04303184524178505, test acc = 0.9850000143051147, time = 0.3384218215942383\n",
      "Testing at step=58, batch=40, test loss = 0.06670071929693222, test acc = 0.9750000238418579, time = 0.33628320693969727\n",
      "Testing at step=58, batch=45, test loss = 0.13125842809677124, test acc = 0.949999988079071, time = 0.3394002914428711\n",
      "Step 58 finished in 310.6086883544922, Train loss = 0.03710686067895343, Test loss = 0.0970692015066743; Train Acc = 0.9878666774431865, Test Acc = 0.9733000087738037\n",
      "Training at step=59, batch=0, train loss = 0.028866060078144073, train acc = 0.9900000095367432, time = 0.9519176483154297\n",
      "Training at step=59, batch=30, train loss = 0.023843979462981224, train acc = 0.9950000047683716, time = 0.9562020301818848\n",
      "Training at step=59, batch=60, train loss = 0.04402424395084381, train acc = 0.9800000190734863, time = 0.9370646476745605\n",
      "Training at step=59, batch=90, train loss = 0.01131351012736559, train acc = 0.9950000047683716, time = 0.9760415554046631\n",
      "Training at step=59, batch=120, train loss = 0.07078836113214493, train acc = 0.9750000238418579, time = 0.9427564144134521\n",
      "Training at step=59, batch=150, train loss = 0.03561880812048912, train acc = 0.9800000190734863, time = 0.9430191516876221\n",
      "Training at step=59, batch=180, train loss = 0.0898023247718811, train acc = 0.9850000143051147, time = 0.9373047351837158\n",
      "Training at step=59, batch=210, train loss = 0.03377172350883484, train acc = 0.9900000095367432, time = 0.9604618549346924\n",
      "Training at step=59, batch=240, train loss = 0.09449873864650726, train acc = 0.9750000238418579, time = 0.9427649974822998\n",
      "Training at step=59, batch=270, train loss = 0.01688644289970398, train acc = 0.9950000047683716, time = 0.9392564296722412\n",
      "Testing at step=59, batch=0, test loss = 0.14935123920440674, test acc = 0.9700000286102295, time = 0.34151124954223633\n",
      "Testing at step=59, batch=5, test loss = 0.03015410341322422, test acc = 0.9900000095367432, time = 0.33933591842651367\n",
      "Testing at step=59, batch=10, test loss = 0.14878316223621368, test acc = 0.9649999737739563, time = 0.3331108093261719\n",
      "Testing at step=59, batch=15, test loss = 0.23250748217105865, test acc = 0.9549999833106995, time = 0.33714842796325684\n",
      "Testing at step=59, batch=20, test loss = 0.08166196197271347, test acc = 0.9750000238418579, time = 0.34007692337036133\n",
      "Testing at step=59, batch=25, test loss = 0.0773582011461258, test acc = 0.9750000238418579, time = 0.3427422046661377\n",
      "Testing at step=59, batch=30, test loss = 0.1149045079946518, test acc = 0.9649999737739563, time = 0.3437528610229492\n",
      "Testing at step=59, batch=35, test loss = 0.10272989422082901, test acc = 0.9750000238418579, time = 0.33722400665283203\n",
      "Testing at step=59, batch=40, test loss = 0.06607873737812042, test acc = 0.9700000286102295, time = 0.33758020401000977\n",
      "Testing at step=59, batch=45, test loss = 0.20100894570350647, test acc = 0.949999988079071, time = 0.3353860378265381\n",
      "Step 59 finished in 311.2551009654999, Train loss = 0.03793619397406777, Test loss = 0.10225170519202947; Train Acc = 0.9874833446741104, Test Acc = 0.9715000033378601\n",
      "Training at step=60, batch=0, train loss = 0.05073415860533714, train acc = 0.9750000238418579, time = 0.9417364597320557\n",
      "Training at step=60, batch=30, train loss = 0.02052980661392212, train acc = 0.9950000047683716, time = 0.9379618167877197\n",
      "Training at step=60, batch=60, train loss = 0.04615360125899315, train acc = 0.9950000047683716, time = 0.9371259212493896\n",
      "Training at step=60, batch=90, train loss = 0.02038167230784893, train acc = 0.9950000047683716, time = 0.9471399784088135\n",
      "Training at step=60, batch=120, train loss = 0.0252822358161211, train acc = 0.9900000095367432, time = 0.9646949768066406\n",
      "Training at step=60, batch=150, train loss = 0.024355683475732803, train acc = 0.9900000095367432, time = 0.939638614654541\n",
      "Training at step=60, batch=180, train loss = 0.07263760268688202, train acc = 0.9750000238418579, time = 0.9453582763671875\n",
      "Training at step=60, batch=210, train loss = 0.08910679817199707, train acc = 0.9700000286102295, time = 0.9390556812286377\n",
      "Training at step=60, batch=240, train loss = 0.03992311283946037, train acc = 0.9900000095367432, time = 0.949791669845581\n",
      "Training at step=60, batch=270, train loss = 0.02947198413312435, train acc = 0.9850000143051147, time = 0.9358339309692383\n",
      "Testing at step=60, batch=0, test loss = 0.03932942822575569, test acc = 0.9800000190734863, time = 0.3367879390716553\n",
      "Testing at step=60, batch=5, test loss = 0.08578085899353027, test acc = 0.9800000190734863, time = 0.3890879154205322\n",
      "Testing at step=60, batch=10, test loss = 0.10522951930761337, test acc = 0.949999988079071, time = 0.3355977535247803\n",
      "Testing at step=60, batch=15, test loss = 0.1457897126674652, test acc = 0.9700000286102295, time = 0.3566863536834717\n",
      "Testing at step=60, batch=20, test loss = 0.18295840919017792, test acc = 0.9599999785423279, time = 0.36275386810302734\n",
      "Testing at step=60, batch=25, test loss = 0.18679212033748627, test acc = 0.9549999833106995, time = 0.3598308563232422\n",
      "Testing at step=60, batch=30, test loss = 0.20619717240333557, test acc = 0.9649999737739563, time = 0.358492374420166\n",
      "Testing at step=60, batch=35, test loss = 0.089180126786232, test acc = 0.9649999737739563, time = 0.35973405838012695\n",
      "Testing at step=60, batch=40, test loss = 0.07899951189756393, test acc = 0.9800000190734863, time = 0.34091854095458984\n",
      "Testing at step=60, batch=45, test loss = 0.23079660534858704, test acc = 0.949999988079071, time = 0.3425297737121582\n",
      "Step 60 finished in 311.56937742233276, Train loss = 0.03630381292508294, Test loss = 0.10492711279541254; Train Acc = 0.9877666781346003, Test Acc = 0.973500007390976\n",
      "Training at step=61, batch=0, train loss = 0.029626013711094856, train acc = 0.9900000095367432, time = 0.9421730041503906\n",
      "Training at step=61, batch=30, train loss = 0.04218338429927826, train acc = 0.9850000143051147, time = 0.9420936107635498\n",
      "Training at step=61, batch=60, train loss = 0.043306101113557816, train acc = 0.9800000190734863, time = 0.9367265701293945\n",
      "Training at step=61, batch=90, train loss = 0.046129260212183, train acc = 0.9750000238418579, time = 0.9414429664611816\n",
      "Training at step=61, batch=120, train loss = 0.004413792863488197, train acc = 1.0, time = 0.942378044128418\n",
      "Training at step=61, batch=150, train loss = 0.07039694488048553, train acc = 0.9700000286102295, time = 0.9343361854553223\n",
      "Training at step=61, batch=180, train loss = 0.027314774692058563, train acc = 0.9900000095367432, time = 0.941730260848999\n",
      "Training at step=61, batch=210, train loss = 0.028224075213074684, train acc = 0.9950000047683716, time = 0.938453197479248\n",
      "Training at step=61, batch=240, train loss = 0.012161743827164173, train acc = 1.0, time = 0.936011791229248\n",
      "Training at step=61, batch=270, train loss = 0.03850129246711731, train acc = 0.9850000143051147, time = 0.9479653835296631\n",
      "Testing at step=61, batch=0, test loss = 0.0688638761639595, test acc = 0.9800000190734863, time = 0.3526895046234131\n",
      "Testing at step=61, batch=5, test loss = 0.0479247085750103, test acc = 0.9850000143051147, time = 0.36074352264404297\n",
      "Testing at step=61, batch=10, test loss = 0.1090666651725769, test acc = 0.9800000190734863, time = 0.3581581115722656\n",
      "Testing at step=61, batch=15, test loss = 0.10444759577512741, test acc = 0.9800000190734863, time = 0.36134910583496094\n",
      "Testing at step=61, batch=20, test loss = 0.033491894602775574, test acc = 0.9900000095367432, time = 0.3559744358062744\n",
      "Testing at step=61, batch=25, test loss = 0.09513165801763535, test acc = 0.9800000190734863, time = 0.3489816188812256\n",
      "Testing at step=61, batch=30, test loss = 0.13733109831809998, test acc = 0.9750000238418579, time = 0.33739376068115234\n",
      "Testing at step=61, batch=35, test loss = 0.10089080780744553, test acc = 0.9700000286102295, time = 0.33777618408203125\n",
      "Testing at step=61, batch=40, test loss = 0.09136417508125305, test acc = 0.9700000286102295, time = 0.34096765518188477\n",
      "Testing at step=61, batch=45, test loss = 0.045983728021383286, test acc = 0.9850000143051147, time = 0.34101343154907227\n",
      "Step 61 finished in 311.0465040206909, Train loss = 0.03679217264599477, Test loss = 0.09170316889882088; Train Acc = 0.9877333438396454, Test Acc = 0.9745000147819519\n",
      "Training at step=62, batch=0, train loss = 0.02184647135436535, train acc = 0.9950000047683716, time = 0.9462413787841797\n",
      "Training at step=62, batch=30, train loss = 0.0892118290066719, train acc = 0.9800000190734863, time = 0.9584541320800781\n",
      "Training at step=62, batch=60, train loss = 0.009029148146510124, train acc = 1.0, time = 0.9414324760437012\n",
      "Training at step=62, batch=90, train loss = 0.044708408415317535, train acc = 0.9900000095367432, time = 0.9430079460144043\n",
      "Training at step=62, batch=120, train loss = 0.060838356614112854, train acc = 0.9850000143051147, time = 0.9393627643585205\n",
      "Training at step=62, batch=150, train loss = 0.01594405807554722, train acc = 0.9950000047683716, time = 0.9373655319213867\n",
      "Training at step=62, batch=180, train loss = 0.029511068016290665, train acc = 0.9900000095367432, time = 0.9474968910217285\n",
      "Training at step=62, batch=210, train loss = 0.033940114080905914, train acc = 0.9850000143051147, time = 0.9427545070648193\n",
      "Training at step=62, batch=240, train loss = 0.08965887874364853, train acc = 0.9850000143051147, time = 0.9378926753997803\n",
      "Training at step=62, batch=270, train loss = 0.059404630213975906, train acc = 0.9800000190734863, time = 0.940854549407959\n",
      "Testing at step=62, batch=0, test loss = 0.048029348254203796, test acc = 0.9800000190734863, time = 0.33779025077819824\n",
      "Testing at step=62, batch=5, test loss = 0.11506441980600357, test acc = 0.9700000286102295, time = 0.33459925651550293\n",
      "Testing at step=62, batch=10, test loss = 0.11824412643909454, test acc = 0.9649999737739563, time = 0.35513782501220703\n",
      "Testing at step=62, batch=15, test loss = 0.04537877067923546, test acc = 0.9850000143051147, time = 0.3535146713256836\n",
      "Testing at step=62, batch=20, test loss = 0.09980769455432892, test acc = 0.9649999737739563, time = 0.36573171615600586\n",
      "Testing at step=62, batch=25, test loss = 0.1418095976114273, test acc = 0.9700000286102295, time = 0.3912069797515869\n",
      "Testing at step=62, batch=30, test loss = 0.16338157653808594, test acc = 0.9599999785423279, time = 0.3668203353881836\n",
      "Testing at step=62, batch=35, test loss = 0.10536380112171173, test acc = 0.9649999737739563, time = 0.35907554626464844\n",
      "Testing at step=62, batch=40, test loss = 0.10903171449899673, test acc = 0.9850000143051147, time = 0.3604910373687744\n",
      "Testing at step=62, batch=45, test loss = 0.04981204494833946, test acc = 0.9750000238418579, time = 0.3583974838256836\n",
      "Step 62 finished in 311.35189962387085, Train loss = 0.035200212787215905, Test loss = 0.09484899584203958; Train Acc = 0.988550010919571, Test Acc = 0.9736000072956085\n",
      "Training at step=63, batch=0, train loss = 0.021014636382460594, train acc = 0.9950000047683716, time = 0.9605369567871094\n",
      "Training at step=63, batch=30, train loss = 0.037233807146549225, train acc = 0.9900000095367432, time = 0.941096305847168\n",
      "Training at step=63, batch=60, train loss = 0.0417591817677021, train acc = 0.9850000143051147, time = 0.9418947696685791\n",
      "Training at step=63, batch=90, train loss = 0.014839164912700653, train acc = 0.9950000047683716, time = 0.9400665760040283\n",
      "Training at step=63, batch=120, train loss = 0.06377732008695602, train acc = 0.9800000190734863, time = 0.938481330871582\n",
      "Training at step=63, batch=150, train loss = 0.028319649398326874, train acc = 0.9900000095367432, time = 0.9767088890075684\n",
      "Training at step=63, batch=180, train loss = 0.0678768903017044, train acc = 0.9800000190734863, time = 0.983241081237793\n",
      "Training at step=63, batch=210, train loss = 0.027975954115390778, train acc = 0.9900000095367432, time = 0.9448826313018799\n",
      "Training at step=63, batch=240, train loss = 0.05285654217004776, train acc = 0.9800000190734863, time = 0.9381918907165527\n",
      "Training at step=63, batch=270, train loss = 0.01181392278522253, train acc = 0.9950000047683716, time = 0.9405279159545898\n",
      "Testing at step=63, batch=0, test loss = 0.08648218214511871, test acc = 0.9850000143051147, time = 0.33718037605285645\n",
      "Testing at step=63, batch=5, test loss = 0.06067119166254997, test acc = 0.9750000238418579, time = 0.3391447067260742\n",
      "Testing at step=63, batch=10, test loss = 0.1304825097322464, test acc = 0.9649999737739563, time = 0.34284496307373047\n",
      "Testing at step=63, batch=15, test loss = 0.0980021059513092, test acc = 0.9700000286102295, time = 0.34391140937805176\n",
      "Testing at step=63, batch=20, test loss = 0.10241935402154922, test acc = 0.9549999833106995, time = 0.33701419830322266\n",
      "Testing at step=63, batch=25, test loss = 0.02945571020245552, test acc = 0.9950000047683716, time = 0.3381617069244385\n",
      "Testing at step=63, batch=30, test loss = 0.10716462880373001, test acc = 0.9649999737739563, time = 0.33750176429748535\n",
      "Testing at step=63, batch=35, test loss = 0.07524722814559937, test acc = 0.9800000190734863, time = 0.36254215240478516\n",
      "Testing at step=63, batch=40, test loss = 0.12855733931064606, test acc = 0.9700000286102295, time = 0.3622474670410156\n",
      "Testing at step=63, batch=45, test loss = 0.13457530736923218, test acc = 0.9649999737739563, time = 0.36082959175109863\n",
      "Step 63 finished in 311.723669052124, Train loss = 0.03576473058744644, Test loss = 0.08921900052577257; Train Acc = 0.9880333439509074, Test Acc = 0.9765000116825103\n",
      "Training at step=64, batch=0, train loss = 0.028523987159132957, train acc = 0.9850000143051147, time = 0.9667484760284424\n",
      "Training at step=64, batch=30, train loss = 0.020280513912439346, train acc = 0.9950000047683716, time = 0.939037561416626\n",
      "Training at step=64, batch=60, train loss = 0.020184049382805824, train acc = 0.9950000047683716, time = 0.9387173652648926\n",
      "Training at step=64, batch=90, train loss = 0.06793983280658722, train acc = 0.9800000190734863, time = 0.9522731304168701\n",
      "Training at step=64, batch=120, train loss = 0.028113558888435364, train acc = 0.9950000047683716, time = 0.9421086311340332\n",
      "Training at step=64, batch=150, train loss = 0.010300558060407639, train acc = 1.0, time = 0.9462971687316895\n",
      "Training at step=64, batch=180, train loss = 0.04623359069228172, train acc = 0.9950000047683716, time = 0.9398815631866455\n",
      "Training at step=64, batch=210, train loss = 0.011904638260602951, train acc = 1.0, time = 0.9363019466400146\n",
      "Training at step=64, batch=240, train loss = 0.013713202439248562, train acc = 0.9950000047683716, time = 0.9446399211883545\n",
      "Training at step=64, batch=270, train loss = 0.052126165479421616, train acc = 0.9750000238418579, time = 0.9402220249176025\n",
      "Testing at step=64, batch=0, test loss = 0.17392171919345856, test acc = 0.9649999737739563, time = 0.3401918411254883\n",
      "Testing at step=64, batch=5, test loss = 0.049841202795505524, test acc = 0.9850000143051147, time = 0.3711230754852295\n",
      "Testing at step=64, batch=10, test loss = 0.1324889212846756, test acc = 0.9700000286102295, time = 0.33646583557128906\n",
      "Testing at step=64, batch=15, test loss = 0.12931175529956818, test acc = 0.9700000286102295, time = 0.340773344039917\n",
      "Testing at step=64, batch=20, test loss = 0.06148498132824898, test acc = 0.9750000238418579, time = 0.3418121337890625\n",
      "Testing at step=64, batch=25, test loss = 0.1328568011522293, test acc = 0.9649999737739563, time = 0.3418269157409668\n",
      "Testing at step=64, batch=30, test loss = 0.07383006811141968, test acc = 0.9800000190734863, time = 0.3375859260559082\n",
      "Testing at step=64, batch=35, test loss = 0.08549995720386505, test acc = 0.9750000238418579, time = 0.338716983795166\n",
      "Testing at step=64, batch=40, test loss = 0.07778725773096085, test acc = 0.9649999737739563, time = 0.3613901138305664\n",
      "Testing at step=64, batch=45, test loss = 0.12864121794700623, test acc = 0.9700000286102295, time = 0.36333680152893066\n",
      "Step 64 finished in 310.6113178730011, Train loss = 0.033741914705994226, Test loss = 0.10415920343250036; Train Acc = 0.9888500102361043, Test Acc = 0.9730000078678132\n",
      "Training at step=65, batch=0, train loss = 0.03582783043384552, train acc = 0.9800000190734863, time = 0.9643981456756592\n",
      "Training at step=65, batch=30, train loss = 0.02253354713320732, train acc = 0.9900000095367432, time = 0.9427902698516846\n",
      "Training at step=65, batch=60, train loss = 0.013310756534337997, train acc = 1.0, time = 0.9415557384490967\n",
      "Training at step=65, batch=90, train loss = 0.025711610913276672, train acc = 0.9900000095367432, time = 0.947847843170166\n",
      "Training at step=65, batch=120, train loss = 0.05868343263864517, train acc = 0.9900000095367432, time = 0.9591870307922363\n",
      "Training at step=65, batch=150, train loss = 0.04573896527290344, train acc = 0.9950000047683716, time = 0.9458527565002441\n",
      "Training at step=65, batch=180, train loss = 0.012866092845797539, train acc = 1.0, time = 0.9381983280181885\n",
      "Training at step=65, batch=210, train loss = 0.04634786769747734, train acc = 0.9700000286102295, time = 0.9405663013458252\n",
      "Training at step=65, batch=240, train loss = 0.01938028074800968, train acc = 0.9900000095367432, time = 0.9457454681396484\n",
      "Training at step=65, batch=270, train loss = 0.041770901530981064, train acc = 0.9800000190734863, time = 0.94089674949646\n",
      "Testing at step=65, batch=0, test loss = 0.1054784283041954, test acc = 0.9750000238418579, time = 0.33800363540649414\n",
      "Testing at step=65, batch=5, test loss = 0.1328587681055069, test acc = 0.9700000286102295, time = 0.3382601737976074\n",
      "Testing at step=65, batch=10, test loss = 0.06722436845302582, test acc = 0.9850000143051147, time = 0.33966064453125\n",
      "Testing at step=65, batch=15, test loss = 0.08404019474983215, test acc = 0.9800000190734863, time = 0.34127020835876465\n",
      "Testing at step=65, batch=20, test loss = 0.04887160286307335, test acc = 0.9850000143051147, time = 0.3393573760986328\n",
      "Testing at step=65, batch=25, test loss = 0.07723870128393173, test acc = 0.9800000190734863, time = 0.3399183750152588\n",
      "Testing at step=65, batch=30, test loss = 0.1385515332221985, test acc = 0.9750000238418579, time = 0.33594393730163574\n",
      "Testing at step=65, batch=35, test loss = 0.07088680565357208, test acc = 0.9900000095367432, time = 0.34185266494750977\n",
      "Testing at step=65, batch=40, test loss = 0.18509578704833984, test acc = 0.9549999833106995, time = 0.3371121883392334\n",
      "Testing at step=65, batch=45, test loss = 0.08861906081438065, test acc = 0.9700000286102295, time = 0.3369929790496826\n",
      "Step 65 finished in 310.11581540107727, Train loss = 0.03343835181634252, Test loss = 0.09747500989586115; Train Acc = 0.9890666768948237, Test Acc = 0.9749000132083893\n",
      "Training at step=66, batch=0, train loss = 0.060956720262765884, train acc = 0.9800000190734863, time = 0.9548454284667969\n",
      "Training at step=66, batch=30, train loss = 0.02396681345999241, train acc = 0.9900000095367432, time = 0.9592406749725342\n",
      "Training at step=66, batch=60, train loss = 0.01307904627174139, train acc = 0.9950000047683716, time = 0.9420382976531982\n",
      "Training at step=66, batch=90, train loss = 0.011060196906328201, train acc = 1.0, time = 0.9455690383911133\n",
      "Training at step=66, batch=120, train loss = 0.036799587309360504, train acc = 0.9900000095367432, time = 0.9520390033721924\n",
      "Training at step=66, batch=150, train loss = 0.032673321664333344, train acc = 0.9800000190734863, time = 0.9439022541046143\n",
      "Training at step=66, batch=180, train loss = 0.02157113142311573, train acc = 0.9850000143051147, time = 0.940251350402832\n",
      "Training at step=66, batch=210, train loss = 0.01629967987537384, train acc = 0.9950000047683716, time = 0.9462692737579346\n",
      "Training at step=66, batch=240, train loss = 0.10538097470998764, train acc = 0.9850000143051147, time = 0.9454669952392578\n",
      "Training at step=66, batch=270, train loss = 0.029094712808728218, train acc = 0.9950000047683716, time = 0.9395604133605957\n",
      "Testing at step=66, batch=0, test loss = 0.07162586599588394, test acc = 0.9800000190734863, time = 0.33495259284973145\n",
      "Testing at step=66, batch=5, test loss = 0.09939244389533997, test acc = 0.9750000238418579, time = 0.33485984802246094\n",
      "Testing at step=66, batch=10, test loss = 0.10147839784622192, test acc = 0.9750000238418579, time = 0.33855581283569336\n",
      "Testing at step=66, batch=15, test loss = 0.04065476357936859, test acc = 0.9950000047683716, time = 0.3371403217315674\n",
      "Testing at step=66, batch=20, test loss = 0.0394117496907711, test acc = 0.9900000095367432, time = 0.33843183517456055\n",
      "Testing at step=66, batch=25, test loss = 0.264671266078949, test acc = 0.9599999785423279, time = 0.34049439430236816\n",
      "Testing at step=66, batch=30, test loss = 0.1434081792831421, test acc = 0.9649999737739563, time = 0.3394465446472168\n",
      "Testing at step=66, batch=35, test loss = 0.11332662403583527, test acc = 0.9750000238418579, time = 0.33571481704711914\n",
      "Testing at step=66, batch=40, test loss = 0.1685507595539093, test acc = 0.9449999928474426, time = 0.3400578498840332\n",
      "Testing at step=66, batch=45, test loss = 0.06402553617954254, test acc = 0.9750000238418579, time = 0.3422529697418213\n",
      "Step 66 finished in 310.7160322666168, Train loss = 0.03250424024493744, Test loss = 0.10409714095294476; Train Acc = 0.9892833431561788, Test Acc = 0.9743000102043152\n",
      "Training at step=67, batch=0, train loss = 0.007694674655795097, train acc = 1.0, time = 0.9492783546447754\n",
      "Training at step=67, batch=30, train loss = 0.01983756572008133, train acc = 0.9900000095367432, time = 0.938713788986206\n",
      "Training at step=67, batch=60, train loss = 0.007695310283452272, train acc = 1.0, time = 0.9472165107727051\n",
      "Training at step=67, batch=90, train loss = 0.0331856943666935, train acc = 0.9900000095367432, time = 0.9450130462646484\n",
      "Training at step=67, batch=120, train loss = 0.025803107768297195, train acc = 0.9900000095367432, time = 0.9390051364898682\n",
      "Training at step=67, batch=150, train loss = 0.03915158659219742, train acc = 0.9850000143051147, time = 0.9497783184051514\n",
      "Training at step=67, batch=180, train loss = 0.030724693089723587, train acc = 0.9850000143051147, time = 0.979968786239624\n",
      "Training at step=67, batch=210, train loss = 0.018562927842140198, train acc = 0.9850000143051147, time = 0.937408447265625\n",
      "Training at step=67, batch=240, train loss = 0.01174530852586031, train acc = 1.0, time = 0.9465351104736328\n",
      "Training at step=67, batch=270, train loss = 0.04030356556177139, train acc = 0.9850000143051147, time = 0.9390823841094971\n",
      "Testing at step=67, batch=0, test loss = 0.030135786160826683, test acc = 0.9850000143051147, time = 0.3585493564605713\n",
      "Testing at step=67, batch=5, test loss = 0.056061748415231705, test acc = 0.9800000190734863, time = 0.35858845710754395\n",
      "Testing at step=67, batch=10, test loss = 0.15358200669288635, test acc = 0.9649999737739563, time = 0.36199951171875\n",
      "Testing at step=67, batch=15, test loss = 0.11162768304347992, test acc = 0.9649999737739563, time = 0.34128308296203613\n",
      "Testing at step=67, batch=20, test loss = 0.09775038808584213, test acc = 0.9750000238418579, time = 0.33989691734313965\n",
      "Testing at step=67, batch=25, test loss = 0.04987884685397148, test acc = 0.9900000095367432, time = 0.33959126472473145\n",
      "Testing at step=67, batch=30, test loss = 0.04656915366649628, test acc = 0.9750000238418579, time = 0.3399968147277832\n",
      "Testing at step=67, batch=35, test loss = 0.0884428322315216, test acc = 0.9750000238418579, time = 0.3412001132965088\n",
      "Testing at step=67, batch=40, test loss = 0.1381485015153885, test acc = 0.9599999785423279, time = 0.34048008918762207\n",
      "Testing at step=67, batch=45, test loss = 0.1326172798871994, test acc = 0.9700000286102295, time = 0.33790087699890137\n",
      "Step 67 finished in 311.4187970161438, Train loss = 0.03222495273919776, Test loss = 0.10537989240139722; Train Acc = 0.989000009894371, Test Acc = 0.9724000036716461\n",
      "Training at step=68, batch=0, train loss = 0.029226701706647873, train acc = 0.9850000143051147, time = 0.9400627613067627\n",
      "Training at step=68, batch=30, train loss = 0.04871707037091255, train acc = 0.9850000143051147, time = 0.9583632946014404\n",
      "Training at step=68, batch=60, train loss = 0.08386663347482681, train acc = 0.9700000286102295, time = 0.9419481754302979\n",
      "Training at step=68, batch=90, train loss = 0.049713049083948135, train acc = 0.9950000047683716, time = 0.9411666393280029\n",
      "Training at step=68, batch=120, train loss = 0.029093287885189056, train acc = 0.9850000143051147, time = 0.9425501823425293\n",
      "Training at step=68, batch=150, train loss = 0.02071419171988964, train acc = 0.9900000095367432, time = 0.9549436569213867\n",
      "Training at step=68, batch=180, train loss = 0.02311377041041851, train acc = 0.9950000047683716, time = 0.9406533241271973\n",
      "Training at step=68, batch=210, train loss = 0.023547982797026634, train acc = 0.9900000095367432, time = 0.9650304317474365\n",
      "Training at step=68, batch=240, train loss = 0.04267437383532524, train acc = 0.9850000143051147, time = 0.9368481636047363\n",
      "Training at step=68, batch=270, train loss = 0.05096130818128586, train acc = 0.9850000143051147, time = 0.989980936050415\n",
      "Testing at step=68, batch=0, test loss = 0.026775915175676346, test acc = 0.9900000095367432, time = 0.33776021003723145\n",
      "Testing at step=68, batch=5, test loss = 0.1583699733018875, test acc = 0.9549999833106995, time = 0.3340163230895996\n",
      "Testing at step=68, batch=10, test loss = 0.08767425268888474, test acc = 0.9700000286102295, time = 0.3350489139556885\n",
      "Testing at step=68, batch=15, test loss = 0.10603342205286026, test acc = 0.9850000143051147, time = 0.33393383026123047\n",
      "Testing at step=68, batch=20, test loss = 0.06653901189565659, test acc = 0.9850000143051147, time = 0.3902008533477783\n",
      "Testing at step=68, batch=25, test loss = 0.12910524010658264, test acc = 0.9700000286102295, time = 0.33801937103271484\n",
      "Testing at step=68, batch=30, test loss = 0.01635831966996193, test acc = 0.9950000047683716, time = 0.338367223739624\n",
      "Testing at step=68, batch=35, test loss = 0.07904543727636337, test acc = 0.9649999737739563, time = 0.33869099617004395\n",
      "Testing at step=68, batch=40, test loss = 0.06780590862035751, test acc = 0.9800000190734863, time = 0.33475232124328613\n",
      "Testing at step=68, batch=45, test loss = 0.07623757421970367, test acc = 0.9800000190734863, time = 0.33924055099487305\n",
      "Step 68 finished in 311.03580379486084, Train loss = 0.03068277610776325, Test loss = 0.098205160908401; Train Acc = 0.99013334274292, Test Acc = 0.975500009059906\n",
      "Training at step=69, batch=0, train loss = 0.006853064522147179, train acc = 1.0, time = 0.9491171836853027\n",
      "Training at step=69, batch=30, train loss = 0.023839883506298065, train acc = 0.9950000047683716, time = 0.9443318843841553\n",
      "Training at step=69, batch=60, train loss = 0.018212532624602318, train acc = 0.9900000095367432, time = 0.9385910034179688\n",
      "Training at step=69, batch=90, train loss = 0.0312969870865345, train acc = 0.9900000095367432, time = 0.9390296936035156\n",
      "Training at step=69, batch=120, train loss = 0.01285528764128685, train acc = 0.9950000047683716, time = 0.944385290145874\n",
      "Training at step=69, batch=150, train loss = 0.009181614965200424, train acc = 0.9950000047683716, time = 0.9427542686462402\n",
      "Training at step=69, batch=180, train loss = 0.016371874138712883, train acc = 0.9950000047683716, time = 0.943619966506958\n",
      "Training at step=69, batch=210, train loss = 0.03931739926338196, train acc = 0.9950000047683716, time = 0.9441299438476562\n",
      "Training at step=69, batch=240, train loss = 0.008781904354691505, train acc = 1.0, time = 0.9479289054870605\n",
      "Training at step=69, batch=270, train loss = 0.010176843032240868, train acc = 0.9950000047683716, time = 0.9382531642913818\n",
      "Testing at step=69, batch=0, test loss = 0.05043456703424454, test acc = 0.9850000143051147, time = 0.3389708995819092\n",
      "Testing at step=69, batch=5, test loss = 0.10107941925525665, test acc = 0.9750000238418579, time = 0.3354651927947998\n",
      "Testing at step=69, batch=10, test loss = 0.21004845201969147, test acc = 0.9449999928474426, time = 0.3367602825164795\n",
      "Testing at step=69, batch=15, test loss = 0.09247585386037827, test acc = 0.9750000238418579, time = 0.33692026138305664\n",
      "Testing at step=69, batch=20, test loss = 0.06937271356582642, test acc = 0.9800000190734863, time = 0.3348236083984375\n",
      "Testing at step=69, batch=25, test loss = 0.11530590057373047, test acc = 0.9649999737739563, time = 0.34642696380615234\n",
      "Testing at step=69, batch=30, test loss = 0.08482998609542847, test acc = 0.9700000286102295, time = 0.34473156929016113\n",
      "Testing at step=69, batch=35, test loss = 0.11541583389043808, test acc = 0.9649999737739563, time = 0.3374514579772949\n",
      "Testing at step=69, batch=40, test loss = 0.04500620812177658, test acc = 0.9900000095367432, time = 0.37012648582458496\n",
      "Testing at step=69, batch=45, test loss = 0.06159710884094238, test acc = 0.9800000190734863, time = 0.33851146697998047\n",
      "Step 69 finished in 310.17463731765747, Train loss = 0.029988793983745078, Test loss = 0.10017938524484635; Train Acc = 0.9904166754086813, Test Acc = 0.9749000108242035\n",
      "Training at step=70, batch=0, train loss = 0.0314151830971241, train acc = 0.9950000047683716, time = 0.9408848285675049\n",
      "Training at step=70, batch=30, train loss = 0.01719539798796177, train acc = 0.9950000047683716, time = 0.9368898868560791\n",
      "Training at step=70, batch=60, train loss = 0.016141891479492188, train acc = 0.9950000047683716, time = 0.9431686401367188\n",
      "Training at step=70, batch=90, train loss = 0.012472203001379967, train acc = 0.9950000047683716, time = 0.9372255802154541\n",
      "Training at step=70, batch=120, train loss = 0.015241708606481552, train acc = 0.9950000047683716, time = 0.9379570484161377\n",
      "Training at step=70, batch=150, train loss = 0.033296916633844376, train acc = 0.9850000143051147, time = 0.9394931793212891\n",
      "Training at step=70, batch=180, train loss = 0.011092067696154118, train acc = 0.9950000047683716, time = 0.9388017654418945\n",
      "Training at step=70, batch=210, train loss = 0.032337892800569534, train acc = 0.9800000190734863, time = 0.9359400272369385\n",
      "Training at step=70, batch=240, train loss = 0.10105667263269424, train acc = 0.9800000190734863, time = 0.9388284683227539\n",
      "Training at step=70, batch=270, train loss = 0.014456438831984997, train acc = 0.9950000047683716, time = 0.9458160400390625\n",
      "Testing at step=70, batch=0, test loss = 0.12228740006685257, test acc = 0.9599999785423279, time = 0.3404510021209717\n",
      "Testing at step=70, batch=5, test loss = 0.005726523231714964, test acc = 1.0, time = 0.3425452709197998\n",
      "Testing at step=70, batch=10, test loss = 0.0478312149643898, test acc = 0.9750000238418579, time = 0.3374767303466797\n",
      "Testing at step=70, batch=15, test loss = 0.02413848601281643, test acc = 0.9950000047683716, time = 0.33692073822021484\n",
      "Testing at step=70, batch=20, test loss = 0.16756339371204376, test acc = 0.9599999785423279, time = 0.33700108528137207\n",
      "Testing at step=70, batch=25, test loss = 0.07385644316673279, test acc = 0.9800000190734863, time = 0.3388831615447998\n",
      "Testing at step=70, batch=30, test loss = 0.02688967064023018, test acc = 0.9800000190734863, time = 0.34328460693359375\n",
      "Testing at step=70, batch=35, test loss = 0.1853555291891098, test acc = 0.9599999785423279, time = 0.3383922576904297\n",
      "Testing at step=70, batch=40, test loss = 0.08654069900512695, test acc = 0.9800000190734863, time = 0.33808016777038574\n",
      "Testing at step=70, batch=45, test loss = 0.04688193276524544, test acc = 0.9800000190734863, time = 0.33783793449401855\n",
      "Step 70 finished in 309.46878480911255, Train loss = 0.029988146948162465, Test loss = 0.09848350412212312; Train Acc = 0.9899500091870626, Test Acc = 0.9754000091552735\n",
      "Training at step=71, batch=0, train loss = 0.016244756057858467, train acc = 0.9950000047683716, time = 0.9470887184143066\n",
      "Training at step=71, batch=30, train loss = 0.018762685358524323, train acc = 1.0, time = 0.9624061584472656\n",
      "Training at step=71, batch=60, train loss = 0.04418277367949486, train acc = 0.9850000143051147, time = 0.9593737125396729\n",
      "Training at step=71, batch=90, train loss = 0.011676863767206669, train acc = 1.0, time = 0.9648175239562988\n",
      "Training at step=71, batch=120, train loss = 0.015570792369544506, train acc = 0.9950000047683716, time = 0.9395275115966797\n",
      "Training at step=71, batch=150, train loss = 0.047320667654275894, train acc = 0.9800000190734863, time = 0.9381086826324463\n",
      "Training at step=71, batch=180, train loss = 0.009389009326696396, train acc = 0.9950000047683716, time = 0.9454250335693359\n",
      "Training at step=71, batch=210, train loss = 0.03657792508602142, train acc = 0.9850000143051147, time = 0.937614917755127\n",
      "Training at step=71, batch=240, train loss = 0.04801945760846138, train acc = 0.9900000095367432, time = 0.9410967826843262\n",
      "Training at step=71, batch=270, train loss = 0.023783216252923012, train acc = 0.9900000095367432, time = 0.9473462104797363\n",
      "Testing at step=71, batch=0, test loss = 0.08364376425743103, test acc = 0.9850000143051147, time = 0.35317468643188477\n",
      "Testing at step=71, batch=5, test loss = 0.06032286584377289, test acc = 0.9800000190734863, time = 0.336942195892334\n",
      "Testing at step=71, batch=10, test loss = 0.11063889414072037, test acc = 0.9800000190734863, time = 0.33849453926086426\n",
      "Testing at step=71, batch=15, test loss = 0.0402042381465435, test acc = 0.9900000095367432, time = 0.3363063335418701\n",
      "Testing at step=71, batch=20, test loss = 0.14676281809806824, test acc = 0.9800000190734863, time = 0.3349947929382324\n",
      "Testing at step=71, batch=25, test loss = 0.06861940026283264, test acc = 0.9800000190734863, time = 0.33789801597595215\n",
      "Testing at step=71, batch=30, test loss = 0.08283597975969315, test acc = 0.9850000143051147, time = 0.33673524856567383\n",
      "Testing at step=71, batch=35, test loss = 0.17582279443740845, test acc = 0.9599999785423279, time = 0.3387303352355957\n",
      "Testing at step=71, batch=40, test loss = 0.06566060334444046, test acc = 0.9700000286102295, time = 0.33756136894226074\n",
      "Testing at step=71, batch=45, test loss = 0.06390557438135147, test acc = 0.9850000143051147, time = 0.33807992935180664\n",
      "Step 71 finished in 311.4745669364929, Train loss = 0.029884866756231834, Test loss = 0.10227354653179646; Train Acc = 0.9899500087896983, Test Acc = 0.9758000111579895\n",
      "Training at step=72, batch=0, train loss = 0.012256563641130924, train acc = 1.0, time = 0.9503598213195801\n",
      "Training at step=72, batch=30, train loss = 0.008977357298135757, train acc = 1.0, time = 0.9402902126312256\n",
      "Training at step=72, batch=60, train loss = 0.015829872339963913, train acc = 1.0, time = 0.9694480895996094\n",
      "Training at step=72, batch=90, train loss = 0.026663288474082947, train acc = 0.9900000095367432, time = 0.9421508312225342\n",
      "Training at step=72, batch=120, train loss = 0.059664882719516754, train acc = 0.9850000143051147, time = 0.9391622543334961\n",
      "Training at step=72, batch=150, train loss = 0.02516855299472809, train acc = 0.9900000095367432, time = 0.9488658905029297\n",
      "Training at step=72, batch=180, train loss = 0.044569578021764755, train acc = 0.9850000143051147, time = 0.9361751079559326\n",
      "Training at step=72, batch=210, train loss = 0.0053084129467606544, train acc = 1.0, time = 0.9426820278167725\n",
      "Training at step=72, batch=240, train loss = 0.04981313645839691, train acc = 0.9850000143051147, time = 0.9466826915740967\n",
      "Training at step=72, batch=270, train loss = 0.026212677359580994, train acc = 0.9900000095367432, time = 0.9423716068267822\n",
      "Testing at step=72, batch=0, test loss = 0.21387583017349243, test acc = 0.9750000238418579, time = 0.33829474449157715\n",
      "Testing at step=72, batch=5, test loss = 0.044528983533382416, test acc = 0.9900000095367432, time = 0.3387322425842285\n",
      "Testing at step=72, batch=10, test loss = 0.10516135394573212, test acc = 0.9800000190734863, time = 0.33611035346984863\n",
      "Testing at step=72, batch=15, test loss = 0.029820775613188744, test acc = 0.9850000143051147, time = 0.3392517566680908\n",
      "Testing at step=72, batch=20, test loss = 0.15734189748764038, test acc = 0.9549999833106995, time = 0.33742475509643555\n",
      "Testing at step=72, batch=25, test loss = 0.032782990485429764, test acc = 0.9850000143051147, time = 0.34104061126708984\n",
      "Testing at step=72, batch=30, test loss = 0.1893368363380432, test acc = 0.9599999785423279, time = 0.3390021324157715\n",
      "Testing at step=72, batch=35, test loss = 0.10888288170099258, test acc = 0.9750000238418579, time = 0.3367893695831299\n",
      "Testing at step=72, batch=40, test loss = 0.11899113655090332, test acc = 0.9700000286102295, time = 0.34025049209594727\n",
      "Testing at step=72, batch=45, test loss = 0.23859809339046478, test acc = 0.949999988079071, time = 0.33643651008605957\n",
      "Step 72 finished in 311.08070635795593, Train loss = 0.028418385942156118, Test loss = 0.10313522931188344; Train Acc = 0.9908666751782099, Test Acc = 0.9753000092506409\n",
      "Training at step=73, batch=0, train loss = 0.030666038393974304, train acc = 0.9900000095367432, time = 0.939917802810669\n",
      "Training at step=73, batch=30, train loss = 0.05109070613980293, train acc = 0.9800000190734863, time = 0.9754106998443604\n",
      "Training at step=73, batch=60, train loss = 0.01863022707402706, train acc = 0.9950000047683716, time = 0.9618127346038818\n",
      "Training at step=73, batch=90, train loss = 0.04052400588989258, train acc = 0.9850000143051147, time = 0.9409112930297852\n",
      "Training at step=73, batch=120, train loss = 0.007744020316749811, train acc = 0.9950000047683716, time = 0.940605640411377\n",
      "Training at step=73, batch=150, train loss = 0.011020348407328129, train acc = 0.9950000047683716, time = 0.9382410049438477\n",
      "Training at step=73, batch=180, train loss = 0.021568894386291504, train acc = 0.9900000095367432, time = 1.0665786266326904\n",
      "Training at step=73, batch=210, train loss = 0.012296630069613457, train acc = 0.9950000047683716, time = 0.936032772064209\n",
      "Training at step=73, batch=240, train loss = 0.03171564266085625, train acc = 0.9950000047683716, time = 0.9406082630157471\n",
      "Training at step=73, batch=270, train loss = 0.025192787870764732, train acc = 0.9850000143051147, time = 0.9686558246612549\n",
      "Testing at step=73, batch=0, test loss = 0.030844885855913162, test acc = 0.9900000095367432, time = 0.3409862518310547\n",
      "Testing at step=73, batch=5, test loss = 0.09284863620996475, test acc = 0.9750000238418579, time = 0.36066174507141113\n",
      "Testing at step=73, batch=10, test loss = 0.11809089034795761, test acc = 0.9700000286102295, time = 0.36066579818725586\n",
      "Testing at step=73, batch=15, test loss = 0.18282480537891388, test acc = 0.9750000238418579, time = 0.35954713821411133\n",
      "Testing at step=73, batch=20, test loss = 0.11769846826791763, test acc = 0.9649999737739563, time = 0.35961318016052246\n",
      "Testing at step=73, batch=25, test loss = 0.07752995938062668, test acc = 0.9850000143051147, time = 0.3605935573577881\n",
      "Testing at step=73, batch=30, test loss = 0.12460743635892868, test acc = 0.9700000286102295, time = 0.35745930671691895\n",
      "Testing at step=73, batch=35, test loss = 0.11028977483510971, test acc = 0.9750000238418579, time = 0.33835721015930176\n",
      "Testing at step=73, batch=40, test loss = 0.15822595357894897, test acc = 0.9649999737739563, time = 0.3395540714263916\n",
      "Testing at step=73, batch=45, test loss = 0.050352707505226135, test acc = 0.9800000190734863, time = 0.336148738861084\n",
      "Step 73 finished in 310.6203656196594, Train loss = 0.029143331629456953, Test loss = 0.0974259240180254; Train Acc = 0.9904000089565913, Test Acc = 0.9764000153541565\n",
      "Training at step=74, batch=0, train loss = 0.03213713318109512, train acc = 0.9850000143051147, time = 0.9466297626495361\n",
      "Training at step=74, batch=30, train loss = 0.007482482586055994, train acc = 1.0, time = 0.9381277561187744\n",
      "Training at step=74, batch=60, train loss = 0.06843341886997223, train acc = 0.9850000143051147, time = 0.93965744972229\n",
      "Training at step=74, batch=90, train loss = 0.028627004474401474, train acc = 0.9900000095367432, time = 0.9430112838745117\n",
      "Training at step=74, batch=120, train loss = 0.01517284195870161, train acc = 0.9950000047683716, time = 0.9449584484100342\n",
      "Training at step=74, batch=150, train loss = 0.01129972841590643, train acc = 1.0, time = 0.9376499652862549\n",
      "Training at step=74, batch=180, train loss = 0.07157766073942184, train acc = 0.9800000190734863, time = 0.9393584728240967\n",
      "Training at step=74, batch=210, train loss = 0.04353611171245575, train acc = 0.9950000047683716, time = 0.9390873908996582\n",
      "Training at step=74, batch=240, train loss = 0.04310044273734093, train acc = 0.9900000095367432, time = 0.9404056072235107\n",
      "Training at step=74, batch=270, train loss = 0.024067021906375885, train acc = 0.9900000095367432, time = 0.9449951648712158\n",
      "Testing at step=74, batch=0, test loss = 0.16039930284023285, test acc = 0.9549999833106995, time = 0.3386974334716797\n",
      "Testing at step=74, batch=5, test loss = 0.1908494234085083, test acc = 0.9599999785423279, time = 0.340442419052124\n",
      "Testing at step=74, batch=10, test loss = 0.16756896674633026, test acc = 0.9800000190734863, time = 0.33824944496154785\n",
      "Testing at step=74, batch=15, test loss = 0.03790578991174698, test acc = 0.9850000143051147, time = 0.3394203186035156\n",
      "Testing at step=74, batch=20, test loss = 0.11760994791984558, test acc = 0.9649999737739563, time = 0.343339204788208\n",
      "Testing at step=74, batch=25, test loss = 0.1426348090171814, test acc = 0.9800000190734863, time = 0.3560962677001953\n",
      "Testing at step=74, batch=30, test loss = 0.14649824798107147, test acc = 0.9700000286102295, time = 0.33873867988586426\n",
      "Testing at step=74, batch=35, test loss = 0.03758440911769867, test acc = 0.9850000143051147, time = 0.3546462059020996\n",
      "Testing at step=74, batch=40, test loss = 0.050620317459106445, test acc = 0.9850000143051147, time = 0.34233522415161133\n",
      "Testing at step=74, batch=45, test loss = 0.14983679354190826, test acc = 0.9649999737739563, time = 0.3399341106414795\n",
      "Step 74 finished in 309.8764569759369, Train loss = 0.028686699400035043, Test loss = 0.10937235446646809; Train Acc = 0.9900333426396052, Test Acc = 0.9752000093460083\n",
      "Training at step=75, batch=0, train loss = 0.017990842461586, train acc = 0.9950000047683716, time = 0.9476075172424316\n",
      "Training at step=75, batch=30, train loss = 0.004635674878954887, train acc = 1.0, time = 0.9429929256439209\n",
      "Training at step=75, batch=60, train loss = 0.028636157512664795, train acc = 0.9900000095367432, time = 0.9375922679901123\n",
      "Training at step=75, batch=90, train loss = 0.013465169817209244, train acc = 0.9950000047683716, time = 0.9477424621582031\n",
      "Training at step=75, batch=120, train loss = 0.036725059151649475, train acc = 0.9850000143051147, time = 0.9510376453399658\n",
      "Training at step=75, batch=150, train loss = 0.03720996528863907, train acc = 0.9800000190734863, time = 0.9504661560058594\n",
      "Training at step=75, batch=180, train loss = 0.018496073782444, train acc = 0.9900000095367432, time = 0.9408829212188721\n",
      "Training at step=75, batch=210, train loss = 0.06095794215798378, train acc = 0.9750000238418579, time = 0.9401137828826904\n",
      "Training at step=75, batch=240, train loss = 0.06224702298641205, train acc = 0.9800000190734863, time = 0.9506096839904785\n",
      "Training at step=75, batch=270, train loss = 0.028757160529494286, train acc = 0.9900000095367432, time = 0.9454965591430664\n",
      "Testing at step=75, batch=0, test loss = 0.11720595508813858, test acc = 0.9700000286102295, time = 0.3421659469604492\n",
      "Testing at step=75, batch=5, test loss = 0.12923689186573029, test acc = 0.9599999785423279, time = 0.3414478302001953\n",
      "Testing at step=75, batch=10, test loss = 0.1191011592745781, test acc = 0.9700000286102295, time = 0.3378479480743408\n",
      "Testing at step=75, batch=15, test loss = 0.05381878837943077, test acc = 0.9750000238418579, time = 0.46074628829956055\n",
      "Testing at step=75, batch=20, test loss = 0.16340140998363495, test acc = 0.9649999737739563, time = 0.3383970260620117\n",
      "Testing at step=75, batch=25, test loss = 0.193180650472641, test acc = 0.9750000238418579, time = 0.3372840881347656\n",
      "Testing at step=75, batch=30, test loss = 0.20091880857944489, test acc = 0.9599999785423279, time = 0.3389089107513428\n",
      "Testing at step=75, batch=35, test loss = 0.14376991987228394, test acc = 0.9700000286102295, time = 0.33446812629699707\n",
      "Testing at step=75, batch=40, test loss = 0.12260682135820389, test acc = 0.9750000238418579, time = 0.33455514907836914\n",
      "Testing at step=75, batch=45, test loss = 0.08776402473449707, test acc = 0.9649999737739563, time = 0.33878350257873535\n",
      "Step 75 finished in 310.2243480682373, Train loss = 0.02890240269480273, Test loss = 0.10638039711862803; Train Acc = 0.9899000092347463, Test Acc = 0.9752000105381012\n",
      "Training at step=76, batch=0, train loss = 0.00583144836127758, train acc = 1.0, time = 0.9401736259460449\n",
      "Training at step=76, batch=30, train loss = 0.049614544957876205, train acc = 0.9900000095367432, time = 0.9571700096130371\n",
      "Training at step=76, batch=60, train loss = 0.019038530066609383, train acc = 0.9900000095367432, time = 0.9443895816802979\n",
      "Training at step=76, batch=90, train loss = 0.0056466879323124886, train acc = 1.0, time = 0.9436662197113037\n",
      "Training at step=76, batch=120, train loss = 0.08559592068195343, train acc = 0.9850000143051147, time = 0.9419353008270264\n",
      "Training at step=76, batch=150, train loss = 0.0126745430752635, train acc = 1.0, time = 0.948387861251831\n",
      "Training at step=76, batch=180, train loss = 0.05849660560488701, train acc = 0.9850000143051147, time = 0.949988842010498\n",
      "Training at step=76, batch=210, train loss = 0.07182896882295609, train acc = 0.9800000190734863, time = 0.9502298831939697\n",
      "Training at step=76, batch=240, train loss = 0.02258647233247757, train acc = 0.9900000095367432, time = 0.9422786235809326\n",
      "Training at step=76, batch=270, train loss = 0.040271248668432236, train acc = 0.9900000095367432, time = 0.9371497631072998\n",
      "Testing at step=76, batch=0, test loss = 0.3989797830581665, test acc = 0.9549999833106995, time = 0.3378787040710449\n",
      "Testing at step=76, batch=5, test loss = 0.05818207561969757, test acc = 0.9800000190734863, time = 0.3373711109161377\n",
      "Testing at step=76, batch=10, test loss = 0.14916758239269257, test acc = 0.9700000286102295, time = 0.35215139389038086\n",
      "Testing at step=76, batch=15, test loss = 0.14157405495643616, test acc = 0.9649999737739563, time = 0.33475446701049805\n",
      "Testing at step=76, batch=20, test loss = 0.08949267119169235, test acc = 0.9750000238418579, time = 0.3382234573364258\n",
      "Testing at step=76, batch=25, test loss = 0.05981160327792168, test acc = 0.9750000238418579, time = 0.3370974063873291\n",
      "Testing at step=76, batch=30, test loss = 0.031384315341711044, test acc = 0.9850000143051147, time = 0.34084391593933105\n",
      "Testing at step=76, batch=35, test loss = 0.109867163002491, test acc = 0.9800000190734863, time = 0.33908629417419434\n",
      "Testing at step=76, batch=40, test loss = 0.10951128602027893, test acc = 0.9700000286102295, time = 0.3396773338317871\n",
      "Testing at step=76, batch=45, test loss = 0.06157960370182991, test acc = 0.9800000190734863, time = 0.33953857421875\n",
      "Step 76 finished in 310.7333724498749, Train loss = 0.026336309924178448, Test loss = 0.11504919558763504; Train Acc = 0.9908833418289821, Test Acc = 0.974800009727478\n",
      "Training at step=77, batch=0, train loss = 0.01151548232883215, train acc = 0.9950000047683716, time = 0.9401669502258301\n",
      "Training at step=77, batch=30, train loss = 0.01244587916880846, train acc = 0.9950000047683716, time = 0.9444961547851562\n",
      "Training at step=77, batch=60, train loss = 0.05674121528863907, train acc = 0.9850000143051147, time = 0.941540002822876\n",
      "Training at step=77, batch=90, train loss = 0.02147699147462845, train acc = 0.9950000047683716, time = 0.944474458694458\n",
      "Training at step=77, batch=120, train loss = 0.003265978069975972, train acc = 1.0, time = 0.9502959251403809\n",
      "Training at step=77, batch=150, train loss = 0.028959359973669052, train acc = 0.9850000143051147, time = 0.9431416988372803\n",
      "Training at step=77, batch=180, train loss = 0.002979616168886423, train acc = 1.0, time = 0.9355525970458984\n",
      "Training at step=77, batch=210, train loss = 0.01910526119172573, train acc = 0.9900000095367432, time = 0.9357051849365234\n",
      "Training at step=77, batch=240, train loss = 0.08507172763347626, train acc = 0.9750000238418579, time = 0.9364955425262451\n",
      "Training at step=77, batch=270, train loss = 0.06642298400402069, train acc = 0.9800000190734863, time = 0.9530627727508545\n",
      "Testing at step=77, batch=0, test loss = 0.13168230652809143, test acc = 0.9750000238418579, time = 0.3399922847747803\n",
      "Testing at step=77, batch=5, test loss = 0.03989148139953613, test acc = 0.9950000047683716, time = 0.33903002738952637\n",
      "Testing at step=77, batch=10, test loss = 0.11447139084339142, test acc = 0.9599999785423279, time = 0.33803677558898926\n",
      "Testing at step=77, batch=15, test loss = 0.1098746657371521, test acc = 0.9800000190734863, time = 0.3371164798736572\n",
      "Testing at step=77, batch=20, test loss = 0.06419837474822998, test acc = 0.9850000143051147, time = 0.33758997917175293\n",
      "Testing at step=77, batch=25, test loss = 0.10943672060966492, test acc = 0.9850000143051147, time = 0.3393833637237549\n",
      "Testing at step=77, batch=30, test loss = 0.13816192746162415, test acc = 0.9750000238418579, time = 0.33606743812561035\n",
      "Testing at step=77, batch=35, test loss = 0.16110652685165405, test acc = 0.9700000286102295, time = 0.33669090270996094\n",
      "Testing at step=77, batch=40, test loss = 0.12470453977584839, test acc = 0.9649999737739563, time = 0.335681676864624\n",
      "Testing at step=77, batch=45, test loss = 0.08938460052013397, test acc = 0.9850000143051147, time = 0.33547139167785645\n",
      "Step 77 finished in 309.98985385894775, Train loss = 0.027895243344440435, Test loss = 0.12533676378428937; Train Acc = 0.9908333412806193, Test Acc = 0.9716000068187713\n",
      "Training at step=78, batch=0, train loss = 0.0367242656648159, train acc = 0.9900000095367432, time = 0.9392127990722656\n",
      "Training at step=78, batch=30, train loss = 0.04114817455410957, train acc = 0.9850000143051147, time = 0.9483208656311035\n",
      "Training at step=78, batch=60, train loss = 0.04282825440168381, train acc = 0.9750000238418579, time = 0.9793365001678467\n",
      "Training at step=78, batch=90, train loss = 0.04900922626256943, train acc = 0.9800000190734863, time = 0.960507869720459\n",
      "Training at step=78, batch=120, train loss = 0.026276301592588425, train acc = 0.9950000047683716, time = 0.9419639110565186\n",
      "Training at step=78, batch=150, train loss = 0.027055773884058, train acc = 0.9900000095367432, time = 0.9359385967254639\n",
      "Training at step=78, batch=180, train loss = 0.026521921157836914, train acc = 0.9950000047683716, time = 0.937004804611206\n",
      "Training at step=78, batch=210, train loss = 0.024403300136327744, train acc = 0.9950000047683716, time = 0.9434986114501953\n",
      "Training at step=78, batch=240, train loss = 0.03957868739962578, train acc = 0.9850000143051147, time = 0.9359228610992432\n",
      "Training at step=78, batch=270, train loss = 0.008965525776147842, train acc = 0.9950000047683716, time = 0.9362294673919678\n",
      "Testing at step=78, batch=0, test loss = 0.025589562952518463, test acc = 0.9900000095367432, time = 0.3378326892852783\n",
      "Testing at step=78, batch=5, test loss = 0.035622525960206985, test acc = 0.9800000190734863, time = 0.33643364906311035\n",
      "Testing at step=78, batch=10, test loss = 0.03427629545331001, test acc = 0.9900000095367432, time = 0.33488988876342773\n",
      "Testing at step=78, batch=15, test loss = 0.09312576055526733, test acc = 0.9700000286102295, time = 0.33567190170288086\n",
      "Testing at step=78, batch=20, test loss = 0.13587385416030884, test acc = 0.9599999785423279, time = 0.3351428508758545\n",
      "Testing at step=78, batch=25, test loss = 0.1287514716386795, test acc = 0.9649999737739563, time = 0.3360910415649414\n",
      "Testing at step=78, batch=30, test loss = 0.10516443103551865, test acc = 0.9750000238418579, time = 0.3373556137084961\n",
      "Testing at step=78, batch=35, test loss = 0.15214575827121735, test acc = 0.9700000286102295, time = 0.3355138301849365\n",
      "Testing at step=78, batch=40, test loss = 0.14753559231758118, test acc = 0.9449999928474426, time = 0.3389925956726074\n",
      "Testing at step=78, batch=45, test loss = 0.06623437255620956, test acc = 0.9850000143051147, time = 0.3383316993713379\n",
      "Step 78 finished in 310.950891494751, Train loss = 0.027022170471803594, Test loss = 0.11753819422796369; Train Acc = 0.9904333420594533, Test Acc = 0.9721000075340271\n",
      "Training at step=79, batch=0, train loss = 0.03322993218898773, train acc = 0.9800000190734863, time = 0.9786581993103027\n",
      "Training at step=79, batch=30, train loss = 0.002400394296273589, train acc = 1.0, time = 0.9413788318634033\n",
      "Training at step=79, batch=60, train loss = 0.045408476144075394, train acc = 0.9850000143051147, time = 0.9601759910583496\n",
      "Training at step=79, batch=90, train loss = 0.022411255165934563, train acc = 0.9900000095367432, time = 0.9388453960418701\n",
      "Training at step=79, batch=120, train loss = 0.005930934567004442, train acc = 1.0, time = 0.9375221729278564\n",
      "Training at step=79, batch=150, train loss = 0.04667304456233978, train acc = 0.9750000238418579, time = 0.9395356178283691\n",
      "Training at step=79, batch=180, train loss = 0.01844744011759758, train acc = 0.9900000095367432, time = 0.9408948421478271\n",
      "Training at step=79, batch=210, train loss = 0.01779354177415371, train acc = 0.9950000047683716, time = 0.9604523181915283\n",
      "Training at step=79, batch=240, train loss = 0.020821543410420418, train acc = 0.9900000095367432, time = 0.9464507102966309\n",
      "Training at step=79, batch=270, train loss = 0.05006657540798187, train acc = 0.9900000095367432, time = 0.9585914611816406\n",
      "Testing at step=79, batch=0, test loss = 0.14123554527759552, test acc = 0.9599999785423279, time = 0.33593225479125977\n",
      "Testing at step=79, batch=5, test loss = 0.16194826364517212, test acc = 0.9700000286102295, time = 0.3467214107513428\n",
      "Testing at step=79, batch=10, test loss = 0.07998914271593094, test acc = 0.9750000238418579, time = 0.3357815742492676\n",
      "Testing at step=79, batch=15, test loss = 0.13176724314689636, test acc = 0.9599999785423279, time = 0.3366210460662842\n",
      "Testing at step=79, batch=20, test loss = 0.12472070753574371, test acc = 0.9700000286102295, time = 0.3359689712524414\n",
      "Testing at step=79, batch=25, test loss = 0.06763225793838501, test acc = 0.9850000143051147, time = 0.336320161819458\n",
      "Testing at step=79, batch=30, test loss = 0.10057986527681351, test acc = 0.9800000190734863, time = 0.33667874336242676\n",
      "Testing at step=79, batch=35, test loss = 0.012262395583093166, test acc = 0.9950000047683716, time = 0.3361842632293701\n",
      "Testing at step=79, batch=40, test loss = 0.0627906545996666, test acc = 0.9850000143051147, time = 0.3619508743286133\n",
      "Testing at step=79, batch=45, test loss = 0.14608308672904968, test acc = 0.9700000286102295, time = 0.3345963954925537\n",
      "Step 79 finished in 310.6106424331665, Train loss = 0.025715225026166688, Test loss = 0.11263600008562208; Train Acc = 0.9911000082890192, Test Acc = 0.9749000120162964\n",
      "Training at step=80, batch=0, train loss = 0.010527125559747219, train acc = 1.0, time = 0.949237585067749\n",
      "Training at step=80, batch=30, train loss = 0.02690025046467781, train acc = 0.9800000190734863, time = 0.9416217803955078\n",
      "Training at step=80, batch=60, train loss = 0.023696977645158768, train acc = 0.9900000095367432, time = 0.9405112266540527\n",
      "Training at step=80, batch=90, train loss = 0.029779229313135147, train acc = 0.9850000143051147, time = 0.9415888786315918\n",
      "Training at step=80, batch=120, train loss = 0.03979729861021042, train acc = 0.9850000143051147, time = 0.9627642631530762\n",
      "Training at step=80, batch=150, train loss = 0.02488822303712368, train acc = 0.9900000095367432, time = 0.9404244422912598\n",
      "Training at step=80, batch=180, train loss = 0.03373945876955986, train acc = 0.9850000143051147, time = 0.9428157806396484\n",
      "Training at step=80, batch=210, train loss = 0.02040795050561428, train acc = 0.9950000047683716, time = 0.9430301189422607\n",
      "Training at step=80, batch=240, train loss = 0.033842433243989944, train acc = 0.9850000143051147, time = 1.1099729537963867\n",
      "Training at step=80, batch=270, train loss = 0.01680215634405613, train acc = 0.9900000095367432, time = 0.9487156867980957\n",
      "Testing at step=80, batch=0, test loss = 0.10417678952217102, test acc = 0.9700000286102295, time = 0.3396151065826416\n",
      "Testing at step=80, batch=5, test loss = 0.23228363692760468, test acc = 0.9399999976158142, time = 0.3397073745727539\n",
      "Testing at step=80, batch=10, test loss = 0.10168862342834473, test acc = 0.9750000238418579, time = 0.34192442893981934\n",
      "Testing at step=80, batch=15, test loss = 0.12406932562589645, test acc = 0.9649999737739563, time = 0.3346691131591797\n",
      "Testing at step=80, batch=20, test loss = 0.04709344357252121, test acc = 0.9750000238418579, time = 0.3362290859222412\n",
      "Testing at step=80, batch=25, test loss = 0.0702248066663742, test acc = 0.9750000238418579, time = 0.33402037620544434\n",
      "Testing at step=80, batch=30, test loss = 0.07067155092954636, test acc = 0.9750000238418579, time = 0.3364715576171875\n",
      "Testing at step=80, batch=35, test loss = 0.14606255292892456, test acc = 0.9800000190734863, time = 0.33832287788391113\n",
      "Testing at step=80, batch=40, test loss = 0.04932929947972298, test acc = 0.9850000143051147, time = 0.33698582649230957\n",
      "Testing at step=80, batch=45, test loss = 0.1560211032629013, test acc = 0.9700000286102295, time = 0.34203219413757324\n",
      "Step 80 finished in 311.5115144252777, Train loss = 0.025811525316676125, Test loss = 0.11663069568574429; Train Acc = 0.9909333415826161, Test Acc = 0.9736000108718872\n",
      "Training at step=81, batch=0, train loss = 0.059253253042697906, train acc = 0.9800000190734863, time = 0.9423449039459229\n",
      "Training at step=81, batch=30, train loss = 0.005163044203072786, train acc = 1.0, time = 0.9374642372131348\n",
      "Training at step=81, batch=60, train loss = 0.010662531480193138, train acc = 1.0, time = 0.9413669109344482\n",
      "Training at step=81, batch=90, train loss = 0.02347252517938614, train acc = 0.9900000095367432, time = 0.939145565032959\n",
      "Training at step=81, batch=120, train loss = 0.003606179729104042, train acc = 1.0, time = 0.9403924942016602\n",
      "Training at step=81, batch=150, train loss = 0.04407373070716858, train acc = 0.9900000095367432, time = 0.9352946281433105\n",
      "Training at step=81, batch=180, train loss = 0.08732527494430542, train acc = 0.9800000190734863, time = 0.9624330997467041\n",
      "Training at step=81, batch=210, train loss = 0.027175795286893845, train acc = 0.9950000047683716, time = 0.9449756145477295\n",
      "Training at step=81, batch=240, train loss = 0.018819458782672882, train acc = 0.9950000047683716, time = 0.9389145374298096\n",
      "Training at step=81, batch=270, train loss = 0.035448309034109116, train acc = 0.9950000047683716, time = 0.9479739665985107\n",
      "Testing at step=81, batch=0, test loss = 0.13837164640426636, test acc = 0.9700000286102295, time = 0.3383293151855469\n",
      "Testing at step=81, batch=5, test loss = 0.007936271838843822, test acc = 1.0, time = 0.33965086936950684\n",
      "Testing at step=81, batch=10, test loss = 0.16204504668712616, test acc = 0.9549999833106995, time = 0.3355531692504883\n",
      "Testing at step=81, batch=15, test loss = 0.131217360496521, test acc = 0.9750000238418579, time = 0.33789944648742676\n",
      "Testing at step=81, batch=20, test loss = 0.1154053658246994, test acc = 0.9700000286102295, time = 0.3373727798461914\n",
      "Testing at step=81, batch=25, test loss = 0.1415243148803711, test acc = 0.9649999737739563, time = 0.34134650230407715\n",
      "Testing at step=81, batch=30, test loss = 0.04482993483543396, test acc = 0.9900000095367432, time = 0.3417797088623047\n",
      "Testing at step=81, batch=35, test loss = 0.03165905177593231, test acc = 0.9950000047683716, time = 0.3353562355041504\n",
      "Testing at step=81, batch=40, test loss = 0.058411698788404465, test acc = 0.9900000095367432, time = 0.34210991859436035\n",
      "Testing at step=81, batch=45, test loss = 0.08558165282011032, test acc = 0.9750000238418579, time = 0.355496883392334\n",
      "Step 81 finished in 310.68399143218994, Train loss = 0.024764220400247724, Test loss = 0.12191590242087841; Train Acc = 0.9915166747570038, Test Acc = 0.9743000078201294\n",
      "Training at step=82, batch=0, train loss = 0.015009820461273193, train acc = 0.9950000047683716, time = 0.9387223720550537\n",
      "Training at step=82, batch=30, train loss = 0.004370344337075949, train acc = 1.0, time = 0.9403762817382812\n",
      "Training at step=82, batch=60, train loss = 0.01372985728085041, train acc = 0.9950000047683716, time = 0.9381103515625\n",
      "Training at step=82, batch=90, train loss = 0.02551860362291336, train acc = 0.9900000095367432, time = 0.9379422664642334\n",
      "Training at step=82, batch=120, train loss = 0.011738848872482777, train acc = 0.9950000047683716, time = 0.9429302215576172\n",
      "Training at step=82, batch=150, train loss = 0.01892266795039177, train acc = 0.9950000047683716, time = 0.9363303184509277\n",
      "Training at step=82, batch=180, train loss = 0.004729670472443104, train acc = 1.0, time = 0.9707839488983154\n",
      "Training at step=82, batch=210, train loss = 0.01781177707016468, train acc = 0.9900000095367432, time = 0.9405379295349121\n",
      "Training at step=82, batch=240, train loss = 0.008988945744931698, train acc = 0.9950000047683716, time = 0.9795610904693604\n",
      "Training at step=82, batch=270, train loss = 0.007996635511517525, train acc = 0.9950000047683716, time = 0.9466581344604492\n",
      "Testing at step=82, batch=0, test loss = 0.18291215598583221, test acc = 0.9700000286102295, time = 0.3394153118133545\n",
      "Testing at step=82, batch=5, test loss = 0.08625591546297073, test acc = 0.9850000143051147, time = 0.3363363742828369\n",
      "Testing at step=82, batch=10, test loss = 0.2774079442024231, test acc = 0.9549999833106995, time = 0.33497023582458496\n",
      "Testing at step=82, batch=15, test loss = 0.04796434938907623, test acc = 0.9850000143051147, time = 0.33451366424560547\n",
      "Testing at step=82, batch=20, test loss = 0.06663484871387482, test acc = 0.9850000143051147, time = 0.33586668968200684\n",
      "Testing at step=82, batch=25, test loss = 0.07727065682411194, test acc = 0.9850000143051147, time = 0.33939290046691895\n",
      "Testing at step=82, batch=30, test loss = 0.150370791554451, test acc = 0.9649999737739563, time = 0.3419303894042969\n",
      "Testing at step=82, batch=35, test loss = 0.08366917818784714, test acc = 0.9750000238418579, time = 0.33556413650512695\n",
      "Testing at step=82, batch=40, test loss = 0.14062218368053436, test acc = 0.9599999785423279, time = 0.3351099491119385\n",
      "Testing at step=82, batch=45, test loss = 0.06127992644906044, test acc = 0.9850000143051147, time = 0.33503293991088867\n",
      "Step 82 finished in 309.73663210868835, Train loss = 0.023579681238819223, Test loss = 0.11329601518809795; Train Acc = 0.9920500075817108, Test Acc = 0.9768000102043152\n",
      "Training at step=83, batch=0, train loss = 0.013301297090947628, train acc = 0.9950000047683716, time = 0.9556472301483154\n",
      "Training at step=83, batch=30, train loss = 0.04754618555307388, train acc = 0.9850000143051147, time = 1.0682590007781982\n",
      "Training at step=83, batch=60, train loss = 0.03094879537820816, train acc = 0.9950000047683716, time = 0.9392566680908203\n",
      "Training at step=83, batch=90, train loss = 0.0434529073536396, train acc = 0.9850000143051147, time = 0.942131519317627\n",
      "Training at step=83, batch=120, train loss = 0.021042268723249435, train acc = 0.9900000095367432, time = 0.9415934085845947\n",
      "Training at step=83, batch=150, train loss = 0.05727569758892059, train acc = 0.9800000190734863, time = 0.940819501876831\n",
      "Training at step=83, batch=180, train loss = 0.02017713151872158, train acc = 0.9950000047683716, time = 0.9407444000244141\n",
      "Training at step=83, batch=210, train loss = 0.022485366091132164, train acc = 0.9850000143051147, time = 0.9438529014587402\n",
      "Training at step=83, batch=240, train loss = 0.003864970523864031, train acc = 1.0, time = 0.9404232501983643\n",
      "Training at step=83, batch=270, train loss = 0.017655281350016594, train acc = 0.9950000047683716, time = 0.9539108276367188\n",
      "Testing at step=83, batch=0, test loss = 0.014258388429880142, test acc = 0.9950000047683716, time = 0.336733341217041\n",
      "Testing at step=83, batch=5, test loss = 0.14747199416160583, test acc = 0.9700000286102295, time = 0.3408620357513428\n",
      "Testing at step=83, batch=10, test loss = 0.15352152287960052, test acc = 0.9700000286102295, time = 0.34174656867980957\n",
      "Testing at step=83, batch=15, test loss = 0.10428009182214737, test acc = 0.9700000286102295, time = 0.3394014835357666\n",
      "Testing at step=83, batch=20, test loss = 0.21100112795829773, test acc = 0.9549999833106995, time = 0.3575432300567627\n",
      "Testing at step=83, batch=25, test loss = 0.13955073058605194, test acc = 0.9599999785423279, time = 0.35892796516418457\n",
      "Testing at step=83, batch=30, test loss = 0.114759162068367, test acc = 0.9700000286102295, time = 0.34067726135253906\n",
      "Testing at step=83, batch=35, test loss = 0.09476100653409958, test acc = 0.9800000190734863, time = 0.338895320892334\n",
      "Testing at step=83, batch=40, test loss = 0.17322438955307007, test acc = 0.9599999785423279, time = 0.34394264221191406\n",
      "Testing at step=83, batch=45, test loss = 0.1349935680627823, test acc = 0.9750000238418579, time = 0.3379404544830322\n",
      "Step 83 finished in 309.7264287471771, Train loss = 0.02430579581608375, Test loss = 0.12487951844930649; Train Acc = 0.9920333409309388, Test Acc = 0.9715000092983246\n",
      "Training at step=84, batch=0, train loss = 0.006357103120535612, train acc = 1.0, time = 0.9401810169219971\n",
      "Training at step=84, batch=30, train loss = 0.027345918118953705, train acc = 0.9950000047683716, time = 0.9447543621063232\n",
      "Training at step=84, batch=60, train loss = 0.005313395988196135, train acc = 1.0, time = 0.9379591941833496\n",
      "Training at step=84, batch=90, train loss = 0.04037388414144516, train acc = 0.9900000095367432, time = 0.9359409809112549\n",
      "Training at step=84, batch=120, train loss = 0.034659963101148605, train acc = 0.9900000095367432, time = 0.941514253616333\n",
      "Training at step=84, batch=150, train loss = 0.017583737149834633, train acc = 0.9900000095367432, time = 0.9380550384521484\n",
      "Training at step=84, batch=180, train loss = 0.012603122740983963, train acc = 0.9900000095367432, time = 0.9405248165130615\n",
      "Training at step=84, batch=210, train loss = 0.026612773537635803, train acc = 0.9900000095367432, time = 0.9424407482147217\n",
      "Training at step=84, batch=240, train loss = 0.005670687649399042, train acc = 1.0, time = 0.937767744064331\n",
      "Training at step=84, batch=270, train loss = 0.014429070986807346, train acc = 0.9950000047683716, time = 0.9552567005157471\n",
      "Testing at step=84, batch=0, test loss = 0.16442888975143433, test acc = 0.9700000286102295, time = 0.34025049209594727\n",
      "Testing at step=84, batch=5, test loss = 0.169693261384964, test acc = 0.9599999785423279, time = 0.34025096893310547\n",
      "Testing at step=84, batch=10, test loss = 0.05564352124929428, test acc = 0.9750000238418579, time = 0.3385050296783447\n",
      "Testing at step=84, batch=15, test loss = 0.15741121768951416, test acc = 0.949999988079071, time = 0.33754563331604004\n",
      "Testing at step=84, batch=20, test loss = 0.08280007541179657, test acc = 0.9800000190734863, time = 0.3419773578643799\n",
      "Testing at step=84, batch=25, test loss = 0.1930379420518875, test acc = 0.9700000286102295, time = 0.35338592529296875\n",
      "Testing at step=84, batch=30, test loss = 0.16215524077415466, test acc = 0.9649999737739563, time = 0.3376178741455078\n",
      "Testing at step=84, batch=35, test loss = 0.0989159569144249, test acc = 0.9599999785423279, time = 0.33983731269836426\n",
      "Testing at step=84, batch=40, test loss = 0.02888193540275097, test acc = 0.9950000047683716, time = 0.33554935455322266\n",
      "Testing at step=84, batch=45, test loss = 0.1338348090648651, test acc = 0.9750000238418579, time = 0.33400678634643555\n",
      "Step 84 finished in 309.722327709198, Train loss = 0.02323442662678038, Test loss = 0.11439318865537644; Train Acc = 0.9923666737476985, Test Acc = 0.9738000082969666\n",
      "Training at step=85, batch=0, train loss = 0.013098900206387043, train acc = 0.9950000047683716, time = 0.944713830947876\n",
      "Training at step=85, batch=30, train loss = 0.019133562222123146, train acc = 0.9950000047683716, time = 0.9343264102935791\n",
      "Training at step=85, batch=60, train loss = 0.05591525882482529, train acc = 0.9800000190734863, time = 0.9407012462615967\n",
      "Training at step=85, batch=90, train loss = 0.03851626068353653, train acc = 0.9800000190734863, time = 0.9389617443084717\n",
      "Training at step=85, batch=120, train loss = 0.0194095391780138, train acc = 0.9900000095367432, time = 0.9588286876678467\n",
      "Training at step=85, batch=150, train loss = 0.06020865589380264, train acc = 0.9850000143051147, time = 0.9412086009979248\n",
      "Training at step=85, batch=180, train loss = 0.06527699530124664, train acc = 0.9750000238418579, time = 0.9565060138702393\n",
      "Training at step=85, batch=210, train loss = 0.023507094010710716, train acc = 0.9850000143051147, time = 0.9388506412506104\n",
      "Training at step=85, batch=240, train loss = 0.02257627435028553, train acc = 0.9900000095367432, time = 0.95135498046875\n",
      "Training at step=85, batch=270, train loss = 0.025866074487566948, train acc = 0.9900000095367432, time = 0.9357442855834961\n",
      "Testing at step=85, batch=0, test loss = 0.05649945139884949, test acc = 0.9900000095367432, time = 0.3385353088378906\n",
      "Testing at step=85, batch=5, test loss = 0.31718865036964417, test acc = 0.9649999737739563, time = 0.34496426582336426\n",
      "Testing at step=85, batch=10, test loss = 0.010970516130328178, test acc = 0.9900000095367432, time = 0.3421969413757324\n",
      "Testing at step=85, batch=15, test loss = 0.13911505043506622, test acc = 0.9850000143051147, time = 0.3433215618133545\n",
      "Testing at step=85, batch=20, test loss = 0.023686550557613373, test acc = 0.9900000095367432, time = 0.3397829532623291\n",
      "Testing at step=85, batch=25, test loss = 0.050837788730859756, test acc = 0.9750000238418579, time = 0.34050750732421875\n",
      "Testing at step=85, batch=30, test loss = 0.25238239765167236, test acc = 0.9649999737739563, time = 0.3356642723083496\n",
      "Testing at step=85, batch=35, test loss = 0.2022210657596588, test acc = 0.9700000286102295, time = 0.3366055488586426\n",
      "Testing at step=85, batch=40, test loss = 0.07406921684741974, test acc = 0.9750000238418579, time = 0.3508586883544922\n",
      "Testing at step=85, batch=45, test loss = 0.1849604845046997, test acc = 0.9700000286102295, time = 0.3356971740722656\n",
      "Step 85 finished in 310.3752865791321, Train loss = 0.02450398937391583, Test loss = 0.1330459686741233; Train Acc = 0.9916500079631806, Test Acc = 0.9757000076770782\n",
      "Training at step=86, batch=0, train loss = 0.0239313505589962, train acc = 0.9950000047683716, time = 0.9368159770965576\n",
      "Training at step=86, batch=30, train loss = 0.029162494465708733, train acc = 0.9950000047683716, time = 0.9886105060577393\n",
      "Training at step=86, batch=60, train loss = 0.03635435923933983, train acc = 0.9900000095367432, time = 0.9474256038665771\n",
      "Training at step=86, batch=90, train loss = 0.037740081548690796, train acc = 0.9900000095367432, time = 0.9410507678985596\n",
      "Training at step=86, batch=120, train loss = 0.010523165576159954, train acc = 0.9950000047683716, time = 0.9381668567657471\n",
      "Training at step=86, batch=150, train loss = 0.01469865720719099, train acc = 0.9950000047683716, time = 0.9386999607086182\n",
      "Training at step=86, batch=180, train loss = 0.012426954694092274, train acc = 0.9950000047683716, time = 0.9493803977966309\n",
      "Training at step=86, batch=210, train loss = 0.015433570370078087, train acc = 0.9950000047683716, time = 0.9685447216033936\n",
      "Training at step=86, batch=240, train loss = 0.056269530206918716, train acc = 0.9750000238418579, time = 0.9599769115447998\n",
      "Training at step=86, batch=270, train loss = 0.010476252064108849, train acc = 0.9950000047683716, time = 0.9482707977294922\n",
      "Testing at step=86, batch=0, test loss = 0.08544036000967026, test acc = 0.9750000238418579, time = 0.3439815044403076\n",
      "Testing at step=86, batch=5, test loss = 0.17457900941371918, test acc = 0.9700000286102295, time = 0.33524155616760254\n",
      "Testing at step=86, batch=10, test loss = 0.1705380082130432, test acc = 0.9549999833106995, time = 0.3420274257659912\n",
      "Testing at step=86, batch=15, test loss = 0.12424102425575256, test acc = 0.9700000286102295, time = 0.3380744457244873\n",
      "Testing at step=86, batch=20, test loss = 0.07589665055274963, test acc = 0.9800000190734863, time = 0.33600711822509766\n",
      "Testing at step=86, batch=25, test loss = 0.07691051810979843, test acc = 0.9850000143051147, time = 0.3386106491088867\n",
      "Testing at step=86, batch=30, test loss = 0.38563036918640137, test acc = 0.9449999928474426, time = 0.33695316314697266\n",
      "Testing at step=86, batch=35, test loss = 0.05201263353228569, test acc = 0.9750000238418579, time = 0.33933186531066895\n",
      "Testing at step=86, batch=40, test loss = 0.10303503274917603, test acc = 0.9750000238418579, time = 0.33783864974975586\n",
      "Testing at step=86, batch=45, test loss = 0.04063909128308296, test acc = 0.9850000143051147, time = 0.3354928493499756\n",
      "Step 86 finished in 310.52580976486206, Train loss = 0.02521685017738491, Test loss = 0.11430599194020033; Train Acc = 0.9913500082492829, Test Acc = 0.973200010061264\n",
      "Training at step=87, batch=0, train loss = 0.010625472292304039, train acc = 0.9950000047683716, time = 0.98944091796875\n",
      "Training at step=87, batch=30, train loss = 0.0065937768667936325, train acc = 1.0, time = 0.9464144706726074\n",
      "Training at step=87, batch=60, train loss = 0.04424479603767395, train acc = 0.9900000095367432, time = 0.9413089752197266\n",
      "Training at step=87, batch=90, train loss = 0.04645698890089989, train acc = 0.9850000143051147, time = 0.9852337837219238\n",
      "Training at step=87, batch=120, train loss = 0.004047024063766003, train acc = 1.0, time = 0.9369711875915527\n",
      "Training at step=87, batch=150, train loss = 0.002873347606509924, train acc = 1.0, time = 0.938101053237915\n",
      "Training at step=87, batch=180, train loss = 0.010801545344293118, train acc = 0.9950000047683716, time = 0.939483642578125\n",
      "Training at step=87, batch=210, train loss = 0.011653434485197067, train acc = 0.9950000047683716, time = 0.9454431533813477\n",
      "Training at step=87, batch=240, train loss = 0.0030795857310295105, train acc = 1.0, time = 0.9399280548095703\n",
      "Training at step=87, batch=270, train loss = 0.003913989756256342, train acc = 1.0, time = 0.9513309001922607\n",
      "Testing at step=87, batch=0, test loss = 0.048339031636714935, test acc = 0.9900000095367432, time = 0.33759331703186035\n",
      "Testing at step=87, batch=5, test loss = 0.062053728848695755, test acc = 0.9800000190734863, time = 0.45908689498901367\n",
      "Testing at step=87, batch=10, test loss = 0.22648291289806366, test acc = 0.949999988079071, time = 0.37747931480407715\n",
      "Testing at step=87, batch=15, test loss = 0.07847633957862854, test acc = 0.9750000238418579, time = 0.3365139961242676\n",
      "Testing at step=87, batch=20, test loss = 0.07895934581756592, test acc = 0.9700000286102295, time = 0.3380398750305176\n",
      "Testing at step=87, batch=25, test loss = 0.13420197367668152, test acc = 0.9700000286102295, time = 0.33811450004577637\n",
      "Testing at step=87, batch=30, test loss = 0.10279135406017303, test acc = 0.9750000238418579, time = 0.3388521671295166\n",
      "Testing at step=87, batch=35, test loss = 0.13696229457855225, test acc = 0.9649999737739563, time = 0.34211277961730957\n",
      "Testing at step=87, batch=40, test loss = 0.08862178027629852, test acc = 0.9700000286102295, time = 0.33782291412353516\n",
      "Testing at step=87, batch=45, test loss = 0.08183689415454865, test acc = 0.9850000143051147, time = 0.3395388126373291\n",
      "Step 87 finished in 310.2596113681793, Train loss = 0.02276819368513922, Test loss = 0.12465302646160126; Train Acc = 0.9923000073432923, Test Acc = 0.9718000078201294\n",
      "Training at step=88, batch=0, train loss = 0.02352650836110115, train acc = 0.9900000095367432, time = 0.9398725032806396\n",
      "Training at step=88, batch=30, train loss = 0.036918286234140396, train acc = 0.9900000095367432, time = 0.9488656520843506\n",
      "Training at step=88, batch=60, train loss = 0.017035776749253273, train acc = 0.9950000047683716, time = 0.9395053386688232\n",
      "Training at step=88, batch=90, train loss = 0.04234735667705536, train acc = 0.9850000143051147, time = 0.9449777603149414\n",
      "Training at step=88, batch=120, train loss = 0.006838392000645399, train acc = 0.9950000047683716, time = 0.9419505596160889\n",
      "Training at step=88, batch=150, train loss = 0.017676446586847305, train acc = 0.9900000095367432, time = 0.9642271995544434\n",
      "Training at step=88, batch=180, train loss = 0.018276100978255272, train acc = 0.9900000095367432, time = 0.9605419635772705\n",
      "Training at step=88, batch=210, train loss = 0.01052860263735056, train acc = 0.9950000047683716, time = 0.938957691192627\n",
      "Training at step=88, batch=240, train loss = 0.02226843684911728, train acc = 0.9900000095367432, time = 0.9435503482818604\n",
      "Training at step=88, batch=270, train loss = 0.057726383209228516, train acc = 0.9800000190734863, time = 0.9421164989471436\n",
      "Testing at step=88, batch=0, test loss = 0.09390553832054138, test acc = 0.9800000190734863, time = 0.37082457542419434\n",
      "Testing at step=88, batch=5, test loss = 0.08463376760482788, test acc = 0.9800000190734863, time = 0.33739399909973145\n",
      "Testing at step=88, batch=10, test loss = 0.1516430824995041, test acc = 0.9700000286102295, time = 0.33781886100769043\n",
      "Testing at step=88, batch=15, test loss = 0.08820939064025879, test acc = 0.9750000238418579, time = 0.3364226818084717\n",
      "Testing at step=88, batch=20, test loss = 0.056247778236866, test acc = 0.9700000286102295, time = 0.33872246742248535\n",
      "Testing at step=88, batch=25, test loss = 0.14484648406505585, test acc = 0.9599999785423279, time = 0.34243083000183105\n",
      "Testing at step=88, batch=30, test loss = 0.007070157676935196, test acc = 1.0, time = 0.3470423221588135\n",
      "Testing at step=88, batch=35, test loss = 0.007834305986762047, test acc = 0.9950000047683716, time = 0.3492734432220459\n",
      "Testing at step=88, batch=40, test loss = 0.04332270100712776, test acc = 0.9800000190734863, time = 0.3600120544433594\n",
      "Testing at step=88, batch=45, test loss = 0.1631360650062561, test acc = 0.9700000286102295, time = 0.37114524841308594\n",
      "Step 88 finished in 311.0632836818695, Train loss = 0.02403405189203719, Test loss = 0.11834881483577192; Train Acc = 0.9916666746139526, Test Acc = 0.9749000120162964\n",
      "Training at step=89, batch=0, train loss = 0.002289622789248824, train acc = 1.0, time = 0.9631068706512451\n",
      "Training at step=89, batch=30, train loss = 0.001664681825786829, train acc = 1.0, time = 0.9395184516906738\n",
      "Training at step=89, batch=60, train loss = 0.008109796792268753, train acc = 1.0, time = 0.938913106918335\n",
      "Training at step=89, batch=90, train loss = 0.00905476976186037, train acc = 0.9950000047683716, time = 0.9451131820678711\n",
      "Training at step=89, batch=120, train loss = 0.00874834880232811, train acc = 1.0, time = 0.9363806247711182\n",
      "Training at step=89, batch=150, train loss = 0.012848254293203354, train acc = 1.0, time = 0.9613645076751709\n",
      "Training at step=89, batch=180, train loss = 0.02184872142970562, train acc = 0.9900000095367432, time = 0.9372830390930176\n",
      "Training at step=89, batch=210, train loss = 0.0064530279487371445, train acc = 1.0, time = 0.9361870288848877\n",
      "Training at step=89, batch=240, train loss = 0.06234082207083702, train acc = 0.9750000238418579, time = 0.9392378330230713\n",
      "Training at step=89, batch=270, train loss = 0.010628491640090942, train acc = 1.0, time = 0.9385647773742676\n",
      "Testing at step=89, batch=0, test loss = 0.05125045403838158, test acc = 0.9800000190734863, time = 0.3385894298553467\n",
      "Testing at step=89, batch=5, test loss = 0.08793573081493378, test acc = 0.9750000238418579, time = 0.35894012451171875\n",
      "Testing at step=89, batch=10, test loss = 0.1342223733663559, test acc = 0.9599999785423279, time = 0.3439347743988037\n",
      "Testing at step=89, batch=15, test loss = 0.06235959380865097, test acc = 0.9649999737739563, time = 0.3601386547088623\n",
      "Testing at step=89, batch=20, test loss = 0.09578026831150055, test acc = 0.9800000190734863, time = 0.35796165466308594\n",
      "Testing at step=89, batch=25, test loss = 0.1667160540819168, test acc = 0.9649999737739563, time = 0.35701727867126465\n",
      "Testing at step=89, batch=30, test loss = 0.08104374259710312, test acc = 0.9750000238418579, time = 0.3790731430053711\n",
      "Testing at step=89, batch=35, test loss = 0.11592274904251099, test acc = 0.9649999737739563, time = 0.35857129096984863\n",
      "Testing at step=89, batch=40, test loss = 0.04402201622724533, test acc = 0.9800000190734863, time = 0.3592820167541504\n",
      "Testing at step=89, batch=45, test loss = 0.07831845432519913, test acc = 0.9700000286102295, time = 0.3580954074859619\n",
      "Step 89 finished in 311.63677430152893, Train loss = 0.0216949423373444, Test loss = 0.11885764045640826; Train Acc = 0.9926666736602783, Test Acc = 0.9737000107765198\n",
      "Training at step=90, batch=0, train loss = 0.024572858586907387, train acc = 0.9900000095367432, time = 0.9679901599884033\n",
      "Training at step=90, batch=30, train loss = 0.014966360293328762, train acc = 0.9900000095367432, time = 0.9451076984405518\n",
      "Training at step=90, batch=60, train loss = 0.017331749200820923, train acc = 0.9950000047683716, time = 0.9413065910339355\n",
      "Training at step=90, batch=90, train loss = 0.03643117845058441, train acc = 0.9850000143051147, time = 1.0950181484222412\n",
      "Training at step=90, batch=120, train loss = 0.007189538329839706, train acc = 1.0, time = 0.9453027248382568\n",
      "Training at step=90, batch=150, train loss = 0.010158667340874672, train acc = 0.9950000047683716, time = 0.9539575576782227\n",
      "Training at step=90, batch=180, train loss = 0.015151397325098515, train acc = 0.9900000095367432, time = 0.945213794708252\n",
      "Training at step=90, batch=210, train loss = 0.009834527969360352, train acc = 1.0, time = 0.9409422874450684\n",
      "Training at step=90, batch=240, train loss = 0.05020039156079292, train acc = 0.9850000143051147, time = 0.9389200210571289\n",
      "Training at step=90, batch=270, train loss = 0.045941222459077835, train acc = 0.9700000286102295, time = 0.9401938915252686\n",
      "Testing at step=90, batch=0, test loss = 0.040114253759384155, test acc = 0.9900000095367432, time = 0.339263916015625\n",
      "Testing at step=90, batch=5, test loss = 0.24273547530174255, test acc = 0.9599999785423279, time = 0.428722620010376\n",
      "Testing at step=90, batch=10, test loss = 0.03132220357656479, test acc = 0.9850000143051147, time = 0.33513903617858887\n",
      "Testing at step=90, batch=15, test loss = 0.0635661780834198, test acc = 0.9800000190734863, time = 0.3360748291015625\n",
      "Testing at step=90, batch=20, test loss = 0.2728383243083954, test acc = 0.9700000286102295, time = 0.3397355079650879\n",
      "Testing at step=90, batch=25, test loss = 0.08925730735063553, test acc = 0.9800000190734863, time = 0.340590238571167\n",
      "Testing at step=90, batch=30, test loss = 0.2077181041240692, test acc = 0.9449999928474426, time = 0.33742785453796387\n",
      "Testing at step=90, batch=35, test loss = 0.19225120544433594, test acc = 0.9599999785423279, time = 0.3416907787322998\n",
      "Testing at step=90, batch=40, test loss = 0.04288317635655403, test acc = 0.9850000143051147, time = 0.3394582271575928\n",
      "Testing at step=90, batch=45, test loss = 0.16536767780780792, test acc = 0.9649999737739563, time = 0.3385143280029297\n",
      "Step 90 finished in 310.81830191612244, Train loss = 0.02407063201768324, Test loss = 0.12121100593358278; Train Acc = 0.9917666743199031, Test Acc = 0.9736000108718872\n",
      "Training at step=91, batch=0, train loss = 0.01635030470788479, train acc = 0.9950000047683716, time = 0.9420266151428223\n",
      "Training at step=91, batch=30, train loss = 0.02007005177438259, train acc = 0.9950000047683716, time = 0.9409365653991699\n",
      "Training at step=91, batch=60, train loss = 0.10285650938749313, train acc = 0.9800000190734863, time = 0.9438042640686035\n",
      "Training at step=91, batch=90, train loss = 0.010027481243014336, train acc = 1.0, time = 0.9452052116394043\n",
      "Training at step=91, batch=120, train loss = 0.008017512038350105, train acc = 0.9950000047683716, time = 0.9655077457427979\n",
      "Training at step=91, batch=150, train loss = 0.04971441254019737, train acc = 0.9900000095367432, time = 0.9478399753570557\n",
      "Training at step=91, batch=180, train loss = 0.03709396719932556, train acc = 0.9800000190734863, time = 0.938835620880127\n",
      "Training at step=91, batch=210, train loss = 0.01679934374988079, train acc = 0.9950000047683716, time = 0.9426019191741943\n",
      "Training at step=91, batch=240, train loss = 0.07131413370370865, train acc = 0.9800000190734863, time = 0.9421412944793701\n",
      "Training at step=91, batch=270, train loss = 0.012862085364758968, train acc = 0.9950000047683716, time = 0.9397923946380615\n",
      "Testing at step=91, batch=0, test loss = 0.058330897241830826, test acc = 0.9800000190734863, time = 0.3376336097717285\n",
      "Testing at step=91, batch=5, test loss = 0.12219278514385223, test acc = 0.9700000286102295, time = 0.3372633457183838\n",
      "Testing at step=91, batch=10, test loss = 0.11671413481235504, test acc = 0.9750000238418579, time = 0.34188222885131836\n",
      "Testing at step=91, batch=15, test loss = 0.03619543835520744, test acc = 0.9800000190734863, time = 0.33888673782348633\n",
      "Testing at step=91, batch=20, test loss = 0.12589561939239502, test acc = 0.9599999785423279, time = 0.3411266803741455\n",
      "Testing at step=91, batch=25, test loss = 0.040858931839466095, test acc = 0.9800000190734863, time = 0.34070873260498047\n",
      "Testing at step=91, batch=30, test loss = 0.250284880399704, test acc = 0.9599999785423279, time = 0.34676527976989746\n",
      "Testing at step=91, batch=35, test loss = 0.13604287803173065, test acc = 0.9649999737739563, time = 0.3392777442932129\n",
      "Testing at step=91, batch=40, test loss = 0.03631431236863136, test acc = 0.9850000143051147, time = 0.336489200592041\n",
      "Testing at step=91, batch=45, test loss = 0.13405446708202362, test acc = 0.9649999737739563, time = 0.3344569206237793\n",
      "Step 91 finished in 310.92444944381714, Train loss = 0.022870780194255834, Test loss = 0.1154986884444952; Train Acc = 0.9924666738510132, Test Acc = 0.9749000060558319\n",
      "Training at step=92, batch=0, train loss = 0.02987879514694214, train acc = 0.9950000047683716, time = 0.9333689212799072\n",
      "Training at step=92, batch=30, train loss = 0.011505771428346634, train acc = 0.9950000047683716, time = 0.9417636394500732\n",
      "Training at step=92, batch=60, train loss = 0.028257938101887703, train acc = 0.9950000047683716, time = 0.9463200569152832\n",
      "Training at step=92, batch=90, train loss = 0.005069587379693985, train acc = 1.0, time = 0.942577600479126\n",
      "Training at step=92, batch=120, train loss = 0.004634941928088665, train acc = 1.0, time = 0.9578828811645508\n",
      "Training at step=92, batch=150, train loss = 0.008713476359844208, train acc = 0.9950000047683716, time = 0.9436161518096924\n",
      "Training at step=92, batch=180, train loss = 0.027021732181310654, train acc = 0.9900000095367432, time = 0.9404158592224121\n",
      "Training at step=92, batch=210, train loss = 0.02250460535287857, train acc = 0.9950000047683716, time = 0.9381256103515625\n",
      "Training at step=92, batch=240, train loss = 0.0030200439505279064, train acc = 1.0, time = 0.9452168941497803\n",
      "Training at step=92, batch=270, train loss = 0.006071485113352537, train acc = 1.0, time = 0.950300931930542\n",
      "Testing at step=92, batch=0, test loss = 0.08734738081693649, test acc = 0.9700000286102295, time = 0.36930370330810547\n",
      "Testing at step=92, batch=5, test loss = 0.08670089393854141, test acc = 0.9900000095367432, time = 0.3634641170501709\n",
      "Testing at step=92, batch=10, test loss = 0.2272598147392273, test acc = 0.9649999737739563, time = 0.337801456451416\n",
      "Testing at step=92, batch=15, test loss = 0.08035054802894592, test acc = 0.9850000143051147, time = 0.33604907989501953\n",
      "Testing at step=92, batch=20, test loss = 0.007698954548686743, test acc = 0.9950000047683716, time = 0.3374302387237549\n",
      "Testing at step=92, batch=25, test loss = 0.06623011082410812, test acc = 0.9750000238418579, time = 0.3378138542175293\n",
      "Testing at step=92, batch=30, test loss = 0.09002883732318878, test acc = 0.9850000143051147, time = 0.3384969234466553\n",
      "Testing at step=92, batch=35, test loss = 0.1048140823841095, test acc = 0.9750000238418579, time = 0.33805179595947266\n",
      "Testing at step=92, batch=40, test loss = 0.2539515793323517, test acc = 0.9549999833106995, time = 0.3385505676269531\n",
      "Testing at step=92, batch=45, test loss = 0.18241989612579346, test acc = 0.9700000286102295, time = 0.3425142765045166\n",
      "Step 92 finished in 311.49944138526917, Train loss = 0.019084961448873703, Test loss = 0.11907934608869254; Train Acc = 0.9938000059127807, Test Acc = 0.9763000118732452\n",
      "Training at step=93, batch=0, train loss = 0.006591598968952894, train acc = 0.9950000047683716, time = 0.944810152053833\n",
      "Training at step=93, batch=30, train loss = 0.009557762183248997, train acc = 1.0, time = 0.9446542263031006\n",
      "Training at step=93, batch=60, train loss = 0.04275982454419136, train acc = 0.9850000143051147, time = 0.9793286323547363\n",
      "Training at step=93, batch=90, train loss = 0.040718525648117065, train acc = 0.9850000143051147, time = 0.9380512237548828\n",
      "Training at step=93, batch=120, train loss = 0.012646742165088654, train acc = 0.9950000047683716, time = 0.9397003650665283\n",
      "Training at step=93, batch=150, train loss = 0.037572428584098816, train acc = 0.9850000143051147, time = 0.9359469413757324\n",
      "Training at step=93, batch=180, train loss = 0.016032744199037552, train acc = 0.9950000047683716, time = 0.9370138645172119\n",
      "Training at step=93, batch=210, train loss = 0.03362740948796272, train acc = 0.9950000047683716, time = 0.9370760917663574\n",
      "Training at step=93, batch=240, train loss = 0.017566649243235588, train acc = 0.9900000095367432, time = 0.9390673637390137\n",
      "Training at step=93, batch=270, train loss = 0.023148097097873688, train acc = 0.9850000143051147, time = 0.9435906410217285\n",
      "Testing at step=93, batch=0, test loss = 0.2639545202255249, test acc = 0.9549999833106995, time = 0.34140753746032715\n",
      "Testing at step=93, batch=5, test loss = 0.09910191595554352, test acc = 0.9850000143051147, time = 0.3401479721069336\n",
      "Testing at step=93, batch=10, test loss = 0.06557057797908783, test acc = 0.9850000143051147, time = 0.37794947624206543\n",
      "Testing at step=93, batch=15, test loss = 0.09750720858573914, test acc = 0.9750000238418579, time = 0.3755800724029541\n",
      "Testing at step=93, batch=20, test loss = 0.036301419138908386, test acc = 0.9850000143051147, time = 0.3379676342010498\n",
      "Testing at step=93, batch=25, test loss = 0.12373492866754532, test acc = 0.9750000238418579, time = 0.33876609802246094\n",
      "Testing at step=93, batch=30, test loss = 0.029216330498456955, test acc = 0.9850000143051147, time = 0.34062981605529785\n",
      "Testing at step=93, batch=35, test loss = 0.1155150830745697, test acc = 0.9700000286102295, time = 0.34097957611083984\n",
      "Testing at step=93, batch=40, test loss = 0.2069883942604065, test acc = 0.9750000238418579, time = 0.3424968719482422\n",
      "Testing at step=93, batch=45, test loss = 0.08963970839977264, test acc = 0.9750000238418579, time = 0.33837342262268066\n",
      "Step 93 finished in 310.2382884025574, Train loss = 0.02070497801876627, Test loss = 0.13130344804376365; Train Acc = 0.9929500065247218, Test Acc = 0.9734000098705292\n",
      "Training at step=94, batch=0, train loss = 0.03232495114207268, train acc = 0.9950000047683716, time = 0.9432938098907471\n",
      "Training at step=94, batch=30, train loss = 0.0414794385433197, train acc = 0.9750000238418579, time = 0.9366085529327393\n",
      "Training at step=94, batch=60, train loss = 0.056800879538059235, train acc = 0.9950000047683716, time = 0.9436550140380859\n",
      "Training at step=94, batch=90, train loss = 0.01104548666626215, train acc = 0.9950000047683716, time = 0.9462263584136963\n",
      "Training at step=94, batch=120, train loss = 0.005867804866284132, train acc = 1.0, time = 0.9432053565979004\n",
      "Training at step=94, batch=150, train loss = 0.01977621391415596, train acc = 0.9900000095367432, time = 0.9464094638824463\n",
      "Training at step=94, batch=180, train loss = 0.005144436843693256, train acc = 1.0, time = 0.9474260807037354\n",
      "Training at step=94, batch=210, train loss = 0.01757865957915783, train acc = 0.9950000047683716, time = 0.940920352935791\n",
      "Training at step=94, batch=240, train loss = 0.014940369874238968, train acc = 0.9950000047683716, time = 0.9439942836761475\n",
      "Training at step=94, batch=270, train loss = 0.02763587050139904, train acc = 0.9950000047683716, time = 0.9371852874755859\n",
      "Testing at step=94, batch=0, test loss = 0.12978249788284302, test acc = 0.9750000238418579, time = 0.34348559379577637\n",
      "Testing at step=94, batch=5, test loss = 0.28205740451812744, test acc = 0.9599999785423279, time = 0.34181714057922363\n",
      "Testing at step=94, batch=10, test loss = 0.15350881218910217, test acc = 0.9700000286102295, time = 0.33797407150268555\n",
      "Testing at step=94, batch=15, test loss = 0.07310114800930023, test acc = 0.9850000143051147, time = 0.3412489891052246\n",
      "Testing at step=94, batch=20, test loss = 0.1084846556186676, test acc = 0.9549999833106995, time = 0.34133172035217285\n",
      "Testing at step=94, batch=25, test loss = 0.32427987456321716, test acc = 0.9549999833106995, time = 0.34194064140319824\n",
      "Testing at step=94, batch=30, test loss = 0.19510458409786224, test acc = 0.949999988079071, time = 0.3464059829711914\n",
      "Testing at step=94, batch=35, test loss = 0.18235191702842712, test acc = 0.9700000286102295, time = 0.33527374267578125\n",
      "Testing at step=94, batch=40, test loss = 0.08500196784734726, test acc = 0.9900000095367432, time = 0.3461904525756836\n",
      "Testing at step=94, batch=45, test loss = 0.06506478041410446, test acc = 0.9750000238418579, time = 0.3387336730957031\n",
      "Step 94 finished in 310.8674192428589, Train loss = 0.021790543359238655, Test loss = 0.12807490207254887; Train Acc = 0.9925000069538752, Test Acc = 0.973500007390976\n",
      "Training at step=95, batch=0, train loss = 0.019375700503587723, train acc = 0.9900000095367432, time = 0.9581937789916992\n",
      "Training at step=95, batch=30, train loss = 0.013071801513433456, train acc = 0.9950000047683716, time = 0.9410719871520996\n",
      "Training at step=95, batch=60, train loss = 0.0032016579061746597, train acc = 1.0, time = 0.9689745903015137\n",
      "Training at step=95, batch=90, train loss = 0.0677887499332428, train acc = 0.9750000238418579, time = 0.9405512809753418\n",
      "Training at step=95, batch=120, train loss = 0.020989753305912018, train acc = 0.9900000095367432, time = 0.9428272247314453\n",
      "Training at step=95, batch=150, train loss = 0.05544205754995346, train acc = 0.9850000143051147, time = 0.9409096240997314\n",
      "Training at step=95, batch=180, train loss = 0.04653052240610123, train acc = 0.9950000047683716, time = 0.9341940879821777\n",
      "Training at step=95, batch=210, train loss = 0.008417006582021713, train acc = 0.9950000047683716, time = 0.9389567375183105\n",
      "Training at step=95, batch=240, train loss = 0.024963654577732086, train acc = 0.9900000095367432, time = 0.9363782405853271\n",
      "Training at step=95, batch=270, train loss = 0.0066945613361895084, train acc = 1.0, time = 0.9463057518005371\n",
      "Testing at step=95, batch=0, test loss = 0.22023296356201172, test acc = 0.9549999833106995, time = 0.3386576175689697\n",
      "Testing at step=95, batch=5, test loss = 0.09757014364004135, test acc = 0.9800000190734863, time = 0.3380911350250244\n",
      "Testing at step=95, batch=10, test loss = 0.2997075021266937, test acc = 0.9750000238418579, time = 0.33478474617004395\n",
      "Testing at step=95, batch=15, test loss = 0.14626136422157288, test acc = 0.9750000238418579, time = 0.33470678329467773\n",
      "Testing at step=95, batch=20, test loss = 0.052061546593904495, test acc = 0.9800000190734863, time = 0.33429551124572754\n",
      "Testing at step=95, batch=25, test loss = 0.031077025458216667, test acc = 0.9950000047683716, time = 0.3357350826263428\n",
      "Testing at step=95, batch=30, test loss = 0.05281654745340347, test acc = 0.9850000143051147, time = 0.3360121250152588\n",
      "Testing at step=95, batch=35, test loss = 0.10045231878757477, test acc = 0.9649999737739563, time = 0.3393435478210449\n",
      "Testing at step=95, batch=40, test loss = 0.22217659652233124, test acc = 0.949999988079071, time = 0.3359103202819824\n",
      "Testing at step=95, batch=45, test loss = 0.11210672557353973, test acc = 0.9750000238418579, time = 0.33937597274780273\n",
      "Step 95 finished in 310.42580342292786, Train loss = 0.01926681599773777, Test loss = 0.12170176678337156; Train Acc = 0.9936833393573761, Test Acc = 0.9740000057220459\n",
      "Training at step=96, batch=0, train loss = 0.03449511528015137, train acc = 0.9900000095367432, time = 0.9425413608551025\n",
      "Training at step=96, batch=30, train loss = 0.012741523794829845, train acc = 0.9900000095367432, time = 0.9632372856140137\n",
      "Training at step=96, batch=60, train loss = 0.022112691774964333, train acc = 0.9850000143051147, time = 0.9469273090362549\n",
      "Training at step=96, batch=90, train loss = 0.007473161444067955, train acc = 1.0, time = 0.9461750984191895\n",
      "Training at step=96, batch=120, train loss = 0.016397124156355858, train acc = 0.9950000047683716, time = 0.9415268898010254\n",
      "Training at step=96, batch=150, train loss = 0.016209514811635017, train acc = 0.9950000047683716, time = 0.9397785663604736\n",
      "Training at step=96, batch=180, train loss = 0.008488954044878483, train acc = 1.0, time = 0.9862563610076904\n",
      "Training at step=96, batch=210, train loss = 0.023017216473817825, train acc = 0.9950000047683716, time = 0.9383518695831299\n",
      "Training at step=96, batch=240, train loss = 0.006547913420945406, train acc = 1.0, time = 0.9635677337646484\n",
      "Training at step=96, batch=270, train loss = 0.020306725054979324, train acc = 0.9950000047683716, time = 0.9395387172698975\n",
      "Testing at step=96, batch=0, test loss = 0.18163424730300903, test acc = 0.9700000286102295, time = 0.3357093334197998\n",
      "Testing at step=96, batch=5, test loss = 0.03779122978448868, test acc = 0.9900000095367432, time = 0.3632364273071289\n",
      "Testing at step=96, batch=10, test loss = 0.12974977493286133, test acc = 0.9850000143051147, time = 0.3355412483215332\n",
      "Testing at step=96, batch=15, test loss = 0.1462429314851761, test acc = 0.9599999785423279, time = 0.353790283203125\n",
      "Testing at step=96, batch=20, test loss = 0.06335249543190002, test acc = 0.9800000190734863, time = 0.34358739852905273\n",
      "Testing at step=96, batch=25, test loss = 0.055789824575185776, test acc = 0.9850000143051147, time = 0.3385453224182129\n",
      "Testing at step=96, batch=30, test loss = 0.1468832641839981, test acc = 0.9700000286102295, time = 0.3367884159088135\n",
      "Testing at step=96, batch=35, test loss = 0.3152831792831421, test acc = 0.949999988079071, time = 0.33620214462280273\n",
      "Testing at step=96, batch=40, test loss = 0.11493193358182907, test acc = 0.9850000143051147, time = 0.33678746223449707\n",
      "Testing at step=96, batch=45, test loss = 0.269376665353775, test acc = 0.9750000238418579, time = 0.33804917335510254\n",
      "Step 96 finished in 312.22881507873535, Train loss = 0.019333862329173522, Test loss = 0.13418505873531103; Train Acc = 0.9938000059127807, Test Acc = 0.9742000102996826\n",
      "Training at step=97, batch=0, train loss = 0.00916271097958088, train acc = 0.9950000047683716, time = 0.950127124786377\n",
      "Training at step=97, batch=30, train loss = 0.008770345710217953, train acc = 0.9950000047683716, time = 0.9418027400970459\n",
      "Training at step=97, batch=60, train loss = 0.013654652051627636, train acc = 0.9900000095367432, time = 0.9382574558258057\n",
      "Training at step=97, batch=90, train loss = 0.015165379270911217, train acc = 0.9950000047683716, time = 0.9432859420776367\n",
      "Training at step=97, batch=120, train loss = 0.030530709773302078, train acc = 0.9850000143051147, time = 0.9410743713378906\n",
      "Training at step=97, batch=150, train loss = 0.033928439021110535, train acc = 0.9900000095367432, time = 0.9370396137237549\n",
      "Training at step=97, batch=180, train loss = 0.04804069921374321, train acc = 0.9850000143051147, time = 0.9415838718414307\n",
      "Training at step=97, batch=210, train loss = 0.025394678115844727, train acc = 0.9900000095367432, time = 0.9397323131561279\n",
      "Training at step=97, batch=240, train loss = 0.012102816253900528, train acc = 0.9950000047683716, time = 0.939277172088623\n",
      "Training at step=97, batch=270, train loss = 0.009653965942561626, train acc = 0.9950000047683716, time = 0.941007137298584\n",
      "Testing at step=97, batch=0, test loss = 0.09017226845026016, test acc = 0.9800000190734863, time = 0.3383955955505371\n",
      "Testing at step=97, batch=5, test loss = 0.0780150517821312, test acc = 0.9800000190734863, time = 0.3390774726867676\n",
      "Testing at step=97, batch=10, test loss = 0.14919382333755493, test acc = 0.9700000286102295, time = 0.350679874420166\n",
      "Testing at step=97, batch=15, test loss = 0.24520722031593323, test acc = 0.9700000286102295, time = 0.3403332233428955\n",
      "Testing at step=97, batch=20, test loss = 0.13312824070453644, test acc = 0.9850000143051147, time = 0.3388094902038574\n",
      "Testing at step=97, batch=25, test loss = 0.17864041030406952, test acc = 0.9599999785423279, time = 0.3369114398956299\n",
      "Testing at step=97, batch=30, test loss = 0.14102958142757416, test acc = 0.9750000238418579, time = 0.34061551094055176\n",
      "Testing at step=97, batch=35, test loss = 0.0870438665151596, test acc = 0.9750000238418579, time = 0.34290361404418945\n",
      "Testing at step=97, batch=40, test loss = 0.03992876410484314, test acc = 0.9900000095367432, time = 0.33806300163269043\n",
      "Testing at step=97, batch=45, test loss = 0.06881309300661087, test acc = 0.9900000095367432, time = 0.3408079147338867\n",
      "Step 97 finished in 310.28110551834106, Train loss = 0.019870923778701883, Test loss = 0.12906572457402946; Train Acc = 0.9934833393494288, Test Acc = 0.9740000104904175\n",
      "Training at step=98, batch=0, train loss = 0.02569158375263214, train acc = 0.9950000047683716, time = 0.9441101551055908\n",
      "Training at step=98, batch=30, train loss = 0.007768596988171339, train acc = 1.0, time = 0.9380252361297607\n",
      "Training at step=98, batch=60, train loss = 0.02727213315665722, train acc = 0.9900000095367432, time = 0.9631013870239258\n",
      "Training at step=98, batch=90, train loss = 0.0094915721565485, train acc = 0.9950000047683716, time = 0.9420559406280518\n",
      "Training at step=98, batch=120, train loss = 0.019593307748436928, train acc = 0.9900000095367432, time = 0.9380137920379639\n",
      "Training at step=98, batch=150, train loss = 0.00845024362206459, train acc = 1.0, time = 0.9682285785675049\n",
      "Training at step=98, batch=180, train loss = 0.031098486855626106, train acc = 0.9950000047683716, time = 0.9371340274810791\n",
      "Training at step=98, batch=210, train loss = 0.08539802581071854, train acc = 0.9850000143051147, time = 0.9423089027404785\n",
      "Training at step=98, batch=240, train loss = 0.04413380101323128, train acc = 0.9900000095367432, time = 0.9377477169036865\n",
      "Training at step=98, batch=270, train loss = 0.0271317008882761, train acc = 0.9950000047683716, time = 0.940173864364624\n",
      "Testing at step=98, batch=0, test loss = 0.18399742245674133, test acc = 0.9700000286102295, time = 0.3389759063720703\n",
      "Testing at step=98, batch=5, test loss = 0.04202655702829361, test acc = 0.9850000143051147, time = 0.3351924419403076\n",
      "Testing at step=98, batch=10, test loss = 0.06837299466133118, test acc = 0.9900000095367432, time = 0.33764052391052246\n",
      "Testing at step=98, batch=15, test loss = 0.22851896286010742, test acc = 0.9599999785423279, time = 0.33562254905700684\n",
      "Testing at step=98, batch=20, test loss = 0.02727743424475193, test acc = 0.9950000047683716, time = 0.3357243537902832\n",
      "Testing at step=98, batch=25, test loss = 0.20957441627979279, test acc = 0.9649999737739563, time = 0.335357666015625\n",
      "Testing at step=98, batch=30, test loss = 0.34581223130226135, test acc = 0.949999988079071, time = 0.3361630439758301\n",
      "Testing at step=98, batch=35, test loss = 0.11051607131958008, test acc = 0.9800000190734863, time = 0.3355293273925781\n",
      "Testing at step=98, batch=40, test loss = 0.13462910056114197, test acc = 0.9700000286102295, time = 0.3385457992553711\n",
      "Testing at step=98, batch=45, test loss = 0.23328836262226105, test acc = 0.9549999833106995, time = 0.34096646308898926\n",
      "Step 98 finished in 310.54559087753296, Train loss = 0.017559415880047405, Test loss = 0.13665606554597617; Train Acc = 0.9941666722297668, Test Acc = 0.9737000095844269\n",
      "Training at step=99, batch=0, train loss = 0.0022445388603955507, train acc = 1.0, time = 0.9455070495605469\n",
      "Training at step=99, batch=30, train loss = 0.012660877779126167, train acc = 0.9950000047683716, time = 0.9490547180175781\n",
      "Training at step=99, batch=60, train loss = 0.030936121940612793, train acc = 0.9850000143051147, time = 0.9597668647766113\n",
      "Training at step=99, batch=90, train loss = 0.006005812436342239, train acc = 0.9950000047683716, time = 0.9479055404663086\n",
      "Training at step=99, batch=120, train loss = 0.02660508081316948, train acc = 0.9850000143051147, time = 0.9416353702545166\n",
      "Training at step=99, batch=150, train loss = 0.0038679770659655333, train acc = 1.0, time = 0.9407627582550049\n",
      "Training at step=99, batch=180, train loss = 0.000862862216308713, train acc = 1.0, time = 0.9403667449951172\n",
      "Training at step=99, batch=210, train loss = 0.011196251958608627, train acc = 1.0, time = 0.9423346519470215\n",
      "Training at step=99, batch=240, train loss = 0.0034275995567440987, train acc = 1.0, time = 0.9372718334197998\n",
      "Training at step=99, batch=270, train loss = 0.0018621523631736636, train acc = 1.0, time = 0.9462075233459473\n",
      "Testing at step=99, batch=0, test loss = 0.1336521953344345, test acc = 0.949999988079071, time = 0.33997368812561035\n",
      "Testing at step=99, batch=5, test loss = 0.07136248052120209, test acc = 0.9750000238418579, time = 0.3433220386505127\n",
      "Testing at step=99, batch=10, test loss = 0.07901916652917862, test acc = 0.9750000238418579, time = 0.34692978858947754\n",
      "Testing at step=99, batch=15, test loss = 0.0782010480761528, test acc = 0.9850000143051147, time = 0.362856388092041\n",
      "Testing at step=99, batch=20, test loss = 0.11535565555095673, test acc = 0.9700000286102295, time = 0.33771800994873047\n",
      "Testing at step=99, batch=25, test loss = 0.07778879255056381, test acc = 0.9750000238418579, time = 0.3378641605377197\n",
      "Testing at step=99, batch=30, test loss = 0.07846949249505997, test acc = 0.9900000095367432, time = 0.3375253677368164\n",
      "Testing at step=99, batch=35, test loss = 0.24536210298538208, test acc = 0.9750000238418579, time = 0.3362088203430176\n",
      "Testing at step=99, batch=40, test loss = 0.060943830758333206, test acc = 0.9800000190734863, time = 0.341611385345459\n",
      "Testing at step=99, batch=45, test loss = 0.06513829529285431, test acc = 0.9800000190734863, time = 0.33603763580322266\n",
      "Step 99 finished in 310.3872001171112, Train loss = 0.01958089542943829, Test loss = 0.1252339239604771; Train Acc = 0.9932833397388459, Test Acc = 0.9734000086784362\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.1\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T22:46:29.903723Z",
     "start_time": "2024-04-07T22:46:29.305515Z"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T00:55:24.433537Z",
     "iopub.status.busy": "2024-04-03T00:55:24.433325Z",
     "iopub.status.idle": "2024-04-03T00:55:24.787803Z",
     "shell.execute_reply": "2024-04-03T00:55:24.787177Z",
     "shell.execute_reply.started": "2024-04-03T00:55:24.433517Z"
    },
    "id": "U0Q0vFm7B6cg"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"2-mnist_full_repacement.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T22:46:29.909125Z",
     "start_time": "2024-04-07T22:46:29.904721Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"2-mnist_full_replacement.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
