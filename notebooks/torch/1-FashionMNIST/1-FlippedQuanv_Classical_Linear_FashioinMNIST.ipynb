{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:07.320372Z",
     "start_time": "2024-04-06T21:07:03.773132Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 73092,
     "status": "ok",
     "timestamp": 1711462301946,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "5ebnFsErY0EM",
    "outputId": "710b1d81-a854-4328-959c-884f3b89fa61"
   },
   "source": [
    "# Install packages\n",
    "!pip install torch torchvision torchaudio pennylane cotengra quimb torchmetrics --upgrade"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:11.191285Z",
     "start_time": "2024-04-06T21:07:07.322167Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7760,
     "status": "ok",
     "timestamp": 1711462309702,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "VN2wD-U7bGse",
    "outputId": "5cf6575c-ec02-4d3d-b222-1464aa8c1914"
   },
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# torch.cfloat won't pass the unitary check, but faster\n",
    "COMPLEX_DTYPE = torch.cfloat\n",
    "REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WmQPQuLbSFw"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:11.308714Z",
     "start_time": "2024-04-06T21:07:11.192556Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7868,
     "status": "ok",
     "timestamp": 1711462317565,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "rUhQUP2dbTja",
    "outputId": "5be0db17-8633-45dd-cde0-19eaec483b24"
   },
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQI8WWY6byQq"
   },
   "source": [
    "# Some Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.368132Z",
     "start_time": "2024-04-06T21:07:11.310586Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2829,
     "status": "ok",
     "timestamp": 1711462320391,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "7B4DTncWbwQl",
    "outputId": "72ab9fd9-1a03-484b-e92a-147788cf05aa"
   },
   "source": [
    "ket = {\n",
    "    '0':torch.tensor([1.,0.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '1':torch.tensor([0.,1.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '+':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) + torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device)),\n",
    "    '-':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) - torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device))\n",
    "}\n",
    "\n",
    "pauli = {\n",
    "    'I':torch.tensor([[1.,0.],[0.,1.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'X':torch.tensor([[0.,1.],[1.,0.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Y':torch.tensor([[0., -1.j],[1.j, 0]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Z':torch.tensor([[1.,0.],[0.,-1.]], dtype = COMPLEX_DTYPE, device=device)\n",
    "}\n",
    "\n",
    "def tensor_product(*args):\n",
    "  input_list = [a for a in args]\n",
    "  return functools.reduce(torch.kron, input_list)\n",
    "\n",
    "def multi_qubit_identity(n_qubits:int):\n",
    "  assert n_qubits>0\n",
    "  if n_qubits == 1:\n",
    "    return pauli['I']\n",
    "  else:\n",
    "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
    "\n",
    "pauli_words_su4 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
    "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
    "\n",
    "pauli_words_su8 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      if not key1+key2+key3 == 'III':\n",
    "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
    "\n",
    "pauli_words_su16 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        if not key1+key2+key3+key4 == 'IIII':\n",
    "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
    "              pauli[key1],\n",
    "              pauli[key2],\n",
    "              pauli[key3],\n",
    "              pauli[key4]\n",
    "          )\n",
    "\n",
    "pauli_words_su32 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        for key5 in pauli.keys():\n",
    "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
    "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
    "                pauli[key1],\n",
    "                pauli[key2],\n",
    "                pauli[key3],\n",
    "                pauli[key4],\n",
    "                pauli[key5]\n",
    "            )\n",
    "\n",
    "def su32_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = list(pauli_words_su32.values())\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, torch.stack(dict_values))\n",
    "\n",
    "  return torch.matrix_exp(1j*generator)\n",
    "\n",
    "def su4_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = torch.stack(list(pauli_words_su4.values()))\n",
    "\n",
    "  generator = torch.einsum(\"i, ijk -> jk\", params, dict_values)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.375546Z",
     "start_time": "2024-04-06T21:07:13.369704Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1711462320391,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "Xs0c2F1eBnGc"
   },
   "source": [
    "def measure_sv(\n",
    "    state,\n",
    "    observable\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Measure a statevector with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not\n",
    "  \"\"\"\n",
    "  expectation_value = torch.conj(state)@observable@state\n",
    "  return torch.real(expectation_value)\n",
    "\n",
    "def measure_dm(\n",
    "    rho,\n",
    "    observable\n",
    "):\n",
    "  \"\"\"\n",
    "  Measure a density matrix with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not.\n",
    "  \"\"\"\n",
    "  product = torch.matmul(rho, observable)\n",
    "\n",
    "  # Calculate the trace, which is the sum of diagonal elements\n",
    "  trace = torch.trace(product)\n",
    "\n",
    "  # The expectation value should be real for physical observables\n",
    "  return torch.real(trace)\n",
    "\n",
    "vmap_measure_sv = torch.vmap(measure_sv, in_dims=(None, 0), out_dims=0)\n",
    "vmap_measure_dm = torch.vmap(measure_dm, in_dims=(None, 0), out_dims=0)\n",
    "\n",
    "# assuming the input patch observables (hermitianized) has shape (batchsize,n_patches, c, h, w)\n",
    "# assuming the input set statevectors has shape (c, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_sv_batched_ob = torch.vmap(measure_sv, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "# assuming the input set desnity matrices has shape (batchsize, n_patches, c, 2**n, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_dm_batched_ob = torch.vmap(measure_dm, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "def bitstring_to_state(bitstring:str):\n",
    "  \"\"\"\n",
    "  Convert a bit string, like '0101001' or '+-+-101'\n",
    "  to a statevector. Each character in the bitstring must be among\n",
    "  0, 1, + and -\n",
    "  \"\"\"\n",
    "  assert len(bitstring)>0\n",
    "  for c in bitstring:\n",
    "    assert c in ['0', '1', '+', '-']\n",
    "  single_qubit_states = [ket[c] for c in bitstring]\n",
    "  return tensor_product(*single_qubit_states)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFZqT6bpElaL"
   },
   "source": [
    "# Flipped Quanvolution Kernel\n",
    "\n",
    "First, let's define the function to extract patches for a single kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.382085Z",
     "start_time": "2024-04-06T21:07:13.376557Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1711462320391,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "He4HdMRHC7T6"
   },
   "source": [
    "def extract_patches(image, patch_size, stride, padding=None):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image with multiple input channels and optional custom padding.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (in_channels, height, width).\n",
    "        patch_size (int): Size of the square patches to extract.\n",
    "        stride (int): Stride between patches.\n",
    "        padding (tuple): Padding value(s) for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n",
    "    pad_l, pad_r, pad_t, pad_b = padding if padding is not None else (0,0,0,0)\n",
    "\n",
    "    if padding is not None:\n",
    "        image = torch.nn.functional.pad(image, (pad_l, pad_r, pad_t, pad_b), mode='constant')\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    height, width = image.shape[-2],  image.shape[-1]\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = image[..., i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    patches = torch.stack(patches, dim=-4)\n",
    "    return patches # has shape (batchsize, n_patches, channel, h, w)\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRqflooEqKN"
   },
   "source": [
    "## A Three-by-Three Quanv Kernel Function\n",
    "\n",
    "For 3 by 3 patches, we only need a 2-qubit parameterised state, generated by the SU4 gate applied to the $|00\\rangle$ state. The first thing to do is to pad the 3 by 3 patches to 4 by 4. However, for input with size\n",
    "\n",
    "$$\n",
    "D_1\\times H_1 \\times W_1\n",
    "$$\n",
    "\n",
    "where $D_1$ is the number of input channels, we will need $D_1$ different two-qubit states, generated by $D_1$ different parameterised circuits. These circuits will produce $D_1$ expectation values. Suppose each circuit is parameterised by $\\theta_i, i\\in \\{1,2,\\cdots, D_1\\}$, the parameterised (pure) state can be written as\n",
    "\n",
    "$$\n",
    "|\\psi (\\theta_i)⟩ = SU4(\\theta_i)|00⟩\n",
    "$$\n",
    "\n",
    "For a channel $M_i$ of patch $M$ with shape $4\\times 4$, the expectation value is calculated by\n",
    "\n",
    "$$\n",
    "⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$\n",
    "\n",
    "Then, a single pixel value of the output channel corresponding this single kernel is\n",
    "\n",
    "$$\n",
    "\\frac{1}{D_1}\\sum_{i=1}^{D_1}⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.387764Z",
     "start_time": "2024-04-06T21:07:13.383020Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1711462320391,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "Yzn4KEt5ErG7"
   },
   "source": [
    "def generate_2q_param_state(theta):\n",
    "  state = bitstring_to_state('00')\n",
    "  state = torch.matmul(su4_op(theta), state)\n",
    "  return state\n",
    "\n",
    "vmap_generate_2q_param_state = torch.vmap(generate_2q_param_state, in_dims=0, out_dims = 0)\n",
    "\n",
    "def single_kernel_op_over_batched_patches(thetas, patch):\n",
    "  # patch has shape (c_in, h, w)\n",
    "  # thetas has shape (c_in, 4^2-1) for SU4 gates\n",
    "  n_theta = thetas.shape[-2]\n",
    "  n_channel = patch.shape[-3]\n",
    "  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n",
    "  states = vmap_generate_2q_param_state(thetas)\n",
    "  #print(\"States shape\", states.shape)\n",
    "  patch = torch.nn.functional.pad(patch, (0, 1, 0, 1), mode='constant')\n",
    "  patch_t = torch.einsum(\"...jk->...kj\", patch)\n",
    "  herm_patch = (patch_t+patch)/2 # has dim (batchsize, num_patches, c, h, w)\n",
    "  #print(\"Herm patch shape\", herm_patch.shape)\n",
    "  channel_out = vmap_measure_channel_sv_batched_ob(states, herm_patch) # has dim (batchsize,n_patches, c)\n",
    "  return torch.sum(channel_out, axis = -1)/n_theta # has dim (batchszie, n_patches)\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4BXEKd8I67_"
   },
   "source": [
    "## Multiple Output Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.391567Z",
     "start_time": "2024-04-06T21:07:13.388733Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1711462320392,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "72vkHV_BI80l"
   },
   "source": [
    "# For multiple channel output\n",
    "# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n",
    "vmap_vmap_single_kernel_op_through_extracted_patches = torch.vmap(single_kernel_op_over_batched_patches, in_dims=(0, None), out_dims=-2) # output has dim (batchsize, c_out, n_patches)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuZ6G5-sKK_J"
   },
   "source": [
    "## Pytorch Module for Three-by-Three Flipped Quanv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.398665Z",
     "start_time": "2024-04-06T21:07:13.392811Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1711462320392,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "Gww_XdJ5KPJt"
   },
   "source": [
    "class FlippedQuanv3x3(torch.nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, padding):\n",
    "    super(FlippedQuanv3x3, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.pad_l, self.pad_r, self.pad_t, self.pad_b = padding if padding is not None else (0,0,0,0)\n",
    "    self.pad = (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n",
    "    self.weight= torch.nn.Parameter(torch.randn((out_channels, in_channels, 4**2-1)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn((out_channels, 1)).type(COMPLEX_DTYPE))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batchsize ,c_in, h, w)\n",
    "    # weight has shape (c_out, c_in, 15)\n",
    "    # bias has shape (c_out, 1)\n",
    "    x = x.type(COMPLEX_DTYPE)\n",
    "    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n",
    "    h_out = (h_in-3+(self.pad_t+self.pad_b))//self.stride +1\n",
    "    w_out = (w_in-3+(self.pad_l+self.pad_r))//self.stride +1\n",
    "\n",
    "    #print(self.weight.shape)\n",
    "    #print(patches.shape)\n",
    "\n",
    "    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n",
    "\n",
    "\n",
    "    out = out + self.bias\n",
    "    return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n",
    "  \n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_channels={self.in_channels}, out_channels={self.out_channels}, stride={self.stride}, padding={self.padding}'\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yumZTjDVu63"
   },
   "source": [
    "## Flipped Quanvolution Replacing Conv2d\n",
    "\n",
    "First, let's just replace `Conv2d` with `FlippedQuanv3x3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:16.031580Z",
     "start_time": "2024-04-06T21:07:13.400651Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1711462320392,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "W4RC5ou-VzbE",
    "outputId": "f2e92dd2-63f4-4ed1-d191-6b016304911e"
   },
   "source": [
    "class HybridNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HybridNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None),\n",
    "        FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(16*28*28, 10).type(REAL_DTYPE)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "net = HybridNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T09:49:01.623325Z",
     "start_time": "2024-04-06T21:07:16.032868Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18075888,
     "status": "ok",
     "timestamp": 1711488461440,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "x4ZY59q1WS4x",
    "outputId": "0e7cbfcd-1078-4c56-c675-1afbfa385c85"
   },
   "source": [
    "import torchmetrics\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.1\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T09:49:02.821336Z",
     "start_time": "2024-04-07T09:49:02.274958Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "executionInfo": {
     "elapsed": 1244,
     "status": "ok",
     "timestamp": 1711488462684,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "U0Q0vFm7B6cg",
    "outputId": "a805910a-6214-4218-d940-429f1bae5d2e"
   },
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"1-fashionmnist_quanv_classical_linear.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T09:49:02.834617Z",
     "start_time": "2024-04-07T09:49:02.822697Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"1-fashionmnist_quanv_classical_linear.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO9ycf2GLNwAh4wkdecvhq8",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
