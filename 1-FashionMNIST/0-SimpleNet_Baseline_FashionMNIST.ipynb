{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:15:36.030435Z",
     "start_time": "2024-04-06T20:13:41.133148Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 618301,
     "status": "ok",
     "timestamp": 1712091541763,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "acHJk38WrBOi",
    "outputId": "ba1ec555-f0d0-45fd-df87-2f537a902cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.1+cu116)\r\n",
      "Collecting torch\r\n",
      "  Downloading torch-2.2.2-cp39-cp39-manylinux1_x86_64.whl (755.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\r\n",
      "Collecting torchvision\r\n",
      "  Downloading torchvision-0.17.2-cp39-cp39-manylinux1_x86_64.whl (6.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.12.1+cu116)\r\n",
      "Collecting torchaudio\r\n",
      "  Downloading torchaudio-2.2.2-cp39-cp39-manylinux1_x86_64.whl (3.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pennylane\r\n",
      "  Downloading PennyLane-0.35.1-py3-none-any.whl (1.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting cotengra\r\n",
      "  Downloading cotengra-0.5.6-py3-none-any.whl (148 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.0/148.0 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting quimb\r\n",
      "  Downloading quimb-1.7.3-py3-none-any.whl (500 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.7/500.7 kB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting torchmetrics\r\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\r\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\r\n",
      "Collecting sympy\r\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\r\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\r\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\r\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3\r\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\r\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting triton==2.2.0\r\n",
      "  Downloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\r\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\r\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting typing-extensions>=4.8.0\r\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26\r\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\r\n",
      "Collecting semantic-version>=2.7\r\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\r\n",
      "Collecting rustworkx\r\n",
      "  Downloading rustworkx-0.14.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\r\n",
      "Collecting appdirs\r\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Collecting autograd\r\n",
      "  Downloading autograd-1.6.2-py3-none-any.whl (49 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting autoray>=0.6.1\r\n",
      "  Downloading autoray-0.6.9-py3-none-any.whl (49 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\r\n",
      "Collecting toml\r\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\r\n",
      "Collecting pennylane-lightning>=0.35\r\n",
      "  Downloading PennyLane_Lightning-0.35.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\r\n",
      "Collecting numba>=0.39\r\n",
      "  Downloading numba-0.59.1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\r\n",
      "Collecting cytoolz>=0.8.0\r\n",
      "  Downloading cytoolz-0.12.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting lightning-utilities>=0.8.0\r\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\r\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\r\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0\r\n",
      "  Downloading llvmlite-0.42.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\r\n",
      "Collecting mpmath>=0.19\r\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: mpmath, appdirs, typing-extensions, triton, toml, sympy, semantic-version, rustworkx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, llvmlite, cytoolz, autoray, autograd, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, lightning-utilities, cotengra, quimb, nvidia-cusolver-cu12, torch, torchvision, torchmetrics, torchaudio, pennylane-lightning, pennylane\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.4.0\r\n",
      "    Uninstalling typing_extensions-4.4.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.4.0\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.12.1+cu116\r\n",
      "    Uninstalling torch-1.12.1+cu116:\r\n",
      "      Successfully uninstalled torch-1.12.1+cu116\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.13.1+cu116\r\n",
      "    Uninstalling torchvision-0.13.1+cu116:\r\n",
      "      Successfully uninstalled torchvision-0.13.1+cu116\r\n",
      "  Attempting uninstall: torchaudio\r\n",
      "    Found existing installation: torchaudio 0.12.1+cu116\r\n",
      "    Uninstalling torchaudio-0.12.1+cu116:\r\n",
      "      Successfully uninstalled torchaudio-0.12.1+cu116\r\n",
      "Successfully installed appdirs-1.4.4 autograd-1.6.2 autoray-0.6.9 cotengra-0.5.6 cytoolz-0.12.3 lightning-utilities-0.11.2 llvmlite-0.42.0 mpmath-1.3.0 numba-0.59.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pennylane-0.35.1 pennylane-lightning-0.35.1 quimb-1.7.3 rustworkx-0.14.2 semantic-version-2.10.0 sympy-1.12 toml-0.10.2 torch-2.2.2 torchaudio-2.2.2 torchmetrics-1.3.2 torchvision-0.17.2 triton-2.2.0 typing-extensions-4.11.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio pennylane cotengra quimb torchmetrics --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:15:40.761798Z",
     "start_time": "2024-04-06T20:15:36.031982Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8020,
     "status": "ok",
     "timestamp": 1712091549780,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "FB_BuVzCrMNk",
    "outputId": "235fd337-f4c4-4ba7-9547-7b96570eac2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "\n",
    "\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "#REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:15:48.999706Z",
     "start_time": "2024-04-06T20:15:40.763128Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5513,
     "status": "ok",
     "timestamp": 1712091555290,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "K7wJnOiEralR",
    "outputId": "a21dab31-4609-4119-cbdd-0b84f74ac35f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:02<00:00, 13013609.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 347217.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:03<00:00, 1383208.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 18129535.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n",
      "tensor([4, 1, 7, 3, 5, 8, 2, 6, 4, 6, 6, 3, 6, 9, 9, 7, 8, 7, 7, 7, 2, 7, 3, 8,\n",
      "        8, 8, 2, 2, 8, 2, 1, 5, 8, 7, 8, 2, 1, 4, 8, 7, 0, 0, 8, 9, 7, 5, 3, 6,\n",
      "        9, 3, 9, 8, 7, 1, 8, 8, 0, 7, 3, 0, 6, 4, 5, 8])\n",
      "tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.7176,\n",
      "         0.5686,  0.8588,  0.2235,  0.0902,  0.9059,  0.5529,  0.6000,  0.6078,\n",
      "         0.6392,  0.6314,  0.6157,  0.8039,  0.4039, -0.0039,  0.9922,  0.6314,\n",
      "         0.7490, -0.6235, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000])\n"
     ]
    }
   ],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:15:52.939979Z",
     "start_time": "2024-04-06T20:15:49.001868Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1712091555780,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "t6wDzOaJrmZ4",
    "outputId": "e216a519-beac-46f4-de29-cd0e9e280630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "SimpleNet(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=12544, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class SimpleNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SimpleNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=3),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Conv2d(32, 16, kernel_size=3),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(16*28*28, 10),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "net = SimpleNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:43:06.239753Z",
     "start_time": "2024-04-06T20:15:52.942021Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2289206,
     "status": "ok",
     "timestamp": 1712093844984,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "PDNHjKy4sSWZ",
    "outputId": "a0ef5fa2-455a-4017-d32f-bc0e38c7cdab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 600, Number of test batches = 100\n",
      "Print every train batch = 120, Print every test batch = 20\n",
      "Training at step=0, batch=0, train loss = 2.306792765239672, train acc = 0.10999999940395355, time = 0.18320798873901367\n",
      "Training at step=0, batch=120, train loss = 0.7192115776557848, train acc = 0.75, time = 0.008365869522094727\n",
      "Training at step=0, batch=240, train loss = 0.7989091330869033, train acc = 0.75, time = 0.008182048797607422\n",
      "Training at step=0, batch=360, train loss = 0.4937506498953356, train acc = 0.7799999713897705, time = 0.008211135864257812\n",
      "Training at step=0, batch=480, train loss = 0.5181211140449858, train acc = 0.8399999737739563, time = 0.008206605911254883\n",
      "Testing at step=0, batch=0, test loss = 0.6360847186959648, test acc = 0.75, time = 0.0016481876373291016\n",
      "Testing at step=0, batch=20, test loss = 0.5886414608914923, test acc = 0.7599999904632568, time = 0.001678466796875\n",
      "Testing at step=0, batch=40, test loss = 0.4538919952490481, test acc = 0.8199999928474426, time = 0.0015566349029541016\n",
      "Testing at step=0, batch=60, test loss = 0.6404311024974219, test acc = 0.7699999809265137, time = 0.001562356948852539\n",
      "Testing at step=0, batch=80, test loss = 0.5357739669468177, test acc = 0.800000011920929, time = 0.0015544891357421875\n",
      "Step 0 finished in 16.621954202651978, Train loss = 0.7241927799136987, Test loss = 0.552099417133723; Train Acc = 0.7505166640629372, Test Acc = 0.7998999989032746\n",
      "Training at step=1, batch=0, train loss = 0.46399548580780836, train acc = 0.8299999833106995, time = 0.008294105529785156\n",
      "Training at step=1, batch=120, train loss = 0.40264857414024574, train acc = 0.8899999856948853, time = 0.008233308792114258\n",
      "Training at step=1, batch=240, train loss = 0.4986857496317565, train acc = 0.8299999833106995, time = 0.008230209350585938\n",
      "Training at step=1, batch=360, train loss = 0.6168165248964933, train acc = 0.7900000214576721, time = 0.008227825164794922\n",
      "Training at step=1, batch=480, train loss = 0.469224702977591, train acc = 0.8100000023841858, time = 0.00818490982055664\n",
      "Testing at step=1, batch=0, test loss = 0.3711550374113526, test acc = 0.8600000143051147, time = 0.0017352104187011719\n",
      "Testing at step=1, batch=20, test loss = 0.5452622015875417, test acc = 0.7900000214576721, time = 0.0019927024841308594\n",
      "Testing at step=1, batch=40, test loss = 0.38660541707551666, test acc = 0.8399999737739563, time = 0.0015904903411865234\n",
      "Testing at step=1, batch=60, test loss = 0.4591920123697774, test acc = 0.8299999833106995, time = 0.0015597343444824219\n",
      "Testing at step=1, batch=80, test loss = 0.36857745237536593, test acc = 0.8799999952316284, time = 0.0015590190887451172\n",
      "Step 1 finished in 16.391932010650635, Train loss = 0.49692725900398776, Test loss = 0.5064217838536715; Train Acc = 0.8246166641513507, Test Acc = 0.8216999971866608\n",
      "Training at step=2, batch=0, train loss = 0.3371824946261799, train acc = 0.8799999952316284, time = 0.008341789245605469\n",
      "Training at step=2, batch=120, train loss = 0.4848131618693195, train acc = 0.8500000238418579, time = 0.00818490982055664\n",
      "Training at step=2, batch=240, train loss = 0.31114203338905677, train acc = 0.8999999761581421, time = 0.00825190544128418\n",
      "Training at step=2, batch=360, train loss = 0.48018303521439276, train acc = 0.8299999833106995, time = 0.008258819580078125\n",
      "Training at step=2, batch=480, train loss = 0.4668734893780957, train acc = 0.8399999737739563, time = 0.00821828842163086\n",
      "Testing at step=2, batch=0, test loss = 0.4212198128844494, test acc = 0.8399999737739563, time = 0.0015990734100341797\n",
      "Testing at step=2, batch=20, test loss = 0.35084438522175426, test acc = 0.8399999737739563, time = 0.0015833377838134766\n",
      "Testing at step=2, batch=40, test loss = 0.443658368693325, test acc = 0.8199999928474426, time = 0.0015799999237060547\n",
      "Testing at step=2, batch=60, test loss = 0.39005118180216813, test acc = 0.8799999952316284, time = 0.0015728473663330078\n",
      "Testing at step=2, batch=80, test loss = 0.64570032695551, test acc = 0.7599999904632568, time = 0.0015799999237060547\n",
      "Step 2 finished in 16.37726402282715, Train loss = 0.46781972211485234, Test loss = 0.48556077941750453; Train Acc = 0.8363000000516574, Test Acc = 0.8260999971628189\n",
      "Training at step=3, batch=0, train loss = 0.4243443713220897, train acc = 0.8600000143051147, time = 0.008396148681640625\n",
      "Training at step=3, batch=120, train loss = 0.3715240127980108, train acc = 0.8899999856948853, time = 0.008167505264282227\n",
      "Training at step=3, batch=240, train loss = 0.5189557425643915, train acc = 0.8500000238418579, time = 0.00819540023803711\n",
      "Training at step=3, batch=360, train loss = 0.4298189555258507, train acc = 0.8299999833106995, time = 0.008165597915649414\n",
      "Training at step=3, batch=480, train loss = 0.4381475860195585, train acc = 0.8299999833106995, time = 0.008190631866455078\n",
      "Testing at step=3, batch=0, test loss = 0.5603308217612496, test acc = 0.8100000023841858, time = 0.0016579627990722656\n",
      "Testing at step=3, batch=20, test loss = 0.4667419089405675, test acc = 0.8700000047683716, time = 0.0015654563903808594\n",
      "Testing at step=3, batch=40, test loss = 0.44732270621781217, test acc = 0.8500000238418579, time = 0.0015532970428466797\n",
      "Testing at step=3, batch=60, test loss = 0.5504358674905664, test acc = 0.7900000214576721, time = 0.0015568733215332031\n",
      "Testing at step=3, batch=80, test loss = 0.3684473062859875, test acc = 0.8899999856948853, time = 0.0015611648559570312\n",
      "Step 3 finished in 16.288172721862793, Train loss = 0.4504732604781617, Test loss = 0.4715339074227788; Train Acc = 0.8435833312074343, Test Acc = 0.8336999982595443\n",
      "Training at step=4, batch=0, train loss = 0.4363385129708158, train acc = 0.8299999833106995, time = 0.00825810432434082\n",
      "Training at step=4, batch=120, train loss = 0.5314069536940424, train acc = 0.8399999737739563, time = 0.008304119110107422\n",
      "Training at step=4, batch=240, train loss = 0.4863975285878008, train acc = 0.8600000143051147, time = 0.008259296417236328\n",
      "Training at step=4, batch=360, train loss = 0.38242816838291227, train acc = 0.8299999833106995, time = 0.008382320404052734\n",
      "Training at step=4, batch=480, train loss = 0.3381365888493791, train acc = 0.8600000143051147, time = 0.008234739303588867\n",
      "Testing at step=4, batch=0, test loss = 0.3083508542660185, test acc = 0.8999999761581421, time = 0.0016639232635498047\n",
      "Testing at step=4, batch=20, test loss = 0.559700677947077, test acc = 0.8100000023841858, time = 0.0015718936920166016\n",
      "Testing at step=4, batch=40, test loss = 0.4951218078975044, test acc = 0.8100000023841858, time = 0.0016014575958251953\n",
      "Testing at step=4, batch=60, test loss = 0.4563012213848767, test acc = 0.8500000238418579, time = 0.0015704631805419922\n",
      "Testing at step=4, batch=80, test loss = 0.4160543933651401, test acc = 0.8600000143051147, time = 0.0015521049499511719\n",
      "Step 4 finished in 16.330678462982178, Train loss = 0.438984516942279, Test loss = 0.4662736931406332; Train Acc = 0.8485499987999598, Test Acc = 0.836800001859665\n",
      "Training at step=5, batch=0, train loss = 0.41948765694591794, train acc = 0.8700000047683716, time = 0.008250713348388672\n",
      "Training at step=5, batch=120, train loss = 0.6085427798412801, train acc = 0.7799999713897705, time = 0.00827336311340332\n",
      "Training at step=5, batch=240, train loss = 0.3744132394386542, train acc = 0.8700000047683716, time = 0.008208990097045898\n",
      "Training at step=5, batch=360, train loss = 0.4300533373282562, train acc = 0.8299999833106995, time = 0.008177518844604492\n",
      "Training at step=5, batch=480, train loss = 0.4087986574662619, train acc = 0.8399999737739563, time = 0.008290529251098633\n",
      "Testing at step=5, batch=0, test loss = 0.4695604929186263, test acc = 0.8199999928474426, time = 0.001619577407836914\n",
      "Testing at step=5, batch=20, test loss = 0.4860410174948722, test acc = 0.8199999928474426, time = 0.0015723705291748047\n",
      "Testing at step=5, batch=40, test loss = 0.35736931296642516, test acc = 0.8600000143051147, time = 0.001573324203491211\n",
      "Testing at step=5, batch=60, test loss = 0.44521422310437186, test acc = 0.8399999737739563, time = 0.0015969276428222656\n",
      "Testing at step=5, batch=80, test loss = 0.3091224456738059, test acc = 0.8799999952316284, time = 0.0015666484832763672\n",
      "Step 5 finished in 16.337376356124878, Train loss = 0.4327017824841274, Test loss = 0.46753177408702895; Train Acc = 0.8490333315730095, Test Acc = 0.8341999977827073\n",
      "Training at step=6, batch=0, train loss = 0.45890517201631253, train acc = 0.8500000238418579, time = 0.008316516876220703\n",
      "Training at step=6, batch=120, train loss = 0.41506585721568084, train acc = 0.7900000214576721, time = 0.008203506469726562\n",
      "Training at step=6, batch=240, train loss = 0.2884103736707081, train acc = 0.9100000262260437, time = 0.008191585540771484\n",
      "Training at step=6, batch=360, train loss = 0.453539878902758, train acc = 0.8399999737739563, time = 0.008274078369140625\n",
      "Training at step=6, batch=480, train loss = 0.39554968045110195, train acc = 0.8199999928474426, time = 0.008143901824951172\n",
      "Testing at step=6, batch=0, test loss = 0.6872105906942013, test acc = 0.7400000095367432, time = 0.0016100406646728516\n",
      "Testing at step=6, batch=20, test loss = 0.3344630602418485, test acc = 0.9300000071525574, time = 0.0016031265258789062\n",
      "Testing at step=6, batch=40, test loss = 0.5501747292698542, test acc = 0.8299999833106995, time = 0.0016088485717773438\n",
      "Testing at step=6, batch=60, test loss = 0.45642097206697924, test acc = 0.8600000143051147, time = 0.0016334056854248047\n",
      "Testing at step=6, batch=80, test loss = 0.48577676394970254, test acc = 0.8199999928474426, time = 0.0015959739685058594\n",
      "Step 6 finished in 16.335495233535767, Train loss = 0.42687215068410805, Test loss = 0.45957655937722336; Train Acc = 0.8525833331545194, Test Acc = 0.8395999950170517\n",
      "Training at step=7, batch=0, train loss = 0.48803011375398747, train acc = 0.8899999856948853, time = 0.008328676223754883\n",
      "Training at step=7, batch=120, train loss = 0.39368673787946884, train acc = 0.8299999833106995, time = 0.008137702941894531\n",
      "Training at step=7, batch=240, train loss = 0.40454729056086064, train acc = 0.8500000238418579, time = 0.00830388069152832\n",
      "Training at step=7, batch=360, train loss = 0.32373223175444793, train acc = 0.8899999856948853, time = 0.008215188980102539\n",
      "Training at step=7, batch=480, train loss = 0.5212297319156919, train acc = 0.8100000023841858, time = 0.008296489715576172\n",
      "Testing at step=7, batch=0, test loss = 0.42035555270719044, test acc = 0.8399999737739563, time = 0.0015969276428222656\n",
      "Testing at step=7, batch=20, test loss = 0.5692979740058497, test acc = 0.7799999713897705, time = 0.0015766620635986328\n",
      "Testing at step=7, batch=40, test loss = 0.41017400816411703, test acc = 0.8999999761581421, time = 0.001575469970703125\n",
      "Testing at step=7, batch=60, test loss = 0.42915623678437803, test acc = 0.8299999833106995, time = 0.0015799999237060547\n",
      "Testing at step=7, batch=80, test loss = 0.5106044828717562, test acc = 0.8399999737739563, time = 0.0015795230865478516\n",
      "Step 7 finished in 16.37747073173523, Train loss = 0.4228418993531209, Test loss = 0.47277055483167; Train Acc = 0.8538999994595845, Test Acc = 0.8327999979257583\n",
      "Training at step=8, batch=0, train loss = 0.3655015256848981, train acc = 0.8500000238418579, time = 0.008415699005126953\n",
      "Training at step=8, batch=120, train loss = 0.5165872559478756, train acc = 0.8500000238418579, time = 0.008231401443481445\n",
      "Training at step=8, batch=240, train loss = 0.33491016210869423, train acc = 0.8899999856948853, time = 0.008188486099243164\n",
      "Training at step=8, batch=360, train loss = 0.4273836706980078, train acc = 0.8700000047683716, time = 0.008322477340698242\n",
      "Training at step=8, batch=480, train loss = 0.46053264686629886, train acc = 0.8399999737739563, time = 0.008348703384399414\n",
      "Testing at step=8, batch=0, test loss = 0.34516584883616636, test acc = 0.8500000238418579, time = 0.0016057491302490234\n",
      "Testing at step=8, batch=20, test loss = 0.5111371089419473, test acc = 0.8700000047683716, time = 0.0015788078308105469\n",
      "Testing at step=8, batch=40, test loss = 0.6002053788307389, test acc = 0.7799999713897705, time = 0.0016219615936279297\n",
      "Testing at step=8, batch=60, test loss = 0.5211527804278253, test acc = 0.8199999928474426, time = 0.001580953598022461\n",
      "Testing at step=8, batch=80, test loss = 0.5368436106821856, test acc = 0.8199999928474426, time = 0.0015997886657714844\n",
      "Step 8 finished in 16.390426874160767, Train loss = 0.4180452636112983, Test loss = 0.44983546665118446; Train Acc = 0.8549333331982295, Test Acc = 0.8423000007867814\n",
      "Training at step=9, batch=0, train loss = 0.284836047433183, train acc = 0.8999999761581421, time = 0.008371829986572266\n",
      "Training at step=9, batch=120, train loss = 0.32037470754335323, train acc = 0.8799999952316284, time = 0.008157014846801758\n",
      "Training at step=9, batch=240, train loss = 0.28958465714803217, train acc = 0.8799999952316284, time = 0.008152484893798828\n",
      "Training at step=9, batch=360, train loss = 0.4391540291618158, train acc = 0.8500000238418579, time = 0.008129596710205078\n",
      "Training at step=9, batch=480, train loss = 0.5192846415216763, train acc = 0.8299999833106995, time = 0.008167505264282227\n",
      "Testing at step=9, batch=0, test loss = 0.47225576888292714, test acc = 0.8500000238418579, time = 0.001750946044921875\n",
      "Testing at step=9, batch=20, test loss = 0.4780916940466631, test acc = 0.8500000238418579, time = 0.0015783309936523438\n",
      "Testing at step=9, batch=40, test loss = 0.31681198792598436, test acc = 0.8600000143051147, time = 0.0016031265258789062\n",
      "Testing at step=9, batch=60, test loss = 0.4597937025537878, test acc = 0.8399999737739563, time = 0.0015707015991210938\n",
      "Testing at step=9, batch=80, test loss = 0.3803351186632028, test acc = 0.8399999737739563, time = 0.0015978813171386719\n",
      "Step 9 finished in 16.323655605316162, Train loss = 0.41488398308193497, Test loss = 0.45123776578624586; Train Acc = 0.8559500005841255, Test Acc = 0.8421999996900559\n",
      "Training at step=10, batch=0, train loss = 0.4448896218884789, train acc = 0.8500000238418579, time = 0.00830984115600586\n",
      "Training at step=10, batch=120, train loss = 0.6128450514883765, train acc = 0.7799999713897705, time = 0.00824284553527832\n",
      "Training at step=10, batch=240, train loss = 0.3893809247290784, train acc = 0.8600000143051147, time = 0.008170366287231445\n",
      "Training at step=10, batch=360, train loss = 0.5860890376629866, train acc = 0.7599999904632568, time = 0.00833582878112793\n",
      "Training at step=10, batch=480, train loss = 0.510062452288967, train acc = 0.8199999928474426, time = 0.008243560791015625\n",
      "Testing at step=10, batch=0, test loss = 0.4761172259900186, test acc = 0.8399999737739563, time = 0.0015780925750732422\n",
      "Testing at step=10, batch=20, test loss = 0.47637549676419433, test acc = 0.8100000023841858, time = 0.0015676021575927734\n",
      "Testing at step=10, batch=40, test loss = 0.39981457780274354, test acc = 0.8299999833106995, time = 0.0016129016876220703\n",
      "Testing at step=10, batch=60, test loss = 0.31555159889898127, test acc = 0.8700000047683716, time = 0.001600027084350586\n",
      "Testing at step=10, batch=80, test loss = 0.34972161958485, test acc = 0.8999999761581421, time = 0.0016269683837890625\n",
      "Step 10 finished in 16.38085389137268, Train loss = 0.41353249836053935, Test loss = 0.44585788796957615; Train Acc = 0.8566833313306172, Test Acc = 0.8432999956607818\n",
      "Training at step=11, batch=0, train loss = 0.4821007034546612, train acc = 0.8600000143051147, time = 0.008311748504638672\n",
      "Training at step=11, batch=120, train loss = 0.3468526387872101, train acc = 0.8899999856948853, time = 0.008321285247802734\n",
      "Training at step=11, batch=240, train loss = 0.41412007472087553, train acc = 0.8299999833106995, time = 0.008181095123291016\n",
      "Training at step=11, batch=360, train loss = 0.5051528069638244, train acc = 0.8500000238418579, time = 0.008161783218383789\n",
      "Training at step=11, batch=480, train loss = 0.42167626833945543, train acc = 0.8600000143051147, time = 0.00831747055053711\n",
      "Testing at step=11, batch=0, test loss = 0.3421301291738657, test acc = 0.8700000047683716, time = 0.0017180442810058594\n",
      "Testing at step=11, batch=20, test loss = 0.3737172128474163, test acc = 0.8700000047683716, time = 0.0015692710876464844\n",
      "Testing at step=11, batch=40, test loss = 0.31619276197422796, test acc = 0.8999999761581421, time = 0.0016033649444580078\n",
      "Testing at step=11, batch=60, test loss = 0.40520569964733, test acc = 0.8500000238418579, time = 0.001558542251586914\n",
      "Testing at step=11, batch=80, test loss = 0.3749530826793573, test acc = 0.8500000238418579, time = 0.0015649795532226562\n",
      "Step 11 finished in 16.33412003517151, Train loss = 0.4088233507591385, Test loss = 0.44709694612122874; Train Acc = 0.8583833326896032, Test Acc = 0.8416000020503998\n",
      "Training at step=12, batch=0, train loss = 0.3308330454881368, train acc = 0.9200000166893005, time = 0.008368968963623047\n",
      "Training at step=12, batch=120, train loss = 0.5652058466304525, train acc = 0.7799999713897705, time = 0.00822138786315918\n",
      "Training at step=12, batch=240, train loss = 0.4974552925307535, train acc = 0.800000011920929, time = 0.008175373077392578\n",
      "Training at step=12, batch=360, train loss = 0.33683023947903123, train acc = 0.8700000047683716, time = 0.008253812789916992\n",
      "Training at step=12, batch=480, train loss = 0.48475954139686983, train acc = 0.8399999737739563, time = 0.008309364318847656\n",
      "Testing at step=12, batch=0, test loss = 0.40783377830960726, test acc = 0.8100000023841858, time = 0.0015778541564941406\n",
      "Testing at step=12, batch=20, test loss = 0.47404626923072807, test acc = 0.8299999833106995, time = 0.0016603469848632812\n",
      "Testing at step=12, batch=40, test loss = 0.411755536143348, test acc = 0.8500000238418579, time = 0.0015649795532226562\n",
      "Testing at step=12, batch=60, test loss = 0.520994651462511, test acc = 0.8299999833106995, time = 0.0015933513641357422\n",
      "Testing at step=12, batch=80, test loss = 0.3863402984029285, test acc = 0.8600000143051147, time = 0.0016057491302490234\n",
      "Step 12 finished in 16.265769481658936, Train loss = 0.40584988674752465, Test loss = 0.45087096249492165; Train Acc = 0.8594999985893568, Test Acc = 0.839699998497963\n",
      "Training at step=13, batch=0, train loss = 0.5956236837774095, train acc = 0.8100000023841858, time = 0.008299827575683594\n",
      "Training at step=13, batch=120, train loss = 0.3583460536355371, train acc = 0.8799999952316284, time = 0.008309125900268555\n",
      "Training at step=13, batch=240, train loss = 0.38666976070093356, train acc = 0.8500000238418579, time = 0.00817108154296875\n",
      "Training at step=13, batch=360, train loss = 0.4536201599269787, train acc = 0.8600000143051147, time = 0.008180856704711914\n",
      "Training at step=13, batch=480, train loss = 0.2936629612917794, train acc = 0.9100000262260437, time = 0.008203268051147461\n",
      "Testing at step=13, batch=0, test loss = 0.4036594382073331, test acc = 0.8299999833106995, time = 0.0016772747039794922\n",
      "Testing at step=13, batch=20, test loss = 0.5522390850724406, test acc = 0.8399999737739563, time = 0.0015745162963867188\n",
      "Testing at step=13, batch=40, test loss = 0.47570836330445915, test acc = 0.8500000238418579, time = 0.0015559196472167969\n",
      "Testing at step=13, batch=60, test loss = 0.4197503674378714, test acc = 0.8600000143051147, time = 0.0015578269958496094\n",
      "Testing at step=13, batch=80, test loss = 0.503192069214697, test acc = 0.8299999833106995, time = 0.001565694808959961\n",
      "Step 13 finished in 16.30500054359436, Train loss = 0.4033505810142347, Test loss = 0.4463709409901185; Train Acc = 0.8614666668574015, Test Acc = 0.8415999990701676\n",
      "Training at step=14, batch=0, train loss = 0.36056805565617805, train acc = 0.8700000047683716, time = 0.008346796035766602\n",
      "Training at step=14, batch=120, train loss = 0.28240523978749055, train acc = 0.8999999761581421, time = 0.008167266845703125\n",
      "Training at step=14, batch=240, train loss = 0.45738037003284854, train acc = 0.8399999737739563, time = 0.008286476135253906\n",
      "Training at step=14, batch=360, train loss = 0.5241879203362414, train acc = 0.8399999737739563, time = 0.008231878280639648\n",
      "Training at step=14, batch=480, train loss = 0.3447960377470436, train acc = 0.8999999761581421, time = 0.008215665817260742\n",
      "Testing at step=14, batch=0, test loss = 0.3929341815207006, test acc = 0.8799999952316284, time = 0.0015871524810791016\n",
      "Testing at step=14, batch=20, test loss = 0.3471005221551554, test acc = 0.8899999856948853, time = 0.0015692710876464844\n",
      "Testing at step=14, batch=40, test loss = 0.38933346383259093, test acc = 0.8799999952316284, time = 0.0015742778778076172\n",
      "Testing at step=14, batch=60, test loss = 0.5327985521853444, test acc = 0.8500000238418579, time = 0.0016067028045654297\n",
      "Testing at step=14, batch=80, test loss = 0.41633485805965614, test acc = 0.8299999833106995, time = 0.0016093254089355469\n",
      "Step 14 finished in 16.351501941680908, Train loss = 0.4020389145885073, Test loss = 0.44616095016774054; Train Acc = 0.8613166657090187, Test Acc = 0.841199997663498\n",
      "Training at step=15, batch=0, train loss = 0.4855686250394936, train acc = 0.8299999833106995, time = 0.008470535278320312\n",
      "Training at step=15, batch=120, train loss = 0.49935739694552056, train acc = 0.8100000023841858, time = 0.008179903030395508\n",
      "Training at step=15, batch=240, train loss = 0.2500289043931851, train acc = 0.9100000262260437, time = 0.008156299591064453\n",
      "Training at step=15, batch=360, train loss = 0.6214149320785645, train acc = 0.8100000023841858, time = 0.011501073837280273\n",
      "Training at step=15, batch=480, train loss = 0.29903756442929513, train acc = 0.8999999761581421, time = 0.008102178573608398\n",
      "Testing at step=15, batch=0, test loss = 0.3681293932527923, test acc = 0.8600000143051147, time = 0.0017247200012207031\n",
      "Testing at step=15, batch=20, test loss = 0.6758527647618859, test acc = 0.8100000023841858, time = 0.001577138900756836\n",
      "Testing at step=15, batch=40, test loss = 0.48277268429833997, test acc = 0.7900000214576721, time = 0.0016078948974609375\n",
      "Testing at step=15, batch=60, test loss = 0.42389930585532354, test acc = 0.8799999952316284, time = 0.0015707015991210938\n",
      "Testing at step=15, batch=80, test loss = 0.5841290857043293, test acc = 0.7799999713897705, time = 0.00159454345703125\n",
      "Step 15 finished in 16.32817792892456, Train loss = 0.39930673672730693, Test loss = 0.4427615080082741; Train Acc = 0.8619499995311102, Test Acc = 0.8433000004291534\n",
      "Training at step=16, batch=0, train loss = 0.3630646164069074, train acc = 0.8899999856948853, time = 0.008265495300292969\n",
      "Training at step=16, batch=120, train loss = 0.3536447150864215, train acc = 0.8899999856948853, time = 0.008148670196533203\n",
      "Training at step=16, batch=240, train loss = 0.3449266499953378, train acc = 0.8799999952316284, time = 0.008273601531982422\n",
      "Training at step=16, batch=360, train loss = 0.3342819975368064, train acc = 0.9100000262260437, time = 0.00816202163696289\n",
      "Training at step=16, batch=480, train loss = 0.3539657967316693, train acc = 0.8500000238418579, time = 0.008328437805175781\n",
      "Testing at step=16, batch=0, test loss = 0.5558324523431785, test acc = 0.8199999928474426, time = 0.0015671253204345703\n",
      "Testing at step=16, batch=20, test loss = 0.5041871688051114, test acc = 0.7900000214576721, time = 0.001573324203491211\n",
      "Testing at step=16, batch=40, test loss = 0.4214547968506942, test acc = 0.8600000143051147, time = 0.0015590190887451172\n",
      "Testing at step=16, batch=60, test loss = 0.3927164252353928, test acc = 0.8700000047683716, time = 0.0015549659729003906\n",
      "Testing at step=16, batch=80, test loss = 0.41366925811288874, test acc = 0.8500000238418579, time = 0.0015628337860107422\n",
      "Step 16 finished in 16.339972019195557, Train loss = 0.3981798736447315, Test loss = 0.4408723513357787; Train Acc = 0.8622166670362155, Test Acc = 0.8437999999523162\n",
      "Training at step=17, batch=0, train loss = 0.513972807084246, train acc = 0.8299999833106995, time = 0.008277416229248047\n",
      "Training at step=17, batch=120, train loss = 0.3194454647976628, train acc = 0.8999999761581421, time = 0.008269309997558594\n",
      "Training at step=17, batch=240, train loss = 0.38174202940903035, train acc = 0.9200000166893005, time = 0.008187294006347656\n",
      "Training at step=17, batch=360, train loss = 0.5931338243154357, train acc = 0.800000011920929, time = 0.008164405822753906\n",
      "Training at step=17, batch=480, train loss = 0.6380872652805768, train acc = 0.8500000238418579, time = 0.008205175399780273\n",
      "Testing at step=17, batch=0, test loss = 0.44020192904332645, test acc = 0.8500000238418579, time = 0.0015764236450195312\n",
      "Testing at step=17, batch=20, test loss = 0.3560723472727107, test acc = 0.9100000262260437, time = 0.0015561580657958984\n",
      "Testing at step=17, batch=40, test loss = 0.5684928337918397, test acc = 0.7599999904632568, time = 0.001558542251586914\n",
      "Testing at step=17, batch=60, test loss = 0.3370845955410504, test acc = 0.8799999952316284, time = 0.0015566349029541016\n",
      "Testing at step=17, batch=80, test loss = 0.4658792950559545, test acc = 0.8100000023841858, time = 0.0015788078308105469\n",
      "Step 17 finished in 16.32947874069214, Train loss = 0.3953990972129021, Test loss = 0.4485337268648546; Train Acc = 0.8636666651566823, Test Acc = 0.8423999983072281\n",
      "Training at step=18, batch=0, train loss = 0.4013833428609928, train acc = 0.8700000047683716, time = 0.008481502532958984\n",
      "Training at step=18, batch=120, train loss = 0.48736717187014433, train acc = 0.8600000143051147, time = 0.00816965103149414\n",
      "Training at step=18, batch=240, train loss = 0.4911610077247277, train acc = 0.8100000023841858, time = 0.008251428604125977\n",
      "Training at step=18, batch=360, train loss = 0.5200733072238457, train acc = 0.8399999737739563, time = 0.00819849967956543\n",
      "Training at step=18, batch=480, train loss = 0.33101611406097975, train acc = 0.9300000071525574, time = 0.008453607559204102\n",
      "Testing at step=18, batch=0, test loss = 0.3474117315400063, test acc = 0.8600000143051147, time = 0.0016407966613769531\n",
      "Testing at step=18, batch=20, test loss = 0.40387554158929573, test acc = 0.8299999833106995, time = 0.0016088485717773438\n",
      "Testing at step=18, batch=40, test loss = 0.5264257478351203, test acc = 0.8600000143051147, time = 0.00156402587890625\n",
      "Testing at step=18, batch=60, test loss = 0.3084997599124523, test acc = 0.9200000166893005, time = 0.0015587806701660156\n",
      "Testing at step=18, batch=80, test loss = 0.372198216694904, test acc = 0.8600000143051147, time = 0.001569986343383789\n",
      "Step 18 finished in 16.33426594734192, Train loss = 0.39489482967001077, Test loss = 0.4439812703938793; Train Acc = 0.8632666664322217, Test Acc = 0.8397000002861023\n",
      "Training at step=19, batch=0, train loss = 0.4000604649639491, train acc = 0.8999999761581421, time = 0.00857400894165039\n",
      "Training at step=19, batch=120, train loss = 0.31513850016853534, train acc = 0.8799999952316284, time = 0.008130073547363281\n",
      "Training at step=19, batch=240, train loss = 0.47016160220121583, train acc = 0.8100000023841858, time = 0.008203268051147461\n",
      "Training at step=19, batch=360, train loss = 0.3344975491062065, train acc = 0.8399999737739563, time = 0.00819706916809082\n",
      "Training at step=19, batch=480, train loss = 0.3807212305384926, train acc = 0.8799999952316284, time = 0.008230209350585938\n",
      "Testing at step=19, batch=0, test loss = 0.3191269919862804, test acc = 0.8799999952316284, time = 0.0015978813171386719\n",
      "Testing at step=19, batch=20, test loss = 0.7127772024510927, test acc = 0.7699999809265137, time = 0.0016102790832519531\n",
      "Testing at step=19, batch=40, test loss = 0.4433202936000249, test acc = 0.8199999928474426, time = 0.0015633106231689453\n",
      "Testing at step=19, batch=60, test loss = 0.4430991270485808, test acc = 0.8600000143051147, time = 0.0015985965728759766\n",
      "Testing at step=19, batch=80, test loss = 0.4109788940597319, test acc = 0.8399999737739563, time = 0.0015795230865478516\n",
      "Step 19 finished in 16.342129230499268, Train loss = 0.3929343544997562, Test loss = 0.4446033300533724; Train Acc = 0.8640833325187365, Test Acc = 0.841499999165535\n",
      "Training at step=20, batch=0, train loss = 0.24854419857518031, train acc = 0.9300000071525574, time = 0.008266687393188477\n",
      "Training at step=20, batch=120, train loss = 0.32730533471015155, train acc = 0.8600000143051147, time = 0.00816965103149414\n",
      "Training at step=20, batch=240, train loss = 0.2940426179554855, train acc = 0.8899999856948853, time = 0.008186101913452148\n",
      "Training at step=20, batch=360, train loss = 0.39856525095176676, train acc = 0.8299999833106995, time = 0.00813746452331543\n",
      "Training at step=20, batch=480, train loss = 0.3787240726287554, train acc = 0.8700000047683716, time = 0.008173227310180664\n",
      "Testing at step=20, batch=0, test loss = 0.499119315925435, test acc = 0.8299999833106995, time = 0.0016281604766845703\n",
      "Testing at step=20, batch=20, test loss = 0.46917301929707017, test acc = 0.8299999833106995, time = 0.0016367435455322266\n",
      "Testing at step=20, batch=40, test loss = 0.45798743936757513, test acc = 0.8399999737739563, time = 0.0015642642974853516\n",
      "Testing at step=20, batch=60, test loss = 0.34296276878622733, test acc = 0.8999999761581421, time = 0.0015664100646972656\n",
      "Testing at step=20, batch=80, test loss = 0.4351763120441285, test acc = 0.8899999856948853, time = 0.0015652179718017578\n",
      "Step 20 finished in 16.363621950149536, Train loss = 0.39202740234704114, Test loss = 0.4382791899872641; Train Acc = 0.8638999985655149, Test Acc = 0.8446999973058701\n",
      "Training at step=21, batch=0, train loss = 0.34377269592131837, train acc = 0.8600000143051147, time = 0.008270025253295898\n",
      "Training at step=21, batch=120, train loss = 0.5222656551552047, train acc = 0.8399999737739563, time = 0.00813603401184082\n",
      "Training at step=21, batch=240, train loss = 0.4025953652786734, train acc = 0.8600000143051147, time = 0.008184194564819336\n",
      "Training at step=21, batch=360, train loss = 0.32702474221616223, train acc = 0.8999999761581421, time = 0.008150339126586914\n",
      "Training at step=21, batch=480, train loss = 0.37078024892935674, train acc = 0.8700000047683716, time = 0.008217096328735352\n",
      "Testing at step=21, batch=0, test loss = 0.40493656259583927, test acc = 0.8999999761581421, time = 0.0015821456909179688\n",
      "Testing at step=21, batch=20, test loss = 0.4300774526082856, test acc = 0.8299999833106995, time = 0.0015664100646972656\n",
      "Testing at step=21, batch=40, test loss = 0.24108586041176316, test acc = 0.9300000071525574, time = 0.0015676021575927734\n",
      "Testing at step=21, batch=60, test loss = 0.30486196658511555, test acc = 0.9100000262260437, time = 0.0015709400177001953\n",
      "Testing at step=21, batch=80, test loss = 0.5012550075864387, test acc = 0.8299999833106995, time = 0.0015611648559570312\n",
      "Step 21 finished in 16.279796361923218, Train loss = 0.3909713845312867, Test loss = 0.44244549322227045; Train Acc = 0.8638500000039736, Test Acc = 0.8428999978303909\n",
      "Training at step=22, batch=0, train loss = 0.39784736102750423, train acc = 0.8399999737739563, time = 0.00821375846862793\n",
      "Training at step=22, batch=120, train loss = 0.43445298235019164, train acc = 0.7900000214576721, time = 0.008160114288330078\n",
      "Training at step=22, batch=240, train loss = 0.3453159724733082, train acc = 0.8999999761581421, time = 0.008245706558227539\n",
      "Training at step=22, batch=360, train loss = 0.3529886137970289, train acc = 0.8600000143051147, time = 0.008208751678466797\n",
      "Training at step=22, batch=480, train loss = 0.5068128506596618, train acc = 0.8799999952316284, time = 0.008170366287231445\n",
      "Testing at step=22, batch=0, test loss = 0.5378922967686433, test acc = 0.800000011920929, time = 0.0016257762908935547\n",
      "Testing at step=22, batch=20, test loss = 0.3387782667611454, test acc = 0.8899999856948853, time = 0.0015673637390136719\n",
      "Testing at step=22, batch=40, test loss = 0.3300899943407936, test acc = 0.8600000143051147, time = 0.001611948013305664\n",
      "Testing at step=22, batch=60, test loss = 0.3961067805713911, test acc = 0.8799999952316284, time = 0.0015707015991210938\n",
      "Testing at step=22, batch=80, test loss = 0.5761657193420695, test acc = 0.7799999713897705, time = 0.001561880111694336\n",
      "Step 22 finished in 16.27732563018799, Train loss = 0.38971470383321666, Test loss = 0.44415084990656484; Train Acc = 0.8640666660666466, Test Acc = 0.842399999499321\n",
      "Training at step=23, batch=0, train loss = 0.4108724655573721, train acc = 0.8799999952316284, time = 0.008408308029174805\n",
      "Training at step=23, batch=120, train loss = 0.36395014813186477, train acc = 0.8799999952316284, time = 0.008152961730957031\n",
      "Training at step=23, batch=240, train loss = 0.4550788368501943, train acc = 0.8399999737739563, time = 0.011482954025268555\n",
      "Training at step=23, batch=360, train loss = 0.4714363556072253, train acc = 0.8199999928474426, time = 0.008133172988891602\n",
      "Training at step=23, batch=480, train loss = 0.2755701845432259, train acc = 0.9100000262260437, time = 0.00813150405883789\n",
      "Testing at step=23, batch=0, test loss = 0.3915101607607497, test acc = 0.8199999928474426, time = 0.0016567707061767578\n",
      "Testing at step=23, batch=20, test loss = 0.3152396660230897, test acc = 0.8999999761581421, time = 0.001554250717163086\n",
      "Testing at step=23, batch=40, test loss = 0.30888517205744664, test acc = 0.8799999952316284, time = 0.0015575885772705078\n",
      "Testing at step=23, batch=60, test loss = 0.6948287318176481, test acc = 0.800000011920929, time = 0.0016331672668457031\n",
      "Testing at step=23, batch=80, test loss = 0.5507325925683437, test acc = 0.8199999928474426, time = 0.0015783309936523438\n",
      "Step 23 finished in 16.301844596862793, Train loss = 0.38867765984135844, Test loss = 0.4486006883545853; Train Acc = 0.8644166669249534, Test Acc = 0.8418999993801117\n",
      "Training at step=24, batch=0, train loss = 0.368486928983053, train acc = 0.8399999737739563, time = 0.008258819580078125\n",
      "Training at step=24, batch=120, train loss = 0.3407595817217648, train acc = 0.8700000047683716, time = 0.008148431777954102\n",
      "Training at step=24, batch=240, train loss = 0.3612578585201276, train acc = 0.8700000047683716, time = 0.008177757263183594\n",
      "Training at step=24, batch=360, train loss = 0.38375825020397697, train acc = 0.8799999952316284, time = 0.008263587951660156\n",
      "Training at step=24, batch=480, train loss = 0.39288777604553426, train acc = 0.8700000047683716, time = 0.008146047592163086\n",
      "Testing at step=24, batch=0, test loss = 0.4252098766805754, test acc = 0.8399999737739563, time = 0.001739501953125\n",
      "Testing at step=24, batch=20, test loss = 0.48222435094501137, test acc = 0.8299999833106995, time = 0.0015752315521240234\n",
      "Testing at step=24, batch=40, test loss = 0.4337374977637182, test acc = 0.8399999737739563, time = 0.001598358154296875\n",
      "Testing at step=24, batch=60, test loss = 0.32238382421171957, test acc = 0.8999999761581421, time = 0.001566171646118164\n",
      "Testing at step=24, batch=80, test loss = 0.35605187154190127, test acc = 0.8600000143051147, time = 0.0015592575073242188\n",
      "Step 24 finished in 16.35592484474182, Train loss = 0.3868514841211871, Test loss = 0.4446956141143987; Train Acc = 0.8657999992370605, Test Acc = 0.8412999975681305\n",
      "Training at step=25, batch=0, train loss = 0.28691833561123714, train acc = 0.9100000262260437, time = 0.008256196975708008\n",
      "Training at step=25, batch=120, train loss = 0.5086924941701415, train acc = 0.800000011920929, time = 0.008224725723266602\n",
      "Training at step=25, batch=240, train loss = 0.3669131975337679, train acc = 0.8299999833106995, time = 0.008214473724365234\n",
      "Training at step=25, batch=360, train loss = 0.4521081481054639, train acc = 0.8199999928474426, time = 0.008167505264282227\n",
      "Training at step=25, batch=480, train loss = 0.4671981981324878, train acc = 0.8500000238418579, time = 0.00818014144897461\n",
      "Testing at step=25, batch=0, test loss = 0.4978175213554855, test acc = 0.7699999809265137, time = 0.0015778541564941406\n",
      "Testing at step=25, batch=20, test loss = 0.37810401908611974, test acc = 0.8899999856948853, time = 0.00156402587890625\n",
      "Testing at step=25, batch=40, test loss = 0.35857797404428704, test acc = 0.8799999952316284, time = 0.0015628337860107422\n",
      "Testing at step=25, batch=60, test loss = 0.5653842304498891, test acc = 0.7900000214576721, time = 0.0015575885772705078\n",
      "Testing at step=25, batch=80, test loss = 0.6052845037992824, test acc = 0.8100000023841858, time = 0.0015552043914794922\n",
      "Step 25 finished in 16.321732997894287, Train loss = 0.386140101886074, Test loss = 0.44302855552838133; Train Acc = 0.8666333319743474, Test Acc = 0.8440000027418136\n",
      "Training at step=26, batch=0, train loss = 0.4071363466235928, train acc = 0.8899999856948853, time = 0.008300065994262695\n",
      "Training at step=26, batch=120, train loss = 0.4372732178482562, train acc = 0.8500000238418579, time = 0.008226871490478516\n",
      "Training at step=26, batch=240, train loss = 0.47906373556606313, train acc = 0.8600000143051147, time = 0.008395910263061523\n",
      "Training at step=26, batch=360, train loss = 0.3488840917906967, train acc = 0.8799999952316284, time = 0.008227825164794922\n",
      "Training at step=26, batch=480, train loss = 0.2954736295446962, train acc = 0.8700000047683716, time = 0.008142232894897461\n",
      "Testing at step=26, batch=0, test loss = 0.5188165884315269, test acc = 0.8199999928474426, time = 0.001699686050415039\n",
      "Testing at step=26, batch=20, test loss = 0.37384260805496333, test acc = 0.8899999856948853, time = 0.0016481876373291016\n",
      "Testing at step=26, batch=40, test loss = 0.3653087141939965, test acc = 0.8899999856948853, time = 0.0015599727630615234\n",
      "Testing at step=26, batch=60, test loss = 0.4119423424349978, test acc = 0.8700000047683716, time = 0.0016214847564697266\n",
      "Testing at step=26, batch=80, test loss = 0.344762532842275, test acc = 0.8700000047683716, time = 0.0016188621520996094\n",
      "Step 26 finished in 16.333544492721558, Train loss = 0.3862648052921272, Test loss = 0.44065409169371855; Train Acc = 0.8664000006516774, Test Acc = 0.8423999983072281\n",
      "Training at step=27, batch=0, train loss = 0.30838421170664687, train acc = 0.8999999761581421, time = 0.008396387100219727\n",
      "Training at step=27, batch=120, train loss = 0.41979377154749414, train acc = 0.8399999737739563, time = 0.008208274841308594\n",
      "Training at step=27, batch=240, train loss = 0.32845090677496125, train acc = 0.8700000047683716, time = 0.008156776428222656\n",
      "Training at step=27, batch=360, train loss = 0.45672493469509107, train acc = 0.8500000238418579, time = 0.008202791213989258\n",
      "Training at step=27, batch=480, train loss = 0.32239415163934815, train acc = 0.8899999856948853, time = 0.008277416229248047\n",
      "Testing at step=27, batch=0, test loss = 0.3848776938084606, test acc = 0.8700000047683716, time = 0.0015721321105957031\n",
      "Testing at step=27, batch=20, test loss = 0.5318200796981287, test acc = 0.8299999833106995, time = 0.0016329288482666016\n",
      "Testing at step=27, batch=40, test loss = 0.3667166994770993, test acc = 0.8899999856948853, time = 0.001565694808959961\n",
      "Testing at step=27, batch=60, test loss = 0.5334957057079621, test acc = 0.8299999833106995, time = 0.0016024112701416016\n",
      "Testing at step=27, batch=80, test loss = 0.3742825010916486, test acc = 0.8399999737739563, time = 0.0015828609466552734\n",
      "Step 27 finished in 16.306112051010132, Train loss = 0.3848183766074416, Test loss = 0.44217450055998264; Train Acc = 0.8662333339452744, Test Acc = 0.8437000000476838\n",
      "Training at step=28, batch=0, train loss = 0.3794528470194883, train acc = 0.8799999952316284, time = 0.008230447769165039\n",
      "Training at step=28, batch=120, train loss = 0.4539035541598215, train acc = 0.8500000238418579, time = 0.008186578750610352\n",
      "Training at step=28, batch=240, train loss = 0.4806437314070929, train acc = 0.800000011920929, time = 0.008160114288330078\n",
      "Training at step=28, batch=360, train loss = 0.4513057364260969, train acc = 0.8299999833106995, time = 0.008220434188842773\n",
      "Training at step=28, batch=480, train loss = 0.3126438378137954, train acc = 0.8999999761581421, time = 0.008180618286132812\n",
      "Testing at step=28, batch=0, test loss = 0.31268034295318914, test acc = 0.8899999856948853, time = 0.0016977787017822266\n",
      "Testing at step=28, batch=20, test loss = 0.5774617453806066, test acc = 0.8100000023841858, time = 0.0015625953674316406\n",
      "Testing at step=28, batch=40, test loss = 0.4638256556259053, test acc = 0.8500000238418579, time = 0.00156402587890625\n",
      "Testing at step=28, batch=60, test loss = 0.45413594740939706, test acc = 0.7799999713897705, time = 0.0015964508056640625\n",
      "Testing at step=28, batch=80, test loss = 0.4649980802807802, test acc = 0.8500000238418579, time = 0.0015726089477539062\n",
      "Step 28 finished in 16.31789469718933, Train loss = 0.3834550898684994, Test loss = 0.4391943531927337; Train Acc = 0.866099999845028, Test Acc = 0.8410999995470047\n",
      "Training at step=29, batch=0, train loss = 0.23957050411872788, train acc = 0.8999999761581421, time = 0.008372783660888672\n",
      "Training at step=29, batch=120, train loss = 0.4296058761561412, train acc = 0.8600000143051147, time = 0.008228302001953125\n",
      "Training at step=29, batch=240, train loss = 0.24760155926659252, train acc = 0.9100000262260437, time = 0.00825810432434082\n",
      "Training at step=29, batch=360, train loss = 0.302594687083626, train acc = 0.9200000166893005, time = 0.008200645446777344\n",
      "Training at step=29, batch=480, train loss = 0.3878782883139971, train acc = 0.8500000238418579, time = 0.008266687393188477\n",
      "Testing at step=29, batch=0, test loss = 0.4736449099644328, test acc = 0.8500000238418579, time = 0.0015721321105957031\n",
      "Testing at step=29, batch=20, test loss = 0.4901404776130887, test acc = 0.8600000143051147, time = 0.001588582992553711\n",
      "Testing at step=29, batch=40, test loss = 0.5832767869620917, test acc = 0.7900000214576721, time = 0.0015838146209716797\n",
      "Testing at step=29, batch=60, test loss = 0.30154480294839214, test acc = 0.8700000047683716, time = 0.0015673637390136719\n",
      "Testing at step=29, batch=80, test loss = 0.5244008791680084, test acc = 0.8199999928474426, time = 0.001569509506225586\n",
      "Step 29 finished in 16.319704294204712, Train loss = 0.38206512736210807, Test loss = 0.4458890277168032; Train Acc = 0.8669666661818822, Test Acc = 0.8429000008106232\n",
      "Training at step=30, batch=0, train loss = 0.3977482768458154, train acc = 0.8600000143051147, time = 0.008251428604125977\n",
      "Training at step=30, batch=120, train loss = 0.37970702948020074, train acc = 0.8500000238418579, time = 0.008209228515625\n",
      "Training at step=30, batch=240, train loss = 0.3051608558961491, train acc = 0.8700000047683716, time = 0.00821995735168457\n",
      "Training at step=30, batch=360, train loss = 0.5009933821117064, train acc = 0.8100000023841858, time = 0.008355379104614258\n",
      "Training at step=30, batch=480, train loss = 0.38529247896773755, train acc = 0.8299999833106995, time = 0.008203983306884766\n",
      "Testing at step=30, batch=0, test loss = 0.5788780901193644, test acc = 0.7799999713897705, time = 0.001611471176147461\n",
      "Testing at step=30, batch=20, test loss = 0.41629087518315977, test acc = 0.8500000238418579, time = 0.001592397689819336\n",
      "Testing at step=30, batch=40, test loss = 0.3682302351440556, test acc = 0.8500000238418579, time = 0.0015659332275390625\n",
      "Testing at step=30, batch=60, test loss = 0.4647455741888748, test acc = 0.8500000238418579, time = 0.0016896724700927734\n",
      "Testing at step=30, batch=80, test loss = 0.3262173797663307, test acc = 0.8899999856948853, time = 0.0016016960144042969\n",
      "Step 30 finished in 16.275881052017212, Train loss = 0.38160090938986563, Test loss = 0.44265690210940895; Train Acc = 0.8671666660904884, Test Acc = 0.8420999991893768\n",
      "Training at step=31, batch=0, train loss = 0.26799523034360695, train acc = 0.8899999856948853, time = 0.008575677871704102\n",
      "Training at step=31, batch=120, train loss = 0.5659002426324833, train acc = 0.8299999833106995, time = 0.008184671401977539\n",
      "Training at step=31, batch=240, train loss = 0.21643081327217925, train acc = 0.8999999761581421, time = 0.008197307586669922\n",
      "Training at step=31, batch=360, train loss = 0.34489619936025356, train acc = 0.8899999856948853, time = 0.008179664611816406\n",
      "Training at step=31, batch=480, train loss = 0.25172874244989596, train acc = 0.8899999856948853, time = 0.008108377456665039\n",
      "Testing at step=31, batch=0, test loss = 0.5468016775828456, test acc = 0.7799999713897705, time = 0.001653432846069336\n",
      "Testing at step=31, batch=20, test loss = 0.6037459902888104, test acc = 0.800000011920929, time = 0.0015988349914550781\n",
      "Testing at step=31, batch=40, test loss = 0.48728478623765276, test acc = 0.8100000023841858, time = 0.0015709400177001953\n",
      "Testing at step=31, batch=60, test loss = 0.38778059150377614, test acc = 0.8399999737739563, time = 0.001619577407836914\n",
      "Testing at step=31, batch=80, test loss = 0.3922524141641917, test acc = 0.8100000023841858, time = 0.0016126632690429688\n",
      "Step 31 finished in 16.316965341567993, Train loss = 0.38076344475618923, Test loss = 0.44243547639436337; Train Acc = 0.8676333334048589, Test Acc = 0.8434999990463257\n",
      "Training at step=32, batch=0, train loss = 0.2986992774742527, train acc = 0.8999999761581421, time = 0.008289813995361328\n",
      "Training at step=32, batch=120, train loss = 0.33824672502165287, train acc = 0.8700000047683716, time = 0.008228063583374023\n",
      "Training at step=32, batch=240, train loss = 0.5155192756942055, train acc = 0.800000011920929, time = 0.008135795593261719\n",
      "Training at step=32, batch=360, train loss = 0.4306442653793866, train acc = 0.8600000143051147, time = 0.008221149444580078\n",
      "Training at step=32, batch=480, train loss = 0.17053745278225754, train acc = 0.949999988079071, time = 0.008150815963745117\n",
      "Testing at step=32, batch=0, test loss = 0.396213718483786, test acc = 0.8799999952316284, time = 0.0015969276428222656\n",
      "Testing at step=32, batch=20, test loss = 0.3697348852544027, test acc = 0.8399999737739563, time = 0.0015993118286132812\n",
      "Testing at step=32, batch=40, test loss = 0.436417780553573, test acc = 0.8100000023841858, time = 0.001558542251586914\n",
      "Testing at step=32, batch=60, test loss = 0.38083268897292877, test acc = 0.8600000143051147, time = 0.0015749931335449219\n",
      "Testing at step=32, batch=80, test loss = 0.48493248739684064, test acc = 0.8399999737739563, time = 0.0015685558319091797\n",
      "Step 32 finished in 16.32347083091736, Train loss = 0.3810019425682988, Test loss = 0.43980076299260984; Train Acc = 0.866433334449927, Test Acc = 0.8427999985218048\n",
      "Training at step=33, batch=0, train loss = 0.32704726583615373, train acc = 0.8799999952316284, time = 0.008379697799682617\n",
      "Training at step=33, batch=120, train loss = 0.27926806558654366, train acc = 0.9200000166893005, time = 0.008309602737426758\n",
      "Training at step=33, batch=240, train loss = 0.3842032655588126, train acc = 0.8500000238418579, time = 0.008177042007446289\n",
      "Training at step=33, batch=360, train loss = 0.26946012941810227, train acc = 0.9200000166893005, time = 0.008240461349487305\n",
      "Training at step=33, batch=480, train loss = 0.306200149523537, train acc = 0.8700000047683716, time = 0.008190631866455078\n",
      "Testing at step=33, batch=0, test loss = 0.32140724475781796, test acc = 0.8999999761581421, time = 0.0016484260559082031\n",
      "Testing at step=33, batch=20, test loss = 0.46678489626365793, test acc = 0.8199999928474426, time = 0.00156402587890625\n",
      "Testing at step=33, batch=40, test loss = 0.3889241479864123, test acc = 0.8399999737739563, time = 0.0015387535095214844\n",
      "Testing at step=33, batch=60, test loss = 0.35730537378268346, test acc = 0.8799999952316284, time = 0.0015642642974853516\n",
      "Testing at step=33, batch=80, test loss = 0.481521390423554, test acc = 0.8299999833106995, time = 0.0015645027160644531\n",
      "Step 33 finished in 16.301995754241943, Train loss = 0.378944651344464, Test loss = 0.44077238884776576; Train Acc = 0.8679999991257985, Test Acc = 0.842399999499321\n",
      "Training at step=34, batch=0, train loss = 0.39360996869236403, train acc = 0.8399999737739563, time = 0.00831151008605957\n",
      "Training at step=34, batch=120, train loss = 0.44877782159069324, train acc = 0.8399999737739563, time = 0.008395671844482422\n",
      "Training at step=34, batch=240, train loss = 0.56676544720969, train acc = 0.8399999737739563, time = 0.008282184600830078\n",
      "Training at step=34, batch=360, train loss = 0.42374940002207395, train acc = 0.8600000143051147, time = 0.008193731307983398\n",
      "Training at step=34, batch=480, train loss = 0.37554593154071414, train acc = 0.8899999856948853, time = 0.008265495300292969\n",
      "Testing at step=34, batch=0, test loss = 0.3870505404446272, test acc = 0.8700000047683716, time = 0.0015766620635986328\n",
      "Testing at step=34, batch=20, test loss = 0.39148417377619255, test acc = 0.8399999737739563, time = 0.0016088485717773438\n",
      "Testing at step=34, batch=40, test loss = 0.4904746387363105, test acc = 0.8299999833106995, time = 0.0015952587127685547\n",
      "Testing at step=34, batch=60, test loss = 0.6220861934924312, test acc = 0.8100000023841858, time = 0.0015685558319091797\n",
      "Testing at step=34, batch=80, test loss = 0.4573250935840527, test acc = 0.8600000143051147, time = 0.0015664100646972656\n",
      "Step 34 finished in 16.28002905845642, Train loss = 0.3791005369108216, Test loss = 0.44192386297030006; Train Acc = 0.8672666655977567, Test Acc = 0.8420999991893768\n",
      "Training at step=35, batch=0, train loss = 0.3669408496723022, train acc = 0.8700000047683716, time = 0.008285999298095703\n",
      "Training at step=35, batch=120, train loss = 0.39969276181972396, train acc = 0.8399999737739563, time = 0.008161306381225586\n",
      "Training at step=35, batch=240, train loss = 0.35910287164679083, train acc = 0.8600000143051147, time = 0.008301496505737305\n",
      "Training at step=35, batch=360, train loss = 0.45773897831163524, train acc = 0.8500000238418579, time = 0.008211135864257812\n",
      "Training at step=35, batch=480, train loss = 0.33535177865973426, train acc = 0.8700000047683716, time = 0.008220672607421875\n",
      "Testing at step=35, batch=0, test loss = 0.3365691589166759, test acc = 0.8999999761581421, time = 0.0015740394592285156\n",
      "Testing at step=35, batch=20, test loss = 0.5198072816329649, test acc = 0.75, time = 0.0015914440155029297\n",
      "Testing at step=35, batch=40, test loss = 0.4873547854887196, test acc = 0.8399999737739563, time = 0.0016117095947265625\n",
      "Testing at step=35, batch=60, test loss = 0.3698175868717958, test acc = 0.8799999952316284, time = 0.0015840530395507812\n",
      "Testing at step=35, batch=80, test loss = 0.46291615340549597, test acc = 0.800000011920929, time = 0.0016448497772216797\n",
      "Step 35 finished in 16.377490758895874, Train loss = 0.3778101001685021, Test loss = 0.442416402493248; Train Acc = 0.868116666773955, Test Acc = 0.8424000012874603\n",
      "Training at step=36, batch=0, train loss = 0.379447506738417, train acc = 0.8500000238418579, time = 0.008307695388793945\n",
      "Training at step=36, batch=120, train loss = 0.4277723998024719, train acc = 0.8199999928474426, time = 0.008199453353881836\n",
      "Training at step=36, batch=240, train loss = 0.4253938160767134, train acc = 0.8500000238418579, time = 0.008130311965942383\n",
      "Training at step=36, batch=360, train loss = 0.46728975771578446, train acc = 0.8100000023841858, time = 0.008249998092651367\n",
      "Training at step=36, batch=480, train loss = 0.3552097593557996, train acc = 0.8500000238418579, time = 0.008260488510131836\n",
      "Testing at step=36, batch=0, test loss = 0.617246466333154, test acc = 0.7400000095367432, time = 0.0015795230865478516\n",
      "Testing at step=36, batch=20, test loss = 0.3829164051786079, test acc = 0.8799999952316284, time = 0.001561880111694336\n",
      "Testing at step=36, batch=40, test loss = 0.3935853892174395, test acc = 0.8199999928474426, time = 0.001562356948852539\n",
      "Testing at step=36, batch=60, test loss = 0.6325232038076363, test acc = 0.7900000214576721, time = 0.0015530586242675781\n",
      "Testing at step=36, batch=80, test loss = 0.3568459947538215, test acc = 0.8500000238418579, time = 0.0015544891357421875\n",
      "Step 36 finished in 16.27703619003296, Train loss = 0.3768275849588019, Test loss = 0.44713814255681633; Train Acc = 0.868983334004879, Test Acc = 0.8387999987602234\n",
      "Training at step=37, batch=0, train loss = 0.2856152141349865, train acc = 0.9100000262260437, time = 0.008255243301391602\n",
      "Training at step=37, batch=120, train loss = 0.4607710789942305, train acc = 0.8100000023841858, time = 0.008194923400878906\n",
      "Training at step=37, batch=240, train loss = 0.3047179528068546, train acc = 0.8899999856948853, time = 0.008171558380126953\n",
      "Training at step=37, batch=360, train loss = 0.3585465082604751, train acc = 0.8700000047683716, time = 0.008164167404174805\n",
      "Training at step=37, batch=480, train loss = 0.34693321266606925, train acc = 0.8600000143051147, time = 0.008197307586669922\n",
      "Testing at step=37, batch=0, test loss = 0.2740060437091732, test acc = 0.8899999856948853, time = 0.0017361640930175781\n",
      "Testing at step=37, batch=20, test loss = 0.4325455457443882, test acc = 0.8700000047683716, time = 0.0016794204711914062\n",
      "Testing at step=37, batch=40, test loss = 0.3910593208107322, test acc = 0.8299999833106995, time = 0.0016460418701171875\n",
      "Testing at step=37, batch=60, test loss = 0.49328299637108153, test acc = 0.8700000047683716, time = 0.0015740394592285156\n",
      "Testing at step=37, batch=80, test loss = 0.38527810732220674, test acc = 0.8600000143051147, time = 0.0015604496002197266\n",
      "Step 37 finished in 16.350407123565674, Train loss = 0.37695693279788517, Test loss = 0.44063130031348186; Train Acc = 0.8689499991138776, Test Acc = 0.8430999994277955\n",
      "Training at step=38, batch=0, train loss = 0.3041593328326601, train acc = 0.8899999856948853, time = 0.008333444595336914\n",
      "Training at step=38, batch=120, train loss = 0.5378427301002936, train acc = 0.800000011920929, time = 0.008188486099243164\n",
      "Training at step=38, batch=240, train loss = 0.34838650826618656, train acc = 0.9100000262260437, time = 0.008280754089355469\n",
      "Training at step=38, batch=360, train loss = 0.2875270051908859, train acc = 0.9100000262260437, time = 0.008198022842407227\n",
      "Training at step=38, batch=480, train loss = 0.3411189047795927, train acc = 0.8700000047683716, time = 0.008168458938598633\n",
      "Testing at step=38, batch=0, test loss = 0.5242366115363778, test acc = 0.800000011920929, time = 0.0017390251159667969\n",
      "Testing at step=38, batch=20, test loss = 0.4883398793095211, test acc = 0.7900000214576721, time = 0.0016002655029296875\n",
      "Testing at step=38, batch=40, test loss = 0.4463431799134397, test acc = 0.8100000023841858, time = 0.0015671253204345703\n",
      "Testing at step=38, batch=60, test loss = 0.3404468813189648, test acc = 0.8899999856948853, time = 0.0015745162963867188\n",
      "Testing at step=38, batch=80, test loss = 0.45244253010796937, test acc = 0.8299999833106995, time = 0.0015594959259033203\n",
      "Step 38 finished in 16.307441234588623, Train loss = 0.3760339600236209, Test loss = 0.44453846517515244; Train Acc = 0.8686166658004125, Test Acc = 0.8385999983549118\n",
      "Training at step=39, batch=0, train loss = 0.37594849094117494, train acc = 0.8799999952316284, time = 0.008385896682739258\n",
      "Training at step=39, batch=120, train loss = 0.3527703482309905, train acc = 0.8500000238418579, time = 0.008136272430419922\n",
      "Training at step=39, batch=240, train loss = 0.3753220167086442, train acc = 0.8799999952316284, time = 0.00818490982055664\n",
      "Training at step=39, batch=360, train loss = 0.4432158661653171, train acc = 0.8600000143051147, time = 0.008289337158203125\n",
      "Training at step=39, batch=480, train loss = 0.3291866404792062, train acc = 0.8700000047683716, time = 0.00820302963256836\n",
      "Testing at step=39, batch=0, test loss = 0.4897679933271539, test acc = 0.7900000214576721, time = 0.001680135726928711\n",
      "Testing at step=39, batch=20, test loss = 0.35516015180013466, test acc = 0.8600000143051147, time = 0.0016105175018310547\n",
      "Testing at step=39, batch=40, test loss = 0.3421191393404374, test acc = 0.8600000143051147, time = 0.0015723705291748047\n",
      "Testing at step=39, batch=60, test loss = 0.4735281891976028, test acc = 0.8100000023841858, time = 0.0015604496002197266\n",
      "Testing at step=39, batch=80, test loss = 0.38615177934644906, test acc = 0.8799999952316284, time = 0.0015974044799804688\n",
      "Step 39 finished in 16.337741136550903, Train loss = 0.37563905556897614, Test loss = 0.4413372682010046; Train Acc = 0.8697000004847845, Test Acc = 0.8435999977588654\n",
      "Training at step=40, batch=0, train loss = 0.47573772158167754, train acc = 0.8799999952316284, time = 0.00846552848815918\n",
      "Training at step=40, batch=120, train loss = 0.3090869387223834, train acc = 0.8700000047683716, time = 0.00826716423034668\n",
      "Training at step=40, batch=240, train loss = 0.395667850631885, train acc = 0.8700000047683716, time = 0.008280038833618164\n",
      "Training at step=40, batch=360, train loss = 0.4825175510711317, train acc = 0.8500000238418579, time = 0.008191823959350586\n",
      "Training at step=40, batch=480, train loss = 0.286177659461011, train acc = 0.8799999952316284, time = 0.008191108703613281\n",
      "Testing at step=40, batch=0, test loss = 0.44133580781518594, test acc = 0.8500000238418579, time = 0.0016226768493652344\n",
      "Testing at step=40, batch=20, test loss = 0.3902433813604828, test acc = 0.8600000143051147, time = 0.0015940666198730469\n",
      "Testing at step=40, batch=40, test loss = 0.4983451098890548, test acc = 0.8199999928474426, time = 0.0015680789947509766\n",
      "Testing at step=40, batch=60, test loss = 0.4103441746843142, test acc = 0.8299999833106995, time = 0.0015325546264648438\n",
      "Testing at step=40, batch=80, test loss = 0.3758970955322105, test acc = 0.8600000143051147, time = 0.001556396484375\n",
      "Step 40 finished in 16.337756872177124, Train loss = 0.3743346896795126, Test loss = 0.4434459131657505; Train Acc = 0.86948333243529, Test Acc = 0.8419999998807907\n",
      "Training at step=41, batch=0, train loss = 0.36313010369713, train acc = 0.9100000262260437, time = 0.008299112319946289\n",
      "Training at step=41, batch=120, train loss = 0.43693816280555076, train acc = 0.8500000238418579, time = 0.008287429809570312\n",
      "Training at step=41, batch=240, train loss = 0.3642461755536708, train acc = 0.8600000143051147, time = 0.008145809173583984\n",
      "Training at step=41, batch=360, train loss = 0.44809708079028404, train acc = 0.8899999856948853, time = 0.008132219314575195\n",
      "Training at step=41, batch=480, train loss = 0.510635749258653, train acc = 0.8500000238418579, time = 0.008184194564819336\n",
      "Testing at step=41, batch=0, test loss = 0.478781548216593, test acc = 0.8299999833106995, time = 0.001748800277709961\n",
      "Testing at step=41, batch=20, test loss = 0.39306608689966943, test acc = 0.8700000047683716, time = 0.0015842914581298828\n",
      "Testing at step=41, batch=40, test loss = 0.48855725265718625, test acc = 0.8600000143051147, time = 0.0015840530395507812\n",
      "Testing at step=41, batch=60, test loss = 0.5266530610175062, test acc = 0.8700000047683716, time = 0.0015892982482910156\n",
      "Testing at step=41, batch=80, test loss = 0.5324758808533304, test acc = 0.8100000023841858, time = 0.0016322135925292969\n",
      "Step 41 finished in 16.32723617553711, Train loss = 0.37500611709849696, Test loss = 0.4408636946266748; Train Acc = 0.8685666671395302, Test Acc = 0.8426999992132187\n",
      "Training at step=42, batch=0, train loss = 0.39743257796862763, train acc = 0.8799999952316284, time = 0.008380413055419922\n",
      "Training at step=42, batch=120, train loss = 0.29656941434574535, train acc = 0.8600000143051147, time = 0.008192777633666992\n",
      "Training at step=42, batch=240, train loss = 0.2630648340179393, train acc = 0.8700000047683716, time = 0.008179903030395508\n",
      "Training at step=42, batch=360, train loss = 0.3901892715349091, train acc = 0.8500000238418579, time = 0.00820469856262207\n",
      "Training at step=42, batch=480, train loss = 0.2783684509781603, train acc = 0.9200000166893005, time = 0.008332490921020508\n",
      "Testing at step=42, batch=0, test loss = 0.4050318415509176, test acc = 0.8600000143051147, time = 0.0015900135040283203\n",
      "Testing at step=42, batch=20, test loss = 0.469223302706038, test acc = 0.8399999737739563, time = 0.0015687942504882812\n",
      "Testing at step=42, batch=40, test loss = 0.49870647128653617, test acc = 0.8299999833106995, time = 0.0015635490417480469\n",
      "Testing at step=42, batch=60, test loss = 0.4898156955061597, test acc = 0.8399999737739563, time = 0.0015749931335449219\n",
      "Testing at step=42, batch=80, test loss = 0.37604561242342, test acc = 0.8600000143051147, time = 0.0015933513641357422\n",
      "Step 42 finished in 16.349071502685547, Train loss = 0.3743157312207771, Test loss = 0.44055571328655213; Train Acc = 0.8685166669885317, Test Acc = 0.8435000002384185\n",
      "Training at step=43, batch=0, train loss = 0.476391782062011, train acc = 0.8399999737739563, time = 0.008356094360351562\n",
      "Training at step=43, batch=120, train loss = 0.3872613808840869, train acc = 0.8399999737739563, time = 0.008248090744018555\n",
      "Training at step=43, batch=240, train loss = 0.2807917529450552, train acc = 0.8999999761581421, time = 0.008205413818359375\n",
      "Training at step=43, batch=360, train loss = 0.4930400117893252, train acc = 0.8399999737739563, time = 0.008225202560424805\n",
      "Training at step=43, batch=480, train loss = 0.31963104219582067, train acc = 0.8700000047683716, time = 0.00811004638671875\n",
      "Testing at step=43, batch=0, test loss = 0.5866598315181715, test acc = 0.7799999713897705, time = 0.001603841781616211\n",
      "Testing at step=43, batch=20, test loss = 0.3924400801193435, test acc = 0.8399999737739563, time = 0.001561880111694336\n",
      "Testing at step=43, batch=40, test loss = 0.5274108587601831, test acc = 0.8399999737739563, time = 0.0015723705291748047\n",
      "Testing at step=43, batch=60, test loss = 0.24568754143401178, test acc = 0.9100000262260437, time = 0.001634836196899414\n",
      "Testing at step=43, batch=80, test loss = 0.30813014750346446, test acc = 0.8600000143051147, time = 0.0015714168548583984\n",
      "Step 43 finished in 16.306233644485474, Train loss = 0.37318390155305736, Test loss = 0.4416746081631968; Train Acc = 0.8689999980727832, Test Acc = 0.8414000004529953\n",
      "Training at step=44, batch=0, train loss = 0.24552094379957293, train acc = 0.8999999761581421, time = 0.00836491584777832\n",
      "Training at step=44, batch=120, train loss = 0.48130944214265237, train acc = 0.8399999737739563, time = 0.008163213729858398\n",
      "Training at step=44, batch=240, train loss = 0.54996660134284, train acc = 0.8199999928474426, time = 0.008175373077392578\n",
      "Training at step=44, batch=360, train loss = 0.28737937393996615, train acc = 0.8600000143051147, time = 0.00813603401184082\n",
      "Training at step=44, batch=480, train loss = 0.3207495611087408, train acc = 0.8799999952316284, time = 0.008220195770263672\n",
      "Testing at step=44, batch=0, test loss = 0.3319150493225906, test acc = 0.9300000071525574, time = 0.0016143321990966797\n",
      "Testing at step=44, batch=20, test loss = 0.6037039721520929, test acc = 0.800000011920929, time = 0.0015573501586914062\n",
      "Testing at step=44, batch=40, test loss = 0.42347764399971166, test acc = 0.8199999928474426, time = 0.0016033649444580078\n",
      "Testing at step=44, batch=60, test loss = 0.32853163709826494, test acc = 0.8899999856948853, time = 0.0015854835510253906\n",
      "Testing at step=44, batch=80, test loss = 0.3436937484686446, test acc = 0.9100000262260437, time = 0.0015645027160644531\n",
      "Step 44 finished in 16.334267377853394, Train loss = 0.37286180652315065, Test loss = 0.44076472481171175; Train Acc = 0.8688833328088125, Test Acc = 0.8420999974012375\n",
      "Training at step=45, batch=0, train loss = 0.35394163479454527, train acc = 0.8500000238418579, time = 0.008285045623779297\n",
      "Training at step=45, batch=120, train loss = 0.4409348923453992, train acc = 0.8299999833106995, time = 0.00816655158996582\n",
      "Training at step=45, batch=240, train loss = 0.33899343606713617, train acc = 0.8899999856948853, time = 0.008214235305786133\n",
      "Training at step=45, batch=360, train loss = 0.5568980720821495, train acc = 0.8500000238418579, time = 0.00816965103149414\n",
      "Training at step=45, batch=480, train loss = 0.4071643997630542, train acc = 0.8299999833106995, time = 0.008225679397583008\n",
      "Testing at step=45, batch=0, test loss = 0.36001811257279653, test acc = 0.8700000047683716, time = 0.0015730857849121094\n",
      "Testing at step=45, batch=20, test loss = 0.47513151906310547, test acc = 0.8600000143051147, time = 0.0015606880187988281\n",
      "Testing at step=45, batch=40, test loss = 0.5452602527487848, test acc = 0.8199999928474426, time = 0.0015680789947509766\n",
      "Testing at step=45, batch=60, test loss = 0.3885863092606131, test acc = 0.8799999952316284, time = 0.0015630722045898438\n",
      "Testing at step=45, batch=80, test loss = 0.3428703333154741, test acc = 0.8700000047683716, time = 0.001575469970703125\n",
      "Step 45 finished in 16.26971745491028, Train loss = 0.3722577047651607, Test loss = 0.4431547412096776; Train Acc = 0.8694499992330869, Test Acc = 0.8403999984264374\n",
      "Training at step=46, batch=0, train loss = 0.3899759682121482, train acc = 0.8700000047683716, time = 0.008233308792114258\n",
      "Training at step=46, batch=120, train loss = 0.3515554715188832, train acc = 0.8799999952316284, time = 0.008141279220581055\n",
      "Training at step=46, batch=240, train loss = 0.31716912935149827, train acc = 0.8799999952316284, time = 0.008193731307983398\n",
      "Training at step=46, batch=360, train loss = 0.34504369429449205, train acc = 0.8999999761581421, time = 0.008198261260986328\n",
      "Training at step=46, batch=480, train loss = 0.44868698699865156, train acc = 0.8299999833106995, time = 0.008196353912353516\n",
      "Testing at step=46, batch=0, test loss = 0.5118134747945097, test acc = 0.8100000023841858, time = 0.0016007423400878906\n",
      "Testing at step=46, batch=20, test loss = 0.3278416712908161, test acc = 0.8899999856948853, time = 0.001560211181640625\n",
      "Testing at step=46, batch=40, test loss = 0.3367834323028518, test acc = 0.8999999761581421, time = 0.0015773773193359375\n",
      "Testing at step=46, batch=60, test loss = 0.40937593605013106, test acc = 0.8399999737739563, time = 0.0015540122985839844\n",
      "Testing at step=46, batch=80, test loss = 0.38261556642066197, test acc = 0.8899999856948853, time = 0.0015859603881835938\n",
      "Step 46 finished in 16.262735843658447, Train loss = 0.3720130643562256, Test loss = 0.4419356023222133; Train Acc = 0.8692333334684372, Test Acc = 0.843299994468689\n",
      "Training at step=47, batch=0, train loss = 0.2614520105347369, train acc = 0.9100000262260437, time = 0.008484363555908203\n",
      "Training at step=47, batch=120, train loss = 0.271214606090568, train acc = 0.8700000047683716, time = 0.008229255676269531\n",
      "Training at step=47, batch=240, train loss = 0.38532895235790293, train acc = 0.8500000238418579, time = 0.008167743682861328\n",
      "Training at step=47, batch=360, train loss = 0.4452288464443053, train acc = 0.8500000238418579, time = 0.00814962387084961\n",
      "Training at step=47, batch=480, train loss = 0.5396556071214796, train acc = 0.7799999713897705, time = 0.008160114288330078\n",
      "Testing at step=47, batch=0, test loss = 0.723310912237385, test acc = 0.7699999809265137, time = 0.0015854835510253906\n",
      "Testing at step=47, batch=20, test loss = 0.5253042180287006, test acc = 0.800000011920929, time = 0.0015518665313720703\n",
      "Testing at step=47, batch=40, test loss = 0.4114950435755745, test acc = 0.8199999928474426, time = 0.001603841781616211\n",
      "Testing at step=47, batch=60, test loss = 0.29084594675361675, test acc = 0.9200000166893005, time = 0.001554250717163086\n",
      "Testing at step=47, batch=80, test loss = 0.5822227725571824, test acc = 0.800000011920929, time = 0.0015599727630615234\n",
      "Step 47 finished in 16.30566167831421, Train loss = 0.37196440453727025, Test loss = 0.4425722902504565; Train Acc = 0.8688666661580403, Test Acc = 0.8407999992370605\n",
      "Training at step=48, batch=0, train loss = 0.3250990132012028, train acc = 0.8799999952316284, time = 0.008382558822631836\n",
      "Training at step=48, batch=120, train loss = 0.29279456438517626, train acc = 0.8799999952316284, time = 0.008161544799804688\n",
      "Training at step=48, batch=240, train loss = 0.42355225427369725, train acc = 0.8299999833106995, time = 0.008201122283935547\n",
      "Training at step=48, batch=360, train loss = 0.28477743748929923, train acc = 0.8899999856948853, time = 0.008152961730957031\n",
      "Training at step=48, batch=480, train loss = 0.2920809036877283, train acc = 0.8899999856948853, time = 0.008206605911254883\n",
      "Testing at step=48, batch=0, test loss = 0.3922510143139477, test acc = 0.8299999833106995, time = 0.001565694808959961\n",
      "Testing at step=48, batch=20, test loss = 0.33566919674966267, test acc = 0.8899999856948853, time = 0.0015666484832763672\n",
      "Testing at step=48, batch=40, test loss = 0.37118182321963666, test acc = 0.8500000238418579, time = 0.0016109943389892578\n",
      "Testing at step=48, batch=60, test loss = 0.3828562877203078, test acc = 0.8500000238418579, time = 0.0015881061553955078\n",
      "Testing at step=48, batch=80, test loss = 0.457554673227161, test acc = 0.8700000047683716, time = 0.0016009807586669922\n",
      "Step 48 finished in 16.278668642044067, Train loss = 0.37113999641702433, Test loss = 0.44499338978624053; Train Acc = 0.8701833320657412, Test Acc = 0.8402999997138977\n",
      "Training at step=49, batch=0, train loss = 0.4066780392481335, train acc = 0.8399999737739563, time = 0.008224010467529297\n",
      "Training at step=49, batch=120, train loss = 0.23947255777632429, train acc = 0.9399999976158142, time = 0.008185148239135742\n",
      "Training at step=49, batch=240, train loss = 0.3789676227840201, train acc = 0.8399999737739563, time = 0.008160114288330078\n",
      "Training at step=49, batch=360, train loss = 0.47937836741265594, train acc = 0.8500000238418579, time = 0.008143186569213867\n",
      "Training at step=49, batch=480, train loss = 0.268802074316428, train acc = 0.8999999761581421, time = 0.008199214935302734\n",
      "Testing at step=49, batch=0, test loss = 0.42369626165675933, test acc = 0.8600000143051147, time = 0.0015876293182373047\n",
      "Testing at step=49, batch=20, test loss = 0.48858484425707205, test acc = 0.9200000166893005, time = 0.0015575885772705078\n",
      "Testing at step=49, batch=40, test loss = 0.4980667930033327, test acc = 0.800000011920929, time = 0.0016634464263916016\n",
      "Testing at step=49, batch=60, test loss = 0.4161737974765389, test acc = 0.8399999737739563, time = 0.0015690326690673828\n",
      "Testing at step=49, batch=80, test loss = 0.3647167234468972, test acc = 0.8799999952316284, time = 0.0015826225280761719\n",
      "Step 49 finished in 16.302788972854614, Train loss = 0.3702063590262085, Test loss = 0.44307134853297186; Train Acc = 0.8707666679223378, Test Acc = 0.841599999666214\n",
      "Training at step=50, batch=0, train loss = 0.30480576066415566, train acc = 0.8899999856948853, time = 0.008320093154907227\n",
      "Training at step=50, batch=120, train loss = 0.2527835057947546, train acc = 0.9300000071525574, time = 0.008186578750610352\n",
      "Training at step=50, batch=240, train loss = 0.44988987185776724, train acc = 0.8500000238418579, time = 0.00817108154296875\n",
      "Training at step=50, batch=360, train loss = 0.36370987023580115, train acc = 0.8500000238418579, time = 0.008246898651123047\n",
      "Training at step=50, batch=480, train loss = 0.30700874781916876, train acc = 0.8999999761581421, time = 0.008176088333129883\n",
      "Testing at step=50, batch=0, test loss = 0.4713565136286404, test acc = 0.800000011920929, time = 0.0015819072723388672\n",
      "Testing at step=50, batch=20, test loss = 0.38692788898392005, test acc = 0.8399999737739563, time = 0.0015835762023925781\n",
      "Testing at step=50, batch=40, test loss = 0.3840249609631093, test acc = 0.8500000238418579, time = 0.0015826225280761719\n",
      "Testing at step=50, batch=60, test loss = 0.26710353229635225, test acc = 0.9200000166893005, time = 0.0015759468078613281\n",
      "Testing at step=50, batch=80, test loss = 0.4713715826696117, test acc = 0.8500000238418579, time = 0.0015697479248046875\n",
      "Step 50 finished in 16.338237285614014, Train loss = 0.3706414732305488, Test loss = 0.44601802531980106; Train Acc = 0.8692999982833862, Test Acc = 0.8407999986410141\n",
      "Training at step=51, batch=0, train loss = 0.225373558344426, train acc = 0.9599999785423279, time = 0.008464574813842773\n",
      "Training at step=51, batch=120, train loss = 0.45482235211583344, train acc = 0.8199999928474426, time = 0.008130550384521484\n",
      "Training at step=51, batch=240, train loss = 0.4984121717163311, train acc = 0.8399999737739563, time = 0.008159637451171875\n",
      "Training at step=51, batch=360, train loss = 0.3408213550642335, train acc = 0.8500000238418579, time = 0.008184432983398438\n",
      "Training at step=51, batch=480, train loss = 0.27172472598157205, train acc = 0.9100000262260437, time = 0.008208036422729492\n",
      "Testing at step=51, batch=0, test loss = 0.32364344307729637, test acc = 0.8899999856948853, time = 0.0016891956329345703\n",
      "Testing at step=51, batch=20, test loss = 0.4446333130906083, test acc = 0.8600000143051147, time = 0.0015654563903808594\n",
      "Testing at step=51, batch=40, test loss = 0.5974789686037714, test acc = 0.7799999713897705, time = 0.0015697479248046875\n",
      "Testing at step=51, batch=60, test loss = 0.3346361043712296, test acc = 0.8600000143051147, time = 0.0015673637390136719\n",
      "Testing at step=51, batch=80, test loss = 0.5040388651646375, test acc = 0.8199999928474426, time = 0.0015578269958496094\n",
      "Step 51 finished in 16.33314347267151, Train loss = 0.3698875488010253, Test loss = 0.45011608375792983; Train Acc = 0.8699499999483427, Test Acc = 0.8407999962568283\n",
      "Training at step=52, batch=0, train loss = 0.23528604708528988, train acc = 0.9200000166893005, time = 0.008272409439086914\n",
      "Training at step=52, batch=120, train loss = 0.5018977955684809, train acc = 0.8799999952316284, time = 0.008168935775756836\n",
      "Training at step=52, batch=240, train loss = 0.4297959842326899, train acc = 0.8799999952316284, time = 0.008167028427124023\n",
      "Training at step=52, batch=360, train loss = 0.37786270829233437, train acc = 0.8700000047683716, time = 0.008207559585571289\n",
      "Training at step=52, batch=480, train loss = 0.3671161351766061, train acc = 0.8700000047683716, time = 0.008332014083862305\n",
      "Testing at step=52, batch=0, test loss = 0.44816403228541524, test acc = 0.8399999737739563, time = 0.0017588138580322266\n",
      "Testing at step=52, batch=20, test loss = 0.5624180129911096, test acc = 0.7799999713897705, time = 0.001608133316040039\n",
      "Testing at step=52, batch=40, test loss = 0.5238741490534947, test acc = 0.8500000238418579, time = 0.0015747547149658203\n",
      "Testing at step=52, batch=60, test loss = 0.39431781889144096, test acc = 0.8399999737739563, time = 0.0015680789947509766\n",
      "Testing at step=52, batch=80, test loss = 0.4429333074878696, test acc = 0.8500000238418579, time = 0.0015707015991210938\n",
      "Step 52 finished in 16.338804006576538, Train loss = 0.3698156540749429, Test loss = 0.4461151586586039; Train Acc = 0.870066666205724, Test Acc = 0.8419999974966049\n",
      "Training at step=53, batch=0, train loss = 0.4745546803953119, train acc = 0.8500000238418579, time = 0.008280515670776367\n",
      "Training at step=53, batch=120, train loss = 0.4088129765610667, train acc = 0.8299999833106995, time = 0.008249521255493164\n",
      "Training at step=53, batch=240, train loss = 0.3924897058581946, train acc = 0.8500000238418579, time = 0.008198022842407227\n",
      "Training at step=53, batch=360, train loss = 0.3445897317038149, train acc = 0.8500000238418579, time = 0.008202791213989258\n",
      "Training at step=53, batch=480, train loss = 0.4418262408923569, train acc = 0.8700000047683716, time = 0.00824427604675293\n",
      "Testing at step=53, batch=0, test loss = 0.3648022640891499, test acc = 0.8600000143051147, time = 0.0017282962799072266\n",
      "Testing at step=53, batch=20, test loss = 0.5042338355801015, test acc = 0.8600000143051147, time = 0.0015826225280761719\n",
      "Testing at step=53, batch=40, test loss = 0.41557469185599943, test acc = 0.8600000143051147, time = 0.0015790462493896484\n",
      "Testing at step=53, batch=60, test loss = 0.4084642166865883, test acc = 0.8899999856948853, time = 0.0016095638275146484\n",
      "Testing at step=53, batch=80, test loss = 0.5747738811206164, test acc = 0.8100000023841858, time = 0.0016155242919921875\n",
      "Step 53 finished in 16.356070518493652, Train loss = 0.36856873851933103, Test loss = 0.4430431083210141; Train Acc = 0.8705333336194356, Test Acc = 0.8421999984979629\n",
      "Training at step=54, batch=0, train loss = 0.32622700635939256, train acc = 0.8999999761581421, time = 0.00823831558227539\n",
      "Training at step=54, batch=120, train loss = 0.27587158565870906, train acc = 0.8999999761581421, time = 0.008186101913452148\n",
      "Training at step=54, batch=240, train loss = 0.46630014493299526, train acc = 0.8399999737739563, time = 0.00821065902709961\n",
      "Training at step=54, batch=360, train loss = 0.38133523630692145, train acc = 0.8600000143051147, time = 0.008190393447875977\n",
      "Training at step=54, batch=480, train loss = 0.31326133480537444, train acc = 0.8799999952316284, time = 0.008179903030395508\n",
      "Testing at step=54, batch=0, test loss = 0.3775251323873457, test acc = 0.8600000143051147, time = 0.0016739368438720703\n",
      "Testing at step=54, batch=20, test loss = 0.3262587976060152, test acc = 0.8799999952316284, time = 0.0016012191772460938\n",
      "Testing at step=54, batch=40, test loss = 0.6276905592238264, test acc = 0.7599999904632568, time = 0.0015726089477539062\n",
      "Testing at step=54, batch=60, test loss = 0.3606762330510441, test acc = 0.8700000047683716, time = 0.0015807151794433594\n",
      "Testing at step=54, batch=80, test loss = 0.7014318932576019, test acc = 0.800000011920929, time = 0.0015873908996582031\n",
      "Step 54 finished in 16.340840339660645, Train loss = 0.3692027265070494, Test loss = 0.4481766839417139; Train Acc = 0.8709499987959862, Test Acc = 0.8407999962568283\n",
      "Training at step=55, batch=0, train loss = 0.4108327281571149, train acc = 0.8600000143051147, time = 0.008395195007324219\n",
      "Training at step=55, batch=120, train loss = 0.38658016027604086, train acc = 0.8600000143051147, time = 0.008159637451171875\n",
      "Training at step=55, batch=240, train loss = 0.3511534955852075, train acc = 0.8500000238418579, time = 0.008139848709106445\n",
      "Training at step=55, batch=360, train loss = 0.32920879260866115, train acc = 0.8799999952316284, time = 0.008172750473022461\n",
      "Training at step=55, batch=480, train loss = 0.34033382644580834, train acc = 0.9100000262260437, time = 0.00825047492980957\n",
      "Testing at step=55, batch=0, test loss = 0.4312659709499881, test acc = 0.8500000238418579, time = 0.0015947818756103516\n",
      "Testing at step=55, batch=20, test loss = 0.427495339919488, test acc = 0.8600000143051147, time = 0.0015926361083984375\n",
      "Testing at step=55, batch=40, test loss = 0.33225637030677746, test acc = 0.8700000047683716, time = 0.0015790462493896484\n",
      "Testing at step=55, batch=60, test loss = 0.4995452452971847, test acc = 0.8100000023841858, time = 0.001569986343383789\n",
      "Testing at step=55, batch=80, test loss = 0.5952179216708404, test acc = 0.8299999833106995, time = 0.0015654563903808594\n",
      "Step 55 finished in 16.318145275115967, Train loss = 0.36844183212547504, Test loss = 0.4471587999956025; Train Acc = 0.8704333338141441, Test Acc = 0.8427999997138977\n",
      "Training at step=56, batch=0, train loss = 0.2462777896800503, train acc = 0.8899999856948853, time = 0.00823664665222168\n",
      "Training at step=56, batch=120, train loss = 0.35371805778408044, train acc = 0.8799999952316284, time = 0.00828695297241211\n",
      "Training at step=56, batch=240, train loss = 0.3837049407841681, train acc = 0.8600000143051147, time = 0.00818324089050293\n",
      "Training at step=56, batch=360, train loss = 0.3055337417914772, train acc = 0.8600000143051147, time = 0.008233070373535156\n",
      "Training at step=56, batch=480, train loss = 0.28898562838345143, train acc = 0.9100000262260437, time = 0.008185148239135742\n",
      "Testing at step=56, batch=0, test loss = 0.5730808099419266, test acc = 0.800000011920929, time = 0.0015866756439208984\n",
      "Testing at step=56, batch=20, test loss = 0.4401632450791769, test acc = 0.8500000238418579, time = 0.0015587806701660156\n",
      "Testing at step=56, batch=40, test loss = 0.27376111567921635, test acc = 0.8999999761581421, time = 0.0015959739685058594\n",
      "Testing at step=56, batch=60, test loss = 0.29854963502940984, test acc = 0.8999999761581421, time = 0.0015521049499511719\n",
      "Testing at step=56, batch=80, test loss = 0.29108428798497693, test acc = 0.9100000262260437, time = 0.0015919208526611328\n",
      "Step 56 finished in 16.28672766685486, Train loss = 0.3675972017742772, Test loss = 0.4460429072850199; Train Acc = 0.8705666669209798, Test Acc = 0.8430000007152557\n",
      "Training at step=57, batch=0, train loss = 0.3203110704101465, train acc = 0.8799999952316284, time = 0.008363008499145508\n",
      "Training at step=57, batch=120, train loss = 0.4450452730199286, train acc = 0.8799999952316284, time = 0.008224725723266602\n",
      "Training at step=57, batch=240, train loss = 0.4573312005167154, train acc = 0.8700000047683716, time = 0.008204936981201172\n",
      "Training at step=57, batch=360, train loss = 0.4116689937333085, train acc = 0.8700000047683716, time = 0.008173465728759766\n",
      "Training at step=57, batch=480, train loss = 0.24164728212369113, train acc = 0.8999999761581421, time = 0.008242368698120117\n",
      "Testing at step=57, batch=0, test loss = 0.31650338145808116, test acc = 0.8700000047683716, time = 0.0016760826110839844\n",
      "Testing at step=57, batch=20, test loss = 0.45267436992329324, test acc = 0.8600000143051147, time = 0.0015711784362792969\n",
      "Testing at step=57, batch=40, test loss = 0.5447161551357729, test acc = 0.800000011920929, time = 0.0015726089477539062\n",
      "Testing at step=57, batch=60, test loss = 0.43278958612029605, test acc = 0.8600000143051147, time = 0.0015621185302734375\n",
      "Testing at step=57, batch=80, test loss = 0.3356809491396048, test acc = 0.8799999952316284, time = 0.0015745162963867188\n",
      "Step 57 finished in 16.338386297225952, Train loss = 0.36750784227029837, Test loss = 0.44631579973806096; Train Acc = 0.8709333344300588, Test Acc = 0.8408999991416931\n",
      "Training at step=58, batch=0, train loss = 0.3032894285846726, train acc = 0.9100000262260437, time = 0.008353710174560547\n",
      "Training at step=58, batch=120, train loss = 0.41751005153421666, train acc = 0.8700000047683716, time = 0.008251190185546875\n",
      "Training at step=58, batch=240, train loss = 0.4613405046007446, train acc = 0.8399999737739563, time = 0.008199214935302734\n",
      "Training at step=58, batch=360, train loss = 0.40009629153641435, train acc = 0.8500000238418579, time = 0.00823831558227539\n",
      "Training at step=58, batch=480, train loss = 0.4997572386287759, train acc = 0.8299999833106995, time = 0.008275508880615234\n",
      "Testing at step=58, batch=0, test loss = 0.5763981053425264, test acc = 0.800000011920929, time = 0.0017199516296386719\n",
      "Testing at step=58, batch=20, test loss = 0.4736594719138359, test acc = 0.8299999833106995, time = 0.0015590190887451172\n",
      "Testing at step=58, batch=40, test loss = 0.44850833447204375, test acc = 0.800000011920929, time = 0.0015633106231689453\n",
      "Testing at step=58, batch=60, test loss = 0.6176954154531594, test acc = 0.7900000214576721, time = 0.0015707015991210938\n",
      "Testing at step=58, batch=80, test loss = 0.4063527126159345, test acc = 0.8500000238418579, time = 0.0015578269958496094\n",
      "Step 58 finished in 16.379230976104736, Train loss = 0.36694221496587287, Test loss = 0.4458979387438196; Train Acc = 0.8711666655540466, Test Acc = 0.843299999833107\n",
      "Training at step=59, batch=0, train loss = 0.4204621103195936, train acc = 0.8199999928474426, time = 0.008459806442260742\n",
      "Training at step=59, batch=120, train loss = 0.37841694850152474, train acc = 0.8399999737739563, time = 0.00818490982055664\n",
      "Training at step=59, batch=240, train loss = 0.3901330272704353, train acc = 0.8100000023841858, time = 0.008203744888305664\n",
      "Training at step=59, batch=360, train loss = 0.3549865193194765, train acc = 0.8399999737739563, time = 0.008129119873046875\n",
      "Training at step=59, batch=480, train loss = 0.4324697560915138, train acc = 0.8399999737739563, time = 0.008248567581176758\n",
      "Testing at step=59, batch=0, test loss = 0.30322271014808044, test acc = 0.8799999952316284, time = 0.0015819072723388672\n",
      "Testing at step=59, batch=20, test loss = 0.5599161906743126, test acc = 0.7799999713897705, time = 0.0015819072723388672\n",
      "Testing at step=59, batch=40, test loss = 0.5701369014193243, test acc = 0.8100000023841858, time = 0.0015664100646972656\n",
      "Testing at step=59, batch=60, test loss = 0.35393534318485176, test acc = 0.8600000143051147, time = 0.0015769004821777344\n",
      "Testing at step=59, batch=80, test loss = 0.6465380564644059, test acc = 0.8199999928474426, time = 0.0016014575958251953\n",
      "Step 59 finished in 16.32874846458435, Train loss = 0.36620530233261034, Test loss = 0.4442043239995258; Train Acc = 0.8708999994397163, Test Acc = 0.8414000004529953\n",
      "Training at step=60, batch=0, train loss = 0.4222867077736894, train acc = 0.8100000023841858, time = 0.008297920227050781\n",
      "Training at step=60, batch=120, train loss = 0.4186769598858588, train acc = 0.8399999737739563, time = 0.00823354721069336\n",
      "Training at step=60, batch=240, train loss = 0.3782972830592049, train acc = 0.8500000238418579, time = 0.008224010467529297\n",
      "Training at step=60, batch=360, train loss = 0.5393781902786118, train acc = 0.8199999928474426, time = 0.00817251205444336\n",
      "Training at step=60, batch=480, train loss = 0.36376236015677565, train acc = 0.8700000047683716, time = 0.008208513259887695\n",
      "Testing at step=60, batch=0, test loss = 0.3654689803757931, test acc = 0.8399999737739563, time = 0.0016109943389892578\n",
      "Testing at step=60, batch=20, test loss = 0.31946960800416246, test acc = 0.8600000143051147, time = 0.0015578269958496094\n",
      "Testing at step=60, batch=40, test loss = 0.31700174898597655, test acc = 0.8999999761581421, time = 0.0016202926635742188\n",
      "Testing at step=60, batch=60, test loss = 0.48814291020390743, test acc = 0.8100000023841858, time = 0.001558065414428711\n",
      "Testing at step=60, batch=80, test loss = 0.43872844694538565, test acc = 0.7900000214576721, time = 0.0016961097717285156\n",
      "Step 60 finished in 16.282410144805908, Train loss = 0.36674700440713937, Test loss = 0.44714379587375364; Train Acc = 0.8713833344976107, Test Acc = 0.8408000010251999\n",
      "Training at step=61, batch=0, train loss = 0.3474761768382002, train acc = 0.8600000143051147, time = 0.00825643539428711\n",
      "Training at step=61, batch=120, train loss = 0.42145065181370917, train acc = 0.8799999952316284, time = 0.008213520050048828\n",
      "Training at step=61, batch=240, train loss = 0.4104217836517245, train acc = 0.8399999737739563, time = 0.008190393447875977\n",
      "Training at step=61, batch=360, train loss = 0.27969515702162967, train acc = 0.8799999952316284, time = 0.008163690567016602\n",
      "Training at step=61, batch=480, train loss = 0.4076966417451006, train acc = 0.8700000047683716, time = 0.008283376693725586\n",
      "Testing at step=61, batch=0, test loss = 0.4367915170292946, test acc = 0.8700000047683716, time = 0.0017170906066894531\n",
      "Testing at step=61, batch=20, test loss = 0.40078455921136397, test acc = 0.8399999737739563, time = 0.0015931129455566406\n",
      "Testing at step=61, batch=40, test loss = 0.5330308984510037, test acc = 0.8199999928474426, time = 0.0015649795532226562\n",
      "Testing at step=61, batch=60, test loss = 0.518679571032433, test acc = 0.7400000095367432, time = 0.0016591548919677734\n",
      "Testing at step=61, batch=80, test loss = 0.6038093894203902, test acc = 0.7799999713897705, time = 0.0016033649444580078\n",
      "Step 61 finished in 16.30970573425293, Train loss = 0.3657762624026784, Test loss = 0.44664857389386997; Train Acc = 0.8713500003019968, Test Acc = 0.8411999982595444\n",
      "Training at step=62, batch=0, train loss = 0.4483286137437096, train acc = 0.8399999737739563, time = 0.008277177810668945\n",
      "Training at step=62, batch=120, train loss = 0.44509842193643384, train acc = 0.8600000143051147, time = 0.008203268051147461\n",
      "Training at step=62, batch=240, train loss = 0.47577028125534165, train acc = 0.8299999833106995, time = 0.008184194564819336\n",
      "Training at step=62, batch=360, train loss = 0.3392356828922546, train acc = 0.9200000166893005, time = 0.008186817169189453\n",
      "Training at step=62, batch=480, train loss = 0.49293541972463173, train acc = 0.800000011920929, time = 0.008466005325317383\n",
      "Testing at step=62, batch=0, test loss = 0.47374183291844724, test acc = 0.8700000047683716, time = 0.0015823841094970703\n",
      "Testing at step=62, batch=20, test loss = 0.48935946826743637, test acc = 0.8399999737739563, time = 0.0015795230865478516\n",
      "Testing at step=62, batch=40, test loss = 0.4383419663889461, test acc = 0.8299999833106995, time = 0.0015997886657714844\n",
      "Testing at step=62, batch=60, test loss = 0.43661338160987273, test acc = 0.8399999737739563, time = 0.0015647411346435547\n",
      "Testing at step=62, batch=80, test loss = 0.3652625149311754, test acc = 0.8799999952316284, time = 0.0015606880187988281\n",
      "Step 62 finished in 16.290489196777344, Train loss = 0.36548416218459795, Test loss = 0.44433916521675615; Train Acc = 0.8718166677157084, Test Acc = 0.8428999966382981\n",
      "Training at step=63, batch=0, train loss = 0.3579056859549372, train acc = 0.8700000047683716, time = 0.008359432220458984\n",
      "Training at step=63, batch=120, train loss = 0.402838274177812, train acc = 0.8700000047683716, time = 0.008132696151733398\n",
      "Training at step=63, batch=240, train loss = 0.39768667811583663, train acc = 0.8600000143051147, time = 0.00821232795715332\n",
      "Training at step=63, batch=360, train loss = 0.3519164886940611, train acc = 0.8600000143051147, time = 0.00815439224243164\n",
      "Training at step=63, batch=480, train loss = 0.41420608360338806, train acc = 0.8500000238418579, time = 0.008195877075195312\n",
      "Testing at step=63, batch=0, test loss = 0.40438479224745083, test acc = 0.8899999856948853, time = 0.0015795230865478516\n",
      "Testing at step=63, batch=20, test loss = 0.458554276177779, test acc = 0.8399999737739563, time = 0.001623392105102539\n",
      "Testing at step=63, batch=40, test loss = 0.4387214134414802, test acc = 0.8199999928474426, time = 0.0015916824340820312\n",
      "Testing at step=63, batch=60, test loss = 0.4258890261169021, test acc = 0.8700000047683716, time = 0.0015935897827148438\n",
      "Testing at step=63, batch=80, test loss = 0.5976658551900481, test acc = 0.7900000214576721, time = 0.0016307830810546875\n",
      "Step 63 finished in 16.344587802886963, Train loss = 0.36560676325569363, Test loss = 0.4477410317046986; Train Acc = 0.8716666678587596, Test Acc = 0.8403999990224839\n",
      "Training at step=64, batch=0, train loss = 0.44823958739733216, train acc = 0.8899999856948853, time = 0.00832056999206543\n",
      "Training at step=64, batch=120, train loss = 0.29000641091054985, train acc = 0.8700000047683716, time = 0.008179903030395508\n",
      "Training at step=64, batch=240, train loss = 0.29729337021763436, train acc = 0.8799999952316284, time = 0.008168935775756836\n",
      "Training at step=64, batch=360, train loss = 0.3417850409979348, train acc = 0.8700000047683716, time = 0.008221149444580078\n",
      "Training at step=64, batch=480, train loss = 0.2965986608964124, train acc = 0.8799999952316284, time = 0.008173704147338867\n",
      "Testing at step=64, batch=0, test loss = 0.5563591725375818, test acc = 0.8399999737739563, time = 0.0017092227935791016\n",
      "Testing at step=64, batch=20, test loss = 0.4650745279073059, test acc = 0.8100000023841858, time = 0.0015873908996582031\n",
      "Testing at step=64, batch=40, test loss = 0.6950334322341979, test acc = 0.75, time = 0.0015926361083984375\n",
      "Testing at step=64, batch=60, test loss = 0.45799906082287656, test acc = 0.8600000143051147, time = 0.0016109943389892578\n",
      "Testing at step=64, batch=80, test loss = 0.3003549092360953, test acc = 0.8700000047683716, time = 0.0016324520111083984\n",
      "Step 64 finished in 16.367743730545044, Train loss = 0.364763840645611, Test loss = 0.4477704099477397; Train Acc = 0.8721833351254463, Test Acc = 0.8418999993801117\n",
      "Training at step=65, batch=0, train loss = 0.4077096634513066, train acc = 0.8299999833106995, time = 0.008486032485961914\n",
      "Training at step=65, batch=120, train loss = 0.3590909261314985, train acc = 0.8600000143051147, time = 0.008203744888305664\n",
      "Training at step=65, batch=240, train loss = 0.31063091016244204, train acc = 0.8799999952316284, time = 0.008220195770263672\n",
      "Training at step=65, batch=360, train loss = 0.3269904562846459, train acc = 0.8799999952316284, time = 0.008219718933105469\n",
      "Training at step=65, batch=480, train loss = 0.4612344832144037, train acc = 0.8199999928474426, time = 0.008199214935302734\n",
      "Testing at step=65, batch=0, test loss = 0.682650076299902, test acc = 0.7599999904632568, time = 0.0017228126525878906\n",
      "Testing at step=65, batch=20, test loss = 0.35311271838990854, test acc = 0.8899999856948853, time = 0.001577138900756836\n",
      "Testing at step=65, batch=40, test loss = 0.40349650875878645, test acc = 0.8600000143051147, time = 0.0015683174133300781\n",
      "Testing at step=65, batch=60, test loss = 0.33166399473571917, test acc = 0.8899999856948853, time = 0.0015666484832763672\n",
      "Testing at step=65, batch=80, test loss = 0.49014007383053715, test acc = 0.8399999737739563, time = 0.001569509506225586\n",
      "Step 65 finished in 16.375093698501587, Train loss = 0.36370048572451624, Test loss = 0.4466981701844875; Train Acc = 0.8722333329916, Test Acc = 0.841599999666214\n",
      "Training at step=66, batch=0, train loss = 0.3175550818585194, train acc = 0.8299999833106995, time = 0.00830984115600586\n",
      "Training at step=66, batch=120, train loss = 0.4594633334132818, train acc = 0.8299999833106995, time = 0.00819849967956543\n",
      "Training at step=66, batch=240, train loss = 0.35396327268578737, train acc = 0.8399999737739563, time = 0.008156299591064453\n",
      "Training at step=66, batch=360, train loss = 0.45659846715244146, train acc = 0.8600000143051147, time = 0.00825357437133789\n",
      "Training at step=66, batch=480, train loss = 0.31320544789488186, train acc = 0.8799999952316284, time = 0.008224725723266602\n",
      "Testing at step=66, batch=0, test loss = 0.395576354167601, test acc = 0.8500000238418579, time = 0.0015854835510253906\n",
      "Testing at step=66, batch=20, test loss = 0.46954167473450353, test acc = 0.8199999928474426, time = 0.0015680789947509766\n",
      "Testing at step=66, batch=40, test loss = 0.4092724660771017, test acc = 0.8700000047683716, time = 0.0015895366668701172\n",
      "Testing at step=66, batch=60, test loss = 0.46720195864377245, test acc = 0.8100000023841858, time = 0.0015883445739746094\n",
      "Testing at step=66, batch=80, test loss = 0.39091192917884177, test acc = 0.8500000238418579, time = 0.0015785694122314453\n",
      "Step 66 finished in 16.35933518409729, Train loss = 0.3646975131269089, Test loss = 0.4477221237890805; Train Acc = 0.8720666664838791, Test Acc = 0.8416000014543533\n",
      "Training at step=67, batch=0, train loss = 0.4185800739570567, train acc = 0.8600000143051147, time = 0.008430957794189453\n",
      "Training at step=67, batch=120, train loss = 0.34795494524756554, train acc = 0.8799999952316284, time = 0.008140325546264648\n",
      "Training at step=67, batch=240, train loss = 0.3092063512570018, train acc = 0.9100000262260437, time = 0.00822138786315918\n",
      "Training at step=67, batch=360, train loss = 0.4240696393687763, train acc = 0.8100000023841858, time = 0.008215904235839844\n",
      "Training at step=67, batch=480, train loss = 0.4631458989355886, train acc = 0.8999999761581421, time = 0.008275032043457031\n",
      "Testing at step=67, batch=0, test loss = 0.5251873342188267, test acc = 0.800000011920929, time = 0.0015804767608642578\n",
      "Testing at step=67, batch=20, test loss = 0.3655676206863746, test acc = 0.8600000143051147, time = 0.0016074180603027344\n",
      "Testing at step=67, batch=40, test loss = 0.3966049906739636, test acc = 0.8500000238418579, time = 0.0015652179718017578\n",
      "Testing at step=67, batch=60, test loss = 0.4651780223324913, test acc = 0.8500000238418579, time = 0.0015912055969238281\n",
      "Testing at step=67, batch=80, test loss = 0.3683522792998699, test acc = 0.8399999737739563, time = 0.0015861988067626953\n",
      "Step 67 finished in 16.288353204727173, Train loss = 0.3636125404646156, Test loss = 0.44630593698580007; Train Acc = 0.8727333337068558, Test Acc = 0.8417000013589859\n",
      "Training at step=68, batch=0, train loss = 0.20885393250029974, train acc = 0.9200000166893005, time = 0.008271932601928711\n",
      "Training at step=68, batch=120, train loss = 0.2803428368273205, train acc = 0.9100000262260437, time = 0.008168220520019531\n",
      "Training at step=68, batch=240, train loss = 0.2942226178244544, train acc = 0.9200000166893005, time = 0.00821065902709961\n",
      "Training at step=68, batch=360, train loss = 0.4537325780474131, train acc = 0.8600000143051147, time = 0.008117198944091797\n",
      "Training at step=68, batch=480, train loss = 0.3771222438348415, train acc = 0.8500000238418579, time = 0.008188724517822266\n",
      "Testing at step=68, batch=0, test loss = 0.3835823678059203, test acc = 0.8799999952316284, time = 0.0015990734100341797\n",
      "Testing at step=68, batch=20, test loss = 0.4086430841958568, test acc = 0.8500000238418579, time = 0.0015652179718017578\n",
      "Testing at step=68, batch=40, test loss = 0.477512064025867, test acc = 0.800000011920929, time = 0.0016384124755859375\n",
      "Testing at step=68, batch=60, test loss = 0.3357587687912937, test acc = 0.8799999952316284, time = 0.0015854835510253906\n",
      "Testing at step=68, batch=80, test loss = 0.39721548926571315, test acc = 0.8799999952316284, time = 0.0016045570373535156\n",
      "Step 68 finished in 16.362316846847534, Train loss = 0.36312624155507733, Test loss = 0.45058819499615543; Train Acc = 0.872466665605704, Test Acc = 0.8429000002145767\n",
      "Training at step=69, batch=0, train loss = 0.36645174301974415, train acc = 0.8899999856948853, time = 0.00829172134399414\n",
      "Training at step=69, batch=120, train loss = 0.2370695423491297, train acc = 0.9200000166893005, time = 0.008220434188842773\n",
      "Training at step=69, batch=240, train loss = 0.49438872980500065, train acc = 0.8100000023841858, time = 0.008164405822753906\n",
      "Training at step=69, batch=360, train loss = 0.21671211149149855, train acc = 0.9100000262260437, time = 0.008150815963745117\n",
      "Training at step=69, batch=480, train loss = 0.3464599495522178, train acc = 0.8700000047683716, time = 0.008152246475219727\n",
      "Testing at step=69, batch=0, test loss = 0.347212607943673, test acc = 0.8600000143051147, time = 0.0015878677368164062\n",
      "Testing at step=69, batch=20, test loss = 0.3897818148155847, test acc = 0.8600000143051147, time = 0.001590728759765625\n",
      "Testing at step=69, batch=40, test loss = 0.47702519236731145, test acc = 0.8500000238418579, time = 0.001573324203491211\n",
      "Testing at step=69, batch=60, test loss = 0.5613738117471164, test acc = 0.8100000023841858, time = 0.0015869140625\n",
      "Testing at step=69, batch=80, test loss = 0.5434408049125954, test acc = 0.8199999928474426, time = 0.0016002655029296875\n",
      "Step 69 finished in 16.309574127197266, Train loss = 0.36293659115780896, Test loss = 0.4567997851229367; Train Acc = 0.8725666667024294, Test Acc = 0.8373000001907349\n",
      "Training at step=70, batch=0, train loss = 0.2851507086140171, train acc = 0.8899999856948853, time = 0.00822305679321289\n",
      "Training at step=70, batch=120, train loss = 0.4100300694769755, train acc = 0.8199999928474426, time = 0.008262157440185547\n",
      "Training at step=70, batch=240, train loss = 0.5205215038524481, train acc = 0.8899999856948853, time = 0.008265972137451172\n",
      "Training at step=70, batch=360, train loss = 0.31995430215114573, train acc = 0.8799999952316284, time = 0.008173942565917969\n",
      "Training at step=70, batch=480, train loss = 0.4330816417251773, train acc = 0.8299999833106995, time = 0.008202314376831055\n",
      "Testing at step=70, batch=0, test loss = 0.3697861034781269, test acc = 0.8500000238418579, time = 0.0015892982482910156\n",
      "Testing at step=70, batch=20, test loss = 0.5128645533743748, test acc = 0.8500000238418579, time = 0.0015606880187988281\n",
      "Testing at step=70, batch=40, test loss = 0.5673647999210056, test acc = 0.8399999737739563, time = 0.001575469970703125\n",
      "Testing at step=70, batch=60, test loss = 0.4567996205667776, test acc = 0.8399999737739563, time = 0.0015702247619628906\n",
      "Testing at step=70, batch=80, test loss = 0.5552087371207687, test acc = 0.7900000214576721, time = 0.0015676021575927734\n",
      "Step 70 finished in 16.30706810951233, Train loss = 0.3632048186695782, Test loss = 0.4490810826567905; Train Acc = 0.873083332280318, Test Acc = 0.841000000834465\n",
      "Training at step=71, batch=0, train loss = 0.38234339288957275, train acc = 0.8799999952316284, time = 0.008427143096923828\n",
      "Training at step=71, batch=120, train loss = 0.3822569317334042, train acc = 0.8500000238418579, time = 0.008234024047851562\n",
      "Training at step=71, batch=240, train loss = 0.4099347559647238, train acc = 0.8999999761581421, time = 0.008201837539672852\n",
      "Training at step=71, batch=360, train loss = 0.2453144345014103, train acc = 0.9100000262260437, time = 0.008160829544067383\n",
      "Training at step=71, batch=480, train loss = 0.3749233674737831, train acc = 0.8799999952316284, time = 0.008152008056640625\n",
      "Testing at step=71, batch=0, test loss = 0.2451167450388516, test acc = 0.9100000262260437, time = 0.001596212387084961\n",
      "Testing at step=71, batch=20, test loss = 0.3817084219226961, test acc = 0.8899999856948853, time = 0.0016503334045410156\n",
      "Testing at step=71, batch=40, test loss = 0.5162279625166994, test acc = 0.8799999952316284, time = 0.0015633106231689453\n",
      "Testing at step=71, batch=60, test loss = 0.4424954228422223, test acc = 0.8299999833106995, time = 0.001611471176147461\n",
      "Testing at step=71, batch=80, test loss = 0.3752813626441953, test acc = 0.8500000238418579, time = 0.0016176700592041016\n",
      "Step 71 finished in 16.30306601524353, Train loss = 0.3627129646235563, Test loss = 0.44794136760320635; Train Acc = 0.8713000005483628, Test Acc = 0.8418999987840653\n",
      "Training at step=72, batch=0, train loss = 0.39763701786965183, train acc = 0.8799999952316284, time = 0.008400440216064453\n",
      "Training at step=72, batch=120, train loss = 0.4543334705154987, train acc = 0.8500000238418579, time = 0.008196830749511719\n",
      "Training at step=72, batch=240, train loss = 0.3596932879809046, train acc = 0.8600000143051147, time = 0.00820159912109375\n",
      "Training at step=72, batch=360, train loss = 0.27504955233173506, train acc = 0.9100000262260437, time = 0.008208990097045898\n",
      "Training at step=72, batch=480, train loss = 0.4620612254747165, train acc = 0.8299999833106995, time = 0.008289098739624023\n",
      "Testing at step=72, batch=0, test loss = 0.4297700884382759, test acc = 0.8199999928474426, time = 0.0015876293182373047\n",
      "Testing at step=72, batch=20, test loss = 0.5716043458658949, test acc = 0.8100000023841858, time = 0.0015668869018554688\n",
      "Testing at step=72, batch=40, test loss = 0.42178062481479084, test acc = 0.8199999928474426, time = 0.0015554428100585938\n",
      "Testing at step=72, batch=60, test loss = 0.42029240326776734, test acc = 0.8500000238418579, time = 0.001554727554321289\n",
      "Testing at step=72, batch=80, test loss = 0.41658790555585745, test acc = 0.8500000238418579, time = 0.0015599727630615234\n",
      "Step 72 finished in 16.29769277572632, Train loss = 0.3623701610116893, Test loss = 0.44847369509307305; Train Acc = 0.8717333335677783, Test Acc = 0.8405999976396561\n",
      "Training at step=73, batch=0, train loss = 0.4277099643197808, train acc = 0.8799999952316284, time = 0.008240938186645508\n",
      "Training at step=73, batch=120, train loss = 0.3445245340540319, train acc = 0.8500000238418579, time = 0.008254289627075195\n",
      "Training at step=73, batch=240, train loss = 0.39564402159409967, train acc = 0.8299999833106995, time = 0.008149862289428711\n",
      "Training at step=73, batch=360, train loss = 0.5332326032686394, train acc = 0.7799999713897705, time = 0.008212566375732422\n",
      "Training at step=73, batch=480, train loss = 0.2908585205189405, train acc = 0.8700000047683716, time = 0.008292675018310547\n",
      "Testing at step=73, batch=0, test loss = 0.5102985619183312, test acc = 0.800000011920929, time = 0.0017359256744384766\n",
      "Testing at step=73, batch=20, test loss = 0.38961639537021286, test acc = 0.8399999737739563, time = 0.0016074180603027344\n",
      "Testing at step=73, batch=40, test loss = 0.3415613580147965, test acc = 0.8600000143051147, time = 0.0016071796417236328\n",
      "Testing at step=73, batch=60, test loss = 0.4251972430284965, test acc = 0.8100000023841858, time = 0.0015861988067626953\n",
      "Testing at step=73, batch=80, test loss = 0.5740868742424116, test acc = 0.8100000023841858, time = 0.0015790462493896484\n",
      "Step 73 finished in 16.30047607421875, Train loss = 0.3622125433461815, Test loss = 0.46139682425909995; Train Acc = 0.8724333330988884, Test Acc = 0.8351999992132186\n",
      "Training at step=74, batch=0, train loss = 0.4023461341383953, train acc = 0.8700000047683716, time = 0.008257389068603516\n",
      "Training at step=74, batch=120, train loss = 0.29050070835175734, train acc = 0.8999999761581421, time = 0.008144617080688477\n",
      "Training at step=74, batch=240, train loss = 0.47733032942337417, train acc = 0.8500000238418579, time = 0.008347034454345703\n",
      "Training at step=74, batch=360, train loss = 0.44319215359135566, train acc = 0.8700000047683716, time = 0.008232831954956055\n",
      "Training at step=74, batch=480, train loss = 0.40190818067174605, train acc = 0.8600000143051147, time = 0.008176088333129883\n",
      "Testing at step=74, batch=0, test loss = 0.3991715996464963, test acc = 0.8500000238418579, time = 0.0015828609466552734\n",
      "Testing at step=74, batch=20, test loss = 0.5461816024091555, test acc = 0.7699999809265137, time = 0.0016036033630371094\n",
      "Testing at step=74, batch=40, test loss = 0.32650121321904224, test acc = 0.8700000047683716, time = 0.0016124248504638672\n",
      "Testing at step=74, batch=60, test loss = 0.4555455454597002, test acc = 0.8399999737739563, time = 0.001569509506225586\n",
      "Testing at step=74, batch=80, test loss = 0.3242978721413078, test acc = 0.8999999761581421, time = 0.0015554428100585938\n",
      "Step 74 finished in 16.32986307144165, Train loss = 0.36201472663092715, Test loss = 0.44893532201345415; Train Acc = 0.8720666665832202, Test Acc = 0.8397999978065491\n",
      "Training at step=75, batch=0, train loss = 0.417177411838918, train acc = 0.8299999833106995, time = 0.0083465576171875\n",
      "Training at step=75, batch=120, train loss = 0.40942385832141803, train acc = 0.8500000238418579, time = 0.008131980895996094\n",
      "Training at step=75, batch=240, train loss = 0.32599758667281015, train acc = 0.8500000238418579, time = 0.008232831954956055\n",
      "Training at step=75, batch=360, train loss = 0.3005678130562677, train acc = 0.9100000262260437, time = 0.008149147033691406\n",
      "Training at step=75, batch=480, train loss = 0.31941553351634455, train acc = 0.8799999952316284, time = 0.008162736892700195\n",
      "Testing at step=75, batch=0, test loss = 0.5696095562313659, test acc = 0.7699999809265137, time = 0.00157928466796875\n",
      "Testing at step=75, batch=20, test loss = 0.3273401893572336, test acc = 0.8899999856948853, time = 0.0015592575073242188\n",
      "Testing at step=75, batch=40, test loss = 0.4085250429609308, test acc = 0.8799999952316284, time = 0.0015707015991210938\n",
      "Testing at step=75, batch=60, test loss = 0.41995598090215586, test acc = 0.8700000047683716, time = 0.0016324520111083984\n",
      "Testing at step=75, batch=80, test loss = 0.414728457567412, test acc = 0.8500000238418579, time = 0.0015604496002197266\n",
      "Step 75 finished in 16.32447099685669, Train loss = 0.36123952000950055, Test loss = 0.45081401828685375; Train Acc = 0.873466666440169, Test Acc = 0.841299996972084\n",
      "Training at step=76, batch=0, train loss = 0.2914887608153283, train acc = 0.8999999761581421, time = 0.008292436599731445\n",
      "Training at step=76, batch=120, train loss = 0.37489471236732363, train acc = 0.8399999737739563, time = 0.008200645446777344\n",
      "Training at step=76, batch=240, train loss = 0.312063846810055, train acc = 0.8799999952316284, time = 0.008157491683959961\n",
      "Training at step=76, batch=360, train loss = 0.48928969145963136, train acc = 0.8199999928474426, time = 0.008171558380126953\n",
      "Training at step=76, batch=480, train loss = 0.36614213903391823, train acc = 0.9200000166893005, time = 0.008280038833618164\n",
      "Testing at step=76, batch=0, test loss = 0.4235269210879652, test acc = 0.8399999737739563, time = 0.0015912055969238281\n",
      "Testing at step=76, batch=20, test loss = 0.6969231672692903, test acc = 0.800000011920929, time = 0.0016248226165771484\n",
      "Testing at step=76, batch=40, test loss = 0.37933856688351114, test acc = 0.8799999952316284, time = 0.0015759468078613281\n",
      "Testing at step=76, batch=60, test loss = 0.42968265332250494, test acc = 0.8199999928474426, time = 0.0015583038330078125\n",
      "Testing at step=76, batch=80, test loss = 0.2335554904119693, test acc = 0.8899999856948853, time = 0.0015568733215332031\n",
      "Step 76 finished in 16.336588144302368, Train loss = 0.3614797657886581, Test loss = 0.44950052381254607; Train Acc = 0.8734666658441226, Test Acc = 0.8414999973773957\n",
      "Training at step=77, batch=0, train loss = 0.4273859626546549, train acc = 0.8399999737739563, time = 0.008258342742919922\n",
      "Training at step=77, batch=120, train loss = 0.4618852230625922, train acc = 0.800000011920929, time = 0.00818634033203125\n",
      "Training at step=77, batch=240, train loss = 0.3643360322550304, train acc = 0.8700000047683716, time = 0.008214235305786133\n",
      "Training at step=77, batch=360, train loss = 0.44545593200751304, train acc = 0.8600000143051147, time = 0.008184194564819336\n",
      "Training at step=77, batch=480, train loss = 0.41154078780534353, train acc = 0.9300000071525574, time = 0.008229494094848633\n",
      "Testing at step=77, batch=0, test loss = 0.31639796665457076, test acc = 0.8700000047683716, time = 0.0015816688537597656\n",
      "Testing at step=77, batch=20, test loss = 0.36195102246682387, test acc = 0.8600000143051147, time = 0.0016560554504394531\n",
      "Testing at step=77, batch=40, test loss = 0.43591193445297677, test acc = 0.8500000238418579, time = 0.0015635490417480469\n",
      "Testing at step=77, batch=60, test loss = 0.29845143185818995, test acc = 0.8899999856948853, time = 0.0015642642974853516\n",
      "Testing at step=77, batch=80, test loss = 0.48268917270233813, test acc = 0.8500000238418579, time = 0.00156402587890625\n",
      "Step 77 finished in 16.25864028930664, Train loss = 0.36131739552717657, Test loss = 0.44961136867285156; Train Acc = 0.8724166669448217, Test Acc = 0.8416999965906143\n",
      "Training at step=78, batch=0, train loss = 0.30929231626015125, train acc = 0.9200000166893005, time = 0.008344173431396484\n",
      "Training at step=78, batch=120, train loss = 0.23416837919528202, train acc = 0.9300000071525574, time = 0.008173465728759766\n",
      "Training at step=78, batch=240, train loss = 0.4379291336185806, train acc = 0.8299999833106995, time = 0.008173465728759766\n",
      "Training at step=78, batch=360, train loss = 0.44845265907426024, train acc = 0.8500000238418579, time = 0.008189678192138672\n",
      "Training at step=78, batch=480, train loss = 0.39486373024534016, train acc = 0.8799999952316284, time = 0.008236885070800781\n",
      "Testing at step=78, batch=0, test loss = 0.5151296959035964, test acc = 0.800000011920929, time = 0.001672506332397461\n",
      "Testing at step=78, batch=20, test loss = 0.3582734083850781, test acc = 0.8999999761581421, time = 0.0015969276428222656\n",
      "Testing at step=78, batch=40, test loss = 0.2920873249373783, test acc = 0.8500000238418579, time = 0.0016202926635742188\n",
      "Testing at step=78, batch=60, test loss = 0.5024249064586972, test acc = 0.8500000238418579, time = 0.0015912055969238281\n",
      "Testing at step=78, batch=80, test loss = 0.378683877167074, test acc = 0.8899999856948853, time = 0.0015807151794433594\n",
      "Step 78 finished in 16.300514936447144, Train loss = 0.3617254718440013, Test loss = 0.452324695144462; Train Acc = 0.8733166660865148, Test Acc = 0.8402999967336655\n",
      "Training at step=79, batch=0, train loss = 0.23825220975215855, train acc = 0.9200000166893005, time = 0.008448600769042969\n",
      "Training at step=79, batch=120, train loss = 0.3426346543754808, train acc = 0.8600000143051147, time = 0.008144617080688477\n",
      "Training at step=79, batch=240, train loss = 0.25564116614166377, train acc = 0.8999999761581421, time = 0.008130073547363281\n",
      "Training at step=79, batch=360, train loss = 0.24445452604078807, train acc = 0.8899999856948853, time = 0.008186817169189453\n",
      "Training at step=79, batch=480, train loss = 0.3036422251943398, train acc = 0.8899999856948853, time = 0.008225202560424805\n",
      "Testing at step=79, batch=0, test loss = 0.34251193418577924, test acc = 0.8999999761581421, time = 0.0015850067138671875\n",
      "Testing at step=79, batch=20, test loss = 0.26538714014968623, test acc = 0.8899999856948853, time = 0.0015933513641357422\n",
      "Testing at step=79, batch=40, test loss = 0.5475438249639446, test acc = 0.7799999713897705, time = 0.001560211181640625\n",
      "Testing at step=79, batch=60, test loss = 0.5245235128622818, test acc = 0.8500000238418579, time = 0.0015659332275390625\n",
      "Testing at step=79, batch=80, test loss = 0.6164730671084504, test acc = 0.7599999904632568, time = 0.0015683174133300781\n",
      "Step 79 finished in 16.3092942237854, Train loss = 0.36019280823374544, Test loss = 0.448960812640516; Train Acc = 0.873149999777476, Test Acc = 0.8406999987363816\n",
      "Training at step=80, batch=0, train loss = 0.393911072049804, train acc = 0.8500000238418579, time = 0.008322477340698242\n",
      "Training at step=80, batch=120, train loss = 0.4309389512556718, train acc = 0.8199999928474426, time = 0.008222341537475586\n",
      "Training at step=80, batch=240, train loss = 0.23868568584528801, train acc = 0.8999999761581421, time = 0.008168935775756836\n",
      "Training at step=80, batch=360, train loss = 0.35167056869581714, train acc = 0.8700000047683716, time = 0.008191823959350586\n",
      "Training at step=80, batch=480, train loss = 0.2367530067422047, train acc = 0.8999999761581421, time = 0.00817728042602539\n",
      "Testing at step=80, batch=0, test loss = 0.47816886934366165, test acc = 0.8500000238418579, time = 0.0015957355499267578\n",
      "Testing at step=80, batch=20, test loss = 0.4965957321925889, test acc = 0.8399999737739563, time = 0.0015964508056640625\n",
      "Testing at step=80, batch=40, test loss = 0.5178723567737149, test acc = 0.8600000143051147, time = 0.0015559196472167969\n",
      "Testing at step=80, batch=60, test loss = 0.3938070366633707, test acc = 0.8700000047683716, time = 0.0015726089477539062\n",
      "Testing at step=80, batch=80, test loss = 0.4089443391189099, test acc = 0.8799999952316284, time = 0.0015933513641357422\n",
      "Step 80 finished in 16.34911870956421, Train loss = 0.3599218268905857, Test loss = 0.4503938489706764; Train Acc = 0.87323333243529, Test Acc = 0.8398999989032745\n",
      "Training at step=81, batch=0, train loss = 0.2822574350542935, train acc = 0.8899999856948853, time = 0.008285999298095703\n",
      "Training at step=81, batch=120, train loss = 0.3396176414040373, train acc = 0.8999999761581421, time = 0.008186578750610352\n",
      "Training at step=81, batch=240, train loss = 0.26663638814559115, train acc = 0.9300000071525574, time = 0.008168458938598633\n",
      "Training at step=81, batch=360, train loss = 0.33057731503827453, train acc = 0.8600000143051147, time = 0.008174419403076172\n",
      "Training at step=81, batch=480, train loss = 0.3122772141567261, train acc = 0.8899999856948853, time = 0.008245468139648438\n",
      "Testing at step=81, batch=0, test loss = 0.5584473633528105, test acc = 0.8100000023841858, time = 0.0016596317291259766\n",
      "Testing at step=81, batch=20, test loss = 0.3457954609470499, test acc = 0.8700000047683716, time = 0.001592874526977539\n",
      "Testing at step=81, batch=40, test loss = 0.43305481536646406, test acc = 0.8199999928474426, time = 0.0015649795532226562\n",
      "Testing at step=81, batch=60, test loss = 0.4171230684054253, test acc = 0.8500000238418579, time = 0.0015637874603271484\n",
      "Testing at step=81, batch=80, test loss = 0.3226712543500574, test acc = 0.8899999856948853, time = 0.0015687942504882812\n",
      "Step 81 finished in 16.32597279548645, Train loss = 0.3598761029452354, Test loss = 0.45609418764797455; Train Acc = 0.872833333214124, Test Acc = 0.838699996471405\n",
      "Training at step=82, batch=0, train loss = 0.40551048790252936, train acc = 0.8600000143051147, time = 0.008236408233642578\n",
      "Training at step=82, batch=120, train loss = 0.49046867092194985, train acc = 0.8500000238418579, time = 0.00825810432434082\n",
      "Training at step=82, batch=240, train loss = 0.3363091435488219, train acc = 0.8600000143051147, time = 0.00829458236694336\n",
      "Training at step=82, batch=360, train loss = 0.26483291963556005, train acc = 0.8999999761581421, time = 0.008252143859863281\n",
      "Training at step=82, batch=480, train loss = 0.2257959888655077, train acc = 0.8899999856948853, time = 0.008225679397583008\n",
      "Testing at step=82, batch=0, test loss = 0.4085601990844556, test acc = 0.8500000238418579, time = 0.0017430782318115234\n",
      "Testing at step=82, batch=20, test loss = 0.5106351054962883, test acc = 0.8100000023841858, time = 0.001611471176147461\n",
      "Testing at step=82, batch=40, test loss = 0.4441583968229601, test acc = 0.8199999928474426, time = 0.001577138900756836\n",
      "Testing at step=82, batch=60, test loss = 0.4279910717245741, test acc = 0.8199999928474426, time = 0.0015687942504882812\n",
      "Testing at step=82, batch=80, test loss = 0.5214204395261285, test acc = 0.8299999833106995, time = 0.0015616416931152344\n",
      "Step 82 finished in 16.36276364326477, Train loss = 0.36044983534097413, Test loss = 0.4513148754722978; Train Acc = 0.8735666670401891, Test Acc = 0.8410000002384186\n",
      "Training at step=83, batch=0, train loss = 0.4261976431905942, train acc = 0.8899999856948853, time = 0.008408546447753906\n",
      "Training at step=83, batch=120, train loss = 0.30895996942640525, train acc = 0.9100000262260437, time = 0.008216142654418945\n",
      "Training at step=83, batch=240, train loss = 0.3336744272436025, train acc = 0.8899999856948853, time = 0.008194208145141602\n",
      "Training at step=83, batch=360, train loss = 0.3783960564486078, train acc = 0.8799999952316284, time = 0.008186578750610352\n",
      "Training at step=83, batch=480, train loss = 0.45366261947187064, train acc = 0.8399999737739563, time = 0.008163213729858398\n",
      "Testing at step=83, batch=0, test loss = 0.5391939798809572, test acc = 0.8100000023841858, time = 0.0016033649444580078\n",
      "Testing at step=83, batch=20, test loss = 0.4137214435700923, test acc = 0.8600000143051147, time = 0.0015559196472167969\n",
      "Testing at step=83, batch=40, test loss = 0.423345827479353, test acc = 0.8399999737739563, time = 0.0015716552734375\n",
      "Testing at step=83, batch=60, test loss = 0.7589386128293449, test acc = 0.8199999928474426, time = 0.0015637874603271484\n",
      "Testing at step=83, batch=80, test loss = 0.40622024496687487, test acc = 0.8299999833106995, time = 0.0015680789947509766\n",
      "Step 83 finished in 16.328482627868652, Train loss = 0.35947222321623085, Test loss = 0.44904243154676565; Train Acc = 0.8736833327015241, Test Acc = 0.8422999989986419\n",
      "Training at step=84, batch=0, train loss = 0.3654795129733708, train acc = 0.8700000047683716, time = 0.008240222930908203\n",
      "Training at step=84, batch=120, train loss = 0.3894264415682322, train acc = 0.8999999761581421, time = 0.008176326751708984\n",
      "Training at step=84, batch=240, train loss = 0.450664575710733, train acc = 0.8700000047683716, time = 0.00817561149597168\n",
      "Training at step=84, batch=360, train loss = 0.37131771057304436, train acc = 0.8799999952316284, time = 0.008158445358276367\n",
      "Training at step=84, batch=480, train loss = 0.3504801748671075, train acc = 0.8500000238418579, time = 0.00820612907409668\n",
      "Testing at step=84, batch=0, test loss = 0.47184760176799556, test acc = 0.8299999833106995, time = 0.0017440319061279297\n",
      "Testing at step=84, batch=20, test loss = 0.4398751379321743, test acc = 0.8500000238418579, time = 0.0015916824340820312\n",
      "Testing at step=84, batch=40, test loss = 0.4347154877507653, test acc = 0.8899999856948853, time = 0.0016109943389892578\n",
      "Testing at step=84, batch=60, test loss = 0.35810634473058583, test acc = 0.8700000047683716, time = 0.0015828609466552734\n",
      "Testing at step=84, batch=80, test loss = 0.39388030263523793, test acc = 0.8600000143051147, time = 0.0015811920166015625\n",
      "Step 84 finished in 16.329955577850342, Train loss = 0.3595587197505558, Test loss = 0.45360892587293367; Train Acc = 0.874049998819828, Test Acc = 0.841800000667572\n",
      "Training at step=85, batch=0, train loss = 0.31063447213216105, train acc = 0.8999999761581421, time = 0.008275747299194336\n",
      "Training at step=85, batch=120, train loss = 0.34480091701484034, train acc = 0.8600000143051147, time = 0.00815725326538086\n",
      "Training at step=85, batch=240, train loss = 0.2009859521638564, train acc = 0.9399999976158142, time = 0.008196830749511719\n",
      "Training at step=85, batch=360, train loss = 0.3013723503242434, train acc = 0.8899999856948853, time = 0.00820016860961914\n",
      "Training at step=85, batch=480, train loss = 0.38425438603799633, train acc = 0.8600000143051147, time = 0.008243560791015625\n",
      "Testing at step=85, batch=0, test loss = 0.5723758982118751, test acc = 0.8100000023841858, time = 0.0017192363739013672\n",
      "Testing at step=85, batch=20, test loss = 0.47761746250925957, test acc = 0.8199999928474426, time = 0.0015935897827148438\n",
      "Testing at step=85, batch=40, test loss = 0.5037707695743611, test acc = 0.8500000238418579, time = 0.0015838146209716797\n",
      "Testing at step=85, batch=60, test loss = 0.5161465382768984, test acc = 0.7900000214576721, time = 0.0015785694122314453\n",
      "Testing at step=85, batch=80, test loss = 0.29709417468429433, test acc = 0.8899999856948853, time = 0.0015659332275390625\n",
      "Step 85 finished in 16.28391742706299, Train loss = 0.3592685418591143, Test loss = 0.4530980207620145; Train Acc = 0.8741999992728233, Test Acc = 0.8416999989748001\n",
      "Training at step=86, batch=0, train loss = 0.275266399516177, train acc = 0.9300000071525574, time = 0.008279800415039062\n",
      "Training at step=86, batch=120, train loss = 0.35307047428620686, train acc = 0.9200000166893005, time = 0.008250713348388672\n",
      "Training at step=86, batch=240, train loss = 0.3981848536356544, train acc = 0.8500000238418579, time = 0.008210420608520508\n",
      "Training at step=86, batch=360, train loss = 0.3425386142952816, train acc = 0.8600000143051147, time = 0.008175373077392578\n",
      "Training at step=86, batch=480, train loss = 0.4057373714003951, train acc = 0.8600000143051147, time = 0.00820612907409668\n",
      "Testing at step=86, batch=0, test loss = 0.4865717455090127, test acc = 0.8500000238418579, time = 0.0016760826110839844\n",
      "Testing at step=86, batch=20, test loss = 0.42161710508999356, test acc = 0.8299999833106995, time = 0.0016474723815917969\n",
      "Testing at step=86, batch=40, test loss = 0.4502891798873378, test acc = 0.8500000238418579, time = 0.001562356948852539\n",
      "Testing at step=86, batch=60, test loss = 0.5283117012713535, test acc = 0.8299999833106995, time = 0.0016040802001953125\n",
      "Testing at step=86, batch=80, test loss = 0.2893356241606389, test acc = 0.8799999952316284, time = 0.0015931129455566406\n",
      "Step 86 finished in 16.310580492019653, Train loss = 0.35877775542117896, Test loss = 0.4588927263376976; Train Acc = 0.8735666665434837, Test Acc = 0.837700001001358\n",
      "Training at step=87, batch=0, train loss = 0.33451498966148685, train acc = 0.8600000143051147, time = 0.008339881896972656\n",
      "Training at step=87, batch=120, train loss = 0.42533404607358866, train acc = 0.8299999833106995, time = 0.008166074752807617\n",
      "Training at step=87, batch=240, train loss = 0.2694631226052588, train acc = 0.9200000166893005, time = 0.008249521255493164\n",
      "Training at step=87, batch=360, train loss = 0.284215183732501, train acc = 0.9100000262260437, time = 0.00818324089050293\n",
      "Training at step=87, batch=480, train loss = 0.30010016653292176, train acc = 0.8899999856948853, time = 0.008235454559326172\n",
      "Testing at step=87, batch=0, test loss = 0.3627557348484796, test acc = 0.8700000047683716, time = 0.0016012191772460938\n",
      "Testing at step=87, batch=20, test loss = 0.444139810708845, test acc = 0.8299999833106995, time = 0.0015614032745361328\n",
      "Testing at step=87, batch=40, test loss = 0.48371042227506517, test acc = 0.8600000143051147, time = 0.0015609264373779297\n",
      "Testing at step=87, batch=60, test loss = 0.37719931205431356, test acc = 0.9100000262260437, time = 0.0015583038330078125\n",
      "Testing at step=87, batch=80, test loss = 0.3771968900417891, test acc = 0.9100000262260437, time = 0.0015599727630615234\n",
      "Step 87 finished in 16.297797441482544, Train loss = 0.35911754573968846, Test loss = 0.4552346369213209; Train Acc = 0.8738833332061767, Test Acc = 0.840099995136261\n",
      "Training at step=88, batch=0, train loss = 0.31955285520116167, train acc = 0.8600000143051147, time = 0.008341550827026367\n",
      "Training at step=88, batch=120, train loss = 0.3192644608694533, train acc = 0.8799999952316284, time = 0.008159875869750977\n",
      "Training at step=88, batch=240, train loss = 0.4403642737085346, train acc = 0.8100000023841858, time = 0.008168220520019531\n",
      "Training at step=88, batch=360, train loss = 0.2441322811573493, train acc = 0.949999988079071, time = 0.008177757263183594\n",
      "Training at step=88, batch=480, train loss = 0.36426174163007125, train acc = 0.8999999761581421, time = 0.008125782012939453\n",
      "Testing at step=88, batch=0, test loss = 0.5616341172867606, test acc = 0.8399999737739563, time = 0.0015857219696044922\n",
      "Testing at step=88, batch=20, test loss = 0.48019261470077124, test acc = 0.8199999928474426, time = 0.0015668869018554688\n",
      "Testing at step=88, batch=40, test loss = 0.411611933413149, test acc = 0.8299999833106995, time = 0.001605987548828125\n",
      "Testing at step=88, batch=60, test loss = 0.47955662810812855, test acc = 0.8399999737739563, time = 0.0015990734100341797\n",
      "Testing at step=88, batch=80, test loss = 0.4230271702510892, test acc = 0.8100000023841858, time = 0.0015876293182373047\n",
      "Step 88 finished in 16.270127534866333, Train loss = 0.35837290449861314, Test loss = 0.45133526194077705; Train Acc = 0.8740666663646698, Test Acc = 0.8408999979496002\n",
      "Training at step=89, batch=0, train loss = 0.41793607740481115, train acc = 0.8899999856948853, time = 0.00825047492980957\n",
      "Training at step=89, batch=120, train loss = 0.2959699203101974, train acc = 0.8899999856948853, time = 0.008186578750610352\n",
      "Training at step=89, batch=240, train loss = 0.3026269243731954, train acc = 0.8799999952316284, time = 0.008255720138549805\n",
      "Training at step=89, batch=360, train loss = 0.39966191860415734, train acc = 0.8399999737739563, time = 0.008532047271728516\n",
      "Training at step=89, batch=480, train loss = 0.37405717176863973, train acc = 0.8500000238418579, time = 0.0082855224609375\n",
      "Testing at step=89, batch=0, test loss = 0.48049033389615536, test acc = 0.8799999952316284, time = 0.00160980224609375\n",
      "Testing at step=89, batch=20, test loss = 0.3173748024539126, test acc = 0.8700000047683716, time = 0.0016102790832519531\n",
      "Testing at step=89, batch=40, test loss = 0.3159076878284072, test acc = 0.8600000143051147, time = 0.0015566349029541016\n",
      "Testing at step=89, batch=60, test loss = 0.4194533980637398, test acc = 0.8600000143051147, time = 0.0015714168548583984\n",
      "Testing at step=89, batch=80, test loss = 0.39030941159090055, test acc = 0.8600000143051147, time = 0.0015590190887451172\n",
      "Step 89 finished in 16.32606291770935, Train loss = 0.35911593912841994, Test loss = 0.45272722605786947; Train Acc = 0.8742166660229365, Test Acc = 0.8423999977111817\n",
      "Training at step=90, batch=0, train loss = 0.44293693577912097, train acc = 0.8600000143051147, time = 0.008273601531982422\n",
      "Training at step=90, batch=120, train loss = 0.3175922896083995, train acc = 0.8600000143051147, time = 0.008167743682861328\n",
      "Training at step=90, batch=240, train loss = 0.35372652395199233, train acc = 0.8799999952316284, time = 0.008190393447875977\n",
      "Training at step=90, batch=360, train loss = 0.369874412762486, train acc = 0.8899999856948853, time = 0.008280515670776367\n",
      "Training at step=90, batch=480, train loss = 0.22106596850707827, train acc = 0.9200000166893005, time = 0.008301734924316406\n",
      "Testing at step=90, batch=0, test loss = 0.3955192775092675, test acc = 0.8700000047683716, time = 0.0016939640045166016\n",
      "Testing at step=90, batch=20, test loss = 0.33353713154171916, test acc = 0.9100000262260437, time = 0.00156402587890625\n",
      "Testing at step=90, batch=40, test loss = 0.3187095911678246, test acc = 0.8600000143051147, time = 0.0015854835510253906\n",
      "Testing at step=90, batch=60, test loss = 0.43148019078624555, test acc = 0.8399999737739563, time = 0.001546621322631836\n",
      "Testing at step=90, batch=80, test loss = 0.24256179756057542, test acc = 0.8999999761581421, time = 0.001573801040649414\n",
      "Step 90 finished in 16.30313229560852, Train loss = 0.35882107835837485, Test loss = 0.451101275185917; Train Acc = 0.8746166665355365, Test Acc = 0.8411999970674515\n",
      "Training at step=91, batch=0, train loss = 0.45004325505478904, train acc = 0.8500000238418579, time = 0.0084075927734375\n",
      "Training at step=91, batch=120, train loss = 0.351649142823864, train acc = 0.8500000238418579, time = 0.008240461349487305\n",
      "Training at step=91, batch=240, train loss = 0.20197508276337878, train acc = 0.9399999976158142, time = 0.008203983306884766\n",
      "Training at step=91, batch=360, train loss = 0.48715553453283983, train acc = 0.8799999952316284, time = 0.008146524429321289\n",
      "Training at step=91, batch=480, train loss = 0.4083375328334655, train acc = 0.8600000143051147, time = 0.00819540023803711\n",
      "Testing at step=91, batch=0, test loss = 0.4195763242498511, test acc = 0.8799999952316284, time = 0.0016589164733886719\n",
      "Testing at step=91, batch=20, test loss = 0.6358094527665209, test acc = 0.800000011920929, time = 0.0015940666198730469\n",
      "Testing at step=91, batch=40, test loss = 0.42705948310862873, test acc = 0.8100000023841858, time = 0.0015759468078613281\n",
      "Testing at step=91, batch=60, test loss = 0.3893941760645781, test acc = 0.8500000238418579, time = 0.0015664100646972656\n",
      "Testing at step=91, batch=80, test loss = 0.32759474890443063, test acc = 0.8799999952316284, time = 0.0015642642974853516\n",
      "Step 91 finished in 16.339078426361084, Train loss = 0.3577764956556779, Test loss = 0.45453043503271623; Train Acc = 0.874733332892259, Test Acc = 0.838899998664856\n",
      "Training at step=92, batch=0, train loss = 0.35639949816963196, train acc = 0.8700000047683716, time = 0.00835418701171875\n",
      "Training at step=92, batch=120, train loss = 0.4512539081577419, train acc = 0.8299999833106995, time = 0.00819087028503418\n",
      "Training at step=92, batch=240, train loss = 0.25535676807161567, train acc = 0.8999999761581421, time = 0.00834345817565918\n",
      "Training at step=92, batch=360, train loss = 0.25902555600738564, train acc = 0.9300000071525574, time = 0.008270263671875\n",
      "Training at step=92, batch=480, train loss = 0.35461922356802866, train acc = 0.8899999856948853, time = 0.008209466934204102\n",
      "Testing at step=92, batch=0, test loss = 0.31527755680852615, test acc = 0.8999999761581421, time = 0.0016222000122070312\n",
      "Testing at step=92, batch=20, test loss = 0.39947086149549954, test acc = 0.8999999761581421, time = 0.0015716552734375\n",
      "Testing at step=92, batch=40, test loss = 0.3729245855523961, test acc = 0.8600000143051147, time = 0.0015652179718017578\n",
      "Testing at step=92, batch=60, test loss = 0.3396012388038085, test acc = 0.8899999856948853, time = 0.0015819072723388672\n",
      "Testing at step=92, batch=80, test loss = 0.4737299999825928, test acc = 0.8600000143051147, time = 0.0016019344329833984\n",
      "Step 92 finished in 16.362021446228027, Train loss = 0.35745229997311656, Test loss = 0.45340170089226106; Train Acc = 0.8743333329757055, Test Acc = 0.8397999984025956\n",
      "Training at step=93, batch=0, train loss = 0.36181527063184177, train acc = 0.8500000238418579, time = 0.008442163467407227\n",
      "Training at step=93, batch=120, train loss = 0.3204438913557421, train acc = 0.8700000047683716, time = 0.00818324089050293\n",
      "Training at step=93, batch=240, train loss = 0.36293425363552045, train acc = 0.8600000143051147, time = 0.008216619491577148\n",
      "Training at step=93, batch=360, train loss = 0.20606941324487982, train acc = 0.9300000071525574, time = 0.008172750473022461\n",
      "Training at step=93, batch=480, train loss = 0.361501894374383, train acc = 0.8799999952316284, time = 0.008203744888305664\n",
      "Testing at step=93, batch=0, test loss = 0.587777024077417, test acc = 0.7900000214576721, time = 0.0015647411346435547\n",
      "Testing at step=93, batch=20, test loss = 0.4600028758424415, test acc = 0.8399999737739563, time = 0.001558065414428711\n",
      "Testing at step=93, batch=40, test loss = 0.36695412565261004, test acc = 0.8799999952316284, time = 0.0015926361083984375\n",
      "Testing at step=93, batch=60, test loss = 0.3624207250363517, test acc = 0.8500000238418579, time = 0.0015652179718017578\n",
      "Testing at step=93, batch=80, test loss = 0.5418591157695712, test acc = 0.800000011920929, time = 0.0016057491302490234\n",
      "Step 93 finished in 16.30595588684082, Train loss = 0.3573544797435161, Test loss = 0.4508640189964209; Train Acc = 0.8743166668216388, Test Acc = 0.8393999999761581\n",
      "Training at step=94, batch=0, train loss = 0.4330353507040723, train acc = 0.8999999761581421, time = 0.008279561996459961\n",
      "Training at step=94, batch=120, train loss = 0.2657370599321466, train acc = 0.9200000166893005, time = 0.008306264877319336\n",
      "Training at step=94, batch=240, train loss = 0.2838079707913267, train acc = 0.8799999952316284, time = 0.008224964141845703\n",
      "Training at step=94, batch=360, train loss = 0.35719908754952684, train acc = 0.8700000047683716, time = 0.00822305679321289\n",
      "Training at step=94, batch=480, train loss = 0.34020076440840946, train acc = 0.8700000047683716, time = 0.008201122283935547\n",
      "Testing at step=94, batch=0, test loss = 0.39975646831460826, test acc = 0.8600000143051147, time = 0.0015735626220703125\n",
      "Testing at step=94, batch=20, test loss = 0.6020225681397217, test acc = 0.8199999928474426, time = 0.0015578269958496094\n",
      "Testing at step=94, batch=40, test loss = 0.6433046411056119, test acc = 0.8199999928474426, time = 0.0015683174133300781\n",
      "Testing at step=94, batch=60, test loss = 0.397086158165863, test acc = 0.9100000262260437, time = 0.0016021728515625\n",
      "Testing at step=94, batch=80, test loss = 0.3730559667998276, test acc = 0.8600000143051147, time = 0.0015652179718017578\n",
      "Step 94 finished in 16.34503984451294, Train loss = 0.35745329242142293, Test loss = 0.45214238314779487; Train Acc = 0.8749166665474574, Test Acc = 0.8419000005722046\n",
      "Training at step=95, batch=0, train loss = 0.36427719563353345, train acc = 0.8600000143051147, time = 0.008425235748291016\n",
      "Training at step=95, batch=120, train loss = 0.2935421601301149, train acc = 0.8899999856948853, time = 0.008159399032592773\n",
      "Training at step=95, batch=240, train loss = 0.2392536918263885, train acc = 0.8999999761581421, time = 0.00818181037902832\n",
      "Training at step=95, batch=360, train loss = 0.27894892851375347, train acc = 0.8999999761581421, time = 0.008201837539672852\n",
      "Training at step=95, batch=480, train loss = 0.3057573915355043, train acc = 0.8600000143051147, time = 0.008158683776855469\n",
      "Testing at step=95, batch=0, test loss = 0.3263518968956271, test acc = 0.8999999761581421, time = 0.0015785694122314453\n",
      "Testing at step=95, batch=20, test loss = 0.3942012278800478, test acc = 0.8399999737739563, time = 0.0015552043914794922\n",
      "Testing at step=95, batch=40, test loss = 0.5523702747569871, test acc = 0.8399999737739563, time = 0.0015540122985839844\n",
      "Testing at step=95, batch=60, test loss = 0.4335951054145904, test acc = 0.8100000023841858, time = 0.0015621185302734375\n",
      "Testing at step=95, batch=80, test loss = 0.33621012673448347, test acc = 0.8500000238418579, time = 0.0015561580657958984\n",
      "Step 95 finished in 16.32379961013794, Train loss = 0.3569399476995738, Test loss = 0.452128708554975; Train Acc = 0.8742999988794327, Test Acc = 0.8420999974012375\n",
      "Training at step=96, batch=0, train loss = 0.32089237018805683, train acc = 0.8700000047683716, time = 0.008413314819335938\n",
      "Training at step=96, batch=120, train loss = 0.3434233418412035, train acc = 0.8799999952316284, time = 0.008202075958251953\n",
      "Training at step=96, batch=240, train loss = 0.3296561598999033, train acc = 0.8600000143051147, time = 0.008165121078491211\n",
      "Training at step=96, batch=360, train loss = 0.5128990505006544, train acc = 0.8299999833106995, time = 0.008177518844604492\n",
      "Training at step=96, batch=480, train loss = 0.44737634213042904, train acc = 0.8500000238418579, time = 0.008202314376831055\n",
      "Testing at step=96, batch=0, test loss = 0.3756654723011097, test acc = 0.8500000238418579, time = 0.0016167163848876953\n",
      "Testing at step=96, batch=20, test loss = 0.41233851718787085, test acc = 0.8700000047683716, time = 0.0015878677368164062\n",
      "Testing at step=96, batch=40, test loss = 0.40713519037525286, test acc = 0.8500000238418579, time = 0.0015645027160644531\n",
      "Testing at step=96, batch=60, test loss = 0.37039502882768943, test acc = 0.8700000047683716, time = 0.0015723705291748047\n",
      "Testing at step=96, batch=80, test loss = 0.5257126767353615, test acc = 0.8100000023841858, time = 0.0015902519226074219\n",
      "Step 96 finished in 16.32258653640747, Train loss = 0.3564704577132007, Test loss = 0.45080756458376653; Train Acc = 0.8740499981244405, Test Acc = 0.8418999993801117\n",
      "Training at step=97, batch=0, train loss = 0.2795425841610898, train acc = 0.9200000166893005, time = 0.008289813995361328\n",
      "Training at step=97, batch=120, train loss = 0.5458441965466373, train acc = 0.8700000047683716, time = 0.008178472518920898\n",
      "Training at step=97, batch=240, train loss = 0.2937266614529783, train acc = 0.8700000047683716, time = 0.008191823959350586\n",
      "Training at step=97, batch=360, train loss = 0.40080883266668155, train acc = 0.8500000238418579, time = 0.008244037628173828\n",
      "Training at step=97, batch=480, train loss = 0.4254009218067619, train acc = 0.8399999737739563, time = 0.008164405822753906\n",
      "Testing at step=97, batch=0, test loss = 0.39754834240875636, test acc = 0.8100000023841858, time = 0.0015952587127685547\n",
      "Testing at step=97, batch=20, test loss = 0.561382880434125, test acc = 0.8199999928474426, time = 0.0015683174133300781\n",
      "Testing at step=97, batch=40, test loss = 0.39788001866158496, test acc = 0.8700000047683716, time = 0.0015561580657958984\n",
      "Testing at step=97, batch=60, test loss = 0.3746077315946607, test acc = 0.8500000238418579, time = 0.0015659332275390625\n",
      "Testing at step=97, batch=80, test loss = 0.5040523224472612, test acc = 0.800000011920929, time = 0.0015556812286376953\n",
      "Step 97 finished in 16.290473699569702, Train loss = 0.3567380806623697, Test loss = 0.45632367144975744; Train Acc = 0.8750000002980233, Test Acc = 0.8385999983549118\n",
      "Training at step=98, batch=0, train loss = 0.5555741478381454, train acc = 0.8399999737739563, time = 0.008253097534179688\n",
      "Training at step=98, batch=120, train loss = 0.4554219827839101, train acc = 0.7900000214576721, time = 0.008166790008544922\n",
      "Training at step=98, batch=240, train loss = 0.47623948968553137, train acc = 0.8600000143051147, time = 0.008201360702514648\n",
      "Training at step=98, batch=360, train loss = 0.27831980994975886, train acc = 0.8700000047683716, time = 0.008215188980102539\n",
      "Training at step=98, batch=480, train loss = 0.42137291970513646, train acc = 0.8600000143051147, time = 0.00828695297241211\n",
      "Testing at step=98, batch=0, test loss = 0.5500956014657118, test acc = 0.8299999833106995, time = 0.0015871524810791016\n",
      "Testing at step=98, batch=20, test loss = 0.5743037751419525, test acc = 0.8299999833106995, time = 0.0015761852264404297\n",
      "Testing at step=98, batch=40, test loss = 0.3572133141371692, test acc = 0.8799999952316284, time = 0.0015921592712402344\n",
      "Testing at step=98, batch=60, test loss = 0.28920586152890243, test acc = 0.8899999856948853, time = 0.0015592575073242188\n",
      "Testing at step=98, batch=80, test loss = 0.44353665353743466, test acc = 0.8500000238418579, time = 0.0015606880187988281\n",
      "Step 98 finished in 16.266512870788574, Train loss = 0.3569151278173665, Test loss = 0.45320449728247886; Train Acc = 0.8737666682402293, Test Acc = 0.8398999989032745\n",
      "Training at step=99, batch=0, train loss = 0.37081156231158247, train acc = 0.8899999856948853, time = 0.00838017463684082\n",
      "Training at step=99, batch=120, train loss = 0.3794580049917567, train acc = 0.8299999833106995, time = 0.008189916610717773\n",
      "Training at step=99, batch=240, train loss = 0.36850839077988445, train acc = 0.8600000143051147, time = 0.008151531219482422\n",
      "Training at step=99, batch=360, train loss = 0.5102636475388098, train acc = 0.8299999833106995, time = 0.008135318756103516\n",
      "Training at step=99, batch=480, train loss = 0.24731112565143618, train acc = 0.8899999856948853, time = 0.00813150405883789\n",
      "Testing at step=99, batch=0, test loss = 0.3555087502318485, test acc = 0.8999999761581421, time = 0.0016422271728515625\n",
      "Testing at step=99, batch=20, test loss = 0.3907221496141766, test acc = 0.8700000047683716, time = 0.0015745162963867188\n",
      "Testing at step=99, batch=40, test loss = 0.3814963437670143, test acc = 0.8799999952316284, time = 0.001566171646118164\n",
      "Testing at step=99, batch=60, test loss = 0.38215154426649833, test acc = 0.8700000047683716, time = 0.0015630722045898438\n",
      "Testing at step=99, batch=80, test loss = 0.5728818917130527, test acc = 0.8100000023841858, time = 0.0015711784362792969\n",
      "Step 99 finished in 16.339483499526978, Train loss = 0.3563778870679723, Test loss = 0.45400162282466927; Train Acc = 0.8747333329916, Test Acc = 0.8421999990940094\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.2\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:47:09.750633Z",
     "start_time": "2024-04-06T20:47:09.194520Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 953,
     "status": "ok",
     "timestamp": 1712093845925,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "V7YYHs33saIx",
    "outputId": "928859bd-acd3-46ea-be13-39efadae9e50"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ8AAAGACAYAAAADNcOYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAAD/+0lEQVR4nOzdd3xUVfr48c/0mZSZFJIQEloCCTWA1BBEQRQpigoiCIiuhVVEhXVty09FXUXs4tfFXRuiK7ZVAamuIhqKZZHeQw+EkF6m3/v7Y8jAmAQSSCYBnvfrlZfm3HPPPfdMSO48c85zNKqqqgghhBBCCCGEEEIIUQ+0Dd0BIYQQQgghhBBCCHHhkuCTEEIIIYQQQgghhKg3EnwSQgghhBBCCCGEEPVGgk9CCCGEEEIIIYQQot5I8EkIIYQQQgghhBBC1BsJPgkhhBBCCCGEEEKIeiPBJyGEEEIIIYQQQghRbyT4JIQQQgghhBBCCCHqjQSfhBBCCCGEEEIIIUS9keCTEEJchP7zn/+QmprKpk2bGrorQgghhBAXlUOHDpGamso777zT0F0RImgk+CSEOGsSwKhexdhU9/X77783dBeFEEIIUQsfffQRqamp3HjjjQ3dFXEGFcGd6r7++c9/NnQXhbjo6Bu6A0IIcSG77777SExMrFTeokWLBuiNEEIIIc7WwoULSUhIYOPGjezfv5+WLVs2dJfEGQwfPpz+/ftXKu/QoUMD9EaIi5sEn4QQoh7179+fzp07N3Q3hBBCCHEODh48yPr163njjTd4/PHHWbhwIffee29Dd6tK5eXlhISENHQ3GoUOHTowYsSIhu6GEAJZdieECIKtW7dyxx13cMkll9CtWzcmTpxYadmZ2+3mjTfe4KqrrqJz58707t2bsWPHkpmZ6a+Tm5vLo48+Sv/+/enUqRP9+vXj7rvv5tChQ9Ve+5133iE1NZXDhw9XOvbSSy/RqVMnioqKANi3bx9TpkwhIyODzp07079/f6ZOnUpJSUndDEQVTl3z//777zNgwADS0tIYP348O3furFR/zZo13HzzzXTt2pUePXpw9913s2fPnkr1cnJyeOyxx+jXrx+dOnVi4MCBPPHEE7hcroB6LpeL5557jj59+tC1a1cmT55Mfn5+vd2vEEIIcT5auHAhNpuNyy67jMGDB7Nw4cIq6xUXF/Pss88ycOBAOnXqRP/+/XnooYcC/rY6nU5mz57N4MGD6dy5M/369ePee+/lwIEDAKxbt47U1FTWrVsX0HbFM8N//vMff9kjjzxCt27dOHDgAHfeeSfdunXjwQcfBODXX3/lvvvu4/LLL6dTp05cdtllPPvsszgcjkr93rNnD/fffz99+vQhLS2NwYMH88orrwCwdu1aUlNTWbFiRZXjkpqayvr166scj02bNpGamsqXX35Z6diPP/5Iamoq33//PQClpaX8/e9/949deno6t912G1u2bKmy7boycOBAJk2axE8//cSIESPo3LkzQ4cOZfny5ZXqHjx4kPvuu49evXrRpUsXRo8ezcqVKyvVO9NrfKpPPvmEQYMG0alTJ0aOHMnGjRvr4zaFaHAy80kIUa927drFuHHjCA0N5Y477kCv1/PJJ58wYcIEPvzwQ7p06QLAG2+8wVtvvcWNN95IWloapaWlbN68mS1btpCRkQHAlClT2L17N+PHjychIYH8/HwyMzM5cuRIlUvbAIYMGcILL7zAkiVLuOOOOwKOLVmyhIyMDGw2Gy6Xi9tvvx2Xy8X48eNp0qQJOTk5rFy5kuLiYsLDw8/q/ktLSysFczQaDZGRkQFlX331FWVlZdx88804nU7mzZvHxIkTWbhwIU2aNAFg9erV3HnnnSQmJnLvvfficDj48MMPGTt2LP/5z3/8Y5CTk8OoUaMoKSlh9OjRJCUlkZOTw7Jly3A4HBiNRv91n3nmGaxWK/feey+HDx9m7ty5PPXUU7z66qtndb9CCCHEhWjhwoVceeWVGI1Ghg8fzscff8zGjRtJS0vz1ykrK2PcuHHs2bOHkSNH0qFDBwoKCvjuu+/IyckhKioKr9fLpEmTWLNmDcOGDeOWW26hrKyMzMxMdu7ceVbL8j0eD7fffjvdu3fn4Ycfxmw2A7B06VIcDgdjx44lIiKCjRs38uGHH3L06FFef/11//nbt29n3Lhx6PV6brrpJhISEjhw4ADfffcdU6dOpXfv3sTHx/vH4I/j0qJFC7p161Zl3zp37kzz5s1ZsmQJ119/fcCxxYsXY7PZ6NevHwBPPPEEy5YtY/z48SQnJ1NYWMhvv/3Gnj176NixY63HBcBut1f5oZrVakWvP/lWeN++fUydOpUxY8Zw/fXX88UXX3D//ffz9ttv+59Djx8/zpgxY7Db7UyYMIHIyEi+/PJL7r77bl5//XX/2NTmNV60aBFlZWXcdNNNaDQa3n77baZMmcK3336LwWA4q3sWotFShRDiLH3xxRdqSkqKunHjxmrr3HPPPWrHjh3VAwcO+MtycnLUbt26qePGjfOXXXvttepdd91VbTtFRUVqSkqK+vbbb9e6nzfddJN6/fXXB5Rt2LBBTUlJUb/88ktVVVV169atakpKirpkyZJat1+VirGp6qtTp07+egcPHlRTUlLUtLQ09ejRo5X69+yzz/rLRowYoaanp6sFBQX+sm3btqnt2rVTH3roIX/ZQw89pLZr167K10VRlID+3Xrrrf4yVVXVZ599Vm3fvr1aXFxcJ+MghBBCnO82bdqkpqSkqJmZmaqq+v6W9u/fX33mmWcC6r322mtqSkqKunz58kptVPyt/fzzz9WUlBT1vffeq7bO2rVr1ZSUFHXt2rUBxyueGb744gt/2cMPP6ympKSoL774YqX27HZ7pbK33npLTU1NVQ8fPuwvGzdunNqtW7eAslP7o6qq+tJLL6mdOnUKeD7Iy8tTO3TooL7++uuVrnOql156Se3YsaNaWFjoL3M6nWqPHj3URx991F/WvXt3dcaMGadtq6Yqxqq6r/Xr1/vrDhgwQE1JSVGXLVvmLyspKVEzMjLU6667zl/297//XU1JSVF/+eUXf1lpaak6cOBAdcCAAarX61VVtWavcUX/evXqFTAu3377rZqSkqJ+9913dTIOQjQmsuxOCFFvvF4vmZmZDBo0iObNm/vLY2NjGT58OL/99hulpaWA7xOoXbt2sW/fvirbMpvNGAwGfv75Z/8yuZoaMmQIW7ZsCZjqvGTJEoxGI4MGDQIgLCwMgJ9++gm73V6r9k/n8ccf57333gv4+te//lWp3qBBg4iLi/N/n5aWRpcuXfjhhx8AOHbsGNu2beP6668nIiLCX69du3b07dvXX09RFL799lsGDBhQZa4pjUYT8P3o0aMDynr06IHX661ymaIQQghxMaqYhdy7d2/A97d06NChLF68GK/X66+3fPly2rVrV2l2UMU5FXUiIyMZP358tXXOxtixYyuVVcyAAl8eqPz8fLp164aqqmzduhWA/Px8fvnlF0aOHEmzZs2q7c+IESNwuVwsXbrUX7Z48WI8Hg/XXnvtafs2dOhQ3G53wDK2zMxMiouLGTp0qL/MarWyYcMGcnJyanjXZ3bTTTdVeg577733aNOmTUC92NjYgNctLCyM6667jq1bt5KbmwvADz/8QFpaGj169PDXCw0N5aabbuLw4cPs3r0bqN1rPHToUGw2m//7irYPHjx4jncuROMjwSchRL3Jz8/HbrfTunXrSseSk5NRFIUjR44Avl3hSkpKGDx4MNdccw3PP/8827dv99c3Go08+OCDrFq1ioyMDMaNG8e//vUv/wPB6Vx99dVotVoWL14MgKqqLF26lP79+/uDTs2bN+e2227js88+o0+fPtx+++189NFH55zvKS0tjb59+wZ89enTp1K9qnbMadWqlT8IlJ2dDVDtWBYUFPgfLEtLS2nbtm2N+vfHB02r1Qr4clYIIYQQFzuv18s333xD7969OXToEPv372f//v2kpaVx/Phx1qxZ46974MCBM/79PXDgAK1btw5Y8nWu9Ho9TZs2rVSenZ3NI488Qq9evejWrRvp6en+gEjFh38VQY6UlJTTXiM5OZnOnTsH5LpauHAhXbt2PeOuf+3atSMpKYklS5b4yxYvXkxkZGTAM9GDDz7Irl27uPzyyxk1ahSzZ88+5yBMy5YtKz2H9e3b1//8d2q9PwaGWrVqBRDwLFbVc1hSUpL/ONTuNY6Pjw/4viIQJc9h4kIkwSchRKPQs2dPVqxYwbPPPkvbtm35/PPPueGGG/jss8/8dW699VaWLVvGtGnTMJlMvPbaawwdOtT/6V114uLi6NGjh/+h5/fffyc7Ozvg0zbwJe1csGABkyZNwuFw8MwzzzBs2DCOHj1a9zfcSGi1Vf8ZUFU1yD0RQgghGp+1a9eSm5vLN998w1VXXeX/euCBBwCqTTx+LqqbAaUoSpXlRqOx0t9zr9fLbbfdxsqVK7njjjv4v//7P9577z1mzpx52rZO57rrruOXX37h6NGjHDhwgN9///2Ms54qDB06lHXr1pGfn4/L5eK7777jqquuCgjQDB06lG+//Zbp06cTGxvLO++8w7Bhw/yzuy9EOp2uynJ5DhMXIgk+CSHqTVRUFBaLhb1791Y6lpWVhVarDfjEJyIigpEjR/Lyyy+zcuVKUlNTmT17dsB5LVq04E9/+hPvvvsuixYtwu128+67756xL0OGDGH79u1kZWWxePFiLBYLAwYMqFQvNTWVe+65h48++oiPPvqInJwcPv7447O4+9rZv39/pbJ9+/aRkJAAnJyhVN1YRkZGEhISQlRUFGFhYezatat+OyyEEEJcBBYuXEh0dDSvvfZapa/hw4ezYsUK/+5xLVq0OOPf3xYtWrB3717cbne1dSpmIf9x9nVtlsTv3LmTffv28cgjj3DXXXcxaNAg+vbtS2xsbEC9irQIVe2w+0dDhw5Fp9OxaNEiFixYgMFgYMiQITXqz9ChQ/F4PCxfvpxVq1ZRWlrKsGHDKtWLjY1l3LhxvPnmm/z3v/8lIiKCOXPm1Oga52L//v2VAj4VqSBOfRar7jms4jjU7DUW4mIkwSchRL3R6XRkZGTw3//+l0OHDvnLjx8/zqJFi+jevbt/2nNBQUHAuaGhobRo0QKXywX4ditxOp0BdVq0aEFoaKi/zukMHjwYnU7HN998w9KlS7n88ssJCQnxHy8tLcXj8QSck5KSglarDWg/OzubPXv21HAEau7bb78NyHGwceNGNmzYQP/+/QHfw1j79u356quvAqZi79y5k8zMTC677DLAN5Np0KBBfP/992zatKnSdeSTNCGEEKJmHA4Hy5cv5/LLL+fqq6+u9DVu3DjKysr47rvvALjqqqvYvn07K1asqNRWxd/fq666ioKCAj766KNq6yQkJKDT6fjll18Cjtfmw7CKmVCn/t1XVZUPPvggoF5UVBQ9e/bkiy++8C8b+2N/Tq176aWXsmDBAhYuXEi/fv2IioqqUX+Sk5NJSUlh8eLFLF68mJiYGHr27Ok/7vV6KwXboqOjiY2NDXgOy8/PZ8+ePXWanxN8uTVPfd1KS0v56quvaN++PTExMQBcdtllbNy4kfXr1/vrlZeX8+mnn5KQkODPI1WT11iIi1HdLTYWQly0vvjiC3788cdK5bfccgsPPPAAq1ev5uabb+bmm29Gp9PxySef4HK5+Otf/+qvO2zYMHr16kXHjh2JiIhg06ZN/u12wffp06233srVV19NmzZt0Ol0fPvttxw/frzKT87+KDo6mt69e/Pee+9RVlZWacnd2rVreeqpp7j66qtp1aoVXq+Xr7/+Gp1Ox+DBg/31Hn74YX7++Wd27NhRo7FZtWqV/xOxU11yySUBSdhbtGjB2LFjGTt2LC6Xiw8++ICIiAjuuOMOf52HHnqIO++8k5tuuolRo0bhcDj48MMPCQ8P59577/XXmzZtGpmZmUyYMIHRo0eTnJxMbm4uS5cu5d///rf/E1UhhBBCVO+7776jrKyMgQMHVnm8a9euREVFsWDBAoYOHcrtt9/OsmXLuP/++xk5ciQdO3akqKiI7777jhkzZtCuXTuuu+46vvrqK5577jk2btxI9+7dsdvtrFmzhrFjxzJo0CDCw8O5+uqr+fDDD9FoNDRv3pyVK1eSl5dX474nJSXRokULnn/+eXJycggLC2PZsmVV5hKaPn06Y8eO5frrr+emm24iMTGRw4cPs3LlSr7++uuAutdddx333XcfAPfff38tRtM3++n111/HZDIxatSogKWCZWVlXHbZZQwePJh27doREhLC6tWr2bRpE4888oi/3kcffcQbb7zBBx984E8Afzpbt26tdA/ge+7q1q2b//tWrVrxt7/9jU2bNhEdHc0XX3xBXl4ezz33nL/OXXfdxTfffMOdd97JhAkTsNlsfPXVVxw6dIjZs2f776cmr7EQFyMJPgkhzll1n8TdcMMNtG3blo8++oiXXnqJt956C1VVSUtL44UXXqBLly7+uhMmTOC7774jMzMTl8tFs2bNeOCBB7j99tsBaNq0KcOGDWPNmjUsWLAAnU5HUlISr776akBw6HSGDh3K6tWrCQ0N9c8UqpCamkq/fv34/vvvycnJwWKxkJqayr/+9S+6du16dgMDvP7661WWP/fccwHBp+uuuw6tVsvcuXPJy8sjLS2N//f//l/A9Pi+ffvy9ttv8/rrr/P666+j1+vp2bMnf/3rXwPaiouL49NPP+W1115j4cKFlJaWEhcXR//+/QN2vhFCCCFE9RYsWIDJZCIjI6PK41qtlssvv5yFCxdSUFBAZGQkH330EbNnz2bFihV8+eWXREdHk56e7t/RVqfT8a9//Yt//OMfLFq0iOXLlxMREcEll1xCamqqv+3p06fj8XiYP38+RqORq6++moceeojhw4fXqO8Gg4E5c+bwzDPP8NZbb2EymbjyyisZN24cI0aMCKjbrl07/3PDxx9/jNPppFmzZlUuqRswYAA2mw1FUbjiiitqOpSA7zns1VdfxW63V2rbbDYzduxYMjMzWb58Oaqq0qJFC5544gluvvnmWl3nVIsWLWLRokWVyq+//vpKwaf/9//+H7NmzWLv3r0kJibyyiuvcOmll/rrNGnShPnz5/PCCy/w4Ycf4nQ6SU1NZc6cOVx++eX+ejV9jYW42GhUmfsnhBAN5tChQ1xxxRU89NBD/kCbEEIIIURj5PF4uPTSSxkwYADPPvtsQ3enTgwcOJC2bdvy1ltvNXRXhLigSc4nIYQQQgghhBBn9O2335Kfn891113X0F0RQpxnZNmdEEIIIYQQQohqbdiwgR07dvDmm2/SoUMHevXq1dBdEkKcZyT4JIQQQgghhBCiWh9//DELFiygXbt2zJw5s6G7I4Q4D0nOJyGEEEIIIYQQQghRbyTnkxBCCCHEeWzPnj3cdtttdO3alYyMDGbNmoXL5TrjeQUFBTz++ONcfvnldO3aleHDh1e5e+mvv/7KhAkT6NmzJ7179+aOO+5g27Zt9XErQgghhLhAybI7IYQQQojzVFFRERMnTqRVq1bMnj2bnJwcZs6cicPh4PHHHz/tuffffz9ZWVlMmzaN+Ph4Vq1axZNPPolOp2P06NEAZGVlcfvtt9OnTx9eeuklXC4Xb731FrfeeiuLFi0iJiYmGLcphBBCiPOcBJ+EEEIIIc5T8+fPp6ysjDfeeIOIiAgAvF4vM2bMYNKkScTFxVV5Xm5uLuvWreO5557jhhtuACA9PZ1NmzbxzTff+INP3377Laqq8tprr2E2mwFITU1l0KBBZGZmyo5XQgghhKgRCT7VAVVVUZS6T52l1WrqpV1RPRnz4JLxDi4Z7+CTMQ+uuhhvrVaDRqOpox7Vv1WrVpGenu4PPAEMGTKEJ554gszMTH9g6Y88Hg8A4eHhAeVhYWGUl5f7v3e73RiNRkwmk7/sj+ecDXl2unDImAeXjHdwyXgHn4x5cAXz2UmCT3VAUVTy88vqtE29XktkZCjFxeV4PEqdti2qJmMeXDLewSXjHXwy5sFVV+MdFRWKTnf+BJ+ysrIYOXJkQJnVaiUmJoasrKxqz4uPj6dfv37MmTOH1q1b07RpU1atWkVmZiYvvviiv96wYcN4++23efXVV7n11ltxuVy8/PLLxMfHc8UVV5x1v+XZ6cIgYx5cMt7BJeMdfDLmwRXsZycJPgkhhBBCnKeKi4uxWq2Vym02G0VFRac9d/bs2UydOpVhw4YBoNPpmD59OoMHD/bXadWqFe+//z733HMPc+bMASAhIYH33nvvnGdA6fV1u++NTqcN+K+ofzLmwSXjHVwy3sEnYx5cwR5vCT4JIYQQQlxkVFXl0UcfZd++fbz00kvExMSwevVqnn32WWw2mz8gtXfvXqZMmUJGRgbXXXcdTqeTd999lzvvvJP58+fTpEmTs7q+VqshMjK0Lm/Jz2q11Eu7onoy5sEl4x1cMt7BJ2MeXMEabwk+CSGEEEKcp6xWKyUlJZXKi4qKsNls1Z63cuVKli5dyoIFC0hNTQWgd+/e5OXlMXPmTH/w6ZVXXqFJkybMmjXLf26vXr0YMGAAH3zwAdOmTTurfiuKSnFx+Zkr1oJOp8VqtVBcbMfrleUawSBjHlwy3sEl4x18MubBVVfjbbVaajR7SoJPQgghhBDnqaSkpEq5nUpKSsjNzSUpKana83bv3o1OpyMlJSWgvH379nz22WfY7XYsFgu7d++ma9euAXVCQ0Np0aIFBw4cOKe+11c+D69XkVwhQSZjHlwy3sEl4x18MubBFazxlsWUQgghhBDnqf79+7N69WqKi4v9ZUuXLkWr1ZKRkVHteQkJCXi9Xnbs2BFQvmXLFqKjo7FYfFPwmzVrxrZt21DVkzvhlJaWsn//fhISEur4boQQQghxoZKZT0IIIS46iqLg9Xrq+RoaHA4dLpcTr1e2DK5vNRlvnU6PVnthfe42ZswY5s2bx+TJk5k0aRI5OTnMmjWLMWPGEBcX5683ceJEsrOzWbFiBeALWjVr1oz77ruPyZMnExsby08//cSXX37JlClTAtqfPHkyDz74ICNGjMDlcvHuu+/icrm48cYbg36/QgghhDg/SfBJCCHERUNVVYqL87HbS4NyvePHtSiKTBsPlpqMt8UShtUahUZz5i2Bzwc2m425c+fy9NNPM3nyZEJDQxk1ahRTp04NqOcLuHr934eFhfH+++/zyiuv8OKLL1JSUkJiYiKPPPII48eP99cbNGgQr776Ku+88w5Tp07FYDDQoUMHPvjgA1q1ahWs2xRCCCHEeU6jnjqPWpwVr1chP7+sTtvU67VERoZSUFAm612DRMY8uGS8g0vG26eoKA+7vZSwsEiMRlO9ByB0Oo3Megqi0423qqq4XE5KSwuwWMKw2aKrrBcVFSpbPAeBPDtdGGTMg0vGO7hkvINPxjy46mq8a/rsJDOfhBBCXBQUxesPPIWFWYNyTb1eKw9PQXSm8TYaTQCUlhYQHh55wS3BE0IIIYRorOSpSwghxEWhYslRRQBCXJwqXv/6zvklhBBCCCFOkuBTI1VU6mTBj3sos7sbuitCCHFBuVBy/YizI6+/EEIIIRobr6Kw+1ARW/bl43R7z3zCCXanhx83ZPPbjlyURp5RSZbdNVLLfj7IotX7GHNFW67q2byhuyOEEEIIIYQQQlz0vIrC0Xw7B4+VcPBYKdm5ZZQ5PNhdHhxOLw6Xb3Z1QpNQmseG0zwujOaxYVhDjGg0vg/CtBqwu7xs21/Alr35bNufj93pCzoZ9FpSm0fQOSmaTklRNI0KqfThWU5BOf/99RA/bjqC0+U7r2XTcMZe0ZaU5hEBfV2/8zjf/noQl0fhr2O7YTE1TBhIgk+NlNvj+wEqlZlPQgghTtGvX48z1nnssScYOvSas2r/3nvvIiQkhFmzXj2r8081atQ19O3bj2nTHj7ntoQQQgghqqOqKnnFDrKPl3H4eBmFJS7KHG7K7G7KHB4cLi+tmoaTlhxNx9ZRlQIwXkUhr9hJUamT4jI3JXYXJWUuSuxuyuweX1sO3//nFTtw1yCn585DRew8VFTjewg16zEadBSUONm8N5/Ne/Phv75gVFS4iSirmSiridJyNxv35FExzykuKoSiUif7j5Yw86P/0T01hmv6tmL7/gJW/HqIvGIHAGEWA16l4WZHSfCpkdLrfSsiK4JQQgghBMCcOe8FfP/nP9/GqFE3MWjQ1f6yhITEs27/L395RHZ7E0IIIUQlbo/C7kOFuP+ws2wTm5lmTULr/fo5BeVs2HWcwjIXDqcHu8uLw+mhuNxFdl65fwZQdQ7llvLTpiPotBraJtpIjAkjt9BOToGd3EJ7rQIzJoOOxJhQmseGkRgbhi3UiNmkx2LUYzHpcHsUDuWWcvCY7+vQsVLKnV5UVUVVfcEyrVZDcjMrHZOi6dQ6ipZx4Wg0kH28jE1Z+Wzem8fOg4W4PQo5Bb5+nqpzUjRX9kykY6soSsrdfPVjFj+cWIL3245cf70wi4EB3RIYeEkCYRZD7Qa9DknwqZEynHjw/+M/bCGEEBe3Tp06VyqLjW1aZXkFp9OByWSuUfutWyeddd+EEEII0XDKHG52HSrC7vDQLaUJZmPdvd3febCQ95ds52h+eZXH05KjGZ7eijaJNn+ZV1H4fVceK9cfIutIMSEmPWEWI2EWPWEhRrQacLkVnB4vLreCoqq0aGolLsJ8YslaGHaXl1+3H+OXbcfYn1Ny2j7qtBqaRoWQEBNKtM1MmNlAqMVAiEmPTqth+4FCNmblkZNfzvYDhWw/UBhwvl7nm2EUHmIgPMRIeIiBsBADYRYDoWYDoWY9oWYDkeEmYiItaM+QR7JFXHjNBvcPEmLCSIgJ4+reLfB4FQpKnOQXO8gvdpJX7MDjVejdIY746JMBP2uokVuubsfASxL55LtdbNlXQHx0CFf1bE56x6YYDbqz6ktdkuBTI2U4MfNJtugWQghRG++88xbz53/Ia6/9g9dee4ldu3Zwxx13c/PNE/jHP2azZs1PHDmSTWhoGF26dGPKlGk0adLEf/4fl91VtDdnznu8+OJz7Ny5nWbNErj33qn07p1+zv396qsv+OSTjzh69AjR0U0YPnwEt9zyJ7Ra39/BkpIS3nzzNdasyaS4uIiIiEg6d05jxoznanRcCCGEOJ+oqsrR/HLW7zrO7kNFhJr1RNvMRNvMNLGaCbUYcLq9OF1enG4v5Q4P+3JK2HWwkMO5Zf6lWIkxoUwZmUZMhKVG11UUFZfHWylgVe7w8PkPe1i5/jDgm0UTbT35gZaqqhzMLWXjnjw27smjXYsIrurZggM5JfywIZuCEqe/rt3pJa/YyensPs0yNa1GQ/uWESTGhmEx6jEbdZhNekLNeppGhxIXaUF/mtnb3VJiGEtbcgrK2bgnj4JiJzERZuKiQoiLDCHSajpjQCnY9DotMRGWGr+OibFhTLupKw6XF5NR16juR4JPjVTFPxqPV4JPQgghasftdjNjxnRGj76ZSZMmY7X6PoUsKMhnwoTbaNIkhsLCAubP/4h7772LDz/8FL2++kcCj8fDU09NZ9SoMdx66x189NFcpk9/iM8/X4jNFnHW/fz88/m8+uqLjBp1E337XsqmTRt4771/UVpayr33PgDA7Nkvs27dav785yk0bRpPXt5x1q5d7W/j1OMJCQkcO3Ys4LgQQgjRUIrLXew+VERWdjF2pwePV8HjVfEqvvd4oSdm5oRbDIRa9Bw8Vsrvu45XWl5VG3FRIZQ73BzKLePpub9yz3WdaNcyssq6Hq/C9v0F/LYzl/U7cykud2MLM5LQJJRmTUKJCjez4teD/gBS/y7NGD0gmRBz4NKtnIJyFq/Zz+rNRyvNKAqzGOjfpRm92sfi8aqU2l2UlPtyMamA0aDDqNdiNOjQaTXkl7nZuT+PA0dLOVZoR6OBdi0i6dk+lu4pMYSHGM96bPxjFBnClT1Czrmdxkqj0TRYUvHTaXw9EsDJmU81SWQmhBDi7KmqistdP79rvYp6xhmsRoO20g4m58rj8XDXXfdwxRVXBZQ/9tgTJ/vm9dKpUxrXXz+U//3vV3r16lNte263mz//+V7S0/sB0KJFS2688VrWrl3N4MFDz6qPXq+X999/myuuuIoHHvgrAL169cHj8TB//odMmHArNlsE27ZtYdCgqxkyZLj/3EGDBvv//9Tjer0Wj0cJOC6EEELUJUVROXy8jKzsIvYeKSY7rxyTXkvoiaVZIWY9JeVudh0q5Ehe1UvUzkSn1dC+ZSSdWkfh8ijkFTs4XuQgr8iB3eXBbNBhMugwGX3/bRodQkpiBG2bR2ALNZJf7GD2fzax/2gJL33yOzcPasuASxJxub0cPFbKvqMl7MkuYuPuPMqdnoBrF5W6KCp1sXVfgb8sNtLCxKvb0b6aIFZcZAi3DW3PiH6tWbLuAOu25tA0KoQBlyTQIzXW/972TPR6LZGRoRQUlOHxKNidHlQVQswStrgQyKvYSJ3M+STBJyGEqC+qqvLch/9j9+Ga70RS19ok2nh03CV1HoCqCBSdas2aTObOfYe9e/dQVlbmLz94cP9pg09arZYePXr7v4+Pb4bJZOLYsWNn3b/9+/dRWFjIwIGDAsoHDrySefPeY+vWLaSnZ5CS0o4lSxYRHd2EPn3SSUpqE1D/1OMZGRm0bCk5q4QQQlTmcHkoKHH6v8CXKDvaZiYy3IROq6XU7mbP4SL2ZBeTlV3s2yXsRGJorVaDBg25hXac7ppvCtWsSShtE23YQo0Y9Fp0Wi16nQZV9eVoKrX7vkrK3USEmejWtkmVu7HVRpTVzCPjLuG9xdv4edsx5i3fybKfD3K8yIGiBuYUtoYauSQlhu4pMbRsGk5OfjmHj5eRfbyMo/nltGoaztA+LWuUMyjKambclSmMuzLlrPt+qsY4e0ecPXk1Gym95HwSQojgaDxL4euM2WwmJCRwOvm2bVt45JFpXHrpZYwfP5GIiCg0Gg2TJt2K0+k6bXsmkwmDIXCKvcFgwOU6fd6G0ykp8SUNjYyMCiiPioo6cbwYgKlTH8JqfYtPPvmQN998jdjYOCZMuI3rrx9Vo+NCCCGCo6Tcxbb9BRwrsNOjXSxNo85tWZPbo3D4eCkHckrZn1PCgZwScvLtJ2f5JNpom2jzLwFTFBW7y0OZw8OxEwGUw7llHD4RRLH/YYbPqbQaDWEhBorLTv/3sILZqKN1vJXW8VZaxIXh8SqU2T2UOdyUOTwY9VraJNpomxjRYLuLmQw6Jl3bkeaxYfznhyyOFfqW8llDDLRsaqVl03A6J0WR3MyGVnvyYSgswUZygq26ZoU4axJ8aqRk5pMQQtQ/jUbDo+MuqbdldxXLwE6nPpbdVdXeqlUrCQsL46mnZvqTeR89eqROr1sbVqsVgIKCgoDy/Px8AMLDfcfDwsK4//6/cP/9f2HPnt189tnHvPTSTJKSkunSpVvA8X379jB//r8DjgshhKg/uw8VsX5XLlv3FXAgp8Sf7Hrh6n2M7J/EoJ7Nz5jwuGL5e0Gpk70nZhxlHSnm4LESPFXs/L37UJE/KbUGsIUZcbi8OFxnno1kNuqIDDcRGW5CVSGv2LeUzauo/sBT06gQkhOspDSPoF1SE0pLHLjcXhRFxauqRIaZiI8ODQjYNFYajYZh6a3o0qYJxwsdtIgLIzLcVOfPHULUhASfGim95HwSQoig0Gg0mIz1s/2sXq9F10geTp1OB3q9PuCBc/nyJQ3WnxYtWhIREcn333/LZZcN8Jd/990KDAYDHTp0rHROcnIb7rtvGosWfc2+fXsrBZfatGl72uNCCHG+KHe4yTpSTKjZQESYCWuoAT01y5sTDPnFDj7+7y5+25EbUJ4YE4rZqGf34SLmf7eb/+3M5bZh7YmLDEFRVPYeKWbDnuNs2ZtPUZkLu9OLw+XL61OVULOeFnHhtIwLp0VcGHFRIRzKLWXXoSJ2HSwkp8BOYWngbCWjXku0zexPmp0YE0Z8dAhRVnOVy7gUVaWo1EVhqZOYCIt/ptIf8w+dzxJjwkiMCWvoboiLnASfGimD7HYnhBCiDvXs2ZtPP/2YV16ZRf/+A9i8eSPLli2u9+sePnyY77//NqBMq9Vy2WUDufXW23n11ReJjIwiPT2DLVs28e9/f8CNN47176J3991/4tJLB5CUlIxOp2Xp0m8wGAz+wNKpxw0GPYsXLww4LoQQ55Myh5sVvxxkxa+HApaJafDl5mnTPIJL0+Lp1CrqnGfeqKrK0fxytu0vYNv+AsodHmyhRiLCTNjCfP+Ni7IQHx2K6US+H49XYcUvB1mQuQ+n24tWo6FXh1g6J0XToWUktjATqqryw4ZsPvluNzsPFfHEuz+TltyEHQcKKCl3V9sfvU5Ly6ZhJMXbSGpmJamZlSY2c6VZOq3jrVya1gyAolInBaVOLCY9FpOeEJPev2t4TWk1Gv9sKCFE/Wl0wac9e/bwzDPPsH79ekJDQxkxYgQPPPAARmP1WyquW7eOW265pcpjrVu3ZunSpf7vc3JyeOaZZ/jpp58wGAxceeWVPProo4SFNa5IsOx2J4QQoi6lp/fj7run8MUXn7J48UI6d+7CrFmvMnbsDfV63XXrVrNu3eqAMp1Oxw8/rGPUqDHo9Xrmz/83X375GdHRTbjttju55ZY/+et27tyFZcu+ITs7G61WQ1JSG55//hVatWpdxXEtSUnJAceFEKKhFJe5yNx0hPW7j6MovqTVOo0vcbXFpCcuykLTqBDio0KJCDPy48YjfPvbQexO3/KxKKtvaVhRqcs3O6fMxW/bj/Hb9mNEW80MvCSBS7s0q5RTSFVV8oud7MkuYs/hYvYeKcbjVbCY9JiNOsxGPYqqsvNgoT/x9ulogCYRZhKahJFTUO7fwa1too3xV6XSPDbwfZRGo+Hyrgl0ahXFu4u3sf1AIb9u921QYTHp6ZwURVpyNPHRoZiNOl/gyKg/q2XotjATtjAJGglxPtCoanWTHIOvqKiIYcOG0apVKyZNmkROTg4zZ87k2muv5fHHH6/2vNLSUnbv3l2p7M4772TChAk89thjgG+r6Btu8D1kT506FYfDwfPPP0+7du146623zrrfXq9Cfn7ZmSvWwr6jJTz1/i/ERFh4/s/pddq2qNqFNLX2fCDjHVwy3uB2u8jLO0J0dDwGQ/UfaNSlmuR8EnWnJuN9pp+DqKhQdLX81FzUXn08O8nvueCTMa9MUVW27Svgh98Ps37XcbxK7d9qJcSEcm1Ga7qnxqDVaFAUlRK7m6IyF5v25rN07X7K7L4ZRHqdhvAQIzqtBr3Ot4taqd1daSladfQ6LW0SrLRvFUUTq5miMt/ys8JS345wR/LKKbUHzlayhhi4cUAb+nZqesZgkaKqrN1ylKP55bRvGUXbRFutZyY1FPn5Dj4Z8+Cqq/Gu6bNTo5r5NH/+fMrKynjjjTeIiIgAwOv1MmPGDCZNmkRcXFyV54WFhdG1a9eAsv/85z8oisLw4cP9ZcuWLWPXrl0sXryYpCTfVsxWq5Xbb7+djRs3kpaWVi/3dTb0et8vcren5tt4CiGEEEIIIere0fxyFmbuIybCTK/2cTRrEhpwvKDEyU+bjvDjhmyOFzn85cnNrGSkxWMLNaIovmCMV1EoLXdzNN83i+hofjkFJU4SY8K4NqMVl5wIOlXQajXYQo1E28x07xjPkF7Nydx0hO9+O8z+nJIqZy/ptBoSY8No08xGUoKVEJMeu8vjS8zt9OJVFFrHW2mTYMNoOH3ew+IyF4ePl5F9vAyvV6FfWrx/h7kz0Wo09O0UX6O6QogLW6MKPq1atYr09HR/4AlgyJAhPPHEE2RmZvpnLdXEokWLaNWqVUBAadWqVaSmpvoDTwAZGRlERETwww8/NKrgk0Hv+yPgrmKHByGEEEIIIcTZyy92+HdVy8oupqDEQe8OcQzp3TIgKbWqqvy06Qj/XrELp9v3ofCCzH0kxITSs10sTaNCWLP5KBuz8vxJsy0mPX07NqV/12aVlqRVx+NVajwjyGjQcWlaM/p1jie30I7d6cWjKHi9Kh6vgkGvpUVcuD9P07myhhqxhhpp3zKyTtoTQlycGlXwKSsri5EjRwaUWa1WYmJiyMrKqnE7x48fZ+3atdx9992V2j818AS+NcmtW7euVfvBYND5Pu2Q6YZCCCGEEELUnldR2Lg7jx0HCykuc1FU5vL/949LyQAWrd7PD79nM6Jfa/p3aYbL7WXu0h38ciJfUUrzCCxGHZv35nM4t4zDuXsDzk9JtHFpl2b0aBdb68DP2SxF02g0xEaG1Po8IYRoCI0q+FRcXIzVaq1UbrPZKCoqqnE7ixcvxuv1Biy5q2g/PDz8nNuvil5ft2uXTUbfS+PxKnXetqhaxTpVyfURHDLewSXjDYpybrsC1VbFigmNhmq3kBZ1p7bjrdNp5O+rEI1QucPDz9tz+GXbMVRVJSLcRGSYbyeyiDAT4SEGwkKMhFkMhJqr3tnseKGdVRuz+XHjEYqqyX2k1WhIjAklqZmV1s2sGHRavs7cR05+OR8u38mKXw/h8XjJK3ai1Wi4vn9rhvRuiVarodzhZv2u4/y87Ri5hXa6tW1Cv7R44qNDq7yWEEKIRhZ8qisLFy6kY8eOtG4dnJ1utFoNkZF1+8dGa/C9NF5FxWoLQXeOW6mKmrNaLQ3dhYuKjHdwXczj7XDoOH5cG/Sgw8Uc8GsIZxpvRdGg1Wqx2UIwm81B6pUQ4nQUVWXHgUJ+2pjNbztycdVi5r/F5NstLcSkx2LSowJ7DhVREYMODzHQs10sTWwWbGG+5WO2ECMxkZZKs5N6tIvlh9+z+fqnveTk+3Z0i4kwc9e1HUluZvPXCzEbyOgcT0ZnyWUkhBA11aiCT1arlZKSkkrlRUVF2Gy2Ks6o7MCBA2zcuJFHH320yvZLS0urbD8+/uz/eCiKSnFx+VmfXxW39+Qf3dzjJXW2ZltUT6fTYrVaKC624/XKcsf6JuMdXDLe4HI5USpyYgRhSbNG4xt3r1eRmU9BUNPx9npVFEWhqKgcu73yph5Wq0UChkIE0b6jxcxdsoP9OSffA8RHh3BpWjMiwo0UlDj9X4WlTkrtHkrLXZQ7PKiA3enF7vSST2DS7Q6tIrmsawLd2jap8ZI2vU7LFd0T6dupKSt+OYjD7eWavq0CckAJIYQ4O43qN2lSUlKl3EslJSXk5uZWytVUnYULF6LVahk6dGiV7e/cuTOgTFVV9u7dS0ZGxtl3nLrPzXTqrqUOpwfdGbYxFXXH61Uk11YQyXgH18U83t4gb+BQEQCRwFNw1Ha8gxWEFEJUzeny8uWPWaz49SCqCmajjt4d4uiXFk9SvBXNGZ59FUWl1OHG7vBQ7vRQ7vBgd3pwur20SbQRdw65kCwmPdf2C84KCiGEuFg0quBT//79mTNnTkDup6VLl6LVamscHPrmm2/o1asXsbGxVba/YMEC9u3bR6tWrQBYs2YNhYWFXHbZZXV2H3VBp9X481bIw7EQQgghhLgQKKrK5qx8Ply+g+NFDgB6d4hjzBVtsYUaa9yOVqvBGmLEGlLzc4QQQjScRhV8GjNmDPPmzWPy5MlMmjSJnJwcZs2axZgxY4iLi/PXmzhxItnZ2axYsSLg/K1bt7Jnzx5uu+22KtsfPHgwb731FlOmTGHatGnY7XZmzZrF5ZdfTlpaWr3eW21pNBoMOi0uj4Jbgk9CCCGEEKKBVORkyi20E24xEB5qxBpiINJqxqpUnm7ocHkoLHVRVOqkoNTJsQI7R/LKOXK8jKP55f6cTtFWExMGp5KW3CTYtySEECLIGlXwyWazMXfuXJ5++mkmT55MaGgoo0aNYurUqQH1fDk7KudpWLhwIUajkcGDB1fZvsFg4O233+aZZ55h2rRp6PV6rrzySh577LF6uZ9zZTDofMGnizQ/ixBCCCGEqF8er8KazUexOz0kxobRPDaM8BOziXIKysncdJQ1m4+QV+ystg0Nvpxrep0GVQWnu/Jz+qkMei0DuiVw3aWtMRsb1dsRIYQQ9aTR/bZPTk7m/fffP22defPmVVn+8MMP8/DDD5/23Li4OGbPnn223Qsqw4ndmDxBzlMihBCi8erXr8cZ6zz22BMMHXrNWV9j164drFq1knHjJp5xR7jFixfy7LMzWLToWyIiIs76mkKI4DuQU8K732zjwLHADXkiw02EWwwB5RaTnuRmVsocHkrKXRSXu3C5fR+QqviCWJ5TYk4mo46IMBMRoUaa2MzENwmlWXQo8U1CiLFZ0MpOzkIIcVFpdMEncdLJ4JPMfBJCCOEzZ857Ad//+c+3MWrUTQwadLW/LCEh8ZyusWvXTt5771+MHHnTGYNPQojzj8er8M2a/SxavQ+vohJmMdA20cah3FJyCx3+3eU0GujYOop+nePp1rYJBn3g7steVSU0zMzxvFKcTi8exffMag0xyg5xQgghAshfhUbMcGJbWMn5JIQQokKnTp0rlcXGNq2yXFwc9uzZwzPPPMP69esJDQ1lxIgRPPDAAxiNp0/EXFBQwCuvvMKqVasoLCwkMTGRcePGMXbs2Ep1V65cyZw5c9i+fTsGg4F27drxwgsv0LRp0/q6LVGHPF6F/GIHeUUOjhc5+Pa3Qxw8MavpkpQYJgxO9Sf7tjs9HMotJa/YQWrzSCLDTdW2azLoCA8x4nGa8JjleVUIIUT1JPjUiBkNvk+XJOeTEEKI2li8eCGffPIRBw8ewGq1MWTIcO6448/odL6/KyUlJbz55musWZNJcXERERGRdO6cxowZz/mX0QEMHz4IgKZN4/n884Vn3Z+jR4/wxhuv8Msv6/B6vaSldWXy5AdITm7jr/PTTz/w3ntvc+DAPnQ6HQkJzbnjjkmkp/er0fGLVVFRERMnTqRVq1bMnj2bnJwcZs6cicPh4PHHHz/tuffffz9ZWVlMmzaN+Ph4Vq1axZNPPolOp2P06NH+el9//TV/+9vf+NOf/sQDDzxAWVkZv/76K05n9TmARMOyOz1s3ZfPxj15bN1XQH6xgz8mcQizGBh3ZQq92sei0ZxcAmcx6WmbGEHb4HZZCCHEBU6CT42YvmLZncx8EkKIeqOqKnhc9dS2FvVMv8P1xoA3fudq/vwP+cc/ZjN69M3ce+8D7Nu3j3/+800UReHuu6cAMHv2y6xbt5o//3kKTZvGk5d3nLVrVwOQnt6PiRNvZ+7cd3jppdmEhoZhNBrOuj/l5WVMmTIJjUbDgw8+itFo4oMP3mXy5DuZO/dj4uKacvjwIaZPf5hBgwbz5z9PRlFUdu/eSUlJCcAZj1/M5s+fT1lZGW+88YY/55bX62XGjBlMmjQpYLfgU+Xm5rJu3Tqee+45brjhBgDS09PZtGkT33zzjT/4VFhYyFNPPcVjjz3GzTff7D//iiuuqN8bE2f044Zslv96EJ1GQ4hZT6jFQKhZT26hg50HC/H+YRc6g15LE5uZaKuZZk1CGdKnpX+2kxBCCFHfJPjUiFUsu5OcT0IIUT9UVaV8wd9RcnY3WB90cW2xXPtYnQSgysvLeOedf3LzzbcwadJkAHr27IPBoGf27Fe4+eYJ2GwRbNu2hUGDrmbIkOH+cwcN8u0UGxkZ6c8ZlZra/pyTiH/zzUKOHj3CvHmf0qpVawC6dbuEkSOH8+mnHzNlylR27tyOx+Nh2rSHCAkJBaB373R/G2c6fjFbtWoV6enpAa/TkCFDeOKJJ8jMzPQHlv7I4/EAEB4eHlAeFhZGeXm5//slS5agKAqjRo2q+86Ls6KqKl//tJcFmftOWy8u0kLn5GjSkqNpERtOeIihTgPdQgghRG1I8KkRq0g4LjmfhBCi/mi4cN6Mbdq0Ebu9nAEDrvAHFwB69OiN0+kkK2sP3bp1JyWlHUuWLCI6ugl9+qSTlNTmNK2emw0b1pOUlOwPPAFYrTZ69OjNxo2/A5Cc3BadTseTT07n2muvp2vXSwgLC/PXP9Pxi1lWVhYjR44MKLNarcTExJCVlVXtefHx8fTr1485c+bQunVrmjZtyqpVq8jMzOTFF1/019uwYQOtW7fmq6++4h//+Ac5OTm0bduWadOmcdlll9XbfYmqKYrKvOU7+OH3bACGpbckpXkEZQ43ZXYPZQ43ISY9nZOiiYsKaeDeCiGEECdJ8KkRk5xPQghRvzQaDZZrH6u3ZXd6vfbMS6frcNldUVEhAH/60/gqjx87lgPA1KkPYbW+xSeffMibb75GbGwcEybcxvXX1/3slpKSEiIjoyqVR0VFsXfvHgBatGjJ88+/wrx57/G3v/0VjUZD797pTJ36ME2bNj3j8YtZcXExVqu1UrnNZqOoqOi0586ePZupU6cybNgwAHQ6HdOnT2fw4MH+Orm5uezdu5fXXnuNv/71r8TExPDRRx9xzz338NVXX9G27dlnBqpIL1BXdCdmjFf890Ljcnv5x9eb+W1HLhpg4pB2DOx+bjtbnqsLfcwbGxnv4JLxDj4Z8+AK9nhL8KkRq5j55PH+MUWkEEKIuqLRaMBQ/W5O59S2XotGE7wPEMLDfUGIv//9hSpz/cTHNwN8S6vuv/8v3H//X9izZzefffYxL700k6SkZLp06VanfbJarRw4sL9SeX5+vr+/AH369KVPn76UlZWydu0aZs9+meeem8Frr/2jRsdF7aiqyqOPPsq+fft46aWXiImJYfXq1Tz77LPYbDZ/QEpVVcrLy3nxxRf9eZ569erF4MGD+de//sWsWbPO6vparYbIyNA6u59TWa2Wemm3Ie07Usybn29g2758DHotD47rTt+0Zg3dLb8LccwbMxnv4JLxDj4Z8+AK1nhL8KkR8+d8kmV3QgghaqBTpzTMZjO5uTlcdtmAGp2TnNyG++6bxqJFX7Nv3166dOmGXu9LMO5ynftuZmlpXVm58r8cOLCPFi1aAb7ZOr/++jPXXnt9pfqhoWFcccWVbN26mW+/XVbr4xcbq9VaZeL1oqIibDZbteetXLmSpUuXsmDBAlJTUwHo3bs3eXl5zJw50x98qphV1adPH/+5BoOBnj17smvXrrPut6KoFBeXn7liLeh0WqxWC8XFdrwXyKzx7ONlfLkqi5+35qACISY9D4zuQrvmNgoKyhq6exfkmDdmMt7BJeMdfDLmwVVX4221Wmo0e0qCT41YxXR0WXYnhBCiJsLDw7n99j/z5puzOXbsGN26dUen05GdfYgff1zF3/8+C7PZzN13/4lLLx1AUlIyOp2WpUu/wWAw+Gc9tWrVCoD//OczLr30csxmM8nJp88LlZm5ipCQwBwzSUltGDbsGj799N/89a8PcOedd/t3u9PpdIwePRaAr776gi1bNtG7dzrR0U04ciSb5cuX0KtX7xodv5glJSVVyu1UUlJCbm4uSUlJ1Z63e/dudDodKSkpAeXt27fns88+w263Y7FYaNOm+tfd6Ty34GR9fbjm9Srn/Qd3xwvtfPnjXtZuPYp6YgJ8z3ax3NA/ibiokEZ3fxfCmJ9PZLyDS8Y7+GTMgytY4y3Bp0asIueT/MMTQghRU2PHjicmJoZPPvmIL774BL1eT0JCIn37Xope7/uz37lzF5Yt+4bs7Gy0Wg1JSW14/vlX/EnBU1La8ac/3cWiRV/z739/QGxsHJ9/vvC0133uuacqld1xx5+59dY7mD37LWbPfplZs55FUbx07tyF//u/fxEX58vX1KZNW1av/pHZs1+huLiIqKhoBg0azJ13/rlGxy9m/fv3Z86cOQG5n5YuXYpWqyUjI6Pa8xISEvB6vezYsYN27dr5y7ds2UJ0dDQWi28K/oABA5g9ezZr1qxh0KBBALhcLn755Rd69OhRj3d28Vq/M5e3v9mK3ekFoFvbJlx3aRLNYyXJvhBCiPOXRlVVSSh0jrxehfz8up36rNdr+eyHLBb+mMWw9JaMvCy5TtsXlen1WiIjQykoKJOAXxDIeAeXjDe43S7y8o4QHR2PwWAMyjVrlHBc1JmajPeZfg6iokLPq0SnRUVFDBs2jNatWzNp0iRycnKYOXMm11xzDY8//ri/3sSJE8nOzmbFihUAlJaWcs0112AwGJg8eTKxsbH89NNPvPvuu0yZMoV77rnHf+59993HL7/8wl/+8hdiYmL497//zZo1a/jss8/8S/Zqq76enc7n33OKovLlj1l8s8aXIy05wcrNg1JoHV85oXxjcb6P+flGxju4ZLyDT8Y8uOpqvGv67CQznxqxipxPbvmHJ4QQQogq2Gw25s6dy9NPP83kyZMJDQ1l1KhRTJ06NaCeoih4vV7/92FhYbz//vu88sorvPjii5SUlJCYmMgjjzzC+PGBuyXOnDmTl19+mZdeeonS0lI6duzIe++9d9aBJ1FZcbmLfy7YwtZ9BQAM6pHI6AFt0J9HgVAhhBDidCT41Iid3O1Ogk9CCCGEqFpycjLvv//+aevMmzevUlnLli159dVXz9h+SEgI06dPZ/r06WfZQ1Gd/GIHv24/xvJfD5Jf7MRo0HLrkHb06dC0obsmhBBC1CkJPjViBoMEn4QQQgghLiSFpU5+2X6MX7YfY/ehIn95XKSFyTd0JjFGcjsJIYS48EjwqREz6HwJx90eScslhBBCCHE+yz5extJ1B1iz5ShexfdspwHaJtro0S6WjM7xWEzyaC6EEOLCJH/hGrGKZXdumfkkhBBCCHFe2nO4iMVr97N+13F/WXIzK706xNEjNZbIcFMD9k4IIYQIDgk+NWL+nE+ScFwIIeqMbPJ6cZPXXwSDqqps2ZfPN6v3s+Ngob+8W9smDO3TkuQEW8N1TgghhGgAEnxqxIyS80kIIeqM7sRSZpfLidEoMw0uVi6XEwCdTh6BRN1TFJX/7czlmzX72Z9TAoBOqyG9Y1Ou7t2CZk1CG7iHQgghRMOQJ69GrCLnkwSfhBDi3Gm1OiyWMEpLfVuZG40mNBpNvV5TUTR4vTLTJlhON96qquJyOSktLcBiCUOrlS3sRd3anJXHx//dxZG8csD3IeJlXRIY3Ks5UVZzA/dOCCGEaFgSfGrE9BU5n2TZnRBC1AmrNQrAH4Cqb1qtFkWR3+HBUpPxtljC/D8HQtSF/GIHH3+7i9925gIQYtJzRfdEBvVIJDzE2MC9E0IIIRoHCT41YpJwXAgh6pZGo8FmiyY8PBKv11Ov19LpNNhsIRQVlcvspyCoyXjrdHqZ8STqjMersPyXgyzI3IvLraDVaBjUI5FrM1oTYpZHbCGEEOJU8pexETuZ80netAghRF3SarVotfU7I0Gv12I2m7HbvbJxRBDIeItgUhSVl+b/7k8m3jbRxoSrUkmMDWvYjgkhhBCNVKMLPu3Zs4dnnnmG9evXExoayogRI3jggQcwGs/8JiEnJ4eXX36ZH374gfLychISErj77ru59tprATh06BBXXHFFpfO6dOnCp59+Wuf3cq78OZ/kIVoIIYQQotH44ffD7DhYiMmoY/yVKfTt1LTec8gJIYQQ57NGFXwqKipi4sSJtGrVitmzZ5OTk8PMmTNxOBw8/vjjpz332LFj3HTTTbRu3Zqnn36asLAwdu3ahcvlqlR32rRp9O7d2/99aGjj3HlElt0JIYQQQjQuRWUuPv8hC4CR/ZPI6BzfwD0SQgghGr9GFXyaP38+ZWVlvPHGG0RERADg9XqZMWMGkyZNIi4urtpzX3jhBZo2bcrbb7/t3047PT29yrotW7aka9eudd39OmeQhONCCCGEEI3Kp9/twu700DIunIGXJDZ0d4QQQojzQqPKurlq1SrS09P9gSeAIUOGoCgKmZmZ1Z5XWlrKkiVLuPnmm/2BpwuBwZ/zSYJPQgghhBANbdv+AtZsyUED3HJ1KlqtLLUTQgghaqJRBZ+ysrJISkoKKLNarcTExJCVlVXteVu2bMHtdqPX6xk/fjwdO3YkIyODF154AbfbXan+k08+Sfv27UlPT2f69OkUFhbW9a3UCX/OJwk+CSGEEEI0KI9X4cPlOwC4/JIEWsdbG7hHQgghxPmjUS27Ky4uxmqt/IfcZrNRVFRU7XnHjx8HYPr06YwePZp7772XjRs38vrrr6PVavnLX/4CgNFoZOzYsfTr1w+r1cqGDRuYM2cOmzdv5rPPPsNgMJx13/X6uo3j6XRaNJqTu91pdRq0ksiyXul02oD/ivol4x1cMt7BJ2MeXDLeor4tXXeAI3nlWEONjOyfdOYThBBCCOHXqIJPZ0tRfDOD+vbtyyOPPAJAnz59KCsr491332Xy5MmYzWZiY2N58skn/ef16tWLtm3bMmnSJFasWMHQoUPP6vparYbIyLpPWl5mPzlrKzzcgtFw4SwpbMysVktDd+GiIuMdXDLewSdjHlwy3qIuqapKbpGDXQcLWbh6HwA3DWxDiPnsP7AUQgghLkaNKvhktVopKSmpVF5UVITNZjvteeALOJ0qPT2dOXPmsH//flJTU6s897LLLiMkJIQtW7acdfBJUVSKi8vP6tzq6HRaLCFG//e5x0sJMTeql+uCo9NpsVotFBfb8cpSx3on4x1cMt7BJ2MeXHU13larRWZPCX7cmM3vu46z53ARxeUnPwxs3zKSPh2q3wBHCCGEEFVrVNGMpKSkSrmdSkpKyM3NrZQL6lRt2rQ5bbtOp7NO+nc6nnrYkU5/ysOvw+nBWMdL+0TVvF6lXl5PUTUZ7+CS8Q4+GfPgkvEW52rP4SLeW7zd/71Oq6FFXDgpzW0M7dMSjaRBEEIIIWqtUQWf+vfvz5w5cwJyPy1duhStVktGRka15yUkJJCSksLq1asZP368v3z16tWYzebTBqe+//57ysvL6dy5c93dSB3RaDTodRo8XhW3PEgLIYQQQtS75b8cBKBzUjTX9G1Fy6ZhGPSS+kAIIYQ4F40q+DRmzBjmzZvH5MmTmTRpEjk5OcyaNYsxY8YQF3dyivPEiRPJzs5mxYoV/rKpU6dyzz338Pe//53LL7+cTZs28e6773L77bcTEhICwMyZM9FoNHTt2hWr1crGjRt566236NSpE4MGDQr6/daEXqfF4/XKjndCCCGEEPUsr8jBbztyARh5WRIt4sIbuEdCCCHEhaFRBZ9sNhtz587l6aefZvLkyYSGhjJq1CimTp0aUE9RFLxeb0DZwIEDefnll3nzzTf5+OOPiY2NZcqUKdx1113+OsnJyXz88cd8+umnOBwO4uLiGDVqFPfddx96faMaCj+DXovD5cUtwSchhBBCiHr13f8Ooagq7VtGSuBJCCGEqEONLuKSnJzM+++/f9o68+bNq7J86NChp00afuONN3LjjTeeS/eCznAi75PMfBJCCCGEqD8Ol4cffs8G4MoezRu4N0IIIcSFRTJYN3L6E0nGJeeTEEIIIUT9Wb35KOVOD7GRFtLaRDd0d4QQQogLigSfGjn/zCcJPgkhhBBC1AtFVVnx6yHAN+tJKzvaCSGEEHVKgk+NnKFi5pNXbeCeCCGEEEJcmDZn5ZGTX47FpCejc9OG7s55QVVV3Nnb8ZQWNnRXhBBCnAcaXc4nEahi2Z3kfBJCCCGEqB/LfzkIQP8u8ZiN8nhcE+4t/8W5+kPKLeGEXDUFTVxKQ3dJCCFEIyYznxq5imV3kvNJCCGEEKLuHcotZeu+AjQauOKSxIbuTr1SygtxZ/2Ce2cmqsd19u0UHsW57lPf/9tLKF34PK5tK+uol6CU5lG+8DkcP85FdTuqr1deiFJeWGfXbWxUVVY+CCEuHPLRTiOnl93uhBBCCCHqzbcncj11T4mhSYQl6NdXXeV4DmxEl9ABrcVat20rHjy71uA5vBVvzm7Uklz/Mc36BZgvvQ19s3a1bNOLfeW/wOtCn9gBkzWSsq2ZOH98H6XgMKY+Y9BodWffZ48T+7LXUfL24z2yA0/2NixX3I2uScuTddwOXP9bgGvTMtAZCLnm0YDjtbqequDe+j2evb+ijUpE37oHuri2aLQN+xm9a9NynL99hbHLEIxdhp2xP6qi4D28Bfeu1aiOEsyXTkQbHnPW11dVBe/Bzehik9CYw866ncbKe3w/KB50sckN3RUhLhoSfGrkTuZ8kuCTEEIIIURdUhSV/+30BWQGdEsI6rVVlx3X5uW4Ni4DVzmakAjMgyajb9q2btr3unF8+yae/etPKdWgjUpEdZSgFuVgXzQTQ7vLMfW+EY0ptEbtujYuQTm2BwwWQgfcSVTz5hwJa4rj5y9wb16Bkn8IQ4eB6Jq2RRsSUbs+qyqOVe+h5O1HYw4HnQG16CjlXz2Nqc8YDB2vwJP1M86181HLCnwnKV7sy14j5PrHq7yeJ3s73iM70Le+BF1U84Bj3oJsnKvew5uzy/d99jbcm1egsVjRt7wEfYsuaJu0QBMahaYOk9DbV/4L5fh+LMMfRmsOrzwOLjvOX78Etx3XL1/gzd6GecBdYI0KrKeqKPmHcO/KxLN7Leops8DKFzxHyPCH0NrOLoeZc92nuDcuRRMeQ8h1/6/OA6N1RSnNx3NoE2p5Eaq9yPdfZxn6Vpdg6DioytfNc+B37MteB1XB2GuUL7h3lq+vUpSDO+sXDKn9av3zXluqquDZ+xu4HehT+tXpz6QQwSDBp0bOn/NJlt0JIYQQQtSpPdlFlNrdhJj0tG0eUeftqx4n3qO7QatFYzCDwYRGa8C9Zy2ujUvBWearqNOjlhdiXzgTU/qYat801+a69uWz8R7aDDo9xrQh6OJT0cUmozFaUJ1lONd9hnv7StzbV+I58Dvm/n9C3yLttO168w7i+vVLAMwZ49CGR6PRaLD0GAG2Zji+/yfe7G14s7cBoAlvgi6uLbqmbdHFtUEbmXjaGTzujUvx7F4LGi3mQfegi2qO44d38Oxfj3P1h7g2LkEtzTvRdgymniNx/fYVStFR7MtfJ2T4I2j0Rt8YqCruTctxrpsPqorrty/RRrfA0DYDfVIP3Dt/wvW/haB4wGDGmHY1SnEunv3rUe3F/rEBwBSKLioRbXQLtBHxaK2xaK2xaMKi0Wh1qC47SvExlOJjqCW5aGNao2/Wvsp79GRvw7Mz88T9LsPUa1TlcdiZCW47GosV1e3Ae3gr5V88juaKu1Aj+uA5loVz96949v6KUnTUf57GFIY+uTfe7K0ohUcoXzgTy7C/oousXWDVs3897o1LfeNYkusL7g1/2D+2waCqKnicoDei0VT9M+M9uovypa+Aq7zysextKKV5mHrfFPBvyZuzG/uKN0H1vbdy/fw5avFxTP0m1HrGnnvvbzhW/gvcDjy71xIy4jE0xpBatVFTnsNbca77BOX4fgAsITb0zU//7/ViobrsKPmH0EbE1/ssPVVV8ez/H7id6JN7n9Msz2BTVRVULxptw4WAJPjUyBl0vl+WMvNJCCGEEFXZs2cPzzzzDOvXryc0NJQRI0bwwAMPYDSe/o1iQUEBr7zyCqtWraKwsJDExETGjRvH2LFjq6yvKAqjRo1iy5YtvPbaa1x99dX1cTtB9fvu4wB0To72pzqoS841H+M+TS4kbUQ8xktGoG/RBceq93yzelZ/hPfYHsyX3obGYKr1NVW3A/vSV/Ee2Q56I5bBD6BP6BBQR2MKxdz/VvRt+uD48T3fLKjlr/mWr8W1qbpdrwfHyn+C4kXfshv6thkBxw2tu6ONeBz3lu/w5uxCyT+IWnIcT8lxPLvXnKhkQReX7AtIJbT3Lek68UbIc3ATzp99eaRM6Tf7gzfmq+7DveVbnGs/8QWedAaMXYdh7DIUjd6ILqY1ZV89hXIsC8cP72IeOAkUD44fP8Cz80ffODdpiZJ/CCXvAM68AzjXfuzvt655mm+JWli0/z69R7bj2fsr3pzdKAVHwFmG98gOvEd2BA6KRgdG88kgYgWtntBRz6CNCJx1pKoqrt++8n/v2vItxi5DAmadqaqCa8sKAIyXXIuuWQcc//0HSv5BShe9iH1lFN7S/IBr6Vt0QZ+Sgb55GhqdHqW8CPviF1DyD2GvCEBFt0D1uPAe2oJ7768o+QcwdrsWQ1LPgD4qJcexr3wbAH1ybzyHNqMc24Pju7cwD5pcL8sRVVXBs+dnPFm/oFTMXiovAq8LTVg05n4T0LfoGnCO58AG7Cv+D7wutJEJvsCqxYomxIZqL8a1fqEvgOZxYcoYj0ajxVuQ7QtWeV3omqehT+yIc8183NtXopTlY7nibjTGMy+9VRUvrl++wLVh8YkSDUrBIewr3sBy9TQ0usC32KrXjWfvr6CqvqClLQ6NKeyMAWZVVVEKDuNc9ynegxsDjrm3r6px8ElVPL5AtN6ELr5dUGZMqapSbdCw7q6h4tm1Gue6T1DtxQBoI5qha9oGXVxbtBHxYDD7fo8azGgMZjQ6w1lfT7EX4/xxLp59v/mutX4Rpt43omvRtc7G1Ht8P579v/tn8Sn2IlRHCRq9CU2IDY3FitZi8y0RTu5T43+PqsuOfdmrKIVHCB39XI1nutY1CT41cv6cTzLzSQghhBB/UFRUxMSJE2nVqhWzZ88mJyeHmTNn4nA4ePzxx0977v33309WVhbTpk0jPj6eVatW8eSTT6LT6Rg9enSl+vPnzycnJ6e+bqVBbNjtm0XTpU10vbTvObgJ8M0AQlFQPU5wO9Da4jB2HR7w5sF8xd2445Jxrv0Ez+61lOcdwnL1/dXm7XFn/Yx720o0Fhu66Oa+WTnWWOzf/xMlZzcYzFiGTEPftPpd6PTN2hE68mkc383Bs+9/2JfPJuSGJ9GGRlaq6/r1Pyh5B9GYwjBdemuVb7Z0kQno+k0AfG92vMey8Obswnt0F95je8Btx3tos++N8G9fgsGCPqE9uqapOP/3NagqhtRLMXS8wt+mRqPB2OlKdPGpePb+hiGlH1rryTHR2uKwXHkv9m9exLNnLc7QCJScPb6ldBqNb7lep6vAWeYbs52ZKMf2oDGHY+o7zjd74ZR70ej06BM7oU/s5LsPrxulIBsl7wDe/EMoRTmoJcdQinPB6/YHnjTmcDTWGFRnOWrRURyrP8Qy5C+Bs26yt/kCWDo9mtBo1OIcXFu+xXTJiJN1Dm5ELcoBowVDSj80BjMh1/0/nGvn4976nS/wpDehb5GGvlV39C26VAqYaENshAx/hPLFL6Ic30f5oufRN2vv+3n0OP31HN/+H960qzH1utE3g8vrwf7fN8FZhjamNebL78R7bA/2b17As+83nOs+wZxedXAaTuTP2vEjnr2/ootuiSElwxcAOA1P9nbfjJ7cvVW3WZqHfemr6JN7Y+o7Dq3FinvXahwr3wHVi655GpYrJ6PRBwZqNWHROH+ci3vrd+B1Y7xkBPbFL/ruLTYJy6DJaAwmNOExOP47B+/BjZQvnIm5/22+pZbVBE4UezGO//7DP7vP0HkwhuTelC96Hu/hrTh+fA/zZXf4X3dv3gEc3/8TJf9QYEMGC9qwKAi4jorq9YDH6Uu073b6Z2ih0WHocDn6Vt2xfzMLz771KPbi0y6HVErzfTP4tv2Aai8CQJfQEXPG+DO+LtXxBcQOoZYWoNpPBEfKi1HtxQEBE1x29C27Yuo73nefdcx7fD/OzA/9S2YxWMBtRynMRinMxr19VeWTNBpM6Tdj7HRlra/n2bfeF6i3F4NWBwYzSmE29mWvoYtPxdRnDLqY1ud0T54jO7B/84JvNuYfqAB5gWXGwiOYeo48Y7u+PHqv4j2yw7ecuQFnPmlU2UbhnHm9Cvn5ZWeuWAt6vZbIyFBe+/g3vv31EMP7tuKG/kl1eg0RqGLMCwrKJNgXBDLewSXjHXwy5sFVV+MdFRWKrh5mwNSXt956izlz5vD9998TEREBwCeffMKMGTP4/vvviYuLq/K83Nxc+vXrx3PPPccNN9zgLx8/fjw6nY65c+cG1M/Pz2fIkCE89NBDPPbYY+c886k+n51q+jNwrNDOI3PWoNVoeO3+foSaz/4T8aoopfmU/XsaaLSE3fqmb9ldDXiO7MDx7Zuo9iI0FiuWq6cGvKlRVRXX74tw/fJF9Y0YQwgZ+iC62Jo9O6puB+VfPYNScAhtbFLg8jXFg3P1v31v4gHzoMn+2TK1GXNVUVDyD+LN2e2bRXR4K6qzNKCONjaJkGsePavZCa7tP+Bc9d7JAqMFyxX3oG/euVJdpbwIjdFyTsvIVFVBLStEdZWhDWviDwApRTmUff438HoCxkpVVcoX/B0lZzeGTleii03G8d0cMIUSNvZF//nl37yA9/AWDGlXY+4zJvCaOTsINYEzIhlvDeYQqM4yype+4gtGnqAJjULfujsA7s2+GVa6+FTMV9yNa+NS32whYwihN8zwB/ncu9f6+gqY+o6r8s27N3cvjp/moeRmBZRrY5IwpPRFn9jZ96a9om+OEpy/fY33wO++AoMZY6cr0TZpidZiQxNiA4MZ14bFuDctA1UFUyiG1j39yyH1bdIxX357tcuI3Dszcfzwtu9cvRE8LrQR8YRc+7eA5VneY1nYl76C6ijxjZE5HF1iR4wtOhPRIpnCA3twH9vv+/nN3QduO+hNmC+7HUNyL+DETKxlr/nySHW/DmO3a3FtXOxbpqp40ZjD0UY28y3NrMhXVkP6Vt0x9brRP5Ou7MunUHKzMPW5CWPakEr1leJjviD2/vX+4JXGYkV1lYPXA1odxs6DMV5ybY1/L4FvVpxj5du+WZU1ZTBj6j0aQ/vLazQT6ky/U1Sv2x+I9b2uJoyXXIux82BUV7lvtmLObt9/ywrA7UB1O8Hr8vcndMysGucwU+zFuH7+DPeOE7MoIxMxD7gTbXgTXL9/g2vzct+YAsZu12DsccNZzYJSCo9Q9vUz4CxD1zQFXbN2vpl8Ft9sJ9zOk8G+ohxffzRaQq5/4rSbLaheD/blr/tmzhkshFzzMLomrfzHg/3sJDOfGrmKhOOy250QQggh/mjVqlWkp6f7A08AQ4YM4YknniAzMzMgsHQqj8f3sBweHpjsOCwsjPLyyvlTXn75ZXr37k3v3r3rrvMNbMOJJXdtE211HngCX24ZAG1081q9wdPHpxJy/RPYl76Ckn+Q8oXP+YIoLbv6AkE/feD/VN/QfgCa0Ej/rBy1KAeNJRzLkL/Uavc3jcGMZfB9lH05A+VYFs7MeZj6/wmcZdi//b8Tszw0GHvdWGmZVo2vodWia9LS16+OV/iCUcf34TkxE0pVFd9slLNcFmNsdxlKQTbuTcvQ2OIIGfxAtbM7tCG2s7rGqTQaLZqwKCBwVofWFoexyzBc//sa55p/o2/eGY3BjPfQZl8Q6MSyQY3Ziua3L31vJLd9j7HLULz5h/Ee3gIaDcZTZn9VMCS0JzQyFFdBGdTgjaLGFErIkL/g/PU/aPQm9K27o23Syv/mWNc0BccP7+A9soPyz/+fP/hivvz2gNllhjZ9UEqO4/rlc5yrP8K1aZlv6WRcG3QxrXBvX3XiZ1L1B5G8eQfwHtyEkpuFMzcLZ1UdBNBoMbS/HOMlI6p8Xcx9xmBI7o1j1bsoeQf9gSdDpysxpY89bUDDkJIBegOO/74FHhea0EgsQx+slBdIF5t0YnbZJ3gOb0F1lODZvdY3A7GKdrUR8ZivvDcgl5a+RRdMGRNw/jQX129f4dnzM0phtu9Yq0swXXqrP+ChelwoJbmoZYWcmNNySmcMvhlZ+hM54oyWSr8/DO3648zNwr19FYbOVwcEO1TFi3356/6ZVrr4dhg6DETf6hLU0jwca/6N98AGX1Bv9xosgyZXu9TW36aq4tn5E47VH4HbAToD2oimJwIjNrQnloP5loX5vvC6cGR+iHJsD86fPsCzey2mXjeiOkrx5h9AyTuIkn8ITYgNfdu+GJJ6njFflqp4AjZR0Cf1wtRnjH9mlcZiRdvqEmh1SRXnein/6mmU4/tw/e9rzBkTqr+Ox4XnwO+4d2biPbjpRABPgyHtakw9rvcHrU29R2PoeAXOX77As2s1rvULQWfAdMm1p72PP1LsxZQvefnkrLyhf6k0k69SH112PHt/xfHD24Rc90SlpZ4V9+z4zjerD70Ry5BpAYGnhiDBp0auYtmdWz41F0IIIcQfZGVlMXJk4LR7q9VKTEwMWVlZ1ZwF8fHx9OvXjzlz5tC6dWuaNm3KqlWryMzM5MUXXwyou3HjRhYtWsSiRYvq5R4aysYTwacubZrUS/sVy0HO9MauKtqwKEKufcwX+Dm0Gfvy1zD1HoPn0CbfkjWNBlP6OIydBgWcp7qdoNGc1YwerTUWyxV3Y1/yEu4dP6Ixh+PO+gW1JNe3hG/gJPQtu9W63epotFp0sUm+2Vm1fLNWHVOfMRiSeqKNSqxVwK+uGbsOw71rNWpJLs7fvsbUezTO33yJ2g0dBvp3RTN1HY7jh3dwbVyKoeMg3JuXA76ZLtUtt6wtjdGCue+4Ko8Zknqii0rEvmI2SoEvUGLodBWGVt2rvCfVWYp707LKubxO0LdJx9TnJv/9KfZiPLvX4t612t/+yY5p0Cd2wthrJLqIZqe9B11Ma0Kuf8I3M2vr9xg6DPTl/KrBDBNDUi80xhDcOzMxdrvGn9vrj7TWWCxXTfHl/MrZ7Vseengzalk+2ohmaKKao4s6sbw1OrHK2VbGDgNQS3JxbVjsCzwZLJgzxqFvmxG4tFNv9AWuapkI3n9Pyb1xrvk3SuERlJzd6E7ZIdO99Xtf4MkUSsg1jwTs8KixxRFy9VQ8+9fjWP1v1JJcHKs/IvT6J6q9lmIvxrnqPX/ARxvXBsvld6K1VT2r9lQh1/4N99b/4vz5c7xHd1K+4O+VKxUdxXtkB87MeehbXoK5XT9UW59K1VRVwbHSt/kAOj2WK++tlAfsdDRaHabeo7F/Mwv31pW+WXZ/2AlS9bhw/vyZL+H/KUnstTGtMfUZgz4+tVK72rBoLAPuwhXdEufaj3H9+h80BjPGzlfVqF+qx4l96auoJblowmOwDH7gjIEnAFO/W/Bmb0fJO4jr928wdR8RcFxVFRw/vOvLNabVY7nqvjrbSfVcSPCpkTPoZOaTEEIIIapWXFyM1Vp5+YDNZqOoqOi0586ePZupU6cybNgwAHQ6HdOnT2fw4MH+OoqiMGPGDG677TYSExM5dOhQdc3VWsWOvnWlYsp/Tab+250eth8oBKB7u5iz6ov78FaUomMY219W5Ztg5UTwydgs9ezuVR+Kftg0yn94H9f2VSeTZOuNhF55D8bWlT/dR3/mZMmnvWSrNEgfg331x/5kytrwJoQNnYYuOrFS/dqMedAkVJ/jKmj0ZkIvnUDp4pdxb16OPtSKciwL9EZCug9He+LnQdcuA9f/vkYpOY5n42Lcu1YDYO4yuMqfmXoZ7yYJGEY9iX3tZ6heNyEZY9BU076h382ova7Hk7MHz5GdeI7uwntsry/vVsbNGJr94c15eATGbldDt7rYnMCIoce10KP2gUp9qzTMrWq4M5zeiKFFB2jRAZ1uDFarheJiO94avhfT9R2NRqtBKc3D0mc0uvB6CG7rQzG26Y1r+494dq7ClOgbd8Veguu3/wAQ0nsUptiqZz/qk7tjbNaWorkP+HJt5e9HH1s5X5FSXkTpF4+jlheCVoel10hMXYfWIum8FkPXwZiSu2P/cR7uQ1vQ2eLQRbdA16QFuqgEvHkHcW7/CaXgMJ6snynN+hnXr60wp9+EPqEjcGLJ6qoPfcFOrY7QwVMwtqp9IFzfshPuFml4DmzE9et/CBt8r/+YqiiUff8W7r2+ZOKa0ChMqRkYU/qiizpzkFB/yRA0XieOX/6Dc82/0ZktmNpfBoC3OBfX1pU4t/8IqnLi/puji26BO+sXlNwsNKZQwq95EF14RM1uJjwC+k+gbMU/cK1fgLlND3TRvkCj5/gB34YLR3aCRkvoVZMxVvPzH+zf4RJ8auT8y+5k5pMQQggh6oiqqjz66KPs27ePl156iZiYGFavXs2zzz6LzWbzB6Q+++wzjh8/zl133VWn19dqNURG1s9uO1brmQMwWzdk41VUmjUJpUOb2FpfQ3GWs3/Jq6guB7bmrbC06Bh43GWn4PgBAKLbdUFvPft7jbzhPgozEyj44WN0oTbiRj+GuVntZ1PVlHr5SHJLsind9APmFh2JG/kgupDT50epyZhfdCIzUHb/SPnOX7Cv+QQAW48hRCcEzvLRZ9zA8aX/xPHrVwAYmyYR06HbaWf11P14h8K1f6553bgmkHbhLME9k1qP99A/1U9HTmHpdTXZ23/EvednbMPvQmsKIXfNPFRnOcbYVsRlDENzSo6tSiJD8bTvQ9mWn2D3j0SmdqpUJe9/X6CWF6KPbErcyL9iimt1dp2NDIVx01FVtYqf63TUATfiOrqXkk0rKd34Pa5j+3B9/TyWpC5EDbyF0s0/4NryHaAhdsT9hHXIqOIiNRNy1UQOv/0g7j0/Yyk/jDkhBVVVOb7kROBJpyfuuqmEpPaq9W59EVfeTL7WQ9G6BZR//y4mbxnO7F2U7/4fpy6v9BzajOfQZv/3Gp2B+Jsexdy8dr/X1Z5XkLP/N8p3/ozjh3eJHzudgh8/o+S3paAqaAwmYobdTVjHS8/YVrB+h0vwqZGr+NTDLTOfhBBCCPEHVquVkpKSSuVFRUXYbNXntVm5ciVLly5lwYIFpKb6PjXv3bs3eXl5zJw5k2HDhlFWVsbLL7/M1KlTcbvduN1uSkt9CaIdDgelpaWEhYVVe43TURSV4uKqsqmcPZ1OW+NZCj/97pvB1Tk5moKC2ic+d27+DtXlACD/958ICW8VcNx9eKvv4T80ihKvBc7iGgE6DsEan4YmJAK7ORT7ubZ3Bvp+txHefhC6qESKnTr/jm5/VJsxvxjpe4+FrA3gcYHeBO2vrPTzprbohSb0M38San2HQRQWVv1vQ8Y7uBrzeKuhiWgj4lEKj3Ds1+/QNWlFyfpvATD2HUdhkeOMbWja9IctP1Gy+Ud0PW4M2DVRcZRR9NsyAEzpYyk3xlBen793zHHoet5ERJfheDZ+Q/GvS7BnbeBw1l/8VUIu/xPu+K5n9TvbzxiDsV0/XNt/5Njy9wkb8RiOX/6DY/0KQEPooLtxxXXGVWg/q+Y1l4zEWFKCa+v3FKya7y/XJ3bE1HEg2rAoXz604wfx5h1AKc3H0ncs9rAWZ/V73dB3PJr9W3Ad3cP+1+/y7cAJGJJ7EtJ3LO7wJqcdr7r6GbdaLZJw/EJgkJxPQgghhKhGUlJSpdxOJSUl5ObmkpRU/U5nu3fvRqfTkZISuESpffv2fPbZZ9jtdgoKCigsLOSJJ57giScCc4I8/PDDNGnShMzMzLPue33N6vZ6ldO2rSgqv+/y5XtKS4qudT9UVcWx5b/+711Zv2LoMzbgE33X4Z2AL99Tnd2nNR4VUIL1TBjRHK8CKGe+3pnG/KJlicLU4waca+dj7DoUxRBWxeunx5g2BOeaf/sSJrfuecaxlPEOrsY63obU/jjXfYJjy8oTOwmq6Nv0QRPbtmb9jW3rD2DZt2di7DDQf8i5cQW4HWijmqNJSAva/euNoTS58jZoexnlaz7Fk/ULAKb0m9GlXFon/TBcch2uXWvxZO+gdPk/8OxZ67tGv1vQtux+ztcw9p2Aqqh49q9H36YPxvYD/DsVAuiiW6P7w+rgs76m0Yop/WYcK/8FXjfaiHhMfcejT+yIQs3/XgTrZ1yCT42c3p/zST1DTSGEEEJcbPr378+cOXMCcj8tXboUrVZLRkb1SxMSEhLwer3s2LGDdu3a+cu3bNlCdHQ0FouFmJgYPvjgg4Dzjh8/zrRp05gyZQp9+/atn5uqZ1lHiim1u7GY9LRNrP2uZ0puFkreQdDpQaNDLctHyd3rS5x9gj/ZeCNI8CoaljHtavRJPdGERlVbx9BhIKrbgS4+9ax3+xMXH31KBs6fP/flbQLQmzD1vqnG52s0GgztL8e55mPc27737Z6p0aC6nbg3+ZLfG7sOq1Fi97qms8VhGTQZ7/H9qK5y9M3a11nb2rBojJ2uxLVhsT/wZOx+PcYOA+qkfY1Wi7n/rcCtddLemejb9sXkdoBGgyG1f5U73zUWjbdnAjgl51Mjm+ophBBCiIY3ZswY5s2bx+TJk5k0aRI5OTnMmjWLMWPGEBd3cjeiiRMnkp2dzYoVKwBf0KpZs2bcd999TJ48mdjYWH766Se+/PJLpkyZAoDJZKJ378C8LhUJx9u0acMll1SR8Po8sOHELnedk6L8H/LVhmvrSsC3zTdeD56sn/Hs+80ffFJVBW/ObgB0cRJ8ElS7w1oFjU5f6+3ZhdBarOhbdsWzz5ck23jJNWhDI2vVhqFtBs6fP0PJO4iSm4UuNhn39h9QnaVowmPQJ/Wsj67XmK5J1UnTz5Wx6zBc238AZ5lv98Tz+N+fRqPB2PGKhu5GjTSirSlEVfw5nxrhVE8hhBBCNCybzcbcuXPR6XRMnjyZl156iVGjRvHII48E1FMUBa/X6/8+LCyM999/nw4dOvDiiy9y991388MPP/DII48wadKkYN9GUP1+IvjUpU3Vu1CpqoJr4zLcu9dWPuYsw7NnHQCG9gPQt/ZtSe/e+yuq6pulrhRkg8sOehPa6OaV2hBCiLpiOLFUTmNrirHz4DPUrkxjDkOf5PuQwbV1JarXg2vjUgCMXYaePmn5eUxjCiVkyF8wXXorpr7jG2R218VIZj41cv6cTzLzSQghhBBVSE5O5v333z9tnXnz5lUqa9myJa+++mqtrpWYmMiOHTtqdU5jcrzQzuHcMjQa6JxU9WwU9+YVONd+7PtG8WJIObl80b1rNXhdaCMT0cW1AbcDtHrUohyUgmzf1uFHTyy5i026YN+4CSEaB31iRyzXPobWGnvWSzaN7S/HsysTz551uKMSUMvy0YREBPzuuxDpYpMClkuL+icznxo5WXYnhBBCCFE3tu337SiWnGAjzFL5jZo3/yDOnz/zf+9Y9R6eo77k4aqq4t62EgBD+8vRaDRojBZ0iR0B8Oz71deG5HsSQgSRvmkK2pCIsz5fG9cGbWQieF04130CgLHzYDR6Yx31UAgfCT41cv6E47LsTgghhBDinBw4VgpAcjNrpWOqx4Xju7fA60HXogv6Vt1B8eBYPhulJBdvzm6UgsOgM2Jom+4/z9C6BwCevb68KyfzPbWp79sRQohzptFoMHS43PeNqoIpFEP7yxuyS+IC1eiCT3v27OG2226ja9euZGRkMGvWLFwuV43OzcnJ4eGHH6ZPnz6kpaUxZMgQFixYEFCnpKSExx57jF69etGtWzfuu+8+jh07Vh+3UicMet/6U1l2J4QQQghxbg7mlADQPDas0jHnr/9ByT+ExhyOuf+fMA+4C22TlqiOEuxLX8V9Ig+KPrk3GlOo/zxdy66g0aLkHcCbsxu1+BigkeCTEOK8YWjbF07MdDJ2HITGaGngHokLUaPK+VRUVMTEiRNp1aoVs2fPJicnh5kzZ+JwOHj88cdPe+6xY8e46aabaN26NU8//TRhYWHs2rWrUuDqgQceYPfu3Tz55JOYTCZeffVV7rzzTr744gv0+kY1HIDMfBJCCCGEqAuqqnIw1zfzqUVseMAxz+Gt/uCS+bI/oQ2xAWC56n7Kv3oKpeCwb9YTVNqOW2sORxefijd7G861viUr2qgENMaQer0fIYSoKxpjCKb0m/Ee2oyx81UN3R1xgWpU0Zb58+dTVlbGG2+8QUREBABer5cZM2YwadKkgC2D/+iFF16gadOmvP322+h0vuSO6enpAXXWr1/PTz/9xDvvvEO/fv0AaN26NUOHDmX58uUMHTq0fm7sHFTkfHJ71QbuiRBCCCHE+et4kQO704tep6Fp9MnAkOosw7HyX4Avl5O+ZTf/MW1YFJar7qN84XPgdaONboE2pnWltvWtu+PN3nYy35PMehJCnGeM7S8HWW4n6lGjWna3atUq0tPT/YEngCFDhqAoCpmZmdWeV1paypIlS7j55pv9gafq2rdarWRknMzcn5SURPv27Vm1alWd3ENdq9jtThKOCyGEEEKcvQM5vllPCU3C/DPLAZzrPkMtK0Bji8PUZ2yl83SxSZiv+LPveM8bqtySW9+qe+A5cZJsXAghhDhVowo+ZWVlkZQUuN2h1WolJiaGrKysas/bsmULbrcbvV7P+PHj6dixIxkZGbzwwgu43e6A9lu3bl3poSEpKem07TckfcXMJ4+CqsrsJyGEEEKIs3HwWOV8T6rixZ31MwDmfhPRGExVnmto1Z2wm55H36Jrlce1oZFoY5P938tOd0IIIUSgRrXsrri4GKu18u4jNpuNoqKias87fvw4ANOnT2f06NHce++9bNy4kddffx2tVstf/vIXf/vh4eGVzrfZbGzevPmc+l4RJKoruhOfyJmNJ18ijVYT8EmdqFsVY66TMQ4KGe/gkvEOPhnz4JLxFmdy8MROd83jTgafvMeywFUOplB08e3OqX1D6+44j+1BY7GhCY85p7aEEEKIC02jCj6dLUXxLUnr27cvjzzyCAB9+vShrKyMd999l8mTJ2M2m+vt+lqthsjI0DNXPAtRUSfbDQ0zE2I21Mt1xElWq+zuEEwy3sEl4x18MubBJeMtqlOx7K7FKTOfvAc2AKBP7IRGe26BS33qpXgObETfukeVS/OEEEKIi1mjCj5ZrVZKSkoqlRcVFWGz2U57HvgCTqdKT09nzpw57N+/n9TUVKxWK0ePHq11+2eiKCrFxeVnfX5VdDotVqsFe7nTX3Y8r5TwEGOdXkecVDHmxcV2vJJjq97JeAeXjHfwyZgHV12Nt9VqkdlTF6Ayh5u8YgcAzU/Z6c5zcBMA+uZp53wNrTmckGseOed2hBBCiAtRowo+VZV7qaSkhNzc3Eq5oE7Vps3pdxRxOp3+9tesWYOqqgGfSO3du5eUlJRz6Dl4PPXzxkJVVHRaDV5Fxe7wYDE2qpfsguT1KvX2eorKZLyDS8Y7+GTMg0vGW1Tl4IlZT01sZkLMvmcppbwQJW8/ALrETg3WNyGEEOJi0Kg+2uvfvz+rV6+muLjYX7Z06VK0Wm3ADnV/lJCQQEpKCqtXrw4oX716NWaz2R+c6t+/P0VFRaxZs8ZfZ+/evWzdupX+/fvX8d3UHb3seCeEEEIIcdb8+Z5OXXJ3YtaTNqY12pCznwEvhBBCiDNrVMGnMWPGEBoayuTJk/npp5/44osvmDVrFmPGjCEuLs5fb+LEiVx55ZUB506dOpXvvvuOv//972RmZjJnzhzeffddbr31VkJCQgDo1q0b/fr147HHHmPJkiV899133HfffaSmpnLVVVcF9V5rw1Cx451XdrsTQgghhKitAyd2umsRd+qSu40A6Jt3bpA+CSGEEBeTRrWGy2azMXfuXJ5++mkmT55MaGgoo0aNYurUqQH1FEXB6/UGlA0cOJCXX36ZN998k48//pjY2FimTJnCXXfdFVDv1Vdf5bnnnuPxxx/H4/HQr18/pk+fjl7fqIYigF7nWyIoywiEEEIIIWqvYtldxcwnVfHiOeTb6bgu8j0JIYQQ4vQaXcQlOTmZ999//7R15s2bV2X50KFDGTp06GnPDQ8P59lnn+XZZ5892y4GXcWyO7csuxNCCCHOSxs2bKBLly4N3Y2LksercPh4GXBypztvzm5w2dGYwtDGVJ9XVAghhBB1o1EtuxNVq1h2JzOfhBBCiPPTTTfdxODBg/m///s/Dh482NDduagcySvHq6hYTHqibWbgZL4nXfNOaLTyOCyEEELUN/lrex4wSMJxIYQQ4rz2wgsv0LJlS/7xj39w1VVXMWbMGD7++GMKCwsbumsXvAM5J/I9xYb5dzv2HNwAyJI7IYQQIlgk+HQe0Otl2Z0QQghxPrvmmmv45z//yapVq/jb3/4GwIwZM7j00ku55557WLp0KS6Xq4F7eWH64053SlkBSt5BQIMusVMD9kwIIYS4eDS6nE+iMn/OJ1l2J4QQQpzXoqKiGD9+POPHj+fAgQMsXLiQhQsXMnXqVMLDwxk8eDAjRoygR48eDd3VC0bFzKfmcSfyPZ1YcqeNaY3WYm2wfgkhhBAXE5n5dB4wVOx2JzOfhBBCiAuGyWTCYrFgMplQVRWNRsN///tfJkyYwMiRI9m9e3dDd/G8p6qqf+ZTi9hwADwHNwKgb965wfolhBBCXGxk5tN5wKDXAeDxqg3cEyGEEEKci9LSUpYtW8bChQv55Zdf0Gg09O/fn8mTJzNgwAC0Wi0rVqzg+eef59FHH+Wzzz5r6C6f1wpKnJQ5POi0Gpo1CUVVPHgObQFA30J2HxRCCCGCRYJP5wH9iZlPsuxOCCGEOD99++23LFy4kJUrV+J0OuncuTOPPfYYQ4cOJTIyMqDu1VdfTXFxMU899VQD9fbCcSDHN+spPjoEg16LJ3sHuO1ozOFoY1o1bOeEEEKIi4gEn84D/oTjEnwSQgghzkv33nsv8fHx3HrrrYwYMYKkpKTT1m/Xrh3XXHNNkHp34Tpw7ES+pxNL7lwblwKgb9UNjUayTwghhBDBIsGn80BFwnHJ+SSEEEKcn+bOnUvv3r1rXD8tLY20tLR67NHFwZ/vKS4Mb+5evAc2gEaDscvQBu6ZEEIIcXGRj3zOAwa9BJ+EEEKI81ltAk+i7hw8seyueWwYzt++BkDfJh2trWlDdksIIYS46Ejw6TxgODHzyS3BJyGEEOK89MorrzBixIhqj1933XW88cYbQezRxSGv2AFAU47jPfA7aDSYuslyRiGEECLYJPh0HqhYdic5n4QQQojz07Jly+jfv3+1xy+77DIWL14cxB5d+BRFxav4dgo2bPONrT65N9qI+IbslhBCCHFRkuDTeaBitzuPV23gngghhBDibBw5coQWLVpUezwxMZHs7Owg9ujCVzFjvJkuHw7+Dmgwdru2QfskhBBCXKwk+HQe8Od8kplPQgghxHkpJCSEw4cPV3v80KFDmEymIPbowlcxY/xqy0YA9Mm90EU2a8guCSGEEBctCT6dBwyy250QQghxXuvVqxeffPIJOTk5lY4dOXKETz755KyTku/Zs4fbbruNrl27kpGRwaxZs3C5XGc8r6CggMcff5zLL7+crl27Mnz4cD7++OOAOqtXr2bq1KkMHDiQLl26MHToUN5++23cbvdZ9TWYPF6FZroCuhgPABqMl8isJyGEEKKh6Bu6A+LM9HrJ+SSEEEKcz+6//35uvPFGhg0bxqhRo2jTpg0Au3bt4osvvkBVVe6///5at1tUVMTEiRNp1aoVs2fPJicnh5kzZ+JwOHj88cfP2KesrCymTZtGfHw8q1at4sknn0Sn0zF69GgA5s+fj8Ph4L777iM+Pp4NGzYwe/Zs9uzZw3PPPVf7gQgit0fhKvOJWU9JPdFFJjRwj4QQQoiLlwSfzgN62e1OCCGEOK8lJSXx0Ucf8cwzz/D+++8HHOvZsyd/+9vfSE5OrnW78+fPp6ysjDfeeIOIiAgAvF4vM2bMYNKkScTFxVV5Xm5uLuvWreO5557jhhtuACA9PZ1NmzbxzTff+INPTz75JFFRUf7zevfujaIovPrqq/z1r38NONbYuN0eOhsPAmDsOqyBeyOEEEJc3CT4dB7w53yS4JMQQghx3mrXrh0ffvgh+fn5HDp0CPAlGj+XAM6qVatIT0/3B54AhgwZwhNPPEFmZqY/sPRHHo8HgPDw8IDysLAwysvL/d9X1bf27dujqiq5ubmNOvjkLS9Gr1FQVNBGJTZ0d4QQQoiLmgSfzgMVOZ9k2Z0QQghx/ouKiqqzoE1WVhYjR44MKLNarcTExJCVlVXtefHx8fTr1485c+bQunVrmjZtyqpVq8jMzOTFF1887TX/97//YTQaSUxs3AEdpawQgDIs2LS6hu2MEEIIcZGT4NN5QC8Jx4UQQogLwtGjR9m6dSslJSWoqlrp+HXXXVer9oqLi7FarZXKbTYbRUVFpz139uzZTJ06lWHDfEvSdDod06dPZ/DgwdWes2/fPj744APGjBlDaGhorfr6RxU5LeuK7sTzUsV/NQ7f/ZdpQuv8WsLnj2Mu6peMd3DJeAefjHlwBXu8Jfh0HtDrNQC4PZUfUoUQQgjR+DmdTh5++GGWL1+OoihoNBp/8Emj0fjr1Tb4dLZUVeXRRx9l3759vPTSS8TExLB69WqeffZZbDabPyB1qtLSUqZMmUJiYiJTp049p+trtRoiI88teFUdq9UCgEkpA6BcG1pv1xI+FWMugkPGO7hkvINPxjy4gjXe5xR8ys7OJjs7mx49evjLtm/fzrvvvovL5WL48OEMGjTonDt5sTPIzCchhBDivPbyyy+zYsUKHnjgAbp168aECROYOXMmsbGxzJ07l2PHjvH888/Xul2r1UpJSUml8qKiImw2W7XnrVy5kqVLl7JgwQJSU1MBXzLxvLw8Zs6cWSn45HK5mDx5MkVFRXzyySeEhITUuq+nUhSV4uLyM1esBZ1Oi9VqobjYjter4Cw8TghQrgmloKCsTq8lfP445qJ+yXgHl4x38MmYB1ddjbfVaqnR7KlzCj4988wzlJeX+3dtOX78OLfccgtut5vQ0FCWLVvGa6+9xlVXXVXjNvfs2cMzzzzD+vXrCQ0NZcSIETzwwAMYjcbTnjdw4EAOHz5cqXzjxo2YTCYA1q1bxy233FKpztChQ3nllVdq3Mdgq5gqLjmfhBBCiPPTsmXLuOGGG7jrrrsoKCgAIC4ujvT0dPr27cstt9zCRx99xIwZM2rVblJSUqXcTiUlJeTm5pKUlFTtebt370an05GSkhJQ3r59ez777DPsdjsWi++TUEVRePDBB9myZQsfffQR8fHxtepjdTz19Fzj9Sq+tst9y+7s2rB6u5bw8Y+5CAoZ7+CS8Q4+GfPgCtZ4n1PwaePGjQHBnK+++gqHw8GiRYtITEzkjjvu4N13361x8KmoqIiJEyfSqlUrZs+eTU5ODjNnzsThcPD444+f8fzBgwfzpz/9KaCsqqDVc889F/BAFhkZWaP+NRSZ+SSEEEKc3/Ly8khLSwPAbDYDYLfb/ccHDx7M//3f/9U6+NS/f3/mzJkTkPtp6dKlaLVaMjIyqj0vISEBr9fLjh07aNeunb98y5YtREdH+wNPADNmzOD777/nnXfe8c+SOh/onL7gk0MX1sA9EUIIIcQ5BZ+KioqIjo72f79y5Up69uxJixYtALjyyitrNaNo/vz5lJWV8cYbb/i3DPZ6vcyYMYNJkyYRFxd32vObNGlC165dz3idtm3b0rlz5xr3q6FJwnEhhBDi/NakSRP/jCeLxYLNZmPv3r3+46WlpTidzlq3O2bMGObNm8fkyZOZNGkSOTk5zJo1izFjxgQ8N02cOJHs7GxWrFgB+IJWzZo147777mPy5MnExsby008/8eWXXzJlyhT/eXPmzGH+/PncfvvtGI1Gfv/9d/+xNm3aEBbWeAM7OlcxAE59eAP3RAghhBDnFHyKiooiOzsb8O228vvvv/Pggw/6j3u9XjweT43bW7VqFenp6f7AE8CQIUN44oknyMzM5IYbbjiX7p5XVFXFXXQMVQ3BULHsToJPQgghxHkpLS2N//3vf/7vBwwYwDvvvENMTAyKovD+++/X6AO0P7LZbMydO5enn36ayZMnExoayqhRoyolBFcUBa/X6/8+LCyM999/n1deeYUXX3yRkpISEhMTeeSRRxg/fry/XmZmJgDvvPMO77zzTkCbH3zwAb179651n4PF4PLlwnIaJPgkhBBCNLRzCj717duXefPmERYWxrp161BVlSuuuMJ/fPfu3bXKC5CVlcXIkSMDyqxWKzExMZXyGVRl4cKFfPrppxgMBnr06MGDDz5Y5fTwu+66i8LCQmJiYhg2bBj333+/fwp8Y+Hc/C2FP84j5IpJ6GMvASTnkxBCCHG+mjBhAkuXLsXlcmE0Grn//vtZv349Dz30EAAtWrTgb3/721m1nZyc7M+/WZ158+ZVKmvZsiWvvvpqrc87H6iKgsFdCoDH0HhnZwkhhBAXi3MKPv3lL39h7969PP/88xgMBh566CGaN28O+HZFWbJkCddcc02N2zs1X8GpbDYbRUVFpz134MCBpKWl0axZMw4ePMicOXO4+eab+eqrr/x9Cg8P54477qBnz56YTCbWrl3Lu+++S1ZWFm+99VYt7ryyiqTgdcXj9D0weY/swNy8JwCqChot6LR1ey3hU5GhvyaZ+sW5k/EOLhnv4JMxD67GPt49evQI2B04Pj6eJUuWsHPnTrRaLUlJSej15/RYJk6hOkrQoKCo4DXKzCchhBCioZ3TU06TJk2YP38+JSUlmEymgOTeiqIwd+5cmjZtes6drInp06f7/79Hjx5kZGQwZMgQ3nnnHZ588kkAOnToQIcOHfz10tPTiY2N5amnnmLjxo3+RKC1pdVqiIwMPaf+/1FJfAvsgKYsl5gmJx+awsIsmE3ycFqfrFbLmSuJOiPjHVwy3sEnYx5cjXG87XY7f/3rX7nqqqu49tpr/eVarTYg2beoO2p5IQClqhmdBPWEEEKIBlcnf43Dwyt/omQ2m2v9QGW1WikpKalUXlRUhM1mq1VbsbGxdO/enS1btpy23pAhQ3jqqafYvHnzWQefFEWluLj8rM6ttk2jbwc+V94RykpP7oaTm1dKmMVQp9cSPjqdFqvVQnGxHa/k16p3Mt7BJeMdfDLmwVVX4221Wup89pTFYmH16tX079+/TtsV1VPLfcndi5STuTOFEEII0XDOKfi0Zs0atmzZwh133OEv+/zzz3njjTdwuVwMHz6chx9+GJ1OV6P2kpKSKuV2KikpITc3l6SkpHPpar3z1HE+Jm14DABKaT6Ky4VG41t2Z3d4MBtqNp7i7Hi9Sp2/nqJ6Mt7BJeMdfDLmwdVYx7t79+6sX7+e0aNHN3RXLgpKuS9dQ7Fi8e8aLIQQQoiGc05/jWfPns327dv93+/YsYMnnniCqKgoevXqxbx58yrtjHI6/fv3Z/Xq1RQXF/vLli5dilarJSMjo1Z9y8nJ4bfffqNz586nrffNN98AnLFesGlMYWjNvqV8SnEuhhMPTh759FwIIcT/b+++w6Oo1geOf2e2ZVM2vVFD7xiUKkhHRFEsqFixggoWUO9Vr12uIv6sWMB2LVdF9NqoCjYUEaVIVyGhB9KTTd0yM78/lizGBEgg2WzC+3mePJLZmTNnT9bsybvveY9odB588EHWrl3Ls88+y8GDBxu6O02eUVIAQKFul8wnIYQQIgicUOZTWloaZ555pv/7zz//nPDwcN577z3sdjsPPvggn3/+OZMmTapRexMmTODdd99lypQpTJ48mczMTGbNmsWECRNITEz0nzdx4kQyMjJYtmwZAAsXLuTbb79lyJAhJCQksHfvXl599VVMJhPXXnut/7q77rqL1q1b07VrV3/B8bfeeouRI0cGX/BJUbBEJ+E6kIbuPIjZpOL26hJ8EkIIIRqh8847D03TePXVV/1zlL/WygTfe//atWsbqIdNS0XNJ6cRSrhkPgkhhBAN7oSCT2VlZYSHH96+9ocffmDQoEHY7b5inz169GDBggU1bi8yMpK3336bxx57jClTphAWFsb48eOZNm1apfN0XUfTNP/3LVq0ICsri8cff5yioiIiIiLo378/t912m3+nO4AOHTqwYMEC3nzzTTweD82bN+emm26qcXAs0MyHgk9GYRYWcwS4wBOESwmEEEIIcXSjR49GUZSG7sZJwx980u1ESeaTEEII0eBOKPiUnJzMpk2bGD9+PLt372b79u1cd911/scLCwurfKp3LO3ateOtt9466jnvvvtupe9TU1OrHKvO5MmTmTx5cq3605As0ckA6M5MzCZfwXWPZD4JIYQQjc7MmTMbugsnFf1Q8EkKjgshhBDB4YSCT+eeey4vvfQSmZmZ7Nixg8jISEaMGOF/fMuWLaSkpJxoH09alpgkAHRnFmazb+fAYCyiKoQQQggRTP6a+WSRZXdCCCFEgzuh4NNNN92Ex+Ph+++/Jzk5mZkzZ+JwOAAoKCjgl19+4eqrr66Tjp6M/JlPhZlYTL5Ufa9mNGSXhBBCCHEcPvvssxqdd/7559drP04Ghq5jHNrtTjKfhBBCiOBwQsEns9nMtGnTqtRkAoiKimLlypUn0vxJzxzty3wyivMIsfqCTrLsTgghhGh87rnnniM+9tdaUBJ8OnFGeREYOgZQZIRglswnIYQQosGdUPDpr0pKSvxbByclJREWFlZXTZ+0TGGRYAkBTznRahFgkmV3QgghRCP09ddfVzmm6zr79u3jgw8+ICMjgyeffLIBetb06CX5AJRiR0eVzCchhBAiCJxw8Gnjxo089dRTrFu3Dl33BUZUVeW0007j7rvvpkePHifcyZOVoiiYIhPQcvYQoziBaMl8EkIIIRqh5s2bV3u8ZcuWDBgwgEmTJvHf//6Xhx56KMA9a3oqltwV4fsgVDKfhBBCiIZ3Qu/GGzZs4Morr2Tr1q2MHz+ee++9l3vvvZfx48ezdetWrrzySjZu3FhXfT0pqZG+pXcxOAEpOC6EEEI0RUOHDmXx4sUN3Y0moSLzqciwA0jmkxBCCBEETijz6dlnnyUxMZH333+f+Pj4So/deuutXHbZZTz77LP85z//OaFOnsxMkYl4gCjD9ymeVzKfhBBCiCZn7969uN3uhu5Gk6CXFADg1EMBZLc7IYQQIgicUPBpw4YNTJkypUrgCSAuLo5LLrmEl19++URucdJTIxMAcOgFAHgk80kIIYRodH799ddqjzudTtasWcO7777LiBEjAtyrpkkvLQB8O90BmCXzSQghhGhwJxR8UlUVTdOO+Liu66iqvOGfCDUyEYAIrQCQ3e6EEEKIxuiqq66qtKtdBcMwMJlMnHXWWdx///0N0LOmxzi07K5ACwHAYqo67kIIIYQIrBMKPvXq1Yv33nuPsWPHVimkmZGRwfvvv8+pp556Qh082ZkOBZ/CtEJUdLya0cA9EkIIIURtvfPOO1WOKYqCw+GgefPmhIeHN0Cvmib9UMHxfO+h4JPZ1JDdEUIIIQQnGHyaPn06V1xxBWPGjGHUqFGkpKQAsHPnTr7++mtUVeXOO++si36etJTQKDBZUTU3sWqx1HwSQgghGqG+ffs2dBdOGro/88lXcNwsmU9CCCFEgzuh4FPXrl356KOPePbZZ/nmm28oKysDwG63c8YZZzB16lSio6PrpKMnK0VRUCMT0PP2EWcqkppPQgghRCO0d+9etm/fzvDhw6t9/JtvvqFjx460aNEiwD1rWgxdwziU+VRR80l2uxNCCCEa3gkFnwDat2/PSy+9hK7r5OXlARATE4Oqqrzyyiu88MILbNu27YQ7ejJTHYm+4JPqlJpPQgghRCM0a9YsiouLjxh8eu+993A4HDz77LMB7lnTopUWgaEDCkWGb9mdWXa7E0IIIRpcnb0bq6pKXFwccXFxUmS8jikO34538aYivJL5JIQQQjQ669ev5/TTTz/i4wMGDGDNmjUB7FHTpBX7Pgg1QiLQUVEUMKmy7E4IIYRoaBIlagQqdryLU4uk5pMQQgjRCDmdTsLCwo74eGhoKAUFBYHrUBOlFfvqPRkhkYBvyV11uwwKIYQQIrAk+NQIqIcyn6TmkxBCCNE4JScns27duiM+vnbtWpKSkgLYo6bJW+QLPukhDgAssuROCCGECAryjtwIVGQ+xarFaF6tgXsjhBBCiNoaO3YsixYt4p133kHXD3+QpGkab7/9NosXL2bs2LEN2MOmoSLzSbP5gk9mKTYuhBBCBIVaFxzfsmVLjc/NysqqbfOiGkpYNLpixowXm7ewobsjhBBCiFqaPHkya9eu5fHHH2fOnDm0adMGgJ07d5KXl0ffvn25+eabG7iXjZ/3UM0nr0Uyn4QQQohgUuvg00UXXVTjtfOGYcg6+zqgKCoeewy20izCPPkN3R0hhBBC1JLVauXNN9/k008/ZdmyZezZsweAnj17cuaZZ3L++efLhi11QDu07M5jjQB8NZ+EEEII0fBqHXx64okn6qMf4hg89jhspVmEeyX4JIQQQjRGqqpy0UUXcdFFFzV0V5qsimV3bksEoGGWzCchhBAiKNQ6+HTBBRfURz/EMWhh8ZALDq2gobsihBBCiFoqKCjg4MGDdO7cudrH//jjD5KSkoiMjAxwz5qWimV3LnMEUCCZT0IIIUSQkHfkRkIPiwcg0pCaT0IIIURj88QTT/Dggw8e8fGHHnqIJ5988rjaTktL49prryU1NZWBAwcya9Ys3G73Ma/Lz8/nwQcfZOjQoaSmpjJ27Fg++OCDKudlZmZy66230qtXL/r27cu//vUviouLj6uv9ckwdLTiAgDKTeEAkvkkhBBCBIlaZz6JBhLhCz5FGQUN2w8hhBBC1NrPP//MZZdddsTHhw0bxrx582rdbmFhIRMnTiQlJYXZs2eTmZnJzJkzKS8vP2qwC+D2228nPT2d6dOnk5yczIoVK3j44YcxmUxccsklAHg8Hm644QYAnn76acrLy3nyySe58847mTt3bq37W5+MsiIwdECh3BQKSM0nIYQQIlgEXfApLS2NGTNmsH79esLCwhg3bhx33HEHVqv1qNcNHz6c/fv3Vzm+ceNGbDab//vMzExmzJjBjz/+iMViYdSoUdx7772Eh4fX+XOpS0pEAgDRODF0HUWKkgohhBCNRl5eHtHR0Ud8PCoqitzc3Fq3O2/ePEpKSnjxxReJiooCQNM0HnnkESZPnkxiYmK112VnZ7N69WqeeOIJLrzwQgAGDBjApk2bWLRokT/49OWXX7J9+3YWL15M27ZtAXA4HFx//fVs3LiRnj171rrP9UUv8dV7UuwOPJpvwxvZ7U4IIYQIDkH1jlzx6Z3H42H27NlMmzaN+fPnM3PmzBpdP3r0aD788MNKX38NWlV8erdr1y6efvppHn74YX788UfuvPPO+npKdUYNj8FrqJgVHd15sKG7I4QQQohaiI+PZ+vWrUd8fMuWLcTExNS63RUrVjBgwAB/4AlgzJgx6LrOypUrj3id1+sFICIiotLx8PBwDMOo1H6nTp38gSeAgQMHEhUVxffff1/r/tYno9RXmkANi8Sr+Z6DWTKfhBBCiKAQVJlPx/vpXYW4uDhSU1OP+Hhj+vTu78xWC2neRDpZDqDt/g1TVLOG7pIQQgghamjkyJG8//77DB48mBEjRlR6bPny5XzyySdMmDCh1u2mp6dX2T3P4XAQHx9Penr6Ea9LTk5m0KBBzJkzhzZt2pCUlMSKFStYuXIl//d//1ep/b8GngAURaFNmzZHbb8h+DOfQqPxeHUALCalIbskhBBCiEOCKvh0pE/vHnroIVauXOlPCz+R9o/26V0wB58sJpVN7pZ0shzAu/s3rKec3dBdEkIIIUQN3XrrraxatYqpU6fSuXNnOnToAMD27dvZtm0b7du357bbbqt1u06nE4fDUeV4ZGQkhYVH36SkIsv8nHPOAcBkMnH//fczevToSu3/PTuqpu0fS11nJWllvv6YwqPRD2VvWS0myX6qR6ZDyxpNsrwxIGS8A0vGO/BkzAMr0OMdVMGn4/30rsKCBQuYP38+FouF3r17c9ddd9GpU6dK7TeWT+/+zmxW2expwXh+Qcvcjl7mRLVXnWwKIYQQIvhERETw4Ycf8vrrr7Ns2TK+/PJLAFq1asWUKVO44YYbarRDXV0xDIN7773XX4ogPj6en376iccff5zIyEh/QKq+qKpCdHRYnbaZZzIoBUITmmFymgAID7PV+X1EVQ6HvaG7cFKR8Q4sGe/AkzEPrECNd1AFn07k07vhw4fTs2dPmjVrxt69e5kzZw6XX345n332GS1btvS331g+vft7FNJuM5Ovh7PXG0NLcx7Gvo2Yuwyu03ue7CTSHlgy3oEl4x14MuaB1RjGOzQ0lNtuu61ShpPL5eKbb77hzjvv5IcffmDTpk21atPhcFBUVFTleGFhIZGRkUe87rvvvmPp0qV88cUX/g/q+vXrR25uLjNnzvQHnxwOB8XFxdW2n5ycXKu+/pWuGzidpcd9fXWUTsOICQmDdgMpWuHbhEbXNPLzS+r0PuIwk0nF4bDjdJahaXpDd6fJk/EOLBnvwJMxD6y6Gm+Hw16j+VdQBZ9OxP333+//d+/evRk4cCBjxozhjTfe4OGHH67Xe9fHp3cVKqKQ4V4NgM3ulrQ058H+DUSfPqZe7nmyk0h7YMl4B5aMd+DJmAdWYxhvwzBYtWoVCxYsYNmyZZSUlBAdHc3YsWNr3Vbbtm2rZG8XFRWRnZ1dJdv7r3bs2IHJZKJjx46Vjnfp0oWPPvqIsrIy7HY7bdu25c8//6zS/507dzJw4MBa9/evvN66/cPCbI8kasD55OeX4HL75k0mVanz+4iqNE2XcQ4gGe/AkvEOPBnzwArUeAdV8Ol4P72rTkJCAqeddhpbtmyp1H5j+fTu71HIip1nNnlaMoYNlKZvIC8rD8Viq9P7nswk0h5YMt6BJeMdeDLmgRXoT++Ox+bNm1mwYAGLFi0iJycHRVE4++yzufLKK0lNTUVRal8ce/DgwcyZM6dS9vjSpUtRVfWowaHmzZujaRp//PEHnTt39h/fsmULsbGx2O12f/tffPEFu3btIiUlBYBVq1ZRUFDAkCFDat3fQPEeeg2YgzgTTgghhDiZBFXw6Xg/vatN+43l07sKf41C2m0m9rui0e0xqGV5uHZvxpzSq17uezKTSHtgyXgHlox34MmYB1awjffevXv54osvWLBgAbt37yYxMZFzzz2Xnj17Mm3aNEaPHk2vXsf/Xj5hwgTeffddpkyZwuTJk8nMzGTWrFlMmDCh0i7BEydOJCMjg2XLlgG+oFKzZs247bbbmDJlCgkJCfz44498+umn3Hrrrf7rRo8ezdy5c7n11luZPn06ZWVlzJo1i6FDhwb1Ri3+3e6k2LgQQggRFIIq+HS8n95VJzMzk7Vr1zJu3LhK7TfGT+8qtE6M4Pc9BeQ4OpFQtgrv7nUSfBJCCCGC1KWXXsrGjRuJjo5m9OjRzJgxg969ewOwZ8+eOrlHZGQkb7/9No899hhTpkwhLCyM8ePHM23atErn6bqOpmn+78PDw3nrrbd49tln+b//+z+Kiopo0aIF99xzD1deeaX/PIvFwuuvv86MGTOYPn06ZrOZUaNGcd9999VJ/+uLZD4JIYQQwSWogk/H++ndwoUL+fbbbxkyZAgJCQns3buXV199FZPJxLXXXuu/rrF+elehXfNIft9TwDatFQmswrv7NwxdR1FlYiWEEEIEmw0bNvgDOkOHDsVsrp9pV7t27XjrrbeOes67775b5Vjr1q157rnnjtl+YmIis2fPPs7eNQzJfBJCCCGCS1AFn47307sWLVqQlZXF448/TlFREREREfTv35/bbrvNv9MdNN5P7yq0bebLBlud42CINRSjvAgtKw1zUocG7pkQQggh/u6BBx5g4cKFTJ06lcjISEaPHs3ZZ59Nv379GrprTZ7nUOaTRTKfhBBCiKAQVMEnOL5P71JTU6v9RK86jfHTuwrtmvmKru/Pc6Gc1h1j5y94d62T4JMQQggRhK644gquuOIK9u7dy4IFC1i4cCHz588nLi6Ofv36oSjKcRUZF8fmlcwnIYQQIqjIO3Ij4gizEh8VAkBWeCcAvLvXN2SXhBBCCHEMLVu25JZbbmHx4sV8/PHHnHPOOfzyyy8YhsEjjzzCAw88wLfffovL5WrorjYZFcvupOaTEEIIERyCLvNJHF275pFkF5SzxdWMoaoJo/AgWkEGpqhmDd01IYQQQhxD9+7d6d69O//85z/5+eef+eKLL1i8eDEfffQRdrud9evlQ6W64F92J5lPQgghRFCQd+RGpmLp3fZMF6ZmXQDw7pKJqhBCCNGYqKrK6aefzsyZM/npp5945pln6N+/f0N3q8mQzCchhBAiuMg7ciPTrrmv6HhahhNT61MB8G7/CcMwGrJbQgghhDhONpuNs88+m1deeaWhu9JkeCXzSQghhAgq8o7cyLSID8dqVilzecmN7gGWEPT8/Wh7NzV014QQQgghgkJF5pPsdieEEEIEB3lHbmTMJpWU5EPZT9keLJ2HAODesLghuyWEEEIIETS8mi8j3CyZT0IIIURQkHfkRqhds4qld4VYe5wJigntwO9oWekN3DMhhBBCiIZ3OPNJaeCeCCGEEAIk+NQotWvuKzqeluFEDY/F3L4fAO6NSxqyW0IIIYQQQeFwzSdTA/dECCGEECDBp0apIvMpI7uE0nIv1lPGAODduQbdmdWQXRNCCCGEaFC6bqDph5bdSeaTEEIIERQk+NQIRYbbiIsMwQB2HnRiimmJqWUPMAzcG5c2dPeEEEIIIRqM51DWE8hud0IIIUSwkHfkRsq/9G5/IQDWU84GwPPHj+hlzgbrlxBCCCFEQ6qo9wQSfBJCCCGChbwjN1JtK4qO7/cFmkzJnVHjUkBz49nydZ3cQzu4Hb20sE7aEkIIIYQIBO+h4JOigEmVqa4QQggRDOQduZFqfyjzKT2jEMMwUBTlcPbTlq8xPK4Tat+zay2lX/yb8q9fPuG+CiGEEEIEin+nO8l6EkIIIYKGvCs3Ui0TwrGYVUrKvRzMKwXA3OY0lIh4DFcxpYueRC8tOK62DcPAvfYzALQDf6KXF9VRr4UQQggh6ldFzSeLSaa5QgghRLCQd+VGymxSaZ0UAUB6hm/pnaKaCBl6A9jC0LPSKf3kYbSs9Fq3re35DT1376HvDLR9W+qq20IIIYQQ9aoi88ksmU9CCCFE0JB35UasfbPKRccBzMmdCDv/QdToZhilBZQueBzP9p8AMNxleA/8gXvTl5T/9B56YWaVNg3DwLXuC983VjsA3n2b6vmZCCGEEELUDa9kPgkhhBBBx9zQHRDHr2PLKJb+soffduRwpW6gqgoAamQioeMeoOybuWh7fqP821dxrfkEoyin0vXePRsIO/9BlJBw/zFt32b07J1gshIyaCLl38xB27sJw9BRFJnECSGEECK4Sc0nIYQQIvjIu3Ij1q1NDGEhZgqK3WzdnVfpMcVqxz76Nqy9zgXwB56U8FjMKaeihMdiOLMo+/oVDF3znWMYuA9lPVm6DsPc5jQw2zDKnH9ZhieEEEIIEbzcFcvuJPNJCCGECBqS+dSIWcwq/bom8s26/fy06SDd28RWelxRVGx9LsLcti9GWSFqXGvUEF+dKC13D6Wfz0DbvwXXLx8R0n8C2oHf0TK3g8mM9ZQxKCYLpmZd0Pb8hnffJkxxrRviaQohhBBC1Jh/2Z1kPgkhhBBBQ96VG7mBPZIBWPdnNmUub7XnmGJbYm7R3R948h1r5StODng2LsWz/afDWU+dhqCGRgFgbtkDAG2v1H0SQjQehteFXpLf0N0QQjQAj2Q+CSGEEEFH3pUbuZSkCJJjQ3F7dX79PatW11ra9sWaOhaA8u/fQMvYBqoJa+rZ/nP8waeDOzDcZXXXcSGEqCeGYVC25FlKPrgLLXNHQ3dHCBFgXqn5JIQQQgQdeVdu5BRF8Wc//bTpQK2vt/a5EFOrU+BQ3SdLx0Go4YeX76mOBJTIRDA0vBnb6qbTQghRj7S9m9AO/A66huuXjzAMo6G7VO/0ggN4dq7x1/AT4mRWUfNJdrsTQgghgoe8KzcBA7oloSjw575Csgpql52kKCr24ZNRo1uAxe7PhPorc4vugCy9E0IEP8MwcK37zP+9duAPX1ZnE+bds4GSTx6ifNmLlP7vQbzyu1qc5CpqPpkl80kIIYQIGkH3rpyWlsa1115LamoqAwcOZNasWbjd7lq18dZbb9GpUycmT55c6fjq1avp1KlTla9p06bV5VMIuOgIG11TYoDjy35SrKGEXvAg4Vc8jeqIr/J4xdI7775NJ0UGgRCiYWi5eyj59JETylbS9m1Cz0oHkxVzu/4AuH79X1D87tLy9uHdu7FO++L54wfKvnwevG5QVPT8/ZQteZrSJc+gFWTU2X0CxTB03FuW4976DYbX1dDdEY2Ux5/5pDRwT4QQQghRIah2uyssLGTixImkpKQwe/ZsMjMzmTlzJuXl5Tz44IM1aiM7O5uXXnqJ2NjYI57zxBNP0LZtW//30dHRJ9z3hjawexJbdubx0+aDnDeoDapSuwmXYrYC1mofMyV3AdWMUZSDUXgQJSq5DnoshBCHaVnplC55GlwluLN3ooSEY+05plZtGIaBa+3nAFi6DsN6yhi8u9ahZ6Wh7dmAuXVqPfS8Zjw7fqb8u9dB92Ju25eQwdegWEOPuz3DMHBvWIT7l48BMHc4HVv/Cbh/W4Rn83K0vRsp3bcZS7cR2HpfcEL30suL0PP2o0YmooRGodTy/aWmDK+L8m9exbtrLQDuNZ9i6XkW1q7DUaz2ermnaJpktzshhBAi+ARV8GnevHmUlJTw4osvEhUVBYCmaTzyyCNMnjyZxMTEY7bx1FNPMXz4cDIyjvyJb4cOHejRo0dddTso9OoYT4jVRE5hOdv3FtCpVd0F1BSLDVNyJ7T9W/Du3YRVgk9CiDrkPbidsiVPg6ccJSwGoyQP18/zUSOTaxUw0vZtRs9KA5MF6yljUEOjsHYfiXvDYlxrPsHUqieKEtg/Rg3DoPy3JZT/9IH/mDf9F0qyd2IfeQum+DZHv17z4N60DMOZiRISgRISjhISgZa5A8+2bwGwnnI21r7jURSVkAGXYe06DNfPH+LdvR7P5mV4037B1v9SzO0H1CpwZLjLcG9cgnvjl1CRhWQLwxTTAjW6BaZmnTG37oViOvGphF5aQNmXz6Nn7wTVjBIaiVGci/uXj3BvWIy1yzAwWzGKc9CLctGLc1DtkdjPvhPFbDvh+4um5XDNJ1MD90QIIYQQFYIq+LRixQoGDBjgDzwBjBkzhoceeoiVK1dy4YUXHvX6NWvWsHz5cpYuXcqdd95Zz70NLjaLiT6dE/hh4wFWbjpYp8EnAHPL7r7g075NWHucWadtCyFOXt6MbZQtfQ68LkzJnbCPvgPXzx/i+f07yr6ZQ+i4+zHFtDhmO75aTxVZT8NRQ6MAX2DGvfUb9Nw9eHeuxdK2zwn1Vy9z4tn2HUZxDpZOgzEltj9yn3Sd3GX/oezXRb5+dR+FpW1fyr6Zg1GUTennM7D1uxRL91HVBoW0vP2UfzMHPW/vEe9hG3AZ1h6jKx1TI5Owj74d777NlK/8L0bhQcq/fRXT7yuw9bsEw12KlpWGlpnmC/YApuROmJI7Y2rWBdURj2frt7jXL8BwFQOg2B0Y5UXgKvHV0TrwB56tX6PYHZg7DMTaeQhqVJJvjEoL0LN3oeXswigtPNyxQ89RdSSixqdgiktBsdjQ8vZRtvRZjOJcFFs4IaNvw5TQFu+On3GtX4hReBD3bwurjk9pIXg9IMEn8TeHaz7JsjshhBAiWARV8Ck9PZ2LLrqo0jGHw0F8fDzp6elHvVbTNB577DFuuukmEhISjnrupEmTKCgoID4+nnPOOYfbb7+dkJCQE+5/QxvYI5kfNh7g1z+yuGJUR2zWuvvEz9SiJ/AhWsbveA9uR8/Z5fvjJXsnamQSIYOv9f+xdyx6cS7uDUtQo5tj6TIk4JkIQtQ1w1UCVru8lmvJu/s3ypa/BJoHU4vu2M+8FcVswzboSvTCg2gHfqfsy+cIPf9BVLvjqG1p+7egZ+7wZz1VUELCsfYYjXvd57jXfIo55TQUVUUvzsW7ZwN6zh7UqETU+LaYYltVu7zLMAz07HTcm5fjTf8VdC8Ant9XYGp1CrY+F2GKbVXpfMOZRfmvH+NJ/xUAW/9LsfQ4C0VRCLvoUcq/fxPvrrW4Vr2P58+VWDqcjrldX9SwaAzDwLPla1yrPwTNgxISgaXLUAxPOUZ5EUZ5MWgeLN1GHjWYZm7RnbDxj+HesAT3+gVoB36n9LNHq/9Z7FyDd+ca3zeqyb8DqhqZhLXPRZjb9AbNg15wAD1vH1rObrzpv2CUFuDZuATPxiWoca0xSgowygqrvUcVioIa1Qy9ONeX9RaZROhZ01AjfVnOlo6DMLc/He/OX/Gm/QJWO2pEHGpEHEp4LKaYligh4TW7lzip+Gs+ybI7IYQQImgEVfDJ6XTicFT9AyMyMpLCwqNPZt9//33Kysq45pprjnhOREQEN9xwA3369MFms/Hzzz/z5ptvkp6ezty5c0+o73W9o4rp0PbAplpsE9wlJZqEKDtZBWWs2nqQkb1b1l1/4ltQdmg5TNkX/670mFaYSemnjxB+9jTM8SlHbMPwuin/bTHl6xb6iuMC2q41hA2/ETU8ps76eryOZ8zBl92gqDLBra3jHe9gYnhclP3yMa6NX6FGNyNsyLWYkzs2dLeqFUzjbbjLKF31Ie4t3wBgSelF2JlTDtWeA7BiGnMrRR8/gu7MonzZbEJPvwxTbAsUS9UPCgzDoPRQ1pOt23Csjsq/T0y9xuDZshy9IAPXN6+gFWSg5+2vpmcKanQyakQc6DoYGui6r+ZR/uGl3KaEtpiiknBv/xltzwZK92zA0r4fpthWaJk78GbuwCgrOnSymYiRkzG363f4NuYIzGNuw7X5a8p++gA9dzeu3N24fp6HuUUXQMW7b7Pv1FY9CRt+Q42D+1WYbVj6nk9I54GU/fhfPLt/Q42Ix5zYDlNiO8yJ7UDX8WRsw7t/G96D28HrRgmLxt7nAqydz0BRD32QYTFBUhvfF2dgDLoMz+7fcG/9Ds+ejeg5uw8Noy+oZI5PQY38y4dBhgG6hpa/H2/WToySfPR838/B3KwzYWfdhlolmKRi6TQAOg045lMNptd4oKWlpTFjxgzWr19PWFgY48aN44477sBqrb6WI/g2Ybn66qurfaxNmzYsXbrU//2aNWt4/vnn+f3331FVlR49enDnnXfSpUuXOn8udcF7KPhkPglfC0IIIUSwCqrg0/HKzc3lhRde4MknnzzqRKtr16507drV//2AAQNISEjg0UcfZePGjfTs2fO47q+qCtHRYcd17bE4HLUrsnrhsPbM+XQTn/2wk7MHtSPMbqm7zqQOo2Dl/1DtEYQ074itWQes8S3J+/4DPDn7KP7s3yScdxthnftXuswwDEr/+IXc5W/hLcwCwJbcHnf2Hrz7tlA0/37ixkwivOvAE+6ipzCLg/P+jSk0kvhzbsIS06zWbdR0zHWvm/zvP8D56xJCO/YmduS1mB1HLnQvqlfb13iwKE1bT86SuXgLswHQ8/ZT9OkMIlJHEjP8Skz2iKNeX77/TwpXf4FisWONa44ltjnWuBaYoxIO/7FfxwzDqPV4l+3ZgutAOmD4ggeHdmozO+KwxrfEEtMMxXz494zuKsWdsx9P7n4Uq42QZh0r/X9RtnMj2Yte9o+b47SziB11DYrp77+rwoi47F/sf+tetIPbKfrEl7FjjkrEmtAa1RaKVlqIXurEW1KI5sxBMVtJHHox5oi//z4OQz39fPK+fc+fiYSiYmvekZAWnfDkH8R1IA3NmYOen1Ep0FRBMVkI6zYQx2ljCGnmW2rnzs0gf8U8SrauxLNjNZ4dqw9fYDJjS25HzLArsLfqVv3gDh6HdtpQirf9RPGWH3Dt+wPvvq3++8WMuBpH7zF1U+A7Ogxa34+ha9W/vrqmAr4aU568A5ijk1DNR34/9YsdDKcOxuvMoWz3FixRiVgTU1Ctx84m9hbl4TqQhqF5CevQu9Lr6EQ01t8px+t4N2vp1q0bH374YaVjxcXF3HjjjQwePNh/LD09neuvv57+/fvz9NNP43a7mTt3Ltdccw0LFy4kPr7qLrkNzS2ZT0IIIUTQCargk8PhoKioqMrxwsJCIiMjj3jd888/T6dOnejduzdOpxMAr9eL1+vF6XQSGhqK2Vz9Ux0zZgyPPvoomzdvPu7gk64bOJ2lx3XtkZhMKg6HHaezDO1Q7YKa6Ns5ns9jQzmQW8q7i7Zw6YgOddepnucR2Wk4ii3c/8eQGwgb147ir17Cu3cTmf97ipC+F2FOaIM3exdazm607N3oTl/QSQmLIfT0CVja98NWcJCS5XPQsneS9ekz5G9aiTmpgy/7wWRBMVswRTerUb0X8GVTOD/5N3rePjzsY+9rdxE66AqsXYbU6I+32oy5N2c3JcvnouftA6Bk2ypKtq/D3ucCbD3PrJMCvNUxDMOXlWAJfI0TQ9coX/M5ri1fE9JrLCGptduJ7O+O9zXe0PRSJ2Ur38e9/ScA1PBY7AMvw7NnE+5t31P023KK/1iN/fTLsXYcUO1SPNfW7yhd8Y5/+VYlJgum6GRMMS0wxbbEFN0cLFYMjws8Lt/285qGYg1BsYUd+gpFDY+t9nVh6DqeXetwbViCNzMdxRp6qGh1OKo9AlNcCtaOAzBFVt7QwZuZRtnPH+Hdv/XoA6KoqJGJqKGRaIWZGCX5VU8Ji8Gc1B5FNeHevso3bhFxhA67AVOLrhQ43fh+m/x9LKIJP+dOyn79DC13D0ZpId6CTLwFmdV2xdZzNEVeK+SXVB2H9kOx7t8FhoEl5RTMLbujhvgChLZDX3ppIVrWTvQyJ6gmX5BGNYHJjDmxHardQRlQVtG+Gol16GTU7mNwbViC4fVgTmqPObE9pvjWmK027Md8jZuh3WBC2w3G5szGvX0Vet5+Qk49Fz22BQUFdfveUiPmWCjyAJ5aXGSHFr3xAmUlGpRU/RlUZYN43wdCBUVHeA3UQl39TnE47I0qe+p4N2sJDw8nNTW10rFPPvkEXdcZO3as/9jy5csxDIPnn3/eX6KgU6dOjBw5kpUrV3L++efXx9M6If6aT43o5yiEEEI0dUEVfGrbtm2V2k5FRUVkZ2fTtm3bI163c+dOfv31V/r0qVr7ok+fPrz22muVPsWrDxUp3nVN0/Rat33x0Pa88L+NfPnLHgaf0oz4qDr8FNgcBpoBGIePmUIIGX0Hrp/n4dm8jPJf/lf1OpMZa88xWFPH+grMagZEJGIf9y/c677AvX5B1cyBilumnIa1z0WYoo+cxWToOmVfvYyetw/F7kCNauarb/Ldm7h3bSBk8LX+2iCG14VRWujbOaqa+i5HG3ND13FvWIx77aegayh2B9bTzsez/Sf0zB2UrZqH6/cfsA24DFOzrnW6HM/wuij78gW0jN+xdBqItde5qBGB+cRZL86j/Nu5aAf+APAtFdI0rKecXX1fXSVgCalRBs/xvMbrg+Euxbv7N1+goUX3KlvT685s3Ju+xPPHCt+yUUXB0m0Utj4XolhCsLXujanDQFw/vIWen0Hp13MpX7cAa+pYzO36oagmDM2D66f38Gz7DgBz616oca19dXQKMtALDoLmQcvZg5azp3ZPQDH5loM17+r7im2FZ8fPuDctxSg8HKzx1QzyBfk1wLNzHeW/foKa0A5LhwGY4lJwb1ji3+4e1Yy51Slgth4qGK2AoaMXZfuWTLnLDvX/wOGu2CNRo5Ix3CXoefswSvLwpP3if9zSdTi2fpegWEKO/bOPa4d9jG8DCb28CD1vH3ruXgzNgxoSgWKPQLE7UOyRKOGxR25PsWAbeqP/Wx3Q/36uNQKlRU+qe9VWe36FqBbYhtxY6ZBmgHLoD+Aav8ZDY7Gc4vuj36D+3leaumD5nRIoJ7pZy18tXLiQlJSUSh/GeTwerFYrNtvh4HZExNEzOxua1HwSQgghgk9QBZ8GDx7MnDlzKtV+Wrp0KaqqMnDgkZdk3Xffff6MpwqPP/44ISEhTJ8+nU6dOh3x2kWLfLsQ9ejRow6eQXA4pX0sXVpHs213Pv/7Po2bxnWv93sqqomQ069AjW6Oe93nKJYQ1NjWmOJaocalYIprjWKrujRRUc3Yel+IuWVPPL+vwPCUYXg9oHkwvC70zDS8u9bi3b0Oc4dB2HqfjxpedWmba/WHaHs2gMmCffQdqPEpeDYuxfXr//DuWkvJwT/BFoZRWgCect9FZhv2UVMxtzz6z97QPGiZO9D2b8W7az16vi/byZxyGrYzJqLaHVi6DMX7x4+4Vs9Hz99P2eL/A6sdc3JnXyCgWRfU6ObHvXzG8Lp9gaf9WwBfoWPPHyuxdBp0KAgVd1zt1oR393rKvnsdDgWUzK1TfbtQrZ4Pioq151mH++kuw7XmUzxblqOERWNNHYul0xlVMsEMXcOTsZ3y0ggMe3Kd99nwuDA0tz+z5Yjn6TpaxjY8f/6Ad+da0A5leigmTMkdMbdKRY1pgeePFXjTf/EvOVPjUggZdDWmhMpBcXNSR0wXPop741Lcvy1Cz8+g/NtXUdZ8irXHmXjSVvuKYqNg7XMh1tRzKmVGGYaOUZSDnrcfLW8vev5+X4BH18Fi82UFmm0oJjOGuwzDVYrhLvXtSOYuQ8vcjpa5HQ7VP/KzhhLSfQRxfc/EmV+Ep6QQo7zYl0m05zdfse6sNFxZaYevURTMHQZiO+38I76+DMPAKC1Az9+PUeZEdSSgRiVX+n/d8JSjZe9Ey0rDcOZgbt8Pc7PjqxOjhkSgNusCx3m9EE3RiWzW8lc5OTn8/PPP3HzzzZWOn3POObz++us899xzXHPNNbjdbp555hmSk5MZMWJEnTyHulaR+WSRzCchhBAiaARV8GnChAm8++67TJkyhcmTJ5OZmcmsWbOYMGFCpbTxiRMnkpGRwbJlywCqLXjpcDgIDQ2lX7/DRV7vuusuWrduTdeuXf0Fx9966y1GjhzZpIJPiqJw6fD2PPKfX/llWxajehfSrvmRly3WJWuXoVi7DK31dabE9tVuWa7l7ce95n94d63D++cPeHeswtw6FXOrUzC17IkaGon79+/xbPoSgJChN/gDAtZTzsbUvBvl38xFL8iA8r8s6VRM4HVR9tXz2Efd6svs+Bvv7vW4t3yNduBP0P6yHMQSQsjAKzF3GOgPJimKiqXzYMwpp+Ja8wmeHavAXYZ393q8u9f7zomIx9LhdCwdB6I6DhfhNQwDozgXLXM7isWOqWXPShlThu6lbPnLvsCT2Ybt9Mvxpv2Ctn8Lnt+/x/Pnj6hxKX9ZrmhFsdoxp5yKqeUpNcq+MnQdoygLvTDLv5uW4SpGL8z0BV0ANa419hE3o0Ym4XIk4l73Oa6f5/kygLqfiTdtNa6f5/kCfIBRnIvrx7dxr1+AtddYLB0GomVux5v+K95d6zDKiygG366HnYdg6Tiw2gBllb56XL4lUVUCWrpvTP5c6cva0TwooVGosa0wxbZCjW0Jmhe9tADj0JeWmYZRkudvQ41qBhjoBQfQMrahZWyrdA9Ti+5Ye47B1LzrEQOJismMrddYrN2G497yDZ5NX2IUZeP66T3fCVY79uE3VfuaUxQVxZGA6kjAnNLrmGPxV7ozG2/GVrR9W9AytmGUF6GEx2LtMRpL58FY7KFYo8MwqSUYkYezCK3dR6KXFuBNW41n+yr0nN2YU07F2udC35K/o1AUBSUsGjUs+sjnWEIwN+ty3AEnIcTRnchmLX+1ePFiNE2rtOQOICUlhbfeeotbbrmFOXPmANC8eXP+85//nHAGVH1t1uI5FHyyWU11fg9R2clc6L8hyHgHlox34MmYB1agx1sxDMM49mmBk5aWxmOPPVZpx5Zp06ZVKiR+1VVXsX//fr755psjtnPVVVcRGhpaaRe7uXPnsmDBAvbv34/H46F58+ace+65TJo06aiFyo9F03Ty8mpS36LmzGaV6Ogw8vNLjnv5wJuLtvHjpgO0a+7gvitPq5uitQ1Ey0rD9cvHVYIBanwb9Jw9YGhYTzsf22nnV7nW8LrRDvwOJgtqaBRKaBSYLJR//TLeXetANWMfNYWQdqcRHR1G7v4DlKx4xx90Ad8yIlPzrphbdPMFvY6x7buh6+g5u/AeCmD8PYBlSuqIqUV39PwMtIN/VgqAKI4ErD3PwtJxEKgmyr+Z49ve3WTBPma6/49478E/ca/9DO0oNXmU8FhfYKfzYNTQKF+QqSQXveAgeuFB9Ly9aLn7fNlc3iPXW7H0GI2t73h/UWjDMHCv+QT3+gW+n0NsS/TcvYf6n0jIgMvQi7Jx/7bIH4xCUfyZQ4Av0KR7fcEkAJMFc5veqDHNUcwhvvpFlhDf9u55+9Dy9/uWcBXngqKihMf6ag05EkA1+7d9rzVrKJb2/bF0HIQa3wZFUXxBtz2/4d39G3ruXkwte2DteRamuNa1bt7wuvD8vgL3pi9RbGHYR9zi30q+vhiGjlGSjxIa5V/6WNPfKYahV1unStReXfweFzVXV+MdExPWqCa93bp14/bbb2fSpEmVjo8dO5ZevXrx2GOP1aidiy++GE3T+OSTTyod37lzJxMnTqR///6cf/75uFwu3nzzTQ4cOMC8efOIizu+zFvDMOptXnL3Cyv4fXc+913TlwE96j67VgghhBC1F3TBp8YoWINP+UUu7n11FW6Pzk3jutG3S/3+wVvfDMPwBXR2/4Z3z2+Ht/UGzO36ETL8plpNZA3dS/nXc/DuXAOqibAzpxBqhZwv3/DVLFJULD3OxNLxDNToZic0STa8Lry71uH5c6Uvg+nv/9spJtS4Vr7C7C7fa+mvtatQTdhH3465ZdWi+FrObvTiHPB6wOvG0DzohZl4tq/0t4ViQo1MRC/KAq2aItfgC85FJvnq5xwqSK3Ywn1Bt+SqS1cNw8D96/9w/7bQf72111isPcf4srDwBf48v3/vD0IpdgfmlNMwt+2DrWUXosLNZP2ynPIt3/iDVyfEFoalXX9fdll0M/TcvWi5e9Bzd6PlZ6CYbb5sqNBIX8ZORBym5t38/a1v9fnH1rFIICTwZMwD62QNPg0YMIDx48dz5513Vjp+xhlnMG7cOO66665jtrFnzx5GjRrFvffeyzXXXFPpsdtuu419+/ZVCkqVlJQwbNgwJkyYwPTp04+r35qm43SWHde1R1JRdP7Wp75h18Ei7roslZ7t6m9Zumi8m4c0VjLegSXjHXgy5oEV6M1agmrZnahb0RE2zurbii9W7mLe19tpnRRBYnTosS8MUoqiYIpvgym+DbbeF6CX5OPduxGjvBhr91G1/qNeUc2EjLiZ8m9fxZu2mpKlL1ARQlRjWxEy5DpMcSl103ezDUv7AVjaD/D1e8cqtKx01JiWmJI6YEpoh2KxYXhceP5YgXvjUt9SvDInKCohI2+pNvAEYIprXW1Gjq3veLzpv+Le9i165g7f0kMA1YwamYAamYwa3Qw1tiWmmJYojsRaFUhXFAVrn4tQ7BHoeft8taf+spzQ97ytWLuPwtJlKEZRTqV7KKqKagvF1n0Eaqeh6NnpeNJ/xSgvAW/5od3dykFRUKOb+75iWmCKbo6he9ELMzEKM9GdvqWCppY9MLc6xZ+dBfjGNqkOd3w8QY05+1AIEZyOd7OWv1qwYAGqqnL22VU3kdixY0eVXfHCwsJo1aoVe/bUcmOEv6mvoKz7ULsKigR+A+RkK/Tf0GS8A0vGO/BkzAMrUOMtwacmbky/1vz6exYHckt58r113H1ZL5Jjj11XpzFQw6Kxdh5yQm0oqomQYZMoV1S8O1ahmCzYep+PucdoFLV+/vdQw6KPuEucYrH5gjVdh+FN+wVP2mrfsrmUU2t9H8VsxdJxIJaOA9Hy9mOU5KFGJqKEx9XZLnyKomDtMfrY55ksKFFHXvqgKAqmhHaYEtrV7L6AGhoF1WRkCSHEyeR4N2v5q0WLFtG3b18SEhKqPNasWTO2bdtWKXOzuLiY3bt3V6qrGUy8studEEIIEXTkXbmJs1lN/OOyXjSLC6Og2M2T769nf07dLhFs7BTVRMjQGwk76zZaTH4O+2nn1lvgqeZ9MmPpcDqhZ007rsDT35limmNu2QPVkVBngSchhBANb8KECYSFhTFlyhR+/PFH/ve//x1xs5ZRo0ZVuX7r1q2kpaVVKTT+1/a3bt3KXXfdxYoVK1i+fDmTJk3C7XZz8cUX19vzOhEe2e1OCCGECDryrnwSiAy38Y/Le9EiPhxniZtZ769jb1ZxQ3crqCiqirVtbyzRSQ3dFSGEEKLGIiMjefvttzGZTEyZMoWnn36a8ePHc88991Q6T9d1NE2rcv2CBQuwWq2MHl19FuvIkSN57rnn2L17N9OmTeP+++8nJCSEd955h5SUlPp4SifMcyjzSXa6E0IIIYKHFByvA8FacPzviss8PD3vN3ZnFhEWYubOCamkJB1917aTiRQHDiwZ78CS8Q48GfPAOlkLjjdW9Tl3Gn/PQlwejZmT+5PQiGtdNgbyey6wZLwDS8Y78GTMAyvQcyeZXZ1Ewu0W7r4slTbJDkrKvTz5/nq27Mpr6G4JIYQQQtQZb8WyO7OpgXsihBBCiAoSfDrJhIZYuGtCKl1aR+Nyazw3fwO/bMts6G4JIYQQQpwwTTfQdF9SvxQcF0IIIYKHvCufhOw2M3dcfAq9Oyeg6QZzP9/C8jV7G7pbQgghhBAnxOM9XNfKbFIasCdCCCGE+CsJPp2kLGaVm87rxvBTm2MA7y/fzsffpaHpsrZWCCGEEI2T5y81KyTzSQghhAge8q58ElNVhStGdeSCwW0BWPzzbh57ew07DzgbuGdCCCGEELVXEXxSFDCpMs0VQgghgoW8K5/kFEXh3NNTuPHcroSFmNmTWcyMd9bw/rI/KXN5G7p7QgghhBA15vb4lt1J1pMQQggRXOSdWQAwoFsS/76xP/27JWIYsHztPu5/fTUbduQ0dNeEEEIIIWqkIvPJUoMtn4UQQggROPLOLPwcYVYmnduNOy9NJSHKTn6Ri+c/3si8r7f7ty0WQgghhAhWFcEns2Q+CSGEEEFF3plFFd3axPDo9X0Z2bsFAF/9upcn/ruO7IKyBu6ZEEIIIcSRVex2J5lPQgghRHCRd2ZRLavFxOUjO3LrhT0ItZnZecDJw//5lbV/ZDV014QQQgghquWuWHYnmU9CCCFEUDE3dAdEcOvVMZ6HE8OZ+/kW0jKcvPTpZrq3ieHcgSl0aBHV0N0TQgghhPDzL7uTzCchRCOn6zqadnJtAKXrCuXlJtxuF5pmNHR3mryajLfJZEato91jJfgkjiku0s4/rziVT1ek8+Uve9m8M4/NO/Po0jqa8wam0KlVdEN3UQghhBACj+x2J4Ro5AzDwOnMo6ysuKG70iByclR0XeoNB0pNxttuD8fhiEFRlBO6lwSfRI2YTSoXD2vPkF7NWbxqNys3HWDb7ny27c6neXwY7ZtH0ibZQZtkB83iQjHVUXRUCCGEEKKmPJpkPgkhGreKwFN4eDRWq+2E/+BvbEwmRbKeAuho420YBm63i+LifAAiI2NP6F4SfBK1khBl55oxnRl7emuW/LyHHzZmsD+7hP3ZJXz/WwYANouJ0X1bcu7AFAlCCSGEECJg3B6p+SSEaLx0XfMHnsLDHQ3dnQZhNqt4vZL5FCjHGm+r1QZAcXE+ERHRJ7QET4JP4rjERdq5anQnxg1qw/Z9BaQfcLIzw8mug0WUuzW+WLmLbbvzmXRuN2IjQxq6u0IIIYQ4Cchud0KIxkzTfL/DKv7gFyIYVLweNc2LqlqPux0JPokT4gizclqnBE7rlACAbhj8si2Td7/8g+37Cnn4P79wzZgunNYpvoF7KoQQQoimzl9wXDKfhBCN2Mm21E4Et7p6Pco7s6hTqqLQv2sSD13blzbJDkrKvbz06SbeWfo7hSXuhu6eEEIIIZow/7I7yXwSQgghgopkPol6kRBl594rfTvkLVm9h+9+y+DHTQcZ2COJs/q2IjEmtKG7KIQQQogmxqNV7HYnWQNCCNFQBg3qfcxz7rvvIc4++9zjan/q1EmEhoYya9Zzx3V9df7883euu+5KmjdvwYcfflZn7YrDgi74lJaWxowZM1i/fj1hYWGMGzeOO+64A6u15msL33rrLZ544gmGDh3K3LlzKz2WmZnJjBkz+PHHH7FYLIwaNYp7772X8PDwun4qJ72KHfK6tYnh0xXppGU4+f63DFb8lsGpneIZ1bslHVpESlqpEEIIIeqEx5/5ZGrgngghxMlrzpz/VPr+ppuuZfz4Sxk58iz/sebNWxx3+3feeQ+mOs5w/eqrpQDs37+PLVs2061b9zptXwRZ8KmwsJCJEyeSkpLC7NmzyczMZObMmZSXl/Pggw/WqI3s7GxeeuklYmOrbgPo8Xi44YYbAHj66acpLy/nySef5M4776wSpBJ1p2tKDF1aR7N9XyFLft7NhrRc1v6Rzdo/skmKCeWMU5I5vXsykWHHX7xMCCGEEOJwzSf5YEsIIRpK9+49qhxLSEiq9ngFl6scm61mG1W1adP2uPtWHV3X+eabZfTsmcrvv29j2bIlQRV8qs3YBLOgWhA/b948SkpKePHFFznjjDMYP348d999N/PmzSMzM7NGbTz11FMMHz6cdu3aVXnsyy+/ZPv27Tz//PMMHz6cs88+m3//+9989913bNy4sa6fjvgLRVHo2DKK2y8+hceu78ugnslYLSoH80r56Ns07nppJbP/t5Flv+5l+74CXG6tobsshBBCiEbGXbHbnRQcF0KIoPXGG3MZNeoMtm7dzOTJ1zJ8+On8738fAfDSSy9w9dWXMmrUGZx//hgeeug+cnJyKl0/deok/vGPO6q0l5a2g5tvvp4RIwZy1VWXsHr1qhr157ff1pGVlcn551/E6acP5Ouvl/l3HvyrJUsWcu21lzN8+Omcc84I7rrrNg4ePOB/PDs7i8cee5Bzzz2T4cMHcvnlFzF//gf+xwcN6s37779bqc3589+vtExx3bo1DBrUm59++pH77/8HZ545hAceuMd//5tvvp4xY4Zz1lnDmDp1Elu3bq7Sz127dnLffXczZsxwRowYyMSJl7FsmS+z61//upubb76uyjWffvoxw4efjtNZWKMxOx5Blfm0YsUKBgwYQFRUlP/YmDFjeOihh1i5ciUXXnjhUa9fs2YNy5cvZ+nSpdx5553Vtt+pUyfatj0cKR04cCBRUVF8//339OzZs86eiziy5vHhXHd2Fy4b0YFftmXyw8YDpGc4Wb89h/Xbfb9YFAWaxYXRuWU0Q3s1o3m8LIsUQgghxNH5M5+k4LgQQgQ1j8fDI4/czyWXXM7kyVNwOCIByM/P46qrriUuLp6CgnzmzXuPqVMn8d//zsdsPnL4wuv18uij9zN+/ASuueYG3nvvbe6//x98/PECIiOjjtqXZcuWEhISwhlnDMVms/Hdd9+wZs0v9Os3wH/O+++/w8svv8DYseOYNOkWvF4va9euoaAgn6SkZAoLC5g8+VoAJk26hWbNmrN37x4yMvYd1/jMmvVvzjxzDI8/Ph5V9b2nHTx4gLPOOofmzVvg8XhYvvxLpk6dxFtvfUCrVq0B2Lt3DzfddC0JCYncccddxMTEsnNnGpmZBwE499wLuOuu29izZxetWqX477do0RecccZQ/8+hPgRV8Ck9PZ2LLrqo0jGHw0F8fDzp6elHvVbTNB577DFuuukmEhISjtj+XwNP4MvIadOmzTHbF3XPbjMzJLU5Q1Kbsz+7mLV/ZrPrQBG7DjopKHazP7uE/dklfL1uH11aRzP81BakdojFpMqEUgghhBBVVQSfJPNJCNGUGIbh380z0KwWtV5q9Hq9XiZNuoURI86sdPz++x/Ge+h3uaZpdO/ekwsuOJt169bQt2//I7bn8Xi46aapDBgwCIBWrVpz8cXn8fPPPzF69NlHve67775h4MDB2O12BgwYRHh4OF99tcQffCouLubNN1/lvPMu4B//+Jf/2jPOGOr/97x571FQkM97731McnIzAE47rU/tBuUvBg0azC233Fbp2LXX3uj/t67r9OnTj23btrBkyUImT54CwJtvvorZbOGVV94gLMyXwNGnTz//dX379icxMYmFC7/wt5+evoPff9/K5Mm3HHd/ayKogk9OpxOHw1HleGRkJIWFR0//ev/99ykrK+Oaa645avsRERHH1f6xmOt4klNRQK2uC6kFq9bJDlonH/7Z5xe5SM8oZOWmA6z9I5ttu/PZtjufWEcIPdrF0iIhjBbx4bRMCCcitG5qRZ1sY97QZLwDS8Y78GTMA0vGW4BkPgkhmh7DMHjiv+vYsb/+lkMdTfsWkdx7xan1EoCqCBT91U8/reTNN19j5840SkpK/Mf37t191OCTqqr07n04yJKc3AybzUZWVtZR+/DzzyspKnIyapSvGLrVamXw4GF8++3X/lpLmzdvpLy8nLFjxx2xnbVrf+XUU3v7A08nqrqx2bVrJ3PnvsTmzRvJz8/zH9+7d3elfgwdOsIfePo7VVUZO3Ycn332MZMm3YLZbGXRoi9ISkrmtNP61knfjySogk/HKzc3lxdeeIEnn3yyVrvi1RVVVYiODquXth0Oe720G+yio8No2yqGkf3bkJVfypKfdvHlz7vJdZbz3fr9lc5NiLZzZr/WjO6fQlSE7YTvfbKOeUOR8Q4sGe/AkzEPLBnvk5vbIzWfhBBNUBPcQyEkJITQ0NBKx7Zt28Ldd0/jjDMGc+WVE4mKikFRFCZPvgaXy33U9mw2GxaLpdIxi8WC2+066nVffbWU8PBwunXrQVFREQADB57B4sUL+PHHFYwYcaa/DlJcXPwR23E6C2nbtmrd6eMVExNT6fvS0hKmT59KVFQUt946jcTEZGw2KzNnzsDtPjw2hYUFxMXFHbXtc845j7feep2ff17JoEGD+PLLJVxwweHlffUlqIJPDofD/wP/q8LCQiIjj7z28Pnnn6dTp0707t0bp9MJ+NL4vF4vTqeT0NBQzGYzDoeD4uLiattPTk4+7n7ruoHTWXrc11fHZFJxOOw4nWVoWsOkWAYLC3De6a05q28LNmzPYffBIvZmF7Mvq4TsgjKy8sv479Lf+XDZn/Tvlsiovi1JSaqaQXcsMuaBJeMdWDLegSdjHlh1Nd4Oh12ypxoxz6GfvUV+hkKIJkJRFO694tQmt+yuujZXrPiO8PBwHn10ZqU6R/WltLSEn376AZfLxbnnjqry+FdfLWHEiDP9dZBycrJJSEisti2HI5KcnOyj3s9qteL1eiodqy7+AVXHZ/PmTWRlZfLkk8/SoUNH//GSkmLgcNmhyMioKgXa/y4hIZF+/QawaNEXGIZOYWEB55xz3lGvqQtBFXxq27ZtldpLRUVFZGdnV6nV9Fc7d+7k119/pU+fqmsq+/Tpw2uvvcbgwYNp27Ytf/75Z6XHDcNg586dDBw48IT6XrEuta5pml5vbTc2Kgq9OsTTq8PhiHOZy8uGHTksW7OPnQec/LDxAD9sPEB0hI0Iu4XwUAvhdguOMCudW0XTLSUGm9V01PvImAeWjHdgyXgHnox5YMl4n9w8Hqn5JIRoehRFOebfME2By1WO2WyuFHj56qsl9Xa/77//FpfLxV133esv2F1hyZKFLFu2FKezkO7dexISEsLixQvo2rV7tW317t2XefP+y8GDB0lKSqr2nPj4BHbv3lnp2K+/rq5RX12ucoBK2V2bNm3gwIEM2rQ5HCvp3bsv3333NbfcciuhoUdenXXuuedz//3/pKAgn9NO60NS0vEn49RUUAWfBg8ezJw5cyrVflq6dCmqqh41OHTffff5M54qPP7444SEhDB9+nQ6derkb/+LL75g165dpKSkALBq1SoKCgoYMmRI/TwpUa/sNjP9uyXRv1sSaRmFLF+zjzW/Z5Ff5CK/qHKK5fI1+zCbVLqmRJPaPo6ubWKIc4Sgqk0wj1UIIYQ4CXm8vmV3UvNJCCEanz59+jF//gc8++wsBg8exubNG/nyy8X1dr9ly5aSlJTMuHEXVsk0cjgiWbJkId98s5zzz7+Ia6+9kVdemY2u65xxxhB03WDdujWMGjWazp27cumll7N06SKmTr2Ra665nmbNWpCRsY89e/b4C3sPHTqCjz76gM6du9GqVWu++mox2dlHr0lVoVu3HtjtoTzzzJNceeU1ZGdn8cYbc4mPr7zZ2rXX3shPP/3AzTffwBVXXE1sbBy7dqVTXl7OFVdM9J83YMAgoqKi2bRpIw8//O8THMmaCarg04QJE3j33XeZMmUKkydPJjMzk1mzZjFhwgQSEw+nt02cOJGMjAyWLVsGQJcuXaq05XA4CA0NpV+/w0XHRo8ezdy5c7n11luZPn06ZWVlzJo1i6FDh9KzZ8/6f4KiXrVrFkm78yK5YlRHsgvKKC7zUFzqoajMQ3Z+GRvScsgpLGdjWi4b03IBMJsU4qPsJETZSYoNpV2rGKJDzSRGhxJutxzjjkIIIYQIJm7Z7U4IIRqtAQMGMWXKbXz00YcsXryAHj1OYdas57jssgvr/F75+XmsXfsrV155TbVLANu370CHDh1Ztmwp559/EVdcMZGoqGjmz3+fJUsWEhoaSrduPYmK8tVmioyM4pVX3mDu3Jd4+eXZlJeXk5yczAUXjPe3ec01N5Cfn8d//vMaqqpw3nkXcvHFnXjxxeeO2d+YmFgee2wmL730HPfccyctW7bi7rvv47333q50XsuWrXjllTeZO/dFnn56Jpqm0bJlK6688ppK55nNZgYOPIPvvvuawYOH1X4Aj4NiGIYRkDvVUFpaGo899hjr168nLCyMcePGMW3atEqFxK+66ir279/PN998c8R2rrrqKkJDQ5k7d26l45mZmcyYMYMff/wRs9nMqFGjuO+++wgPr74afE1omk5eXsmxT6wFs1klOjqM/PwSWT5QRwzDYH9OCb9tz+G3Hb7aUZp+5Je/I9RCs7gwWiVG0DopgtaJESTFhEqmVB2R13hgyXgHnox5YNXVeMfEhDW6mk9paWnMmDGj0tzpjjvuOOomLKtXr+bqq6+u9rE2bdqwdOnSSse+++475syZw++//47FYqFz58489dRTR1xacCz1NXf612ur2ZtZxN2X9aJL6+g6bV9UJb/nAkvGO7AaYrw9Hje5uQeIjU3GYgn8RlrBwGxW5fUdALquc+ml5zNw4BncccfdRz33WK/Lms6dgirzCaBdu3a89dZbRz3n3XffPWY7RzonMTGR2bNnH0/XRCOnKAot4sNpER/O2NNT0HWDPGc5mYeKlmcXlJFdWM7uA05yCstxlnpw7ing9z0F/jasFpXmceE0iw0lOS6M5NhQmsWFkRBlr5dCfEIIIcTRFBYWMnHiRFJSUpg9ezaZmZnMnDmT8vJyHnzwwSNe161bNz788MNKx4qLi7nxxhsZPHhwpeOff/45//rXv7juuuu44447KCkpYc2aNbhcR99BqCFULLuTzCchhBCiKo/Hw44df/Ltt1+TlZXJxRdfGrB7B13wSYhAUVWFuCg7cVF2uqVU/nSjuNTNwbxS9mYVsyezmN2ZRezJLMLt0dl5wMnOA5VrjEVH2OjeJoYebWPpmhJDaIj8ryWEEKL+zZs3j5KSEl588UWioqIA0DSNRx55hMmTJ1cqW/BX4eHhpKamVjr2ySefoOs6Y8eO9R8rKCjg0Ucf5b777uPyyy/3Hx8xYkSdP5e6ULEblOx2J4QQQlSVk5PNjTf6lhBOm3Y3rVunBCzTTP5CFqIaIVYzKUkOUpIc/mO6bnAwr5SMnBIycks4kOv794HcUvKLXP6d9lRFoVViOHGRIURHhBDjsBEdYcNsUtF0w7cTk2ZgUn3nJceFoUrWlBBCiOOwYsUKBgwY4A88AYwZM4aHHnqIlStXcuGFNa+TsXDhQlJSUirVwVyyZAm6rjN+/PijXBk8PIcm0GbJfBJCCCGqSE5uxo8/rmmQe0vwSYgaUlWFZnFhNIurvGWl26Px574CNqXlsSk9l4N5pew6WMSug0U1atduM9OumYP2zSNp29xBm2QHYSFS7FwIIcSxpaenc9FFF1U65nA4iI+PJz09vcbt5OTk8PPPP3PzzTdXOr5hwwbatGnDZ599xiuvvEJmZiYdOnRg+vTpQblTsCy7E0IIIYKTBJ+EOEFWi4nubWLp3iaWy+hAdkEZezKLyCtykV/x5SxHMwxMqopJVTCZFNxujV2ZRZS5vGzemcfmnXn+NhNjQmmbHEHbZpG0SXbQKjFcto0WQghRhdPpxOFwVDkeGRlJYWFhjdtZvHgxmqZVWnIHkJ2dzc6dO3n++ee5++67iY+P57333uOWW27hs88+o0OHDsfd97rOTjKZVH/mU4jVJNlPAVBRYLaxFelvrGS8A6shxlvXT+7VEBWLQRQFgmtbtKaptuNtMikn9N4qwSch6lh8lJ34KHuNztV0nX1ZJezYX0ja/kLSM5xkFZSRmVdKZl4pq7ZkAmA2KbROjKBNMwetEyOICLUQFmIhzG4h3G4hNMQsS/eEEEIctwULFtCtWzfatGlT6bhhGJSWlvJ///d//jpPffv2ZfTo0bz22mvMmjXruO6nqgrR0WHHPrEWNN3w72IbHxeBI+zk3CmqITgcNZv3iLoh4x1YgRzv8nITOTnqCf+R39hJgDWwjjXeuq6gqiqRkaGEhIQc930k+CREAzKpKq2TImidFMGI01oAUFzm8RU1z3CSfsBJeoaT4jIPaRlO0jKc1bZjNqnER4UQF2knLiqEmAgbum7g9uq4PTpur0a43UJqhzjaJDskUCWEEE2Ew+GgqKjqMu/CwkIiIyNr1MaePXvYuHEj9957b7XtA/Tv399/zGKx0KdPH7Zv336cvfbVUXQ6S4/7+up49cMf25YUl6G5PXXavqjKZFJxOOw4nWVommyNXt9kvAOrIcbb7Xah6zqaZgSsCHQwURTfuGuaLplPAVDT8dY0A13XKSwspaxMq/K4w2GvUcBQgk9CBJlwu4UebWPp0TYW8H3qnF1QRlqGLxB1MLeE4jIvJeUeiss8lLs1vJrOgdxSDuQefSK/aNVuoiNs9OoQx6kd44kMt/kLoHs1HZNJoXlcGCFW+dUghBCNQdu2bavUdioqKiI7O5u2bdvWqI0FCxagqipnn312lcfat29/xOtcLlftOvs3df2HVbnn8IRYqYf2xZFpmi7jHUAy3oEVyPHWtJM74lIRAJHAU2DUdrxPNCgqf2EKEeQURSEhOpSE6FAGdEuq8rhX08krcpFTUEZ2QRk5heUUFLkwmVSsZhWrxYTVrJKRW8KGtFzyi1x8s24/36zbX/39gKTYUFKSImid5KBlQjhJMaFEhVtRJGNKCCGCyuDBg5kzZ06l2k9Lly5FVVUGDhxYozYWLVpE3759SUhIqPLYsGHDmD17NqtWrWLkyJEAuN1ufv31V3r37l13T6QOVEyIFcWXWSyEEEKI4CHBJyEaObNJJSHKTkIN6kx5vBpbd+Wz9s9sNqfn4tUM35pyVcVsUij3aBQWu/1ZVBU1p8BXvDUxJpTEaDuKoqBpur++hsWkEh9tJzHaTmJ0KIkxoTjCLDL5F0KIejZhwgTeffddpkyZwuTJk8nMzGTWrFlMmDCBxMRE/3kTJ04kIyODZcuWVbp+69atpKWlce2111bbfrdu3Rg9ejQPPPAABQUFxMfH8/7775OTk8P1119fr8+ttiqKjctOd0IIIUTwkeCTECcRi9nEKe3jOKV93BHPKSx2setgEbsPFrHrYBEZOSVkF5ZR7tbYfeh4TdksJkJDzITazITbLbRMCCclOYKUJActEsIBX9H1PGc5BcVunKVuwkLMREfYiAq3yQ5/QghxDJGRkbz99ts89thjTJkyhbCwMMaPH8+0adMqneerIVK1TsOCBQuwWq2MHj36iPeYOXMmzzzzDE8//TTFxcV069aN//znP3Tq1KnOn8+J8ByqyWKR9w4hhGhQgwYdOzP2vvse4uyzzz3ue2zf/gcrVnzHFVdMrFUR7Hvumc6PP67g/vsf4ayzzjnu+4vaUwxDVlSeKE3TycsrqdM2zWaV6Ogw8vNLZE13gMiYH5nHq5NVUMbB3FJyCstQ8BWnM5kUTKpCuVsjK7+MzPxSsvLLyCkoRz/GrxabxUSY3Ux+keuI64wdYVZiHSE0jwujRXwYzRPCaREfjiPUIksAa0le34EnYx5YdTXeMTFhsstOANTH3Gl/TgkPvL6aqHArz0wdVKdti+rJ77nAkvEOrIYYb4/HTW7uAWJjk7FYGu+OnZs3b6r0/U03Xcv48ZcycuRZ/mPNm7cgOjq6yrVms1qj8V68eAGPP/4ICxcuJyoqqkb9cjoLGTfuLDweD/36nc7TT79Qo+uaspqM97FelzWdO0nmkxDimCxmleZxYTSPq9m22JquU+bSKC33+P+bX+xi98Fidh10sjuzCJdHw3WoOKxJVYgMtxIRaqWkzENBsQuvZuAsceMscbPzQOVd/lRFIcRqIsRmIsRqxm414QizEhlm9f033EZYiBmrxYTtUN0ri1nFpCooioKi+NqwmFWiwm2oqgSyhBCisfMeynySrFkhhGhY3bv3qHIsISGp2uOB9O23X+PxeOjduy9r1qwmPz+P6OiYBu1TBU3TMAwDs7nphmia7jMTQjQYk6oSblcJt1sqHT+9u++/um6QVViG3W7DhE6ozYz6l0wmwzAoKvNQUOQiK7+MfdnF7MsuYV92Mdn5ZeiGQanLS6nLC5zYbksmVSE6wkZ8lJ24yBDio+wkxYSScKh+lc1qAnx/1JSW+3YZBIiJCPE/JoQQouFJzSchhGg8Fi9ewIcfvsfevXtwOCIZM2YsN9xwE+ZDv8OLiop4+eXnWbVqJU5nIVFR0fTo0ZNHHnnCn/UEMHasbzOMpKRkPv54wVHvuWzZUlq0aMmtt05n4sQJfP31V4wfP6HSOdnZWcyZ8yK//PIzJSUlJCUlcf7547nkksv85yxZspD5899n9+5d2O12unTpxl133UtSUjJvvDGXefP+y7JlP1Rq96yzhnLxxZdx/fWTAZg6dRKhoaEMGzaSd955k4yM/cyd+x/i4hJ49dWXWL9+Hbm5OSQkJDBs2EiuvfZGrNbDWUe6rjN//vssWPAZGRn7iYhw0LNnKvfc8wCZmQeZOHECzz77In369Pdfo2kaF100ljPPPItbbrm9tj+yEybBJyFEwKmqQov48COmMiuKgiPUiiPUSqvECHp3PrwDk9ujUVLupdztpdytUe7yUurScJa6KSx24SxxU1DsptTlxePVcHt0XB4Nj1dHNwwMwxfc0g1fW5pukFNYTk5hebV9jQi14PbquNxVa6WE2y3EOGzERIQQYjWhKAqq6suqslpMtIgPo1ViBC3iw7CYDweqDMOgzOWlqMxDhN1KaIj8KhZCiBMlwSchRFNlGAZ43Q1zc3Pd73g9b95/eeWV2VxyyeVMnXoHu3bt4tVXX0bXdW691RcUmT37GVav/ombbrqVpKRkcnNz+PnnnwAYMGAQEydez9tvv8HTT88mLCwcq9VytFuSlZXJhg3rueaaG2jXrj3t2rVn2bIvKwWfCgsLmDzZtwHHpEm30KxZc/bu3UNGxj7/Oe+//w4vv/wCY8eOY9KkW/B6vaxdu4aCgnySkpJrNQ6//76NAwcyuOGGm4iIcJCQkEh+fj4ORyS33jqNiIgI9u7dw5tvvkpubg733feQ/9pnn32KL774hEsuuZw+ffpRWlrCTz/9SFlZKe3atadr1+4sXPhFpeDT6tWryMnJ5pxzxtWqn3VF/uIRQjQqVosJq8UE2E64Ld0wKChykVNYTnZBGTmF5WTll5KZX0ZmXikl5V6KSj2VrrHbzOiGgcutUVzmobjMw57M4qPex6QqJMeGEWIzUVDkorDE7f8jydemiRhHCLGOEGIjQ4iPtBMfFULcof/abWapcSWEEMfgLzguwSchRBNiGAalX/wbPXNHg9zflNgB+3n31dlctLS0hDfeeJXLL7+ayZOnANCnT38sFjOzZz/L1VdPJCzMwbZtWxg58izGjBnrv3bkSN/mGNHR0TRv3gKATp261Kjm0/LlX2IYBqNGjT7U1lnMnfsi+/fv87c1b957FBTk8957H5Oc3AyA007r42+juLiYN998lfPOu4B//ONf/uNnnDH0uMbC6SzktdfeJjExyX8sJiaWqVPv8H/fo8cphITY+fe/H2L69H8SEhLCnj27+eyzj5k06RauuurwbrVDh47w//u8887nmWeewul04nA4AFi06HN69OhJ69Ypx9XfEyXBJyHESUtVFGIcIcQ4QujYMqrK48VlHvKc5disJsJCLL7lgariz1zKdbrIc5aTV+TC49HQDV9AS9d9ywL3ZhaxO7OY4jIP+7KrBqisFhW3x1cfa392Cfuzj1x8t6JOlUlVMJlUIkItvuywMCuOUAuR4TZfFpYjhJgIG9ERNlwenaJSN0UlbkpcXmwhVsKtKvFR9ipLIoUQorGryKKVmk9CiKZGoel8CLlp00bKykoZNmwEXq/Xf7x37364XC7S0tLo2bMXHTt2ZsmShcTGxtG//wDatm1/QvddtmwpHTt2plWrFABGjRrNq6++xLJlS7nmmhsAWLv2V049tbc/8PR3mzdvpLy8nLFj6yZzqF27DpUCT+ALNn700Qd88cWnZGRk4HYfLjGSkbGPtm3bs27drxiGcdR+jBgxmhdeeJZly5Zy0UWXUFBQwMqVP3DXXffWSd+PhwSfhBDiCMLtlmqDNIqiEBpiITTEQsuE8KO2YRgG+UUudmcW4dUMosJ9BdGjwqxYLSZcbo28onJyC8vJdZb7lwBmF5SRXVDmz7wyDNAMA003wKtT5vKSlV92Qs8tMcZOZJgNm8WEzWoi5FBhdv3QfXTdQNMMLBaV+MgQ4qLsxEfZiXWESGaBECLoyLI7IURTpCgK9vPuazLL7goLCwC47rorq308M/MgANOm/QOHYy4ffvhfXn75eRISErnqqmu54ILxtb7nrl072b79T66/fjJFRUUAhIWF07lzl0rBJ6ezkLZt2x2xHaezEIC4uPha96E6MTFVi53Pn/8+L730PJdffjWnntqbiIgItm3byjPPPInb7XsNFBYWYjKZjlos3W63M3LkmSxa9DkXXXQJX321GIvFyvDho+qk78dDgk9CCFGPlL9kV1XHZjWRHBtGcmz1Owm63Bour4Z+KBik6wYeTaeo1OPbDbDUfajOlYs8p4vcQ5lYLreGAoTZLf7sKKvVzL6sIvKcLt+Swf2eau95zOcExEWF0Cw2jGZxvq/IMCtZBWUcyC3lYG4JB/PKMJsUmsWF0Tz+0HmxYf6dCCUzQQhR12TZnRCiqVIUBSwnXnIiGERE+JaA/fvfT5GYmFjl8ZYtfUvgwsPDuf32O7n99jtJS9vBRx99wNNPz6Rt23acckqvWt3zq6+WAPDGG3N54425VR7/44/f6dSpMw5HJDk52Udsx+GIBCAnJ5uEhKp9B7BabZUyugC8Xi9lZVU/NK4uqPftt18zcOBgbrppqv/Yrl07K50TGRmJpmnH3K3vvPMu4IsvPmX79j9ZtGgBw4ePJDQ09Ijn1zcJPgkhRBCzWU3V7qqXHHvkawzDoNytYbOYUFXfm5rZrPoLvJeUesg8VNuquNSNy6NT7vb6irN7tb8s71MwqSrlbi85BeVkF5aRU1COy6ORXVBOdkE5G9Jyj9r/zPwy1m/PqXLcbjMTbjcTarNgMauYTQoWswmzSUHTDdweDZdHx+3R8OoGYSFmfyZauN1CaIgZu9VMiM3k/6/FpGL2fymYTaqvbbOKpeLfEvQSosnyZz7J/+dCCBG0unfvSUhICNnZmQwZMqzK42azWmUzonbt2nPbbdNZuPBzdu3aySmn9MJs9q1O+OuytCNZvvxLunXr4a8xVcHr9fLPf07jq6+W0KlTZ3r37su8ef/l4MGDJCUlVWmnou+LFy+ga9fu1d4rISEBj8dTqZbU2rW/omlVNy+qjstVjsVSeeVFRfCswqmn9kFRFBYt+oIrr7zmiG117tyVDh068vzz/0da2nbuvPOfNepDfZHgkxBCNDGKomC3HfnXu81qolViBK0SI2rdtmEYOEs9HMgpISO3hIwc35ez1ENClJ2k2FCSY0JJig3F49XZf+jx/TklHMwtpbjMl21V5vJS5vIC1e8yWF/sNjOxDpu/uHtkuA39ULDL7fUFuwwDf/DKbFIxmRRcbo1Sl5fSci+l5b7n0KaZg44toujQMkpqaAkRBGTZnRBCBL+IiAiuv/4mXn55NllZWfTqdRomk4mMjH388MMKnnzyKcxmGzfffB1nnDGMtm3bYTKpLF26CIvF4s96SklJAeCTTz7ijDOGEhISQrt2VetCbd68kYyM/UyceD2nntq7yuMDBgzi66+/YsqU27n00stZunQRU6feyDXXXE+zZi3IyNjHnj17uOWW2wgPD+faa2/klVdmo+s6Z5wxBF03WLduDaNGjaZz56707386drudJ5+cwRVXTCQ7O5OPPpqH1VqzzLU+ffrx0Ufz+N//PqRly9Z8+eVi9u3bV+mcVq1aM27cRbz22is4nU569+5LeXk5q1b9yHXXTSI+/vBO4eeeewHPPPMkrVq1pmfP1Br+lOqHBJ+EEELUmKIoRIZZiQyz0rl19DHP75pSORVY1w1Kyj3+nQLLXF48XgOPpuH1+pYUmlUFm9WE1WzCZlFRVYXScq//mqJD15W5vJS7tUP/1tB0HY9XR9MNvJrv315Nx6sZ/vuXubzsy/ay7yjF3Wvqz32FfPnLXgCaxYWRGG33ZapZfF8hVhOhoTZcLg+GYaDgWxbkLHHjLPFQWOKmuMxNuN1CUkyo7ys2jLjIEP9YabqBYRioqlKpNlfFfWQXRCEO82pScFwIIRqDyy67kvj4eD788D3+978PMZvNNG/egtNPP8Of0dSjxyl8+eUiMjIyUFWFtm3b8+STz5KS0gaAjh07c911k1i48HPef/8dEhIS+fjjBVXutWzZUkJCQhg2bESVxwDGjDmHFSu+Zf36tZx2Wh9eeeUN5s59iZdfnk15eTnJycmV6kxdccVEoqKimT//fZYsWUhoaCjduvUkKso3542MjGLGjFm8+OKz3HvvXXTo0JH773+EW2+dXKOxueaaGykoKOD1133LA4cOHcEdd9zFP/85rdJ506f/g2bNmvHFF58xf/77REZGkpp6apVldYMHD+OZZ57knHPOq9H965NiGIZx7NPE0WiaTl7eif8h81d/XSLz97RDUT9kzANLxjuwTubx1g0Dr1fH7dUpLHGT5zxc4L2w2I3ZrGI1q1gtKlazCUUBTfMFwjTNF8iyWU2EhpgJtZkJDbHg9mjs2F/In3sLOJBb2iDPy6QqvmWIoRbCQyxYLSbK3b5AXLnbF5iz20zEOkKIjgghNtJGuN1KYbGLvCJffbB8Zzlur+4PllX8Ny7KTnJsGM3iQmkWG0Z0hC3oA1119RqPiQnDJMGLelcfc6fPftzJFz/uZGTvFlw+smOdti2qdzK/tzQEGe/Aaojx9njc5OYeIDY2GYvFGpB7Bpvqlt2J47dw4ec89dTjfPLJImJj46o8XpPxPtbrsqZzJ8l8EkII0aSpioLVYsJqMRFut9A8rvri7rU1sEcyAM5SN2n7CikoceNya7i9Gi6PhlczsFrNlJV70DUd/dByPkeYFUeYlchQK2F2C84SN5n5pRzMLeVgXil5RS5UxZdlpqoKqqKg6wYuj0a5R8Pt1jAATTcoLHFTWHLk3W+KyzxkFxx7aWMRfy8+n1/pO7NJxW7zBaZCrGZCrL6dEVVVwfTXfhqHdkk89F9VVYiwW4gItVYKlFXU7gqzWwixmvxLHl0eDbdHx+PV8Gi6PxtO03XCQixEhFpwhFqJCLVgMVethSZObl4pOC6EEEIAcOBABvv27eHtt99gxIgzqw08BVrQBZ/S0tKYMWMG69evJywsjHHjxnHHHXdgtR498nvXXXexceNGsrKysFgsdOzYkZtvvplBgwb5z9m3bx8jRlRNtzvllFOYP39+nT8XIYQQTZ8j1EqvjlW33K2vT0wNw8Dt0Skp91BU6qG43ENxqQePVyfEasJu8wWHbFYTpeVe8g7tgJjrLKe41ENkmPXQDow2YhwhhFhM/sCW69Ayxsz8Ug7klJKRW0JWfhleTaeo1LfLYrBoHh/G/Vf3xmaRIJTwkYLjQgghhM+bb77KsmVL6d69J1On3tHQ3QGCLPhUWFjIxIkTSUlJYfbs2WRmZjJz5kzKy8t58MEHj3qtx+PhmmuuISUlBZfLxccff8ykSZN455136N27cmGx6dOn069fP//3YWF18ym4EEIIUd8URfHvghjjCKn3+3k1nYIiF+Vu7dCXb0mfV/PV16rIdDJ0A+UvmVAmVcGj6b5aXaWHAmVlh79Kyj2UlHnRD63+t1pUbBZfrS+r5fDOhRaTr72Sci9FpW6KSj1oukFBkQvPoSWDQgCkJEVgUhXaNo9s6K4IIYQQDepf/3qYf/3r4YbuRiVBFXyaN28eJSUlvPjii0RFRQGgaRqPPPIIkydPJjEx8YjXPv/885W+Hzx4MCNGjODzzz+vEnxq3bo1qampdd19IYQQoskxm1Tiouz10nZFPS6zWUWtYU0pwzAoc3mxmE2yvEpUcsYpzThrYFtKisulXogQQggRZIJq1rZixQoGDBjgDzwBjBkzBl3XWblyZa3aMplMRERE4PEEzxIBIYQQQhxWUY+rpoEn8GV+hYZYJPAkqmWVTDghhBAiKAXVzC09PZ22bdtWOuZwOIiPjyc9Pf2Y1xuGgdfrJT8/nzfeeIPdu3dz6aWXVjnv4YcfpkuXLgwYMID777+fgoKCunoKQgghhBBCCCHEcZMN6UUwqavXY1Atu3M6nTgcjirHIyMjKSwsPOb1H3/8Mffffz8AoaGhPPvss/Tq1cv/uNVq5bLLLmPQoEE4HA42bNjAnDlz2Lx5Mx999BEWi+W4+26u409gK7YqlO2eA0fGPLBkvANLxjvwZMwDS8ZbCCFEY2cy+bI33W4XVqutgXsjhI/b7QLAZDqx8FFQBZ9O1IgRI+jcuTP5+fksXbqUO+64gxdffJEhQ4YAkJCQwMMPP+w/v2/fvnTo0IHJkyezbNkyzj777OO6r6oqREfXT9Fyh6N+6myII5MxDywZ78CS8Q48GfPAkvEWQgjRWKmqCbs9nOLifACsVhtKLZamNwW6rqBpkvkVKEcbb8MwcLtdFBfnY7eHo6on9gFfUAWfHA4HRUVFVY4XFhYSGXnsnUtiYmKIiYkBfAXHCwsLeeqpp/zBp+oMGTKE0NBQtmzZctzBJ103cDpLj+vaIzGZVBwOO05nGZomRTMDQcY8sGS8A0vGO/BkzAOrrsbb4bBL9pQQQogG43D4/p6tCECdbFRVRddl3hQoNRlvuz3c/7o8EUEVfGrbtm2V2k5FRUVkZ2dXqQVVE926dWPFihV11b2jqq9dVTRNlx1bAkzGPLBkvANLxjvwZMwDS8ZbCCFEY6YoCpGRsURERKNp3obuTkCZTAqRkaEUFpZK9lMA1GS8TSbzCWc8VQiq4NPgwYOZM2dOpdpPS5cuRVVVBg4cWOv21q5dS8uWLY96zrfffktpaSk9evQ4rj4LIYQQQgghhBB1SVVVVNXa0N0IKLNZJSQkhLIyTT5ICoBAj3dQBZ8mTJjAu+++y5QpU5g8eTKZmZnMmjWLCRMmkJiY6D9v4sSJZGRksGzZMgC+++47PvvsM4YOHUpycjKFhYUsXLiQH3/8kWeeecZ/3cyZM1EUhdTUVBwOBxs3bmTu3Ll0796dkSNHBvz5CiGEEEIIIYQQQjR1QRV8ioyM5O233+axxx5jypQphIWFMX78eKZNm1bpPF3X0TTN/33Lli1xu908/fTT5OfnEx0dTadOnXj33Xfp27ev/7x27drxwQcfMH/+fMrLy0lMTGT8+PHcdtttmM1BNRRCCCGEEEIIIYQQTYJiGIYspjxBmqaTl1dSp22azSrR0WHk55dIymGAyJgHlox3YMl4B56MeWDV1XjHxIRJwfEAkLlT0yBjHlgy3oEl4x14MuaBFei5kwSf6oBhGOh63Q+jyaTKDkkBJmMeWDLegSXjHXgy5oFVF+OtqspJt611Q5C5U9MhYx5YMt6BJeMdeDLmgRXIuZMEn4QQQgghhBBCCCFEvZG8ciGEEEIIIYQQQghRbyT4JIQQQgghhBBCCCHqjQSfhBBCCCGEEEIIIUS9keCTEEIIIYQQQgghhKg3EnwSQgghhBBCCCGEEPVGgk9CCCGEEEIIIYQQot5I8EkIIYQQQgghhBBC1BsJPgkhhBBCCCGEEEKIeiPBJyGEEEIIIYQQQghRbyT4JIQQQgghhBBCCCHqjQSfhBBCCCGEEEIIIUS9keCTEEIIIYQQQgghhKg3EnwKQmlpaVx77bWkpqYycOBAZs2ahdvtbuhuNXpLlizh5ptvZvDgwaSmpjJu3Dg+/vhjDMOodN5HH33E6NGj6dGjB+eddx7ffvttA/W4aSkpKWHw4MF06tSJTZs2VXpMxrxuffrpp5x//vn06NGDfv36ccMNN1BeXu5//JtvvuG8886jR48ejB49mv/9738N2NvG7euvv+biiy+mV69eDBo0iNtvv529e/dWOU9e47W3e/duHnzwQcaNG0fXrl0ZO3ZstefVZGyLioq477776Nu3L7169eK2224jKyurvp+CCCCZO9UPmTs1LJk7BY7MnQJH5k71J9jnThJ8CjKFhYVMnDgRj8fD7NmzmTZtGvPnz2fmzJkN3bVG76233sJut3PPPffwyiuvMHjwYB544AFeeukl/zmLFi3igQceYMyYMbz22mukpqYydepUfvvtt4breBPx8ssvo2laleMy5nXrlVde4bHHHuPss8/mjTfe4NFHH6VFixb+sV+zZg1Tp04lNTWV1157jTFjxvCvf/2LpUuXNnDPG5/Vq1czdepU2rdvz0svvcR9993H77//znXXXVdpwiqv8eOzfft2vv/+e1q3bk27du2qPaemY3vHHXewcuVKHn74Yf7v//6PnTt3cuONN+L1egPwTER9k7lT/ZG5U8OSuVNgyNwpcGTuVL+Cfu5kiKAyZ84cIzU11cjPz/cfmzdvntGlSxfj4MGDDdexJiA3N7fKsfvvv9849dRTDU3TDMMwjDPPPNOYPn16pXMuvfRS44YbbghIH5uqHTt2GKmpqcYHH3xgdOzY0di4caP/MRnzupOWlmZ07drV+O677454znXXXWdceumllY5Nnz7dGDNmTH13r8l54IEHjOHDhxu6rvuPrVq1yujYsaPx66+/+o/Ja/z4VPxeNgzD+Oc//2mcc845Vc6pydiuW7fO6Nixo/HDDz/4j6WlpRmdOnUyFi1aVA89F4Emc6f6I3OnhiNzp8CQuVNgydypfgX73Ekyn4LMihUrGDBgAFFRUf5jY8aMQdd1Vq5c2XAdawJiYmKqHOvSpQvFxcWUlpayd+9edu3axZgxYyqdc/bZZ7Nq1SpJ3z8BM2bMYMKECbRp06bScRnzuvXJJ5/QokULhgwZUu3jbreb1atXc9ZZZ1U6fvbZZ5OWlsa+ffsC0c0mw+v1EhYWhqIo/mMREREA/iUp8ho/fqp69ClKTcd2xYoVOBwOBg4c6D+nbdu2dOnShRUrVtR9x0XAydyp/sjcqeHI3CkwZO4UWDJ3ql/BPneS4FOQSU9Pp23btpWOORwO4uPjSU9Pb6BeNV1r164lMTGR8PBw//j+/U2+Xbt2eDyeatcii2NbunQpf/75J1OmTKnymIx53dqwYQMdO3bk5ZdfZsCAAXTv3p0JEyawYcMGAPbs2YPH46nyO6YiLVd+x9TOhRdeSFpaGu+99x5FRUXs3buXZ555hq5du3LqqacC8hqvTzUd2/T0dNq0aVNpogu+SZS85psGmTsFlsyd6p/MnQJH5k6BJXOnhtXQcycJPgUZp9OJw+GocjwyMpLCwsIG6FHTtWbNGhYvXsx1110H4B/fv49/xfcy/rVXVlbGzJkzmTZtGuHh4VUelzGvW9nZ2fz44498/vnnPPTQQ7z00ksoisJ1111Hbm6ujHcd6927Ny+++CJPP/00vXv3ZuTIkeTm5vLaa69hMpkAeY3Xp5qOrdPp9H+q+lfyvtp0yNwpcGTuVP9k7hRYMncKLJk7NayGnjtJ8EmclA4ePMi0adPo168fV199dUN3p8l65ZVXiI2N5aKLLmrorpwUDMOgtLSU559/nrPOOoshQ4bwyiuvYBgG//3vfxu6e03OunXr+Mc//sEll1zC22+/zfPPP4+u60yaNKlS0UwhhGgKZO4UGDJ3CiyZOwWWzJ1ObhJ8CjIOh4OioqIqxwsLC4mMjGyAHjU9TqeTG2+8kaioKGbPnu1fG1sxvn8ff6fTWelxUTP79+/nzTff5LbbbqOoqAin00lpaSkApaWllJSUyJjXMYfDQVRUFJ07d/Yfi4qKomvXruzYsUPGu47NmDGD/v37c88999C/f3/OOussXn31VbZu3crnn38OyO+V+lTTsXU4HBQXF1e5Xt5Xmw6ZO9U/mTsFhsydAk/mToElc6eG1dBzJwk+BZnq1lEWFRWRnZ1dZa2xqL3y8nImT55MUVERr7/+eqV0worx/fv4p6enY7FYaNmyZUD72tjt27cPj8fDpEmT6NOnD3369OGmm24C4Oqrr+baa6+VMa9j7du3P+JjLpeLVq1aYbFYqh1vQH7H1FJaWlqlySpAUlIS0dHR7NmzB5DfK/WppmPbtm1bdu7c6S9kWmHnzp3ymm8iZO5Uv2TuFDgydwo8mTsFlsydGlZDz50k+BRkBg8ezE8//eSPPoKv6KCqqpWqzYva83q93HHHHaSnp/P666+TmJhY6fGWLVuSkpLC0qVLKx1fvHgxAwYMwGq1BrK7jV6XLl145513Kn3de++9ADzyyCM89NBDMuZ1bNiwYRQUFLBt2zb/sfz8fLZs2UK3bt2wWq3069ePL7/8stJ1ixcvpl27drRo0SLQXW7UmjVrxtatWysd279/P/n5+TRv3hyQ3yv1qaZjO3jwYAoLC1m1apX/nJ07d7J161YGDx4c0D6L+iFzp/ojc6fAkrlT4MncKbBk7tSwGnruZD7uK0W9mDBhAu+++y5Tpkxh8uTJZGZmMmvWLCZMmFDlDV/UziOPPMK3337LPffcQ3FxMb/99pv/sa5du2K1Wrn11lu56667aNWqFf369WPx4sVs3LhR1nwfB4fDQb9+/ap9rFu3bnTr1g1AxrwOjRw5kh49enDbbbcxbdo0bDYbr776KlarlcsvvxyAm2++mauvvpqHH36YMWPGsHr1ahYuXMizzz7bwL1vfCZMmMDjjz/OjBkzGD58OAUFBf5aHX/dwlZe48enrKyM77//HvBNTIuLi/2Tpb59+xITE1Ojse3VqxeDBg3ivvvu45///Cc2m41nn32WTp06ceaZZzbIcxN1S+ZO9UfmToElc6fAk7lTYMncqX4F+9xJMf6eSyUaXFpaGo899hjr168nLCyMcePGMW3aNInynqDhw4ezf//+ah/7+uuv/Z9cfPTRR7z22mtkZGTQpk0bpk+fzrBhwwLZ1SZr9erVXH311Xz88cf06NHDf1zGvO7k5eXxxBNP8O233+LxeOjduzf33ntvpbTyr7/+mueee46dO3fSrFkzJk2axPjx4xuw142TYRjMmzePDz74gL179xIWFkZqairTpk3zb8FcQV7jtbdv3z5GjBhR7WPvvPOO/w+0moxtUVERTzzxBMuWLcPr9TJo0CDuv/9+CUw0ITJ3qh8yd2p4MneqfzJ3ChyZO9WvYJ87SfBJCCGEEEIIIYQQQtQbqfkkhBBCCCGEEEIIIeqNBJ+EEEIIIYQQQgghRL2R4JMQQgghhBBCCCGEqDcSfBJCCCGEEEIIIYQQ9UaCT0IIIYQQQgghhBCi3kjwSQghhBBCCCGEEELUGwk+CSGEEEIIIYQQQoh6I8EnIYQQQgghhBBCCFFvJPgkhBAB8Mknn9CpUyc2bdrU0F0RQgghhAh6MncSomkxN3QHhBCirnzyySfce++9R3z8ww8/JDU1NXAdEkIIIYQIYjJ3EkIEigSfhBBNzm233UaLFi2qHG/VqlUD9EYIIYQQIrjJ3EkIUd8k+CSEaHIGDx5Mjx49GrobQgghhBCNgsydhBD1TWo+CSFOKvv27aNTp0688cYbvPXWWwwbNoyePXty5ZVX8ueff1Y5f9WqVVx++eWkpqbSu3dvbr75ZtLS0qqcl5mZyX333cegQYPo3r07w4cP56GHHsLtdlc6z+1288QTT9C/f39SU1OZMmUKeXl59fZ8hRBCCCFOhMydhBB1QTKfhBBNTnFxcZVJiaIoREdH+7//7LPPKCkp4fLLL8flcvHuu+8yceJEFixYQFxcHAA//fQTN954Iy1atGDq1KmUl5fz3//+l8suu4xPPvnEn56emZnJ+PHjKSoq4pJLLqFt27ZkZmby5ZdfUl5ejtVq9d93xowZOBwOpk6dyv79+3n77bd59NFHee655+p/YIQQQgghqiFzJyFEfZPgkxCiybnmmmuqHLNarZV2S9mzZw9fffUViYmJgC/d/OKLL+a1117zF96cNWsWkZGRfPjhh0RFRQEwcuRILrjgAmbPns2TTz4JwDPPPENOTg7z58+vlLJ+++23YxhGpX5ERUXx5ptvoigKALqu8+6771JUVERERESdjYEQQgghRE3J3EkIUd8k+CSEaHIefPBB2rRpU+mYqlZeZTxy5Ej/5AmgZ8+enHLKKXz//ffce++9ZGVlsW3bNm644Qb/5Amgc+fOnH766Xz//feAbwK0fPlyhg0bVm2thIqJUoVLLrmk0rHevXvz1ltvsX//fjp37nzcz1kIIYQQ4njJ3EkIUd8k+CSEaHJ69ux5zKKZrVu3rnIsJSWFJUuWAJCRkQFQZSIG0K5dO3788UdKS0spLS2luLiYDh061KhvzZo1q/S9w+EAwOl01uh6IYQQQoi6JnMnIUR9k4LjQggRQH//FLHC31PMhRBCCCGEzJ2EaCok80kIcVLavXt3lWO7du2iefPmwOFP2Xbu3FnlvPT0dKKjowkNDSUkJITw8HC2b99evx0WQgghhGhAMncSQpwIyXwSQpyUli9fTmZmpv/7jRs3smHDBgYPHgxAQkICXbp04bPPPquU1v3nn3+ycuVKhgwZAvg+jRs5ciTffvttpaKcFeRTOSGEEEI0BTJ3EkKcCMl8EkI0OStWrCA9Pb3K8VNPPdVfsLJVq1ZcdtllXHbZZbjdbt555x2ioqK44YYb/Of/4x//4MYbb+TSSy9l/Pjx/u2CIyIimDp1qv+86dOns3LlSq666iouueQS2rVrR3Z2NkuXLuX999/31yYQQgghhAhGMncSQtQ3CT4JIZqcF154odrjTzzxBH379gXg/PPPR1VV3n77bXJzc+nZsycPPPAACQkJ/vNPP/10Xn/9dV544QVeeOEFzGYzffr04e6776Zly5b+8xITE5k/fz7PP/88CxYsoLi4mMTERAYPHkxISEj9PlkhhBBCiBMkcychRH1TDMlrFEKcRPbt28eIESP4xz/+wfXXX9/Q3RFCCCGECGoydxJC1AWp+SSEEEIIIYQQQggh6o0En4QQQgghhBBCCCFEvZHgkxBCCCGEEEIIIYSoN1LzSQghhBBCCCGEEELUG8l8EkIIIYQQQgghhBD1RoJPQgghhBBCCCGEEKLeSPBJCCGEEEIIIYQQQtQbCT4JIYQQQgghhBBCiHojwSchhBBCCCGEEEIIUW8k+CSEEEIIIYQQQggh6o0En4QQQgghhBBCCCFEvZHgkxBCCCGEEEIIIYSoNxJ8EkIIIYQQQgghhBD15v8BtM6FJOqz7Y0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"fashionmnist_simple_cnn.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:43:06.955805Z",
     "start_time": "2024-04-06T20:43:06.810295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"fashionmnist_simple_cnn.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
