{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "authorship_tag": "ABX9TyO9ycf2GLNwAh4wkdecvhq8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ebnFsErY0EM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462301946,
     "user_tz": -660,
     "elapsed": 73092,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "710b1d81-a854-4328-959c-884f3b89fa61",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:07.320372Z",
     "start_time": "2024-04-06T21:07:03.773132Z"
    }
   },
   "source": [
    "# Install packages\n",
    "!pip install torch torchvision torchaudio pennylane pennylane-lightning pennylane-lightning[gpu] cotengra quimb torchmetrics --upgrade"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.17.2)\r\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: pennylane in /usr/local/lib/python3.9/dist-packages (0.35.1)\r\n",
      "Requirement already satisfied: pennylane-lightning in /usr/local/lib/python3.9/dist-packages (0.35.1)\r\n",
      "Requirement already satisfied: cotengra in /usr/local/lib/python3.9/dist-packages (0.5.6)\r\n",
      "Requirement already satisfied: quimb in /usr/local/lib/python3.9/dist-packages (1.7.3)\r\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (1.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from torch) (4.11.0)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.0.106)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch) (2.19.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.2.0)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\r\n",
      "Requirement already satisfied: rustworkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.14.2)\r\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.10.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\r\n",
      "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.6.9)\r\n",
      "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.10.0)\r\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\r\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.4.4)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\r\n",
      "Requirement already satisfied: autograd in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.6.2)\r\n",
      "Requirement already satisfied: pennylane-lightning-gpu in /usr/local/lib/python3.9/dist-packages (from pennylane-lightning) (0.35.1)\r\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\r\n",
      "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.59.1)\r\n",
      "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\r\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.12.3)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (0.11.2)\r\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\r\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.39->quimb) (0.42.0)\r\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import pennylane as qml\n",
    "import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "#prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# torch.cfloat won't pass the unitary check, but faster\n",
    "COMPLEX_DTYPE = torch.cfloat\n",
    "REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VN2wD-U7bGse",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462309702,
     "user_tz": -660,
     "elapsed": 7760,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "5cf6575c-ec02-4d3d-b222-1464aa8c1914",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:11.191285Z",
     "start_time": "2024-04-06T21:07:07.322167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data"
   ],
   "metadata": {
    "id": "9WmQPQuLbSFw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUhQUP2dbTja",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462317565,
     "user_tz": -660,
     "elapsed": 7868,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "5be0db17-8633-45dd-cde0-19eaec483b24",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:11.308714Z",
     "start_time": "2024-04-06T21:07:11.192556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n",
      "tensor([4, 1, 7, 3, 5, 8, 2, 6, 4, 6, 6, 3, 6, 9, 9, 7, 8, 7, 7, 7, 2, 7, 3, 8,\n",
      "        8, 8, 2, 2, 8, 2, 1, 5, 8, 7, 8, 2, 1, 4, 8, 7, 0, 0, 8, 9, 7, 5, 3, 6,\n",
      "        9, 3, 9, 8, 7, 1, 8, 8, 0, 7, 3, 0, 6, 4, 5, 8])\n",
      "tensor([-1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j,  0.7176+0.j,  0.5686+0.j,  0.8588+0.j,  0.2235+0.j,  0.0902+0.j,\n",
      "         0.9059+0.j,  0.5529+0.j,  0.6000+0.j,  0.6078+0.j,  0.6392+0.j,  0.6314+0.j,\n",
      "         0.6157+0.j,  0.8039+0.j,  0.4039+0.j, -0.0039+0.j,  0.9922+0.j,  0.6314+0.j,\n",
      "         0.7490+0.j, -0.6235+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j], dtype=torch.complex64)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Some Utilities"
   ],
   "metadata": {
    "id": "kQI8WWY6byQq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ket = {\n",
    "    '0':torch.tensor([1.,0.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '1':torch.tensor([0.,1.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '+':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) + torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device)),\n",
    "    '-':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) - torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device))\n",
    "}\n",
    "\n",
    "pauli = {\n",
    "    'I':torch.tensor([[1.,0.],[0.,1.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'X':torch.tensor([[0.,1.],[1.,0.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Y':torch.tensor([[0., -1.j],[1.j, 0]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Z':torch.tensor([[1.,0.],[0.,-1.]], dtype = COMPLEX_DTYPE, device=device)\n",
    "}\n",
    "\n",
    "def tensor_product(*args):\n",
    "  input_list = [a for a in args]\n",
    "  return functools.reduce(torch.kron, input_list)\n",
    "\n",
    "def multi_qubit_identity(n_qubits:int):\n",
    "  assert n_qubits>0\n",
    "  if n_qubits == 1:\n",
    "    return pauli['I']\n",
    "  else:\n",
    "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
    "\n",
    "pauli_words_su4 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
    "      #print(key1, key2)\n",
    "      #print(pauli[key1])\n",
    "      #print(pauli[key2])\n",
    "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
    "      #print(pauli_words_su4[key1+key2])\n",
    "\n",
    "pauli_words_su8 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      if not key1+key2+key3 == 'III':\n",
    "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
    "\n",
    "pauli_words_su16 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        if not key1+key2+key3+key4 == 'IIII':\n",
    "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
    "              pauli[key1],\n",
    "              pauli[key2],\n",
    "              pauli[key3],\n",
    "              pauli[key4]\n",
    "          )\n",
    "\n",
    "pauli_words_su32 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        for key5 in pauli.keys():\n",
    "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
    "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
    "                pauli[key1],\n",
    "                pauli[key2],\n",
    "                pauli[key3],\n",
    "                pauli[key4],\n",
    "                pauli[key5]\n",
    "            )\n",
    "\n",
    "def su32_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = list(pauli_words_su32.values())\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, torch.stack(dict_values))\n",
    "  #print(generator.shape)\n",
    "  return torch.matrix_exp(1j*generator)\n",
    "\n",
    "def su4_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = torch.stack(list(pauli_words_su4.values()))\n",
    "  #print(dict_values.shape)\n",
    "  generator = torch.einsum(\"i, ijk -> jk\", params, dict_values)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7B4DTncWbwQl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320391,
     "user_tz": -660,
     "elapsed": 2829,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "72ab9fd9-1a03-484b-e92a-147788cf05aa",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.368132Z",
     "start_time": "2024-04-06T21:07:11.310586Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "def measure_sv(\n",
    "    state,\n",
    "    observable\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Measure a statevector with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not\n",
    "  \"\"\"\n",
    "  expectation_value = torch.conj(state)@observable@state\n",
    "  return torch.real(expectation_value)\n",
    "\n",
    "def measure_dm(\n",
    "    rho,\n",
    "    observable\n",
    "):\n",
    "  \"\"\"\n",
    "  Measure a density matrix with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not.\n",
    "  \"\"\"\n",
    "  product = torch.matmul(rho, observable)\n",
    "\n",
    "  # Calculate the trace, which is the sum of diagonal elements\n",
    "  trace = torch.trace(product)\n",
    "\n",
    "  # The expectation value should be real for physical observables\n",
    "  return torch.real(trace)\n",
    "\n",
    "vmap_measure_sv = torch.vmap(measure_sv, in_dims=(None, 0), out_dims=0)\n",
    "vmap_measure_dm = torch.vmap(measure_dm, in_dims=(None, 0), out_dims=0)\n",
    "\n",
    "# assuming the input patch observables (hermitianized) has shape (batchsize,n_patches, c, h, w)\n",
    "# assuming the input set statevectors has shape (c, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_sv_batched_ob = torch.vmap(measure_sv, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "# assuming the input set desnity matrices has shape (batchsize, n_patches, c, 2**n, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_dm_batched_ob = torch.vmap(measure_dm, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "def bitstring_to_state(bitstring:str):\n",
    "  \"\"\"\n",
    "  Convert a bit string, like '0101001' or '+-+-101'\n",
    "  to a statevector. Each character in the bitstring must be among\n",
    "  0, 1, + and -\n",
    "  \"\"\"\n",
    "  assert len(bitstring)>0\n",
    "  for c in bitstring:\n",
    "    assert c in ['0', '1', '+', '-']\n",
    "  single_qubit_states = [ket[c] for c in bitstring]\n",
    "  return tensor_product(*single_qubit_states)\n",
    "\n",
    "\n",
    "#test_patch = torch.randn(size=[5, 2, 3, 4,4], dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_herm_patch = (torch.einsum(\"...jk->...kj\", test_patch)+test_patch)/2\n",
    "#print(test_herm_patch)\n",
    "#print(\"all patches shape\", test_herm_patch.shape)\n",
    "#test_sv = torch.stack([bitstring_to_state('++')]*3, axis = 0)\n",
    "#print(test_sv)\n",
    "#print(\"all sv shape\", test_sv.shape)\n",
    "#res = vmap_measure_channel_sv_batched_ob(test_sv, test_herm_patch)\n",
    "#print(res)\n",
    "#print(\"res shape\",res.shape)\n",
    "\n",
    "#for batchid in range(test_patch.shape[0]):\n",
    "#  batchitem = test_patch[batchid]\n",
    "#  for patch_idx in range(batchitem.shape[0]):\n",
    "#    patch = batchitem[patch_idx]\n",
    "#    for ch_idx in range(patch.shape[0]):\n",
    "#      print(f\"batchid={batchid}; patchid = {patch_idx}; chid = {ch_idx}\")\n",
    "#      ch = patch[ch_idx]\n",
    "#      #print(ch.shape)\n",
    "#      sv_ch = test_sv[ch_idx]\n",
    "#      #print(sv_ch.shape)\n",
    "#      print(measure_sv(sv_ch, ch))\n"
   ],
   "metadata": {
    "id": "Xs0c2F1eBnGc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320391,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.375546Z",
     "start_time": "2024-04-06T21:07:13.369704Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flipped Quanvolution Kernel\n",
    "\n",
    "First, let's define the function to extract patches for a single kernel."
   ],
   "metadata": {
    "id": "fFZqT6bpElaL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_patches(image, patch_size, stride, padding=None):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image with multiple input channels and optional custom padding.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (in_channels, height, width).\n",
    "        patch_size (int): Size of the square patches to extract.\n",
    "        stride (int): Stride between patches.\n",
    "        padding (tuple): Padding value(s) for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n",
    "    pad_l, pad_r, pad_t, pad_b = padding if padding is not None else (0,0,0,0)\n",
    "\n",
    "    if padding is not None:\n",
    "        image = torch.nn.functional.pad(image, (pad_l, pad_r, pad_t, pad_b), mode='constant')\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    height, width = image.shape[-2],  image.shape[-1]\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = image[..., i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    patches = torch.stack(patches, dim=-4)\n",
    "    return patches # has shape (batchsize, n_patches, channel, h, w)\n",
    "\n",
    "\n",
    "#single_img = torch.stack([torch.arange(32*32*1,dtype = torch.double, device=device).reshape((32,32)).type(torch.cdouble)]*3)\n",
    "\n",
    "#test_img = torch.stack([single_img]*2)\n",
    "#print(test_img[0][...,:4,:4])\n",
    "#patches = extract_patches(test_img, patch_size=4, stride=2,padding=(0,0,0,0))\n",
    "#print(patches.shape)\n",
    "#print(patches[0].shape)\n",
    "#print(patches[0][0])\n"
   ],
   "metadata": {
    "id": "He4HdMRHC7T6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320391,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.382085Z",
     "start_time": "2024-04-06T21:07:13.376557Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Three-by-Three Quanv Kernel Function\n",
    "\n",
    "For 3 by 3 patches, we only need a 2-qubit parameterised state, generated by the SU4 gate applied to the $|00\\rangle$ state. The first thing to do is to pad the 3 by 3 patches to 4 by 4. However, for input with size\n",
    "\n",
    "$$\n",
    "D_1\\times H_1 \\times W_1\n",
    "$$\n",
    "\n",
    "where $D_1$ is the number of input channels, we will need $D_1$ different two-qubit states, generated by $D_1$ different parameterised circuits. These circuits will produce $D_1$ expectation values. Suppose each circuit is parameterised by $\\theta_i, i\\in \\{1,2,\\cdots, D_1\\}$, the parameterised (pure) state can be written as\n",
    "\n",
    "$$\n",
    "|\\psi (\\theta_i)⟩ = SU4(\\theta_i)|00⟩\n",
    "$$\n",
    "\n",
    "For a channel $M_i$ of patch $M$ with shape $4\\times 4$, the expectation value is calculated by\n",
    "\n",
    "$$\n",
    "⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$\n",
    "\n",
    "Then, a single pixel value of the output channel corresponding this single kernel is\n",
    "\n",
    "$$\n",
    "\\frac{1}{D_1}\\sum_{i=1}^{D_1}⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$"
   ],
   "metadata": {
    "id": "9hRqflooEqKN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_2q_param_state(theta):\n",
    "  state = bitstring_to_state('00')\n",
    "  state = torch.matmul(su4_op(theta), state)\n",
    "  return state\n",
    "\n",
    "vmap_generate_2q_param_state = torch.vmap(generate_2q_param_state, in_dims=0, out_dims = 0)\n",
    "\n",
    "def single_kernel_op_over_batched_patches(thetas, patch):\n",
    "  # patch has shape (c_in, h, w)\n",
    "  # thetas has shape (c_in, 4^2-1) for SU4 gates\n",
    "  n_theta = thetas.shape[-2]\n",
    "  n_channel = patch.shape[-3]\n",
    "  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n",
    "  states = vmap_generate_2q_param_state(thetas)\n",
    "  #print(\"States shape\", states.shape)\n",
    "  patch = torch.nn.functional.pad(patch, (0, 1, 0, 1), mode='constant')\n",
    "  patch_t = torch.einsum(\"...jk->...kj\", patch)\n",
    "  herm_patch = (patch_t+patch)/2 # has dim (batchsize, num_patches, c, h, w)\n",
    "  #print(\"Herm patch shape\", herm_patch.shape)\n",
    "  channel_out = vmap_measure_channel_sv_batched_ob(states, herm_patch) # has dim (batchsize,n_patches, c)\n",
    "  return torch.sum(channel_out, axis = -1)/n_theta # has dim (batchszie, n_patches)\n",
    "\n",
    "\n",
    "#test_params = torch.randn((1,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_img = dummy_x.to(device)#.type(torch.cdouble)\n",
    "#print(test_img.shape)\n",
    "#test_patches = extract_patches(test_img, patch_size=3, stride=1,padding=(1,1,1,1))\n",
    "#print(test_patches.shape)\n",
    "#print(test_params.shape)\n",
    "#print(vmap_generate_2q_param_state(test_params))\n",
    "#test_out = single_kernel_op_over_batched_patches(test_params, test_patches)\n",
    "#print(test_out.shape)\n",
    "#h = (32-3+1*2)//1 +1\n",
    "#h**2\n"
   ],
   "metadata": {
    "id": "Yzn4KEt5ErG7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320391,
     "user_tz": -660,
     "elapsed": 5,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.387764Z",
     "start_time": "2024-04-06T21:07:13.383020Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multiple Output Channels"
   ],
   "metadata": {
    "id": "A4BXEKd8I67_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# For multiple channel output\n",
    "# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n",
    "vmap_vmap_single_kernel_op_through_extracted_patches = torch.vmap(single_kernel_op_over_batched_patches, in_dims=(0, None), out_dims=-2) # output has dim (batchsize, c_out, n_patches)\n",
    "\n",
    "#c_in = 1\n",
    "#c_out = 2\n",
    "\n",
    "#test_params2 = torch.randn((c_out, c_in,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_out2 = vmap_vmap_single_kernel_op_through_extracted_patches(test_params2, test_patches)\n",
    "#print(test_out2.shape)\n",
    "#test_out_2_features = test_out2.reshape((-1,c_out, 32, 32))\n",
    "#print(test_out_2_features.shape)\n"
   ],
   "metadata": {
    "id": "72vkHV_BI80l",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320392,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.391567Z",
     "start_time": "2024-04-06T21:07:13.388733Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch Module for Three-by-Three Flipped Quanv"
   ],
   "metadata": {
    "id": "NuZ6G5-sKK_J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FlippedQuanv3x3(torch.nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, padding):\n",
    "    super(FlippedQuanv3x3, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.pad_l, self.pad_r, self.pad_t, self.pad_b = padding if padding is not None else (0,0,0,0)\n",
    "    self.pad = (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n",
    "    self.weight= torch.nn.Parameter(torch.randn((out_channels, in_channels, 4**2-1)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn((out_channels, 1)).type(COMPLEX_DTYPE))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batchsize ,c_in, h, w)\n",
    "    # weight has shape (c_out, c_in, 15)\n",
    "    # bias has shape (c_out, 1)\n",
    "    x = x.type(COMPLEX_DTYPE)\n",
    "    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n",
    "    h_out = (h_in-3+(self.pad_t+self.pad_b))//self.stride +1\n",
    "    w_out = (w_in-3+(self.pad_l+self.pad_r))//self.stride +1\n",
    "\n",
    "    #print(self.weight.shape)\n",
    "    #print(patches.shape)\n",
    "\n",
    "    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n",
    "\n",
    "\n",
    "    out = out + self.bias\n",
    "    return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n",
    "\n",
    "#test_module = FlippedQuanv3x3(in_channels=1, out_channels=2, stride=1, padding=(1,1,1,1)).to(device)\n",
    "#print(dummy_x.shape)\n",
    "#test_out = test_module(dummy_x.to(device))\n",
    "#print(test_out.shape)"
   ],
   "metadata": {
    "id": "Gww_XdJ5KPJt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320392,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.398665Z",
     "start_time": "2024-04-06T21:07:13.392811Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flipped Quanvolution Replacing Conv2d\n",
    "\n",
    "First, let's just replace `Conv2d` with `FlippedQuanv3x3`."
   ],
   "metadata": {
    "id": "1yumZTjDVu63"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class HybridNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HybridNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(16*28*28, 10).type(REAL_DTYPE)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "net = HybridNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4RC5ou-VzbE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320392,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "f2e92dd2-63f4-4ed1-d191-6b016304911e",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:16.031580Z",
     "start_time": "2024-04-06T21:07:13.400651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "HybridNet(\n",
      "  (layers): Sequential(\n",
      "    (0): FlippedQuanv3x3()\n",
      "    (1): FlippedQuanv3x3()\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=12544, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_257/2687080126.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n",
      "/tmp/ipykernel_257/2683249851.py:30: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:301.)\n",
      "  return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "import torchmetrics\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.2\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4ZY59q1WS4x",
    "outputId": "0e7cbfcd-1078-4c56-c675-1afbfa385c85",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711488461440,
     "user_tz": -660,
     "elapsed": 18075888,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-07T09:49:01.623325Z",
     "start_time": "2024-04-06T21:07:16.032868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 600, Number of test batches = 100\n",
      "Print every train batch = 120, Print every test batch = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_257/2687080126.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at step=0, batch=0, train loss = 2.5836989879608154, train acc = 0.07999999821186066, time = 0.6942799091339111\n",
      "Training at step=0, batch=120, train loss = 0.9371321797370911, train acc = 0.6299999952316284, time = 0.7056727409362793\n",
      "Training at step=0, batch=240, train loss = 0.8880257606506348, train acc = 0.6899999976158142, time = 0.7041270732879639\n",
      "Training at step=0, batch=360, train loss = 0.6269816160202026, train acc = 0.7699999809265137, time = 0.7041285037994385\n",
      "Training at step=0, batch=480, train loss = 0.7035717964172363, train acc = 0.7200000286102295, time = 0.7034604549407959\n",
      "Testing at step=0, batch=0, test loss = 0.581850528717041, test acc = 0.7799999713897705, time = 0.2047109603881836\n",
      "Testing at step=0, batch=20, test loss = 0.5622889995574951, test acc = 0.7900000214576721, time = 0.14134502410888672\n",
      "Testing at step=0, batch=40, test loss = 0.6838259100914001, test acc = 0.7799999713897705, time = 0.14039039611816406\n",
      "Testing at step=0, batch=60, test loss = 0.4995146691799164, test acc = 0.8500000238418579, time = 0.142592191696167\n",
      "Testing at step=0, batch=80, test loss = 0.8044507503509521, test acc = 0.7200000286102295, time = 0.14116120338439941\n",
      "Step 0 finished in 456.00109338760376, Train loss = 0.764598686248064, Test loss = 0.6363971829414368; Train Acc = 0.7235833340883255, Test Acc = 0.7703999978303909\n",
      "Training at step=1, batch=0, train loss = 0.7110718488693237, train acc = 0.7400000095367432, time = 0.7150058746337891\n",
      "Training at step=1, batch=120, train loss = 0.5455096364021301, train acc = 0.8199999928474426, time = 0.7054343223571777\n",
      "Training at step=1, batch=240, train loss = 0.5716806650161743, train acc = 0.7699999809265137, time = 0.7043781280517578\n",
      "Training at step=1, batch=360, train loss = 0.47319844365119934, train acc = 0.8100000023841858, time = 0.7042665481567383\n",
      "Training at step=1, batch=480, train loss = 0.5869431495666504, train acc = 0.7900000214576721, time = 0.7045483589172363\n",
      "Testing at step=1, batch=0, test loss = 0.4256950318813324, test acc = 0.8600000143051147, time = 0.21527576446533203\n",
      "Testing at step=1, batch=20, test loss = 0.6368618011474609, test acc = 0.7400000095367432, time = 0.1482100486755371\n",
      "Testing at step=1, batch=40, test loss = 0.6805402636528015, test acc = 0.7599999904632568, time = 0.4181978702545166\n",
      "Testing at step=1, batch=60, test loss = 0.6507700085639954, test acc = 0.800000011920929, time = 0.41753172874450684\n",
      "Testing at step=1, batch=80, test loss = 0.6572731733322144, test acc = 0.75, time = 0.4173121452331543\n",
      "Step 1 finished in 456.12537479400635, Train loss = 0.5784350056449572, Test loss = 0.634054321050644; Train Acc = 0.7925999960303307, Test Acc = 0.7668999999761581\n",
      "Training at step=2, batch=0, train loss = 0.5403960943222046, train acc = 0.7900000214576721, time = 0.7250816822052002\n",
      "Training at step=2, batch=120, train loss = 0.6074568033218384, train acc = 0.8199999928474426, time = 0.7037386894226074\n",
      "Training at step=2, batch=240, train loss = 0.49753284454345703, train acc = 0.8399999737739563, time = 0.7046875953674316\n",
      "Training at step=2, batch=360, train loss = 0.5856130123138428, train acc = 0.7900000214576721, time = 0.7039542198181152\n",
      "Training at step=2, batch=480, train loss = 0.5448890924453735, train acc = 0.800000011920929, time = 0.7046308517456055\n",
      "Testing at step=2, batch=0, test loss = 0.6612063050270081, test acc = 0.7599999904632568, time = 0.1929945945739746\n",
      "Testing at step=2, batch=20, test loss = 0.6898283362388611, test acc = 0.7699999809265137, time = 0.14329242706298828\n",
      "Testing at step=2, batch=40, test loss = 0.4943191111087799, test acc = 0.8600000143051147, time = 0.14353728294372559\n",
      "Testing at step=2, batch=60, test loss = 0.6529456973075867, test acc = 0.7300000190734863, time = 0.14299368858337402\n",
      "Testing at step=2, batch=80, test loss = 0.6678145527839661, test acc = 0.800000011920929, time = 0.14440178871154785\n",
      "Step 2 finished in 456.793842792511, Train loss = 0.5453186816970508, Test loss = 0.5951967200636864; Train Acc = 0.8037833310166995, Test Acc = 0.7811999994516373\n",
      "Training at step=3, batch=0, train loss = 0.5006687641143799, train acc = 0.8199999928474426, time = 0.7103610038757324\n",
      "Training at step=3, batch=120, train loss = 0.6498001217842102, train acc = 0.8100000023841858, time = 0.7330482006072998\n",
      "Training at step=3, batch=240, train loss = 0.5991846323013306, train acc = 0.7200000286102295, time = 0.7061505317687988\n",
      "Training at step=3, batch=360, train loss = 0.3979871869087219, train acc = 0.8600000143051147, time = 0.7036490440368652\n",
      "Training at step=3, batch=480, train loss = 0.40067240595817566, train acc = 0.8399999737739563, time = 0.701552152633667\n",
      "Testing at step=3, batch=0, test loss = 0.5248273611068726, test acc = 0.8500000238418579, time = 0.21711182594299316\n",
      "Testing at step=3, batch=20, test loss = 0.5039570331573486, test acc = 0.7900000214576721, time = 0.14200925827026367\n",
      "Testing at step=3, batch=40, test loss = 0.5787867903709412, test acc = 0.7900000214576721, time = 0.14683985710144043\n",
      "Testing at step=3, batch=60, test loss = 0.34330254793167114, test acc = 0.8799999952316284, time = 0.1477656364440918\n",
      "Testing at step=3, batch=80, test loss = 0.5878751873970032, test acc = 0.7599999904632568, time = 0.14446187019348145\n",
      "Step 3 finished in 458.2344596385956, Train loss = 0.5272795108457407, Test loss = 0.5386574950814247; Train Acc = 0.8111833307147026, Test Acc = 0.8038999962806702\n",
      "Training at step=4, batch=0, train loss = 0.4156323969364166, train acc = 0.8600000143051147, time = 0.7294206619262695\n",
      "Training at step=4, batch=120, train loss = 0.519503116607666, train acc = 0.8299999833106995, time = 0.703869104385376\n",
      "Training at step=4, batch=240, train loss = 0.46468907594680786, train acc = 0.8199999928474426, time = 0.7041590213775635\n",
      "Training at step=4, batch=360, train loss = 0.4092407524585724, train acc = 0.8799999952316284, time = 0.7021641731262207\n",
      "Training at step=4, batch=480, train loss = 0.3036113977432251, train acc = 0.8700000047683716, time = 0.705237865447998\n",
      "Testing at step=4, batch=0, test loss = 0.5691523551940918, test acc = 0.7900000214576721, time = 0.21664142608642578\n",
      "Testing at step=4, batch=20, test loss = 0.34584859013557434, test acc = 0.8600000143051147, time = 0.16241788864135742\n",
      "Testing at step=4, batch=40, test loss = 0.5068615674972534, test acc = 0.800000011920929, time = 0.1520707607269287\n",
      "Testing at step=4, batch=60, test loss = 0.510859489440918, test acc = 0.8299999833106995, time = 0.14288949966430664\n",
      "Testing at step=4, batch=80, test loss = 0.519633948802948, test acc = 0.8399999737739563, time = 0.14717674255371094\n",
      "Step 4 finished in 458.07330298423767, Train loss = 0.508265282958746, Test loss = 0.5419282969832421; Train Acc = 0.8192499990264575, Test Acc = 0.8068999963998794\n",
      "Training at step=5, batch=0, train loss = 0.7953063249588013, train acc = 0.8199999928474426, time = 0.7035589218139648\n",
      "Training at step=5, batch=120, train loss = 0.563596248626709, train acc = 0.7900000214576721, time = 0.703160285949707\n",
      "Training at step=5, batch=240, train loss = 0.48696380853652954, train acc = 0.8500000238418579, time = 0.7036678791046143\n",
      "Training at step=5, batch=360, train loss = 0.6045854091644287, train acc = 0.7799999713897705, time = 0.7051546573638916\n",
      "Training at step=5, batch=480, train loss = 0.4593188762664795, train acc = 0.8600000143051147, time = 0.7048430442810059\n",
      "Testing at step=5, batch=0, test loss = 0.5588992834091187, test acc = 0.7900000214576721, time = 0.21746611595153809\n",
      "Testing at step=5, batch=20, test loss = 0.596833348274231, test acc = 0.800000011920929, time = 0.14188385009765625\n",
      "Testing at step=5, batch=40, test loss = 0.5523189306259155, test acc = 0.75, time = 0.14325332641601562\n",
      "Testing at step=5, batch=60, test loss = 0.510686457157135, test acc = 0.8399999737739563, time = 0.14117813110351562\n",
      "Testing at step=5, batch=80, test loss = 0.5925377607345581, test acc = 0.7900000214576721, time = 0.14351296424865723\n",
      "Step 5 finished in 456.49703431129456, Train loss = 0.5001073582470417, Test loss = 0.5477593141794205; Train Acc = 0.821699997484684, Test Acc = 0.8024999982118607\n",
      "Training at step=6, batch=0, train loss = 0.5131503939628601, train acc = 0.8199999928474426, time = 0.6998727321624756\n",
      "Training at step=6, batch=120, train loss = 0.3779446482658386, train acc = 0.8799999952316284, time = 0.7057740688323975\n",
      "Training at step=6, batch=240, train loss = 0.45099353790283203, train acc = 0.8299999833106995, time = 0.7056541442871094\n",
      "Training at step=6, batch=360, train loss = 0.48642033338546753, train acc = 0.8700000047683716, time = 0.704491376876831\n",
      "Training at step=6, batch=480, train loss = 0.5854506492614746, train acc = 0.8399999737739563, time = 0.7058923244476318\n",
      "Testing at step=6, batch=0, test loss = 0.5053102374076843, test acc = 0.800000011920929, time = 0.20370697975158691\n",
      "Testing at step=6, batch=20, test loss = 0.5549144744873047, test acc = 0.800000011920929, time = 0.15019893646240234\n",
      "Testing at step=6, batch=40, test loss = 0.45852988958358765, test acc = 0.8199999928474426, time = 0.14861106872558594\n",
      "Testing at step=6, batch=60, test loss = 0.5003421902656555, test acc = 0.8199999928474426, time = 0.14943408966064453\n",
      "Testing at step=6, batch=80, test loss = 0.47083571553230286, test acc = 0.8299999833106995, time = 0.1493053436279297\n",
      "Step 6 finished in 456.4067442417145, Train loss = 0.49528150300184887, Test loss = 0.4986539569497108; Train Acc = 0.8241333312789599, Test Acc = 0.8197999978065491\n",
      "Training at step=7, batch=0, train loss = 0.3079851269721985, train acc = 0.9100000262260437, time = 0.7338972091674805\n",
      "Training at step=7, batch=120, train loss = 0.4281565845012665, train acc = 0.8199999928474426, time = 0.7051863670349121\n",
      "Training at step=7, batch=240, train loss = 0.4037320613861084, train acc = 0.8199999928474426, time = 0.7180707454681396\n",
      "Training at step=7, batch=360, train loss = 0.4286593496799469, train acc = 0.8500000238418579, time = 0.7065448760986328\n",
      "Training at step=7, batch=480, train loss = 0.3068290948867798, train acc = 0.9399999976158142, time = 0.7048852443695068\n",
      "Testing at step=7, batch=0, test loss = 0.5811710953712463, test acc = 0.800000011920929, time = 0.19431138038635254\n",
      "Testing at step=7, batch=20, test loss = 0.4622206389904022, test acc = 0.8299999833106995, time = 0.14078903198242188\n",
      "Testing at step=7, batch=40, test loss = 0.5795673727989197, test acc = 0.7699999809265137, time = 0.13730692863464355\n",
      "Testing at step=7, batch=60, test loss = 0.41157257556915283, test acc = 0.800000011920929, time = 0.41759228706359863\n",
      "Testing at step=7, batch=80, test loss = 0.6922125816345215, test acc = 0.7900000214576721, time = 0.41677355766296387\n",
      "Step 7 finished in 458.04687094688416, Train loss = 0.4879501762986183, Test loss = 0.5101201692223549; Train Acc = 0.827483332157135, Test Acc = 0.8208999967575074\n",
      "Training at step=8, batch=0, train loss = 0.41865235567092896, train acc = 0.8899999856948853, time = 0.7163324356079102\n",
      "Training at step=8, batch=120, train loss = 0.38789480924606323, train acc = 0.8399999737739563, time = 0.704984188079834\n",
      "Training at step=8, batch=240, train loss = 0.3257369101047516, train acc = 0.8799999952316284, time = 0.7033274173736572\n",
      "Training at step=8, batch=360, train loss = 0.459367036819458, train acc = 0.8600000143051147, time = 0.7055008411407471\n",
      "Training at step=8, batch=480, train loss = 0.666461706161499, train acc = 0.7300000190734863, time = 0.7065105438232422\n",
      "Testing at step=8, batch=0, test loss = 0.6766992211341858, test acc = 0.7799999713897705, time = 0.21277928352355957\n",
      "Testing at step=8, batch=20, test loss = 0.5956601500511169, test acc = 0.7900000214576721, time = 0.14673829078674316\n",
      "Testing at step=8, batch=40, test loss = 0.4126332402229309, test acc = 0.8500000238418579, time = 0.14756011962890625\n",
      "Testing at step=8, batch=60, test loss = 0.4091559648513794, test acc = 0.8600000143051147, time = 0.14821267127990723\n",
      "Testing at step=8, batch=80, test loss = 0.552273154258728, test acc = 0.8299999833106995, time = 0.14606356620788574\n",
      "Step 8 finished in 457.40761160850525, Train loss = 0.48341333483656246, Test loss = 0.49373015254735947; Train Acc = 0.8296333308021228, Test Acc = 0.8252999985218048\n",
      "Training at step=9, batch=0, train loss = 0.42179977893829346, train acc = 0.8700000047683716, time = 0.7268786430358887\n",
      "Training at step=9, batch=120, train loss = 0.431244432926178, train acc = 0.8399999737739563, time = 0.6929588317871094\n",
      "Training at step=9, batch=240, train loss = 0.44564080238342285, train acc = 0.8299999833106995, time = 0.7064242362976074\n",
      "Training at step=9, batch=360, train loss = 0.4100096523761749, train acc = 0.8399999737739563, time = 0.7048161029815674\n",
      "Training at step=9, batch=480, train loss = 0.4805448651313782, train acc = 0.8100000023841858, time = 0.7045340538024902\n",
      "Testing at step=9, batch=0, test loss = 0.9530547857284546, test acc = 0.7200000286102295, time = 0.3545069694519043\n",
      "Testing at step=9, batch=20, test loss = 0.6226334571838379, test acc = 0.7699999809265137, time = 0.41589856147766113\n",
      "Testing at step=9, batch=40, test loss = 0.5935570001602173, test acc = 0.7900000214576721, time = 0.41712045669555664\n",
      "Testing at step=9, batch=60, test loss = 0.5904956459999084, test acc = 0.8399999737739563, time = 0.41799283027648926\n",
      "Testing at step=9, batch=80, test loss = 0.5456560254096985, test acc = 0.800000011920929, time = 0.41851186752319336\n",
      "Step 9 finished in 456.6105628013611, Train loss = 0.4754742250839869, Test loss = 0.5512288343906403; Train Acc = 0.8317999982833862, Test Acc = 0.806099995970726\n",
      "Training at step=10, batch=0, train loss = 0.40707316994667053, train acc = 0.8199999928474426, time = 0.7327320575714111\n",
      "Training at step=10, batch=120, train loss = 0.3782639801502228, train acc = 0.8600000143051147, time = 0.7340168952941895\n",
      "Training at step=10, batch=240, train loss = 0.43855324387550354, train acc = 0.8899999856948853, time = 0.7046501636505127\n",
      "Training at step=10, batch=360, train loss = 0.624163031578064, train acc = 0.8100000023841858, time = 0.6888973712921143\n",
      "Training at step=10, batch=480, train loss = 0.5831882953643799, train acc = 0.800000011920929, time = 0.705087423324585\n",
      "Testing at step=10, batch=0, test loss = 0.32842883467674255, test acc = 0.8700000047683716, time = 0.2169208526611328\n",
      "Testing at step=10, batch=20, test loss = 0.5066608190536499, test acc = 0.800000011920929, time = 0.14801549911499023\n",
      "Testing at step=10, batch=40, test loss = 0.3708866834640503, test acc = 0.8600000143051147, time = 0.14791297912597656\n",
      "Testing at step=10, batch=60, test loss = 0.5163568258285522, test acc = 0.8100000023841858, time = 0.1421947479248047\n",
      "Testing at step=10, batch=80, test loss = 0.4993497133255005, test acc = 0.8299999833106995, time = 0.1422128677368164\n",
      "Step 10 finished in 461.5884690284729, Train loss = 0.47249131185313065, Test loss = 0.5193875503540039; Train Acc = 0.8323499976595243, Test Acc = 0.8175999963283539\n",
      "Training at step=11, batch=0, train loss = 0.3766229748725891, train acc = 0.8199999928474426, time = 0.6956281661987305\n",
      "Training at step=11, batch=120, train loss = 0.5130823254585266, train acc = 0.7599999904632568, time = 0.7048144340515137\n",
      "Training at step=11, batch=240, train loss = 0.50436931848526, train acc = 0.8299999833106995, time = 0.7029895782470703\n",
      "Training at step=11, batch=360, train loss = 0.38892197608947754, train acc = 0.8299999833106995, time = 0.7037022113800049\n",
      "Training at step=11, batch=480, train loss = 0.7053138613700867, train acc = 0.7900000214576721, time = 0.7063784599304199\n",
      "Testing at step=11, batch=0, test loss = 0.4709154963493347, test acc = 0.8600000143051147, time = 0.21624016761779785\n",
      "Testing at step=11, batch=20, test loss = 0.5053666234016418, test acc = 0.8199999928474426, time = 0.14736008644104004\n",
      "Testing at step=11, batch=40, test loss = 0.5918402075767517, test acc = 0.7799999713897705, time = 0.41739344596862793\n",
      "Testing at step=11, batch=60, test loss = 0.29736843705177307, test acc = 0.8999999761581421, time = 0.4178652763366699\n",
      "Testing at step=11, batch=80, test loss = 0.4542364478111267, test acc = 0.8299999833106995, time = 0.41774463653564453\n",
      "Step 11 finished in 456.159277677536, Train loss = 0.47389601051807406, Test loss = 0.47949127554893495; Train Acc = 0.8329999993244807, Test Acc = 0.8329999989271164\n",
      "Training at step=12, batch=0, train loss = 0.402303010225296, train acc = 0.8500000238418579, time = 0.7219505310058594\n",
      "Training at step=12, batch=120, train loss = 0.3268115520477295, train acc = 0.9100000262260437, time = 0.7046942710876465\n",
      "Training at step=12, batch=240, train loss = 0.5390752553939819, train acc = 0.8100000023841858, time = 0.7047328948974609\n",
      "Training at step=12, batch=360, train loss = 0.6665087938308716, train acc = 0.7900000214576721, time = 0.7054693698883057\n",
      "Training at step=12, batch=480, train loss = 0.48503878712654114, train acc = 0.8500000238418579, time = 0.7060391902923584\n",
      "Testing at step=12, batch=0, test loss = 0.4109826385974884, test acc = 0.8600000143051147, time = 0.21619009971618652\n",
      "Testing at step=12, batch=20, test loss = 0.46577098965644836, test acc = 0.8100000023841858, time = 0.41928625106811523\n",
      "Testing at step=12, batch=40, test loss = 0.5229445099830627, test acc = 0.800000011920929, time = 0.41727614402770996\n",
      "Testing at step=12, batch=60, test loss = 0.4304868280887604, test acc = 0.8100000023841858, time = 0.4176809787750244\n",
      "Testing at step=12, batch=80, test loss = 0.6124879717826843, test acc = 0.8199999928474426, time = 0.41805458068847656\n",
      "Step 12 finished in 456.2666447162628, Train loss = 0.46878914105395475, Test loss = 0.4999197354912758; Train Acc = 0.8355166663726171, Test Acc = 0.8252999973297119\n",
      "Training at step=13, batch=0, train loss = 0.43874961137771606, train acc = 0.8199999928474426, time = 0.7260279655456543\n",
      "Training at step=13, batch=120, train loss = 0.6594992280006409, train acc = 0.8100000023841858, time = 0.7047092914581299\n",
      "Training at step=13, batch=240, train loss = 0.6099026203155518, train acc = 0.8399999737739563, time = 0.7041568756103516\n",
      "Training at step=13, batch=360, train loss = 0.30135735869407654, train acc = 0.9100000262260437, time = 0.7030730247497559\n",
      "Training at step=13, batch=480, train loss = 0.5906194448471069, train acc = 0.7599999904632568, time = 0.7044522762298584\n",
      "Testing at step=13, batch=0, test loss = 0.49416035413742065, test acc = 0.7799999713897705, time = 0.2055361270904541\n",
      "Testing at step=13, batch=20, test loss = 0.4301818609237671, test acc = 0.8399999737739563, time = 0.1457984447479248\n",
      "Testing at step=13, batch=40, test loss = 0.5736342668533325, test acc = 0.7699999809265137, time = 0.41745615005493164\n",
      "Testing at step=13, batch=60, test loss = 0.3847528398036957, test acc = 0.8399999737739563, time = 0.41748738288879395\n",
      "Testing at step=13, batch=80, test loss = 0.523226797580719, test acc = 0.8199999928474426, time = 0.4170217514038086\n",
      "Step 13 finished in 456.856507062912, Train loss = 0.4662287653982639, Test loss = 0.5067566525936127; Train Acc = 0.8343833317359288, Test Acc = 0.8251999974250793\n",
      "Training at step=14, batch=0, train loss = 0.4817822277545929, train acc = 0.8399999737739563, time = 0.7227199077606201\n",
      "Training at step=14, batch=120, train loss = 0.303949236869812, train acc = 0.8899999856948853, time = 0.7052016258239746\n",
      "Training at step=14, batch=240, train loss = 0.4212009906768799, train acc = 0.8799999952316284, time = 0.7056341171264648\n",
      "Training at step=14, batch=360, train loss = 0.3398321866989136, train acc = 0.8700000047683716, time = 0.704207181930542\n",
      "Training at step=14, batch=480, train loss = 0.38202956318855286, train acc = 0.8700000047683716, time = 0.6993482112884521\n",
      "Testing at step=14, batch=0, test loss = 0.45321136713027954, test acc = 0.8399999737739563, time = 0.1739037036895752\n",
      "Testing at step=14, batch=20, test loss = 0.35754722356796265, test acc = 0.8700000047683716, time = 0.14027070999145508\n",
      "Testing at step=14, batch=40, test loss = 0.5431442856788635, test acc = 0.8199999928474426, time = 0.1404104232788086\n",
      "Testing at step=14, batch=60, test loss = 0.4609263241291046, test acc = 0.8399999737739563, time = 0.14225983619689941\n",
      "Testing at step=14, batch=80, test loss = 0.5839819312095642, test acc = 0.8500000238418579, time = 0.14323139190673828\n",
      "Step 14 finished in 457.27523159980774, Train loss = 0.46439996575315795, Test loss = 0.5031320914626122; Train Acc = 0.8353499990701675, Test Acc = 0.8194999969005585\n",
      "Training at step=15, batch=0, train loss = 0.38016387820243835, train acc = 0.8700000047683716, time = 0.70013427734375\n",
      "Training at step=15, batch=120, train loss = 0.5252819657325745, train acc = 0.8100000023841858, time = 0.7056193351745605\n",
      "Training at step=15, batch=240, train loss = 0.5489832162857056, train acc = 0.8299999833106995, time = 0.7049953937530518\n",
      "Training at step=15, batch=360, train loss = 0.3122580647468567, train acc = 0.8700000047683716, time = 0.7002999782562256\n",
      "Training at step=15, batch=480, train loss = 0.3799818158149719, train acc = 0.8700000047683716, time = 0.7027554512023926\n",
      "Testing at step=15, batch=0, test loss = 0.4944506883621216, test acc = 0.8299999833106995, time = 0.4248948097229004\n",
      "Testing at step=15, batch=20, test loss = 0.5267869234085083, test acc = 0.7900000214576721, time = 0.4175150394439697\n",
      "Testing at step=15, batch=40, test loss = 0.5221590995788574, test acc = 0.8199999928474426, time = 0.4173140525817871\n",
      "Testing at step=15, batch=60, test loss = 0.3368505835533142, test acc = 0.8799999952316284, time = 0.4177992343902588\n",
      "Testing at step=15, batch=80, test loss = 0.4475051462650299, test acc = 0.8600000143051147, time = 0.42204833030700684\n",
      "Step 15 finished in 455.6875171661377, Train loss = 0.4609855676690737, Test loss = 0.4897383800148964; Train Acc = 0.8377333325147629, Test Acc = 0.8253999972343444\n",
      "Training at step=16, batch=0, train loss = 0.36863213777542114, train acc = 0.9100000262260437, time = 0.7311935424804688\n",
      "Training at step=16, batch=120, train loss = 0.4382559061050415, train acc = 0.8199999928474426, time = 0.7052912712097168\n",
      "Training at step=16, batch=240, train loss = 0.46391046047210693, train acc = 0.8199999928474426, time = 0.7051424980163574\n",
      "Training at step=16, batch=360, train loss = 0.3935889005661011, train acc = 0.8899999856948853, time = 0.7060692310333252\n",
      "Training at step=16, batch=480, train loss = 0.5795872807502747, train acc = 0.800000011920929, time = 0.7043871879577637\n",
      "Testing at step=16, batch=0, test loss = 0.37092846632003784, test acc = 0.8199999928474426, time = 0.2132551670074463\n",
      "Testing at step=16, batch=20, test loss = 0.5681855082511902, test acc = 0.800000011920929, time = 0.15068554878234863\n",
      "Testing at step=16, batch=40, test loss = 0.6460704207420349, test acc = 0.75, time = 0.14250683784484863\n",
      "Testing at step=16, batch=60, test loss = 0.5046252012252808, test acc = 0.8399999737739563, time = 0.1551070213317871\n",
      "Testing at step=16, batch=80, test loss = 0.47124701738357544, test acc = 0.8100000023841858, time = 0.14240384101867676\n",
      "Step 16 finished in 456.2136297225952, Train loss = 0.4617140822360913, Test loss = 0.4911623142659664; Train Acc = 0.8365999985734621, Test Acc = 0.8263999962806702\n",
      "Training at step=17, batch=0, train loss = 0.42390045523643494, train acc = 0.8999999761581421, time = 0.7275311946868896\n",
      "Training at step=17, batch=120, train loss = 0.4261901080608368, train acc = 0.8500000238418579, time = 0.731903076171875\n",
      "Training at step=17, batch=240, train loss = 0.4052235782146454, train acc = 0.8299999833106995, time = 0.7031280994415283\n",
      "Training at step=17, batch=360, train loss = 0.49176475405693054, train acc = 0.8399999737739563, time = 0.7046117782592773\n",
      "Training at step=17, batch=480, train loss = 0.48318395018577576, train acc = 0.8299999833106995, time = 0.7036502361297607\n",
      "Testing at step=17, batch=0, test loss = 0.40681731700897217, test acc = 0.8299999833106995, time = 0.20215845108032227\n",
      "Testing at step=17, batch=20, test loss = 0.5045398473739624, test acc = 0.8100000023841858, time = 0.141737699508667\n",
      "Testing at step=17, batch=40, test loss = 0.5470139980316162, test acc = 0.8399999737739563, time = 0.14345026016235352\n",
      "Testing at step=17, batch=60, test loss = 0.4775587022304535, test acc = 0.8199999928474426, time = 0.14284372329711914\n",
      "Testing at step=17, batch=80, test loss = 0.6183226108551025, test acc = 0.7699999809265137, time = 0.14017248153686523\n",
      "Step 17 finished in 460.1881334781647, Train loss = 0.4567331436028083, Test loss = 0.5266998848319053; Train Acc = 0.8401499995589257, Test Acc = 0.8142999988794327\n",
      "Training at step=18, batch=0, train loss = 0.43261730670928955, train acc = 0.8600000143051147, time = 0.7169637680053711\n",
      "Training at step=18, batch=120, train loss = 0.4167031943798065, train acc = 0.8500000238418579, time = 0.7063252925872803\n",
      "Training at step=18, batch=240, train loss = 0.5004487633705139, train acc = 0.7699999809265137, time = 0.7054078578948975\n",
      "Training at step=18, batch=360, train loss = 0.6306230425834656, train acc = 0.8199999928474426, time = 0.7059130668640137\n",
      "Training at step=18, batch=480, train loss = 0.3969259560108185, train acc = 0.8500000238418579, time = 0.7071328163146973\n",
      "Testing at step=18, batch=0, test loss = 0.317615270614624, test acc = 0.8799999952316284, time = 0.21545648574829102\n",
      "Testing at step=18, batch=20, test loss = 0.5479483604431152, test acc = 0.7799999713897705, time = 0.14492154121398926\n",
      "Testing at step=18, batch=40, test loss = 0.36460959911346436, test acc = 0.8500000238418579, time = 0.1477818489074707\n",
      "Testing at step=18, batch=60, test loss = 0.4522160589694977, test acc = 0.8100000023841858, time = 0.14759206771850586\n",
      "Testing at step=18, batch=80, test loss = 0.49114975333213806, test acc = 0.8299999833106995, time = 0.14682412147521973\n",
      "Step 18 finished in 458.51345229148865, Train loss = 0.4554745694249868, Test loss = 0.48436667650938037; Train Acc = 0.8384833318988482, Test Acc = 0.8265999960899353\n",
      "Training at step=19, batch=0, train loss = 0.5536739230155945, train acc = 0.7699999809265137, time = 0.7107799053192139\n",
      "Training at step=19, batch=120, train loss = 0.3789885640144348, train acc = 0.8600000143051147, time = 0.6914527416229248\n",
      "Training at step=19, batch=240, train loss = 0.5599356293678284, train acc = 0.8299999833106995, time = 0.6991264820098877\n",
      "Training at step=19, batch=360, train loss = 0.4348159730434418, train acc = 0.8199999928474426, time = 0.6991987228393555\n",
      "Training at step=19, batch=480, train loss = 0.43253958225250244, train acc = 0.8799999952316284, time = 0.7052955627441406\n",
      "Testing at step=19, batch=0, test loss = 0.6611605882644653, test acc = 0.8199999928474426, time = 0.21474337577819824\n",
      "Testing at step=19, batch=20, test loss = 0.4191944897174835, test acc = 0.8299999833106995, time = 0.14136171340942383\n",
      "Testing at step=19, batch=40, test loss = 0.661003589630127, test acc = 0.7900000214576721, time = 0.1415700912475586\n",
      "Testing at step=19, batch=60, test loss = 0.41524994373321533, test acc = 0.8100000023841858, time = 0.4188981056213379\n",
      "Testing at step=19, batch=80, test loss = 0.4027870297431946, test acc = 0.8600000143051147, time = 0.4168236255645752\n",
      "Step 19 finished in 458.5296814441681, Train loss = 0.45384931708375614, Test loss = 0.5046708369255066; Train Acc = 0.8406499988834063, Test Acc = 0.816099995970726\n",
      "Training at step=20, batch=0, train loss = 0.5167936086654663, train acc = 0.7799999713897705, time = 0.7205643653869629\n",
      "Training at step=20, batch=120, train loss = 0.4453178644180298, train acc = 0.8399999737739563, time = 0.7056465148925781\n",
      "Training at step=20, batch=240, train loss = 0.3049720525741577, train acc = 0.8899999856948853, time = 0.7051610946655273\n",
      "Training at step=20, batch=360, train loss = 0.7020414471626282, train acc = 0.800000011920929, time = 0.6993224620819092\n",
      "Training at step=20, batch=480, train loss = 0.3691588342189789, train acc = 0.8100000023841858, time = 0.7050933837890625\n",
      "Testing at step=20, batch=0, test loss = 0.4294397830963135, test acc = 0.7900000214576721, time = 0.22243142127990723\n",
      "Testing at step=20, batch=20, test loss = 0.4061073362827301, test acc = 0.8600000143051147, time = 0.14498186111450195\n",
      "Testing at step=20, batch=40, test loss = 0.5187147855758667, test acc = 0.8299999833106995, time = 0.1479802131652832\n",
      "Testing at step=20, batch=60, test loss = 0.47814705967903137, test acc = 0.8299999833106995, time = 0.14211750030517578\n",
      "Testing at step=20, batch=80, test loss = 0.4875328540802002, test acc = 0.800000011920929, time = 0.14731788635253906\n",
      "Step 20 finished in 456.6799235343933, Train loss = 0.4490342014282942, Test loss = 0.4853870651125908; Train Acc = 0.8432333317399024, Test Acc = 0.8273999977111817\n",
      "Training at step=21, batch=0, train loss = 0.45767566561698914, train acc = 0.8100000023841858, time = 0.700080394744873\n",
      "Training at step=21, batch=120, train loss = 0.409812331199646, train acc = 0.8600000143051147, time = 0.6851084232330322\n",
      "Training at step=21, batch=240, train loss = 0.48901766538619995, train acc = 0.8399999737739563, time = 0.704493522644043\n",
      "Training at step=21, batch=360, train loss = 0.4023078978061676, train acc = 0.8600000143051147, time = 0.705888032913208\n",
      "Training at step=21, batch=480, train loss = 0.3996330499649048, train acc = 0.8799999952316284, time = 0.70278000831604\n",
      "Testing at step=21, batch=0, test loss = 0.4382367432117462, test acc = 0.8700000047683716, time = 0.21632075309753418\n",
      "Testing at step=21, batch=20, test loss = 0.5659179091453552, test acc = 0.800000011920929, time = 0.13734197616577148\n",
      "Testing at step=21, batch=40, test loss = 0.49220508337020874, test acc = 0.8100000023841858, time = 0.1375565528869629\n",
      "Testing at step=21, batch=60, test loss = 0.5118113160133362, test acc = 0.8199999928474426, time = 0.14119267463684082\n",
      "Testing at step=21, batch=80, test loss = 0.5524092316627502, test acc = 0.8199999928474426, time = 0.1406419277191162\n",
      "Step 21 finished in 455.83018016815186, Train loss = 0.4486587506781022, Test loss = 0.47772032260894776; Train Acc = 0.8429333332180977, Test Acc = 0.8350000011920929\n",
      "Training at step=22, batch=0, train loss = 0.3313891291618347, train acc = 0.8700000047683716, time = 0.7077314853668213\n",
      "Training at step=22, batch=120, train loss = 0.44395145773887634, train acc = 0.8700000047683716, time = 0.7059378623962402\n",
      "Training at step=22, batch=240, train loss = 0.34258899092674255, train acc = 0.8500000238418579, time = 0.7048676013946533\n",
      "Training at step=22, batch=360, train loss = 0.5430681705474854, train acc = 0.800000011920929, time = 0.732072114944458\n",
      "Training at step=22, batch=480, train loss = 0.4465436637401581, train acc = 0.8299999833106995, time = 0.7332925796508789\n",
      "Testing at step=22, batch=0, test loss = 0.5940528512001038, test acc = 0.7599999904632568, time = 0.22232341766357422\n",
      "Testing at step=22, batch=20, test loss = 0.3863440454006195, test acc = 0.8799999952316284, time = 0.1502392292022705\n",
      "Testing at step=22, batch=40, test loss = 0.4899102747440338, test acc = 0.8299999833106995, time = 0.14190983772277832\n",
      "Testing at step=22, batch=60, test loss = 0.5265815854072571, test acc = 0.8100000023841858, time = 0.1501779556274414\n",
      "Testing at step=22, batch=80, test loss = 0.6827204823493958, test acc = 0.8100000023841858, time = 0.1512453556060791\n",
      "Step 22 finished in 463.50418758392334, Train loss = 0.4491105587035418, Test loss = 0.5562531456351281; Train Acc = 0.8425999995072683, Test Acc = 0.8115999990701676\n",
      "Training at step=23, batch=0, train loss = 0.373381644487381, train acc = 0.8700000047683716, time = 0.7258889675140381\n",
      "Training at step=23, batch=120, train loss = 0.49694663286209106, train acc = 0.8600000143051147, time = 0.7041523456573486\n",
      "Training at step=23, batch=240, train loss = 0.5928541421890259, train acc = 0.7799999713897705, time = 0.7054927349090576\n",
      "Training at step=23, batch=360, train loss = 0.3851701319217682, train acc = 0.8700000047683716, time = 0.7057764530181885\n",
      "Training at step=23, batch=480, train loss = 0.5011603832244873, train acc = 0.8399999737739563, time = 0.7033603191375732\n",
      "Testing at step=23, batch=0, test loss = 0.3915104269981384, test acc = 0.8700000047683716, time = 0.2162337303161621\n",
      "Testing at step=23, batch=20, test loss = 0.43997547030448914, test acc = 0.8899999856948853, time = 0.14214348793029785\n",
      "Testing at step=23, batch=40, test loss = 0.45554888248443604, test acc = 0.8199999928474426, time = 0.14019393920898438\n",
      "Testing at step=23, batch=60, test loss = 0.5792999863624573, test acc = 0.8399999737739563, time = 0.14200067520141602\n",
      "Testing at step=23, batch=80, test loss = 0.4933737516403198, test acc = 0.8600000143051147, time = 0.14777302742004395\n",
      "Step 23 finished in 456.847975730896, Train loss = 0.44988846031328045, Test loss = 0.4779954707622528; Train Acc = 0.8422333320975304, Test Acc = 0.8309999984502793\n",
      "Training at step=24, batch=0, train loss = 0.38782593607902527, train acc = 0.8700000047683716, time = 0.7065892219543457\n",
      "Training at step=24, batch=120, train loss = 0.7070816159248352, train acc = 0.7799999713897705, time = 0.7053380012512207\n",
      "Training at step=24, batch=240, train loss = 0.44507163763046265, train acc = 0.8500000238418579, time = 0.7053661346435547\n",
      "Training at step=24, batch=360, train loss = 0.44560471177101135, train acc = 0.8100000023841858, time = 0.7039766311645508\n",
      "Training at step=24, batch=480, train loss = 0.42132118344306946, train acc = 0.8799999952316284, time = 0.7039170265197754\n",
      "Testing at step=24, batch=0, test loss = 0.5291650295257568, test acc = 0.8600000143051147, time = 0.20711994171142578\n",
      "Testing at step=24, batch=20, test loss = 0.5094763040542603, test acc = 0.7799999713897705, time = 0.1418313980102539\n",
      "Testing at step=24, batch=40, test loss = 0.49438396096229553, test acc = 0.8500000238418579, time = 0.14365744590759277\n",
      "Testing at step=24, batch=60, test loss = 0.6199904680252075, test acc = 0.7799999713897705, time = 0.14188504219055176\n",
      "Testing at step=24, batch=80, test loss = 0.4084780216217041, test acc = 0.8600000143051147, time = 0.1419672966003418\n",
      "Step 24 finished in 456.1104245185852, Train loss = 0.4473826499531666, Test loss = 0.46263786673545837; Train Acc = 0.843166664938132, Test Acc = 0.8373999965190887\n",
      "Training at step=25, batch=0, train loss = 0.529517650604248, train acc = 0.7699999809265137, time = 0.7115342617034912\n",
      "Training at step=25, batch=120, train loss = 0.5825998783111572, train acc = 0.8100000023841858, time = 0.7047317028045654\n",
      "Training at step=25, batch=240, train loss = 0.35570746660232544, train acc = 0.8999999761581421, time = 0.70570969581604\n",
      "Training at step=25, batch=360, train loss = 0.4182649552822113, train acc = 0.8700000047683716, time = 0.6976146697998047\n",
      "Training at step=25, batch=480, train loss = 0.45445698499679565, train acc = 0.8799999952316284, time = 0.7019665241241455\n",
      "Testing at step=25, batch=0, test loss = 0.4641473889350891, test acc = 0.8299999833106995, time = 0.21703600883483887\n",
      "Testing at step=25, batch=20, test loss = 0.4268237352371216, test acc = 0.8299999833106995, time = 0.14723825454711914\n",
      "Testing at step=25, batch=40, test loss = 0.5594601631164551, test acc = 0.8399999737739563, time = 0.14480018615722656\n",
      "Testing at step=25, batch=60, test loss = 0.5305129885673523, test acc = 0.8500000238418579, time = 0.1324901580810547\n",
      "Testing at step=25, batch=80, test loss = 0.370148241519928, test acc = 0.8700000047683716, time = 0.14181041717529297\n",
      "Step 25 finished in 456.12342858314514, Train loss = 0.4451936722298463, Test loss = 0.467105772793293; Train Acc = 0.843499998251597, Test Acc = 0.8337999969720841\n",
      "Training at step=26, batch=0, train loss = 0.25615912675857544, train acc = 0.9399999976158142, time = 0.7046604156494141\n",
      "Training at step=26, batch=120, train loss = 0.5983080267906189, train acc = 0.8199999928474426, time = 0.7036929130554199\n",
      "Training at step=26, batch=240, train loss = 0.46842700242996216, train acc = 0.8700000047683716, time = 0.710961103439331\n",
      "Training at step=26, batch=360, train loss = 0.26218193769454956, train acc = 0.9399999976158142, time = 0.7045762538909912\n",
      "Training at step=26, batch=480, train loss = 0.3243862986564636, train acc = 0.8899999856948853, time = 0.7048490047454834\n",
      "Testing at step=26, batch=0, test loss = 0.3783702850341797, test acc = 0.8399999737739563, time = 0.24723172187805176\n",
      "Testing at step=26, batch=20, test loss = 0.5155704617500305, test acc = 0.8199999928474426, time = 0.14186882972717285\n",
      "Testing at step=26, batch=40, test loss = 0.38912665843963623, test acc = 0.8700000047683716, time = 0.13950657844543457\n",
      "Testing at step=26, batch=60, test loss = 0.46592071652412415, test acc = 0.8299999833106995, time = 0.1418929100036621\n",
      "Testing at step=26, batch=80, test loss = 0.364628404378891, test acc = 0.8999999761581421, time = 0.14484190940856934\n",
      "Step 26 finished in 456.26277780532837, Train loss = 0.4429330095897118, Test loss = 0.484149377644062; Train Acc = 0.8444333323836326, Test Acc = 0.8311000001430512\n",
      "Training at step=27, batch=0, train loss = 0.3683125376701355, train acc = 0.8500000238418579, time = 0.7139670848846436\n",
      "Training at step=27, batch=120, train loss = 0.4876323938369751, train acc = 0.8199999928474426, time = 0.7127468585968018\n",
      "Training at step=27, batch=240, train loss = 0.5204612612724304, train acc = 0.8100000023841858, time = 0.7033767700195312\n",
      "Training at step=27, batch=360, train loss = 0.42435309290885925, train acc = 0.8799999952316284, time = 0.7042324542999268\n",
      "Training at step=27, batch=480, train loss = 0.3133281469345093, train acc = 0.8700000047683716, time = 0.7045350074768066\n",
      "Testing at step=27, batch=0, test loss = 0.5098335146903992, test acc = 0.8299999833106995, time = 0.3033723831176758\n",
      "Testing at step=27, batch=20, test loss = 0.3995729088783264, test acc = 0.8299999833106995, time = 0.14331316947937012\n",
      "Testing at step=27, batch=40, test loss = 0.5187602043151855, test acc = 0.7900000214576721, time = 0.13831782341003418\n",
      "Testing at step=27, batch=60, test loss = 0.35218706727027893, test acc = 0.8299999833106995, time = 0.13960719108581543\n",
      "Testing at step=27, batch=80, test loss = 0.713105320930481, test acc = 0.7200000286102295, time = 0.1415398120880127\n",
      "Step 27 finished in 454.5793948173523, Train loss = 0.43912770253916583, Test loss = 0.49261617958545684; Train Acc = 0.846066665649414, Test Acc = 0.8242999970912933\n",
      "Training at step=28, batch=0, train loss = 0.4008989632129669, train acc = 0.8500000238418579, time = 0.689687967300415\n",
      "Training at step=28, batch=120, train loss = 0.5200506448745728, train acc = 0.8399999737739563, time = 0.7039716243743896\n",
      "Training at step=28, batch=240, train loss = 0.40780922770500183, train acc = 0.8299999833106995, time = 0.708855152130127\n",
      "Training at step=28, batch=360, train loss = 0.5118582248687744, train acc = 0.8500000238418579, time = 0.7043006420135498\n",
      "Training at step=28, batch=480, train loss = 0.495276540517807, train acc = 0.8600000143051147, time = 0.7053260803222656\n",
      "Testing at step=28, batch=0, test loss = 0.36517632007598877, test acc = 0.8799999952316284, time = 0.21582508087158203\n",
      "Testing at step=28, batch=20, test loss = 0.513538122177124, test acc = 0.8500000238418579, time = 0.14725828170776367\n",
      "Testing at step=28, batch=40, test loss = 0.5758606195449829, test acc = 0.800000011920929, time = 0.14636921882629395\n",
      "Testing at step=28, batch=60, test loss = 0.6866488456726074, test acc = 0.7699999809265137, time = 0.14620304107666016\n",
      "Testing at step=28, batch=80, test loss = 0.30865803360939026, test acc = 0.8700000047683716, time = 0.13512611389160156\n",
      "Step 28 finished in 456.38975739479065, Train loss = 0.44342311975856624, Test loss = 0.4731666254997253; Train Acc = 0.8432833329836528, Test Acc = 0.8357000005245209\n",
      "Training at step=29, batch=0, train loss = 0.4678434729576111, train acc = 0.8399999737739563, time = 0.7145318984985352\n",
      "Training at step=29, batch=120, train loss = 0.42941582202911377, train acc = 0.8500000238418579, time = 0.7318761348724365\n",
      "Training at step=29, batch=240, train loss = 0.5496862530708313, train acc = 0.8100000023841858, time = 0.73223876953125\n",
      "Training at step=29, batch=360, train loss = 0.41414016485214233, train acc = 0.8299999833106995, time = 0.7322630882263184\n",
      "Training at step=29, batch=480, train loss = 0.42386317253112793, train acc = 0.8100000023841858, time = 0.7324192523956299\n",
      "Testing at step=29, batch=0, test loss = 0.31632164120674133, test acc = 0.8899999856948853, time = 0.35666966438293457\n",
      "Testing at step=29, batch=20, test loss = 0.5689898133277893, test acc = 0.7799999713897705, time = 0.4168057441711426\n",
      "Testing at step=29, batch=40, test loss = 0.4202044606208801, test acc = 0.8500000238418579, time = 0.4168989658355713\n",
      "Testing at step=29, batch=60, test loss = 0.37404167652130127, test acc = 0.8399999737739563, time = 0.4175252914428711\n",
      "Testing at step=29, batch=80, test loss = 0.3408221900463104, test acc = 0.8799999952316284, time = 0.41686034202575684\n",
      "Step 29 finished in 466.72936630249023, Train loss = 0.4376265976577997, Test loss = 0.48335735231637955; Train Acc = 0.8463166654109955, Test Acc = 0.828999997973442\n",
      "Training at step=30, batch=0, train loss = 0.2806750237941742, train acc = 0.9100000262260437, time = 0.7215571403503418\n",
      "Training at step=30, batch=120, train loss = 0.26405447721481323, train acc = 0.9100000262260437, time = 0.7045116424560547\n",
      "Training at step=30, batch=240, train loss = 0.31332796812057495, train acc = 0.8700000047683716, time = 0.7022838592529297\n",
      "Training at step=30, batch=360, train loss = 0.7367913126945496, train acc = 0.7900000214576721, time = 0.7043204307556152\n",
      "Training at step=30, batch=480, train loss = 0.4506540298461914, train acc = 0.8199999928474426, time = 0.7045409679412842\n",
      "Testing at step=30, batch=0, test loss = 0.45435887575149536, test acc = 0.8399999737739563, time = 0.4223935604095459\n",
      "Testing at step=30, batch=20, test loss = 0.5460830926895142, test acc = 0.7900000214576721, time = 0.4174511432647705\n",
      "Testing at step=30, batch=40, test loss = 0.5668935179710388, test acc = 0.7699999809265137, time = 0.4180276393890381\n",
      "Testing at step=30, batch=60, test loss = 0.4520436227321625, test acc = 0.8500000238418579, time = 0.41798901557922363\n",
      "Testing at step=30, batch=80, test loss = 0.5555583834648132, test acc = 0.8100000023841858, time = 0.4177565574645996\n",
      "Step 30 finished in 456.4439055919647, Train loss = 0.43776731145878633, Test loss = 0.46058255940675735; Train Acc = 0.8467166646321614, Test Acc = 0.8374999994039536\n",
      "Training at step=31, batch=0, train loss = 0.30470213294029236, train acc = 0.8999999761581421, time = 0.7223272323608398\n",
      "Training at step=31, batch=120, train loss = 0.31679224967956543, train acc = 0.9200000166893005, time = 0.7065556049346924\n",
      "Training at step=31, batch=240, train loss = 0.43138325214385986, train acc = 0.8600000143051147, time = 0.70418381690979\n",
      "Training at step=31, batch=360, train loss = 0.43316590785980225, train acc = 0.8500000238418579, time = 0.7127463817596436\n",
      "Training at step=31, batch=480, train loss = 0.5291745662689209, train acc = 0.800000011920929, time = 0.704218864440918\n",
      "Testing at step=31, batch=0, test loss = 0.5924100279808044, test acc = 0.8199999928474426, time = 0.21728920936584473\n",
      "Testing at step=31, batch=20, test loss = 0.3225539028644562, test acc = 0.8999999761581421, time = 0.14713025093078613\n",
      "Testing at step=31, batch=40, test loss = 0.5708610415458679, test acc = 0.7900000214576721, time = 0.14899516105651855\n",
      "Testing at step=31, batch=60, test loss = 0.5635886192321777, test acc = 0.8500000238418579, time = 0.15101838111877441\n",
      "Testing at step=31, batch=80, test loss = 0.4290068447589874, test acc = 0.8799999952316284, time = 0.4174938201904297\n",
      "Step 31 finished in 456.5207221508026, Train loss = 0.4363722864041726, Test loss = 0.488566555082798; Train Acc = 0.8473333326975504, Test Acc = 0.8291999977827073\n",
      "Training at step=32, batch=0, train loss = 0.3290497958660126, train acc = 0.8999999761581421, time = 0.7216629981994629\n",
      "Training at step=32, batch=120, train loss = 0.5213116407394409, train acc = 0.8500000238418579, time = 0.7035036087036133\n",
      "Training at step=32, batch=240, train loss = 0.4175311028957367, train acc = 0.8700000047683716, time = 0.7055585384368896\n",
      "Training at step=32, batch=360, train loss = 0.3808581531047821, train acc = 0.8999999761581421, time = 0.6913549900054932\n",
      "Training at step=32, batch=480, train loss = 0.6284018754959106, train acc = 0.7599999904632568, time = 0.7057979106903076\n",
      "Testing at step=32, batch=0, test loss = 0.7304959297180176, test acc = 0.7799999713897705, time = 0.21592235565185547\n",
      "Testing at step=32, batch=20, test loss = 0.5167315006256104, test acc = 0.8500000238418579, time = 0.14731907844543457\n",
      "Testing at step=32, batch=40, test loss = 0.38931483030319214, test acc = 0.8899999856948853, time = 0.1466658115386963\n",
      "Testing at step=32, batch=60, test loss = 0.4637361764907837, test acc = 0.8100000023841858, time = 0.14542770385742188\n",
      "Testing at step=32, batch=80, test loss = 0.3017677068710327, test acc = 0.9200000166893005, time = 0.1440582275390625\n",
      "Step 32 finished in 455.9878327846527, Train loss = 0.436791727344195, Test loss = 0.4662397062778473; Train Acc = 0.8470499994357427, Test Acc = 0.8399999982118607\n",
      "Training at step=33, batch=0, train loss = 0.29713204503059387, train acc = 0.8799999952316284, time = 0.7083184719085693\n",
      "Training at step=33, batch=120, train loss = 0.422793984413147, train acc = 0.8500000238418579, time = 0.6574835777282715\n",
      "Training at step=33, batch=240, train loss = 0.33550944924354553, train acc = 0.8899999856948853, time = 0.7053887844085693\n",
      "Training at step=33, batch=360, train loss = 0.4398644268512726, train acc = 0.8399999737739563, time = 0.7036423683166504\n",
      "Training at step=33, batch=480, train loss = 0.3523654043674469, train acc = 0.8600000143051147, time = 0.7045023441314697\n",
      "Testing at step=33, batch=0, test loss = 0.6963317394256592, test acc = 0.7599999904632568, time = 0.21713590621948242\n",
      "Testing at step=33, batch=20, test loss = 0.3382833003997803, test acc = 0.8799999952316284, time = 0.1420912742614746\n",
      "Testing at step=33, batch=40, test loss = 0.4500652253627777, test acc = 0.8700000047683716, time = 0.1427013874053955\n",
      "Testing at step=33, batch=60, test loss = 0.5443277955055237, test acc = 0.7200000286102295, time = 0.149308443069458\n",
      "Testing at step=33, batch=80, test loss = 0.5303916335105896, test acc = 0.8199999928474426, time = 0.1485428810119629\n",
      "Step 33 finished in 458.26359391212463, Train loss = 0.4349016021688779, Test loss = 0.4644082233309746; Train Acc = 0.8468166659275691, Test Acc = 0.8320000004768372\n",
      "Training at step=34, batch=0, train loss = 0.3782455325126648, train acc = 0.8500000238418579, time = 0.7124452590942383\n",
      "Training at step=34, batch=120, train loss = 0.32433560490608215, train acc = 0.9200000166893005, time = 0.7048468589782715\n",
      "Training at step=34, batch=240, train loss = 0.464836984872818, train acc = 0.7900000214576721, time = 0.7049236297607422\n",
      "Training at step=34, batch=360, train loss = 0.3293117582798004, train acc = 0.8899999856948853, time = 0.7039375305175781\n",
      "Training at step=34, batch=480, train loss = 0.36566588282585144, train acc = 0.8500000238418579, time = 0.7053661346435547\n",
      "Testing at step=34, batch=0, test loss = 0.550486147403717, test acc = 0.8299999833106995, time = 0.41751527786254883\n",
      "Testing at step=34, batch=20, test loss = 0.49235087633132935, test acc = 0.8199999928474426, time = 0.4173104763031006\n",
      "Testing at step=34, batch=40, test loss = 0.4693759083747864, test acc = 0.8600000143051147, time = 0.41739535331726074\n",
      "Testing at step=34, batch=60, test loss = 0.3332579731941223, test acc = 0.8799999952316284, time = 0.41719508171081543\n",
      "Testing at step=34, batch=80, test loss = 0.42439815402030945, test acc = 0.8299999833106995, time = 0.4182271957397461\n",
      "Step 34 finished in 455.67069363594055, Train loss = 0.43478305655221144, Test loss = 0.48284188866615296; Train Acc = 0.8479499991734822, Test Acc = 0.8305999946594238\n",
      "Training at step=35, batch=0, train loss = 0.4655101001262665, train acc = 0.8199999928474426, time = 0.722346305847168\n",
      "Training at step=35, batch=120, train loss = 0.4858068823814392, train acc = 0.8199999928474426, time = 0.7052502632141113\n",
      "Training at step=35, batch=240, train loss = 0.45450690388679504, train acc = 0.8799999952316284, time = 0.7047193050384521\n",
      "Training at step=35, batch=360, train loss = 0.383757084608078, train acc = 0.8700000047683716, time = 0.705322265625\n",
      "Training at step=35, batch=480, train loss = 0.4667462110519409, train acc = 0.8199999928474426, time = 0.7045083045959473\n",
      "Testing at step=35, batch=0, test loss = 0.6748159527778625, test acc = 0.8100000023841858, time = 0.20837950706481934\n",
      "Testing at step=35, batch=20, test loss = 0.5571056008338928, test acc = 0.800000011920929, time = 0.14805126190185547\n",
      "Testing at step=35, batch=40, test loss = 0.37650200724601746, test acc = 0.8700000047683716, time = 0.1461653709411621\n",
      "Testing at step=35, batch=60, test loss = 0.5162479877471924, test acc = 0.8199999928474426, time = 0.14205408096313477\n",
      "Testing at step=35, batch=80, test loss = 0.4442530870437622, test acc = 0.8299999833106995, time = 0.14736604690551758\n",
      "Step 35 finished in 456.40640354156494, Train loss = 0.43624265380203725, Test loss = 0.4675870920717716; Train Acc = 0.8477166652679443, Test Acc = 0.8357999992370605\n",
      "Training at step=36, batch=0, train loss = 0.366542249917984, train acc = 0.8700000047683716, time = 0.7280638217926025\n",
      "Training at step=36, batch=120, train loss = 0.342003732919693, train acc = 0.8999999761581421, time = 0.7055497169494629\n",
      "Training at step=36, batch=240, train loss = 0.3357591927051544, train acc = 0.8799999952316284, time = 0.7060413360595703\n",
      "Training at step=36, batch=360, train loss = 0.41206809878349304, train acc = 0.8700000047683716, time = 0.7032546997070312\n",
      "Training at step=36, batch=480, train loss = 0.43420010805130005, train acc = 0.8299999833106995, time = 0.7042891979217529\n",
      "Testing at step=36, batch=0, test loss = 0.4706283509731293, test acc = 0.8500000238418579, time = 0.21592164039611816\n",
      "Testing at step=36, batch=20, test loss = 0.4008530080318451, test acc = 0.8799999952316284, time = 0.1410682201385498\n",
      "Testing at step=36, batch=40, test loss = 0.6133649349212646, test acc = 0.8100000023841858, time = 0.14916276931762695\n",
      "Testing at step=36, batch=60, test loss = 0.416767954826355, test acc = 0.8399999737739563, time = 0.1431276798248291\n",
      "Testing at step=36, batch=80, test loss = 0.5687324404716492, test acc = 0.7900000214576721, time = 0.1405348777770996\n",
      "Step 36 finished in 457.53054213523865, Train loss = 0.43390074945986273, Test loss = 0.45893566340208053; Train Acc = 0.846633332769076, Test Acc = 0.838999999165535\n",
      "Training at step=37, batch=0, train loss = 0.3674776554107666, train acc = 0.8799999952316284, time = 0.721806526184082\n",
      "Training at step=37, batch=120, train loss = 0.4540349543094635, train acc = 0.8899999856948853, time = 0.7039871215820312\n",
      "Training at step=37, batch=240, train loss = 0.5403060913085938, train acc = 0.8100000023841858, time = 0.7033710479736328\n",
      "Training at step=37, batch=360, train loss = 0.4153043031692505, train acc = 0.8600000143051147, time = 0.7055912017822266\n",
      "Training at step=37, batch=480, train loss = 0.36377134919166565, train acc = 0.8899999856948853, time = 0.7049770355224609\n",
      "Testing at step=37, batch=0, test loss = 0.6297474503517151, test acc = 0.8199999928474426, time = 0.25806641578674316\n",
      "Testing at step=37, batch=20, test loss = 0.4905925691127777, test acc = 0.8399999737739563, time = 0.14615702629089355\n",
      "Testing at step=37, batch=40, test loss = 0.3473035395145416, test acc = 0.8999999761581421, time = 0.14468073844909668\n",
      "Testing at step=37, batch=60, test loss = 0.2944945991039276, test acc = 0.8899999856948853, time = 0.14145946502685547\n",
      "Testing at step=37, batch=80, test loss = 0.5803104639053345, test acc = 0.8299999833106995, time = 0.41776180267333984\n",
      "Step 37 finished in 456.12613224983215, Train loss = 0.4306685869395733, Test loss = 0.4903254386782646; Train Acc = 0.84849999944369, Test Acc = 0.8284999984502792\n",
      "Training at step=38, batch=0, train loss = 0.5274378657341003, train acc = 0.8100000023841858, time = 0.7159297466278076\n",
      "Training at step=38, batch=120, train loss = 0.46400538086891174, train acc = 0.8299999833106995, time = 0.7072865962982178\n",
      "Training at step=38, batch=240, train loss = 0.4528145492076874, train acc = 0.8199999928474426, time = 0.7066514492034912\n",
      "Training at step=38, batch=360, train loss = 0.31429415941238403, train acc = 0.8999999761581421, time = 0.705878496170044\n",
      "Training at step=38, batch=480, train loss = 0.47686442732810974, train acc = 0.8600000143051147, time = 0.7027935981750488\n",
      "Testing at step=38, batch=0, test loss = 0.6599875092506409, test acc = 0.7799999713897705, time = 0.4243454933166504\n",
      "Testing at step=38, batch=20, test loss = 0.4511227309703827, test acc = 0.8700000047683716, time = 0.4171600341796875\n",
      "Testing at step=38, batch=40, test loss = 0.3654552698135376, test acc = 0.8700000047683716, time = 0.4163095951080322\n",
      "Testing at step=38, batch=60, test loss = 0.3334263861179352, test acc = 0.8700000047683716, time = 0.4163784980773926\n",
      "Testing at step=38, batch=80, test loss = 0.3991096019744873, test acc = 0.8500000238418579, time = 0.13445329666137695\n",
      "Step 38 finished in 456.6781828403473, Train loss = 0.4334530645608902, Test loss = 0.46400021731853486; Train Acc = 0.8468999988834063, Test Acc = 0.8374999988079072\n",
      "Training at step=39, batch=0, train loss = 0.2972504794597626, train acc = 0.8799999952316284, time = 0.6963362693786621\n",
      "Training at step=39, batch=120, train loss = 0.36793527007102966, train acc = 0.8500000238418579, time = 0.7057380676269531\n",
      "Training at step=39, batch=240, train loss = 0.36453086137771606, train acc = 0.8799999952316284, time = 0.702164888381958\n",
      "Training at step=39, batch=360, train loss = 0.4128932058811188, train acc = 0.800000011920929, time = 0.7059056758880615\n",
      "Training at step=39, batch=480, train loss = 0.4283325970172882, train acc = 0.8399999737739563, time = 0.7066864967346191\n",
      "Testing at step=39, batch=0, test loss = 0.5703340172767639, test acc = 0.7900000214576721, time = 0.18444395065307617\n",
      "Testing at step=39, batch=20, test loss = 0.40763285756111145, test acc = 0.8100000023841858, time = 0.14302420616149902\n",
      "Testing at step=39, batch=40, test loss = 0.6846544742584229, test acc = 0.8199999928474426, time = 0.13930296897888184\n",
      "Testing at step=39, batch=60, test loss = 0.47387078404426575, test acc = 0.8299999833106995, time = 0.1482830047607422\n",
      "Testing at step=39, batch=80, test loss = 0.6608421802520752, test acc = 0.7799999713897705, time = 0.13835954666137695\n",
      "Step 39 finished in 455.9381182193756, Train loss = 0.433488116239508, Test loss = 0.48353770494461057; Train Acc = 0.8469333307941754, Test Acc = 0.831700000166893\n",
      "Training at step=40, batch=0, train loss = 0.40194323658943176, train acc = 0.8500000238418579, time = 0.7112433910369873\n",
      "Training at step=40, batch=120, train loss = 0.5346889495849609, train acc = 0.8100000023841858, time = 0.7061128616333008\n",
      "Training at step=40, batch=240, train loss = 0.2687378525733948, train acc = 0.8999999761581421, time = 0.7052726745605469\n",
      "Training at step=40, batch=360, train loss = 0.4573705196380615, train acc = 0.8299999833106995, time = 0.7037060260772705\n",
      "Training at step=40, batch=480, train loss = 0.46227872371673584, train acc = 0.8299999833106995, time = 0.7045295238494873\n",
      "Testing at step=40, batch=0, test loss = 0.4948413372039795, test acc = 0.8199999928474426, time = 0.422121524810791\n",
      "Testing at step=40, batch=20, test loss = 0.3787209987640381, test acc = 0.8700000047683716, time = 0.4178340435028076\n",
      "Testing at step=40, batch=40, test loss = 0.5098829865455627, test acc = 0.8399999737739563, time = 0.41754770278930664\n",
      "Testing at step=40, batch=60, test loss = 0.4022981524467468, test acc = 0.8500000238418579, time = 0.4177687168121338\n",
      "Testing at step=40, batch=80, test loss = 0.4343763589859009, test acc = 0.8399999737739563, time = 0.416858434677124\n",
      "Step 40 finished in 456.3316595554352, Train loss = 0.43029877682526907, Test loss = 0.46263609200716016; Train Acc = 0.8493499987324079, Test Acc = 0.8343000000715256\n",
      "Training at step=41, batch=0, train loss = 0.337619811296463, train acc = 0.9399999976158142, time = 0.7092075347900391\n",
      "Training at step=41, batch=120, train loss = 0.25367432832717896, train acc = 0.9100000262260437, time = 0.7034029960632324\n",
      "Training at step=41, batch=240, train loss = 0.6024587750434875, train acc = 0.7900000214576721, time = 0.7041199207305908\n",
      "Training at step=41, batch=360, train loss = 0.45332372188568115, train acc = 0.8600000143051147, time = 0.7039239406585693\n",
      "Training at step=41, batch=480, train loss = 0.4911735951900482, train acc = 0.8600000143051147, time = 0.7048568725585938\n",
      "Testing at step=41, batch=0, test loss = 0.6149259805679321, test acc = 0.8199999928474426, time = 0.203660249710083\n",
      "Testing at step=41, batch=20, test loss = 0.490897536277771, test acc = 0.8100000023841858, time = 0.4171781539916992\n",
      "Testing at step=41, batch=40, test loss = 0.4095081388950348, test acc = 0.8500000238418579, time = 0.41709089279174805\n",
      "Testing at step=41, batch=60, test loss = 0.47291773557662964, test acc = 0.8899999856948853, time = 0.4178123474121094\n",
      "Testing at step=41, batch=80, test loss = 0.445402592420578, test acc = 0.8399999737739563, time = 0.41736364364624023\n",
      "Step 41 finished in 455.9295358657837, Train loss = 0.4281548057993253, Test loss = 0.4600663931667805; Train Acc = 0.8499333326021831, Test Acc = 0.8375999999046325\n",
      "Training at step=42, batch=0, train loss = 0.24382337927818298, train acc = 0.8999999761581421, time = 0.698408842086792\n",
      "Training at step=42, batch=120, train loss = 0.31576067209243774, train acc = 0.8799999952316284, time = 0.7038853168487549\n",
      "Training at step=42, batch=240, train loss = 0.5698180794715881, train acc = 0.800000011920929, time = 0.706226110458374\n",
      "Training at step=42, batch=360, train loss = 0.4311457574367523, train acc = 0.8600000143051147, time = 0.7101585865020752\n",
      "Training at step=42, batch=480, train loss = 0.3724265396595001, train acc = 0.8399999737739563, time = 0.7062528133392334\n",
      "Testing at step=42, batch=0, test loss = 0.4287187159061432, test acc = 0.8500000238418579, time = 0.21024179458618164\n",
      "Testing at step=42, batch=20, test loss = 0.40872469544410706, test acc = 0.8600000143051147, time = 0.14246511459350586\n",
      "Testing at step=42, batch=40, test loss = 0.42774349451065063, test acc = 0.8500000238418579, time = 0.141890287399292\n",
      "Testing at step=42, batch=60, test loss = 0.5405309200286865, test acc = 0.8299999833106995, time = 0.14438247680664062\n",
      "Testing at step=42, batch=80, test loss = 0.6246796250343323, test acc = 0.8199999928474426, time = 0.14612078666687012\n",
      "Step 42 finished in 455.71827149391174, Train loss = 0.43051576539874076, Test loss = 0.4785565859079361; Train Acc = 0.8488999993602435, Test Acc = 0.8348000001907349\n",
      "Training at step=43, batch=0, train loss = 0.3025488555431366, train acc = 0.8899999856948853, time = 0.7251451015472412\n",
      "Training at step=43, batch=120, train loss = 0.4151259958744049, train acc = 0.8299999833106995, time = 0.7328641414642334\n",
      "Training at step=43, batch=240, train loss = 0.4470871388912201, train acc = 0.8700000047683716, time = 0.6928164958953857\n",
      "Training at step=43, batch=360, train loss = 0.38235417008399963, train acc = 0.8700000047683716, time = 0.7050909996032715\n",
      "Training at step=43, batch=480, train loss = 0.5555179119110107, train acc = 0.8299999833106995, time = 0.7062692642211914\n",
      "Testing at step=43, batch=0, test loss = 0.48568665981292725, test acc = 0.8500000238418579, time = 0.2160351276397705\n",
      "Testing at step=43, batch=20, test loss = 0.47226840257644653, test acc = 0.8399999737739563, time = 0.13725543022155762\n",
      "Testing at step=43, batch=40, test loss = 0.3671504259109497, test acc = 0.8299999833106995, time = 0.1374211311340332\n",
      "Testing at step=43, batch=60, test loss = 0.3077840209007263, test acc = 0.9200000166893005, time = 0.1404421329498291\n",
      "Testing at step=43, batch=80, test loss = 0.4406183660030365, test acc = 0.8399999737739563, time = 0.33241963386535645\n",
      "Step 43 finished in 462.0984880924225, Train loss = 0.4288333298265934, Test loss = 0.49576325207948685; Train Acc = 0.8496833310524623, Test Acc = 0.8174999970197677\n",
      "Training at step=44, batch=0, train loss = 0.5980778336524963, train acc = 0.75, time = 0.7252092361450195\n",
      "Training at step=44, batch=120, train loss = 0.3042448163032532, train acc = 0.8600000143051147, time = 0.705265998840332\n",
      "Training at step=44, batch=240, train loss = 0.4502311944961548, train acc = 0.8700000047683716, time = 0.7054810523986816\n",
      "Training at step=44, batch=360, train loss = 0.23223252594470978, train acc = 0.9300000071525574, time = 0.7057473659515381\n",
      "Training at step=44, batch=480, train loss = 0.4185255169868469, train acc = 0.8199999928474426, time = 0.7048287391662598\n",
      "Testing at step=44, batch=0, test loss = 0.38968193531036377, test acc = 0.8600000143051147, time = 0.42361927032470703\n",
      "Testing at step=44, batch=20, test loss = 0.49860209226608276, test acc = 0.8299999833106995, time = 0.418612003326416\n",
      "Testing at step=44, batch=40, test loss = 0.46868419647216797, test acc = 0.8199999928474426, time = 0.4186704158782959\n",
      "Testing at step=44, batch=60, test loss = 0.49678799510002136, test acc = 0.8100000023841858, time = 0.41764211654663086\n",
      "Testing at step=44, batch=80, test loss = 0.42802292108535767, test acc = 0.8700000047683716, time = 0.4181396961212158\n",
      "Step 44 finished in 456.83735251426697, Train loss = 0.4247341588387887, Test loss = 0.4739836919307709; Train Acc = 0.8514833305279413, Test Acc = 0.8312999987602234\n",
      "Training at step=45, batch=0, train loss = 0.5127807855606079, train acc = 0.7900000214576721, time = 0.7249114513397217\n",
      "Training at step=45, batch=120, train loss = 0.5004675388336182, train acc = 0.8100000023841858, time = 0.7046763896942139\n",
      "Training at step=45, batch=240, train loss = 0.3923271596431732, train acc = 0.8199999928474426, time = 0.7053420543670654\n",
      "Training at step=45, batch=360, train loss = 0.35949480533599854, train acc = 0.8799999952316284, time = 0.7053003311157227\n",
      "Training at step=45, batch=480, train loss = 0.34469902515411377, train acc = 0.8899999856948853, time = 0.7017459869384766\n",
      "Testing at step=45, batch=0, test loss = 0.4249993562698364, test acc = 0.8399999737739563, time = 0.35656094551086426\n",
      "Testing at step=45, batch=20, test loss = 0.49592551589012146, test acc = 0.8500000238418579, time = 0.4163837432861328\n",
      "Testing at step=45, batch=40, test loss = 0.32569485902786255, test acc = 0.8899999856948853, time = 0.4173922538757324\n",
      "Testing at step=45, batch=60, test loss = 0.5685765743255615, test acc = 0.8100000023841858, time = 0.4178764820098877\n",
      "Testing at step=45, batch=80, test loss = 0.5733910202980042, test acc = 0.8299999833106995, time = 0.4177839756011963\n",
      "Step 45 finished in 455.778240442276, Train loss = 0.42462757356464864, Test loss = 0.46522341966629027; Train Acc = 0.8510333318511645, Test Acc = 0.8382000011205674\n",
      "Training at step=46, batch=0, train loss = 0.49039000272750854, train acc = 0.800000011920929, time = 0.7190148830413818\n",
      "Training at step=46, batch=120, train loss = 0.36420178413391113, train acc = 0.9100000262260437, time = 0.7059831619262695\n",
      "Training at step=46, batch=240, train loss = 0.3536510169506073, train acc = 0.8899999856948853, time = 0.7060337066650391\n",
      "Training at step=46, batch=360, train loss = 0.34290122985839844, train acc = 0.8799999952316284, time = 0.7038729190826416\n",
      "Training at step=46, batch=480, train loss = 0.5139844417572021, train acc = 0.7699999809265137, time = 0.7031993865966797\n",
      "Testing at step=46, batch=0, test loss = 0.3639072775840759, test acc = 0.8799999952316284, time = 0.2224414348602295\n",
      "Testing at step=46, batch=20, test loss = 0.42108994722366333, test acc = 0.8799999952316284, time = 0.14202284812927246\n",
      "Testing at step=46, batch=40, test loss = 0.39602547883987427, test acc = 0.8700000047683716, time = 0.14444684982299805\n",
      "Testing at step=46, batch=60, test loss = 0.5940569043159485, test acc = 0.8299999833106995, time = 0.13994383811950684\n",
      "Testing at step=46, batch=80, test loss = 0.5786557197570801, test acc = 0.7900000214576721, time = 0.14160418510437012\n",
      "Step 46 finished in 456.24431133270264, Train loss = 0.4252054889748494, Test loss = 0.4581734947860241; Train Acc = 0.8506166663765907, Test Acc = 0.8405999994277954\n",
      "Training at step=47, batch=0, train loss = 0.3661458492279053, train acc = 0.8600000143051147, time = 0.7248363494873047\n",
      "Training at step=47, batch=120, train loss = 0.6422917246818542, train acc = 0.7699999809265137, time = 0.7346303462982178\n",
      "Training at step=47, batch=240, train loss = 0.48622196912765503, train acc = 0.8100000023841858, time = 0.7320451736450195\n",
      "Training at step=47, batch=360, train loss = 0.5493990778923035, train acc = 0.8600000143051147, time = 0.7329905033111572\n",
      "Training at step=47, batch=480, train loss = 0.504666805267334, train acc = 0.8600000143051147, time = 0.6917893886566162\n",
      "Testing at step=47, batch=0, test loss = 0.3834351599216461, test acc = 0.8399999737739563, time = 0.22276687622070312\n",
      "Testing at step=47, batch=20, test loss = 0.5133270621299744, test acc = 0.8199999928474426, time = 0.1401059627532959\n",
      "Testing at step=47, batch=40, test loss = 0.5248197317123413, test acc = 0.8600000143051147, time = 0.14036178588867188\n",
      "Testing at step=47, batch=60, test loss = 0.4288289248943329, test acc = 0.8500000238418579, time = 0.14338088035583496\n",
      "Testing at step=47, batch=80, test loss = 0.5705165266990662, test acc = 0.8100000023841858, time = 0.14171171188354492\n",
      "Step 47 finished in 467.7352657318115, Train loss = 0.42447901556889217, Test loss = 0.46407617419958114; Train Acc = 0.8517333330710729, Test Acc = 0.8361999988555908\n",
      "Training at step=48, batch=0, train loss = 0.6242622137069702, train acc = 0.8399999737739563, time = 0.6960422992706299\n",
      "Training at step=48, batch=120, train loss = 0.49843543767929077, train acc = 0.8399999737739563, time = 0.7049784660339355\n",
      "Training at step=48, batch=240, train loss = 0.40513846278190613, train acc = 0.8600000143051147, time = 0.7047789096832275\n",
      "Training at step=48, batch=360, train loss = 0.45307186245918274, train acc = 0.8600000143051147, time = 0.7046470642089844\n",
      "Training at step=48, batch=480, train loss = 0.384185791015625, train acc = 0.8399999737739563, time = 0.7047398090362549\n",
      "Testing at step=48, batch=0, test loss = 0.4066540598869324, test acc = 0.8700000047683716, time = 0.2080531120300293\n",
      "Testing at step=48, batch=20, test loss = 0.4801803231239319, test acc = 0.8100000023841858, time = 0.14252448081970215\n",
      "Testing at step=48, batch=40, test loss = 0.5305193066596985, test acc = 0.7599999904632568, time = 0.1510932445526123\n",
      "Testing at step=48, batch=60, test loss = 0.4791277050971985, test acc = 0.7799999713897705, time = 0.1475210189819336\n",
      "Testing at step=48, batch=80, test loss = 0.5489516258239746, test acc = 0.75, time = 0.14949727058410645\n",
      "Step 48 finished in 456.03391456604004, Train loss = 0.4230356240272522, Test loss = 0.4627136406302452; Train Acc = 0.8516999994715054, Test Acc = 0.8320999991893768\n",
      "Training at step=49, batch=0, train loss = 0.4987051486968994, train acc = 0.8299999833106995, time = 0.7028119564056396\n",
      "Training at step=49, batch=120, train loss = 0.5869541764259338, train acc = 0.8100000023841858, time = 0.7063558101654053\n",
      "Training at step=49, batch=240, train loss = 0.43462321162223816, train acc = 0.8399999737739563, time = 0.7046108245849609\n",
      "Training at step=49, batch=360, train loss = 0.6140182018280029, train acc = 0.7799999713897705, time = 0.7041440010070801\n",
      "Training at step=49, batch=480, train loss = 0.3097210228443146, train acc = 0.8899999856948853, time = 0.7066788673400879\n",
      "Testing at step=49, batch=0, test loss = 0.44902098178863525, test acc = 0.8199999928474426, time = 0.26941585540771484\n",
      "Testing at step=49, batch=20, test loss = 0.36644408106803894, test acc = 0.8500000238418579, time = 0.137315034866333\n",
      "Testing at step=49, batch=40, test loss = 0.5323834419250488, test acc = 0.7799999713897705, time = 0.14262986183166504\n",
      "Testing at step=49, batch=60, test loss = 0.6696960926055908, test acc = 0.7699999809265137, time = 0.1434619426727295\n",
      "Testing at step=49, batch=80, test loss = 0.6231759786605835, test acc = 0.8100000023841858, time = 0.4175455570220947\n",
      "Step 49 finished in 455.9414179325104, Train loss = 0.4234515568117301, Test loss = 0.46379565328359607; Train Acc = 0.8524833324551583, Test Acc = 0.836800001859665\n",
      "Training at step=50, batch=0, train loss = 0.47538816928863525, train acc = 0.8199999928474426, time = 0.7245886325836182\n",
      "Training at step=50, batch=120, train loss = 0.7322688102722168, train acc = 0.8199999928474426, time = 0.7050366401672363\n",
      "Training at step=50, batch=240, train loss = 0.4385502338409424, train acc = 0.8600000143051147, time = 0.7044548988342285\n",
      "Training at step=50, batch=360, train loss = 0.2556154429912567, train acc = 0.9300000071525574, time = 0.7033114433288574\n",
      "Training at step=50, batch=480, train loss = 0.285334974527359, train acc = 0.9200000166893005, time = 0.7049882411956787\n",
      "Testing at step=50, batch=0, test loss = 0.4604271352291107, test acc = 0.7900000214576721, time = 0.1764659881591797\n",
      "Testing at step=50, batch=20, test loss = 0.4427977502346039, test acc = 0.8100000023841858, time = 0.14206790924072266\n",
      "Testing at step=50, batch=40, test loss = 0.5247276425361633, test acc = 0.8299999833106995, time = 0.14217686653137207\n",
      "Testing at step=50, batch=60, test loss = 0.44319820404052734, test acc = 0.8600000143051147, time = 0.1421647071838379\n",
      "Testing at step=50, batch=80, test loss = 0.5334923267364502, test acc = 0.7699999809265137, time = 0.13909268379211426\n",
      "Step 50 finished in 456.8969216346741, Train loss = 0.4218338900059462, Test loss = 0.4680379718542099; Train Acc = 0.852666666607062, Test Acc = 0.8328999954462052\n",
      "Training at step=51, batch=0, train loss = 0.45085984468460083, train acc = 0.8399999737739563, time = 0.7145600318908691\n",
      "Training at step=51, batch=120, train loss = 0.359986811876297, train acc = 0.8700000047683716, time = 0.7040107250213623\n",
      "Training at step=51, batch=240, train loss = 0.40591561794281006, train acc = 0.8500000238418579, time = 0.7044527530670166\n",
      "Training at step=51, batch=360, train loss = 0.5581965446472168, train acc = 0.8700000047683716, time = 0.7044961452484131\n",
      "Training at step=51, batch=480, train loss = 0.38013848662376404, train acc = 0.8899999856948853, time = 0.7044832706451416\n",
      "Testing at step=51, batch=0, test loss = 0.497714638710022, test acc = 0.8299999833106995, time = 0.19951868057250977\n",
      "Testing at step=51, batch=20, test loss = 0.41785064339637756, test acc = 0.8199999928474426, time = 0.14609670639038086\n",
      "Testing at step=51, batch=40, test loss = 0.5861868262290955, test acc = 0.7599999904632568, time = 0.14986729621887207\n",
      "Testing at step=51, batch=60, test loss = 0.685778021812439, test acc = 0.7699999809265137, time = 0.14437150955200195\n",
      "Testing at step=51, batch=80, test loss = 0.40887510776519775, test acc = 0.8500000238418579, time = 0.14195585250854492\n",
      "Step 51 finished in 455.93957328796387, Train loss = 0.42378160516421, Test loss = 0.4796094611287117; Train Acc = 0.8521499984463056, Test Acc = 0.8260999965667725\n",
      "Training at step=52, batch=0, train loss = 0.7177407145500183, train acc = 0.7300000190734863, time = 0.7096381187438965\n",
      "Training at step=52, batch=120, train loss = 0.30105260014533997, train acc = 0.8999999761581421, time = 0.7035248279571533\n",
      "Training at step=52, batch=240, train loss = 0.4571724236011505, train acc = 0.8600000143051147, time = 0.7036585807800293\n",
      "Training at step=52, batch=360, train loss = 0.21701852977275848, train acc = 0.9300000071525574, time = 0.7038257122039795\n",
      "Training at step=52, batch=480, train loss = 0.3846551775932312, train acc = 0.8700000047683716, time = 0.7045021057128906\n",
      "Testing at step=52, batch=0, test loss = 0.4668736159801483, test acc = 0.8199999928474426, time = 0.17519307136535645\n",
      "Testing at step=52, batch=20, test loss = 0.5306933522224426, test acc = 0.800000011920929, time = 0.14927124977111816\n",
      "Testing at step=52, batch=40, test loss = 0.43627825379371643, test acc = 0.8500000238418579, time = 0.1478571891784668\n",
      "Testing at step=52, batch=60, test loss = 0.5000165104866028, test acc = 0.8100000023841858, time = 0.15207386016845703\n",
      "Testing at step=52, batch=80, test loss = 0.2278645932674408, test acc = 0.8999999761581421, time = 0.14755535125732422\n",
      "Step 52 finished in 456.42340898513794, Train loss = 0.42141741221149764, Test loss = 0.46706772699952126; Train Acc = 0.8522999983032544, Test Acc = 0.8349999982118607\n",
      "Training at step=53, batch=0, train loss = 0.3188960552215576, train acc = 0.8999999761581421, time = 0.6985118389129639\n",
      "Training at step=53, batch=120, train loss = 0.320830374956131, train acc = 0.8799999952316284, time = 0.7020370960235596\n",
      "Training at step=53, batch=240, train loss = 0.48154354095458984, train acc = 0.8399999737739563, time = 0.7059564590454102\n",
      "Training at step=53, batch=360, train loss = 0.2685995101928711, train acc = 0.8799999952316284, time = 0.6948654651641846\n",
      "Training at step=53, batch=480, train loss = 0.5975061058998108, train acc = 0.8199999928474426, time = 0.7043650150299072\n",
      "Testing at step=53, batch=0, test loss = 0.4126715362071991, test acc = 0.8299999833106995, time = 0.21088361740112305\n",
      "Testing at step=53, batch=20, test loss = 0.562249481678009, test acc = 0.7900000214576721, time = 0.14993667602539062\n",
      "Testing at step=53, batch=40, test loss = 0.5624715089797974, test acc = 0.8299999833106995, time = 0.14899730682373047\n",
      "Testing at step=53, batch=60, test loss = 0.4962204396724701, test acc = 0.800000011920929, time = 0.14827775955200195\n",
      "Testing at step=53, batch=80, test loss = 0.6309691667556763, test acc = 0.8500000238418579, time = 0.15058493614196777\n",
      "Step 53 finished in 455.78870725631714, Train loss = 0.4194472496459882, Test loss = 0.4694538661837578; Train Acc = 0.8531999991337458, Test Acc = 0.8380999994277955\n",
      "Training at step=54, batch=0, train loss = 0.44458019733428955, train acc = 0.8299999833106995, time = 0.7302958965301514\n",
      "Training at step=54, batch=120, train loss = 0.5690861940383911, train acc = 0.8199999928474426, time = 0.7323400974273682\n",
      "Training at step=54, batch=240, train loss = 0.502577543258667, train acc = 0.800000011920929, time = 0.7334258556365967\n",
      "Training at step=54, batch=360, train loss = 0.4795036017894745, train acc = 0.8299999833106995, time = 0.7288503646850586\n",
      "Training at step=54, batch=480, train loss = 0.41843685507774353, train acc = 0.8700000047683716, time = 0.7018494606018066\n",
      "Testing at step=54, batch=0, test loss = 0.43181660771369934, test acc = 0.8799999952316284, time = 0.4245777130126953\n",
      "Testing at step=54, batch=20, test loss = 0.41272246837615967, test acc = 0.8500000238418579, time = 0.4179983139038086\n",
      "Testing at step=54, batch=40, test loss = 0.501396894454956, test acc = 0.8100000023841858, time = 0.14705753326416016\n",
      "Testing at step=54, batch=60, test loss = 0.4556628465652466, test acc = 0.8600000143051147, time = 0.14719629287719727\n",
      "Testing at step=54, batch=80, test loss = 0.5165034532546997, test acc = 0.8500000238418579, time = 0.14878320693969727\n",
      "Step 54 finished in 468.9777133464813, Train loss = 0.4198978802065055, Test loss = 0.4863953399658203; Train Acc = 0.8526666646202405, Test Acc = 0.8332999974489212\n",
      "Training at step=55, batch=0, train loss = 0.44547221064567566, train acc = 0.8399999737739563, time = 0.714118480682373\n",
      "Training at step=55, batch=120, train loss = 0.33278995752334595, train acc = 0.8700000047683716, time = 0.7311711311340332\n",
      "Training at step=55, batch=240, train loss = 0.511610746383667, train acc = 0.8600000143051147, time = 0.7023768424987793\n",
      "Training at step=55, batch=360, train loss = 0.35992538928985596, train acc = 0.8600000143051147, time = 0.7056455612182617\n",
      "Training at step=55, batch=480, train loss = 0.37541690468788147, train acc = 0.8799999952316284, time = 0.7051866054534912\n",
      "Testing at step=55, batch=0, test loss = 0.566192090511322, test acc = 0.7699999809265137, time = 0.19284701347351074\n",
      "Testing at step=55, batch=20, test loss = 0.4313715696334839, test acc = 0.9200000166893005, time = 0.14192962646484375\n",
      "Testing at step=55, batch=40, test loss = 0.4486232399940491, test acc = 0.8700000047683716, time = 0.142380952835083\n",
      "Testing at step=55, batch=60, test loss = 0.30594202876091003, test acc = 0.9300000071525574, time = 0.1422281265258789\n",
      "Testing at step=55, batch=80, test loss = 0.3477114140987396, test acc = 0.8700000047683716, time = 0.14246535301208496\n",
      "Step 55 finished in 461.7671947479248, Train loss = 0.4223589408149322, Test loss = 0.4477321594953537; Train Acc = 0.8517666666706403, Test Acc = 0.8437999987602234\n",
      "Training at step=56, batch=0, train loss = 0.41018134355545044, train acc = 0.8100000023841858, time = 0.7231359481811523\n",
      "Training at step=56, batch=120, train loss = 0.3284321129322052, train acc = 0.8899999856948853, time = 0.7042696475982666\n",
      "Training at step=56, batch=240, train loss = 0.2367236465215683, train acc = 0.9100000262260437, time = 0.7044692039489746\n",
      "Training at step=56, batch=360, train loss = 0.3188905715942383, train acc = 0.8600000143051147, time = 0.7053327560424805\n",
      "Training at step=56, batch=480, train loss = 0.4376630485057831, train acc = 0.8600000143051147, time = 0.7099945545196533\n",
      "Testing at step=56, batch=0, test loss = 0.45546597242355347, test acc = 0.8500000238418579, time = 0.3815906047821045\n",
      "Testing at step=56, batch=20, test loss = 0.5205065011978149, test acc = 0.7799999713897705, time = 0.4172031879425049\n",
      "Testing at step=56, batch=40, test loss = 0.3866475820541382, test acc = 0.8399999737739563, time = 0.4178471565246582\n",
      "Testing at step=56, batch=60, test loss = 0.37440788745880127, test acc = 0.8600000143051147, time = 0.41725611686706543\n",
      "Testing at step=56, batch=80, test loss = 0.49757319688796997, test acc = 0.8299999833106995, time = 0.41794514656066895\n",
      "Step 56 finished in 456.31082224845886, Train loss = 0.4175121996303399, Test loss = 0.48097401529550554; Train Acc = 0.8537333330512047, Test Acc = 0.8224999994039536\n",
      "Training at step=57, batch=0, train loss = 0.43159282207489014, train acc = 0.8199999928474426, time = 0.7089798450469971\n",
      "Training at step=57, batch=120, train loss = 0.3782888352870941, train acc = 0.8600000143051147, time = 0.7037463188171387\n",
      "Training at step=57, batch=240, train loss = 0.44100695848464966, train acc = 0.8700000047683716, time = 0.7053899765014648\n",
      "Training at step=57, batch=360, train loss = 0.48834431171417236, train acc = 0.800000011920929, time = 0.7044098377227783\n",
      "Training at step=57, batch=480, train loss = 0.4893416464328766, train acc = 0.8299999833106995, time = 0.7051293849945068\n",
      "Testing at step=57, batch=0, test loss = 0.37995830178260803, test acc = 0.8999999761581421, time = 0.2048356533050537\n",
      "Testing at step=57, batch=20, test loss = 0.5044615864753723, test acc = 0.7900000214576721, time = 0.13582444190979004\n",
      "Testing at step=57, batch=40, test loss = 0.6134921908378601, test acc = 0.7900000214576721, time = 0.13570475578308105\n",
      "Testing at step=57, batch=60, test loss = 0.35620152950286865, test acc = 0.8500000238418579, time = 0.13463997840881348\n",
      "Testing at step=57, batch=80, test loss = 0.3757152259349823, test acc = 0.8500000238418579, time = 0.13709139823913574\n",
      "Step 57 finished in 455.950208902359, Train loss = 0.41742238618433475, Test loss = 0.4655445069074631; Train Acc = 0.8543333341677983, Test Acc = 0.838700001835823\n",
      "Training at step=58, batch=0, train loss = 0.4881598651409149, train acc = 0.800000011920929, time = 0.7092423439025879\n",
      "Training at step=58, batch=120, train loss = 0.37944942712783813, train acc = 0.8799999952316284, time = 0.7071568965911865\n",
      "Training at step=58, batch=240, train loss = 0.33118656277656555, train acc = 0.8999999761581421, time = 0.7054219245910645\n",
      "Training at step=58, batch=360, train loss = 0.432092547416687, train acc = 0.8100000023841858, time = 0.7047185897827148\n",
      "Training at step=58, batch=480, train loss = 0.6458247303962708, train acc = 0.7699999809265137, time = 0.7033016681671143\n",
      "Testing at step=58, batch=0, test loss = 0.36789053678512573, test acc = 0.8799999952316284, time = 0.22260022163391113\n",
      "Testing at step=58, batch=20, test loss = 0.48857536911964417, test acc = 0.8299999833106995, time = 0.14607787132263184\n",
      "Testing at step=58, batch=40, test loss = 0.35649871826171875, test acc = 0.8600000143051147, time = 0.14299583435058594\n",
      "Testing at step=58, batch=60, test loss = 0.3666932284832001, test acc = 0.8799999952316284, time = 0.14409470558166504\n",
      "Testing at step=58, batch=80, test loss = 0.37385645508766174, test acc = 0.8500000238418579, time = 0.14522266387939453\n",
      "Step 58 finished in 455.7496111392975, Train loss = 0.4172552980730931, Test loss = 0.45710100799798964; Train Acc = 0.8530499986807505, Test Acc = 0.8406999957561493\n",
      "Training at step=59, batch=0, train loss = 0.30384817719459534, train acc = 0.9200000166893005, time = 0.7276272773742676\n",
      "Training at step=59, batch=120, train loss = 0.4848605990409851, train acc = 0.8199999928474426, time = 0.7054805755615234\n",
      "Training at step=59, batch=240, train loss = 0.38261881470680237, train acc = 0.8600000143051147, time = 0.7033596038818359\n",
      "Training at step=59, batch=360, train loss = 0.3782355487346649, train acc = 0.8799999952316284, time = 0.703131914138794\n",
      "Training at step=59, batch=480, train loss = 0.34499022364616394, train acc = 0.8799999952316284, time = 0.7051496505737305\n",
      "Testing at step=59, batch=0, test loss = 0.4793194532394409, test acc = 0.8199999928474426, time = 0.2145998477935791\n",
      "Testing at step=59, batch=20, test loss = 0.30844536423683167, test acc = 0.8700000047683716, time = 0.15054059028625488\n",
      "Testing at step=59, batch=40, test loss = 0.46167701482772827, test acc = 0.8199999928474426, time = 0.14997482299804688\n",
      "Testing at step=59, batch=60, test loss = 0.42977437376976013, test acc = 0.8600000143051147, time = 0.14918088912963867\n",
      "Testing at step=59, batch=80, test loss = 0.38876307010650635, test acc = 0.8399999737739563, time = 0.146620512008667\n",
      "Step 59 finished in 457.32651591300964, Train loss = 0.41818866930902004, Test loss = 0.4591410656273365; Train Acc = 0.852649998764197, Test Acc = 0.8385999965667724\n",
      "Training at step=60, batch=0, train loss = 0.292142778635025, train acc = 0.8999999761581421, time = 0.7174193859100342\n",
      "Training at step=60, batch=120, train loss = 0.5198968052864075, train acc = 0.8199999928474426, time = 0.7049791812896729\n",
      "Training at step=60, batch=240, train loss = 0.40516895055770874, train acc = 0.8600000143051147, time = 0.7046160697937012\n",
      "Training at step=60, batch=360, train loss = 0.41569292545318604, train acc = 0.8700000047683716, time = 0.7038829326629639\n",
      "Training at step=60, batch=480, train loss = 0.41335052251815796, train acc = 0.8700000047683716, time = 0.7032716274261475\n",
      "Testing at step=60, batch=0, test loss = 0.5599316954612732, test acc = 0.7699999809265137, time = 0.2063584327697754\n",
      "Testing at step=60, batch=20, test loss = 0.3326207995414734, test acc = 0.8999999761581421, time = 0.14224767684936523\n",
      "Testing at step=60, batch=40, test loss = 0.36057034134864807, test acc = 0.8700000047683716, time = 0.14550495147705078\n",
      "Testing at step=60, batch=60, test loss = 0.507261335849762, test acc = 0.8299999833106995, time = 0.14981436729431152\n",
      "Testing at step=60, batch=80, test loss = 0.491483598947525, test acc = 0.8199999928474426, time = 0.1481189727783203\n",
      "Step 60 finished in 456.0115439891815, Train loss = 0.41484043943385285, Test loss = 0.4703906387090683; Train Acc = 0.8555666661262512, Test Acc = 0.8355999982357025\n",
      "Training at step=61, batch=0, train loss = 0.5322074294090271, train acc = 0.800000011920929, time = 0.7097961902618408\n",
      "Training at step=61, batch=120, train loss = 0.3819842040538788, train acc = 0.8500000238418579, time = 0.7048666477203369\n",
      "Training at step=61, batch=240, train loss = 0.3863581120967865, train acc = 0.8799999952316284, time = 0.7048981189727783\n",
      "Training at step=61, batch=360, train loss = 0.5579246282577515, train acc = 0.8399999737739563, time = 0.7054996490478516\n",
      "Training at step=61, batch=480, train loss = 0.5429415702819824, train acc = 0.8100000023841858, time = 0.704491376876831\n",
      "Testing at step=61, batch=0, test loss = 0.3097347319126129, test acc = 0.8899999856948853, time = 0.22701239585876465\n",
      "Testing at step=61, batch=20, test loss = 0.5969995260238647, test acc = 0.8100000023841858, time = 0.14529848098754883\n",
      "Testing at step=61, batch=40, test loss = 0.5035070180892944, test acc = 0.8299999833106995, time = 0.1416771411895752\n",
      "Testing at step=61, batch=60, test loss = 0.4818624258041382, test acc = 0.8500000238418579, time = 0.1450507640838623\n",
      "Testing at step=61, batch=80, test loss = 0.37076088786125183, test acc = 0.8700000047683716, time = 0.14618754386901855\n",
      "Step 61 finished in 457.2420575618744, Train loss = 0.41715171925723554, Test loss = 0.4532590043544769; Train Acc = 0.8538333336512248, Test Acc = 0.8431999975442886\n",
      "Training at step=62, batch=0, train loss = 0.3766246736049652, train acc = 0.8399999737739563, time = 0.7259941101074219\n",
      "Training at step=62, batch=120, train loss = 0.3594967722892761, train acc = 0.8799999952316284, time = 0.7040109634399414\n",
      "Training at step=62, batch=240, train loss = 0.34209439158439636, train acc = 0.8999999761581421, time = 0.7191746234893799\n",
      "Training at step=62, batch=360, train loss = 0.47049397230148315, train acc = 0.8500000238418579, time = 0.7044475078582764\n",
      "Training at step=62, batch=480, train loss = 0.41439324617385864, train acc = 0.8700000047683716, time = 0.7062389850616455\n",
      "Testing at step=62, batch=0, test loss = 0.4491232931613922, test acc = 0.8600000143051147, time = 0.4207606315612793\n",
      "Testing at step=62, batch=20, test loss = 0.2878783941268921, test acc = 0.8999999761581421, time = 0.4173710346221924\n",
      "Testing at step=62, batch=40, test loss = 0.47034117579460144, test acc = 0.8199999928474426, time = 0.4172224998474121\n",
      "Testing at step=62, batch=60, test loss = 0.5369399189949036, test acc = 0.8299999833106995, time = 0.4221632480621338\n",
      "Testing at step=62, batch=80, test loss = 0.6579712629318237, test acc = 0.7900000214576721, time = 0.4178469181060791\n",
      "Step 62 finished in 456.78113532066345, Train loss = 0.41514034022887547, Test loss = 0.4647670415043831; Train Acc = 0.8549833315610885, Test Acc = 0.8407000005245209\n",
      "Training at step=63, batch=0, train loss = 0.33247435092926025, train acc = 0.8399999737739563, time = 0.7199170589447021\n",
      "Training at step=63, batch=120, train loss = 0.2564561665058136, train acc = 0.9200000166893005, time = 0.7043530941009521\n",
      "Training at step=63, batch=240, train loss = 0.36495471000671387, train acc = 0.8899999856948853, time = 0.7014224529266357\n",
      "Training at step=63, batch=360, train loss = 0.40188702940940857, train acc = 0.8600000143051147, time = 0.7035648822784424\n",
      "Training at step=63, batch=480, train loss = 0.5240407586097717, train acc = 0.8399999737739563, time = 0.7055678367614746\n",
      "Testing at step=63, batch=0, test loss = 0.4278065860271454, test acc = 0.8500000238418579, time = 0.35265445709228516\n",
      "Testing at step=63, batch=20, test loss = 0.6215391755104065, test acc = 0.7900000214576721, time = 0.417496919631958\n",
      "Testing at step=63, batch=40, test loss = 0.5757482647895813, test acc = 0.7699999809265137, time = 0.4173552989959717\n",
      "Testing at step=63, batch=60, test loss = 0.4760458469390869, test acc = 0.8399999737739563, time = 0.4167361259460449\n",
      "Testing at step=63, batch=80, test loss = 0.3687545359134674, test acc = 0.8799999952316284, time = 0.41814470291137695\n",
      "Step 63 finished in 456.36812472343445, Train loss = 0.4137588087717692, Test loss = 0.4502100417017937; Train Acc = 0.8549166651566823, Test Acc = 0.8418000000715256\n",
      "Training at step=64, batch=0, train loss = 0.40107211470603943, train acc = 0.8600000143051147, time = 0.7116413116455078\n",
      "Training at step=64, batch=120, train loss = 0.3531159460544586, train acc = 0.8899999856948853, time = 0.7047529220581055\n",
      "Training at step=64, batch=240, train loss = 0.3220336437225342, train acc = 0.8899999856948853, time = 0.704432487487793\n",
      "Training at step=64, batch=360, train loss = 0.3100661039352417, train acc = 0.8899999856948853, time = 0.7065343856811523\n",
      "Training at step=64, batch=480, train loss = 0.39203375577926636, train acc = 0.8299999833106995, time = 0.7007992267608643\n",
      "Testing at step=64, batch=0, test loss = 0.7153317332267761, test acc = 0.75, time = 0.22155523300170898\n",
      "Testing at step=64, batch=20, test loss = 0.414124995470047, test acc = 0.8500000238418579, time = 0.4177558422088623\n",
      "Testing at step=64, batch=40, test loss = 0.6269780993461609, test acc = 0.8299999833106995, time = 0.41898083686828613\n",
      "Testing at step=64, batch=60, test loss = 0.5071141719818115, test acc = 0.8500000238418579, time = 0.4175577163696289\n",
      "Testing at step=64, batch=80, test loss = 0.5441874265670776, test acc = 0.8100000023841858, time = 0.4172537326812744\n",
      "Step 64 finished in 456.116149187088, Train loss = 0.41366427058974903, Test loss = 0.46732057094573975; Train Acc = 0.8549666675925255, Test Acc = 0.8380000001192093\n",
      "Training at step=65, batch=0, train loss = 0.34464001655578613, train acc = 0.8799999952316284, time = 0.721771240234375\n",
      "Training at step=65, batch=120, train loss = 0.5787537097930908, train acc = 0.7900000214576721, time = 0.7066328525543213\n",
      "Training at step=65, batch=240, train loss = 0.37926310300827026, train acc = 0.8700000047683716, time = 0.7056686878204346\n",
      "Training at step=65, batch=360, train loss = 0.407001793384552, train acc = 0.8799999952316284, time = 0.7038860321044922\n",
      "Training at step=65, batch=480, train loss = 0.2857098877429962, train acc = 0.8899999856948853, time = 0.7327516078948975\n",
      "Testing at step=65, batch=0, test loss = 0.3798607885837555, test acc = 0.8600000143051147, time = 0.24923276901245117\n",
      "Testing at step=65, batch=20, test loss = 0.341761976480484, test acc = 0.8799999952316284, time = 0.1408238410949707\n",
      "Testing at step=65, batch=40, test loss = 0.47288963198661804, test acc = 0.8299999833106995, time = 0.14125299453735352\n",
      "Testing at step=65, batch=60, test loss = 0.48832398653030396, test acc = 0.8199999928474426, time = 0.14756464958190918\n",
      "Testing at step=65, batch=80, test loss = 0.44216713309288025, test acc = 0.8299999833106995, time = 0.14862680435180664\n",
      "Step 65 finished in 460.270503282547, Train loss = 0.4127993231763442, Test loss = 0.4535843297839165; Train Acc = 0.8560166664918264, Test Acc = 0.8431999969482422\n",
      "Training at step=66, batch=0, train loss = 0.5295382738113403, train acc = 0.8600000143051147, time = 0.7123978137969971\n",
      "Training at step=66, batch=120, train loss = 0.3890804052352905, train acc = 0.8500000238418579, time = 0.7332441806793213\n",
      "Training at step=66, batch=240, train loss = 0.378996878862381, train acc = 0.8399999737739563, time = 0.6920175552368164\n",
      "Training at step=66, batch=360, train loss = 0.3615051209926605, train acc = 0.8399999737739563, time = 0.7016215324401855\n",
      "Training at step=66, batch=480, train loss = 0.5591135025024414, train acc = 0.8199999928474426, time = 0.7060701847076416\n",
      "Testing at step=66, batch=0, test loss = 0.5762091279029846, test acc = 0.8500000238418579, time = 0.42311930656433105\n",
      "Testing at step=66, batch=20, test loss = 0.4476284086704254, test acc = 0.8399999737739563, time = 0.4028017520904541\n",
      "Testing at step=66, batch=40, test loss = 0.38087284564971924, test acc = 0.8799999952316284, time = 0.41732120513916016\n",
      "Testing at step=66, batch=60, test loss = 0.5856673121452332, test acc = 0.800000011920929, time = 0.4181201457977295\n",
      "Testing at step=66, batch=80, test loss = 0.5238083600997925, test acc = 0.8299999833106995, time = 0.41695666313171387\n",
      "Step 66 finished in 461.7644808292389, Train loss = 0.41236506193876266, Test loss = 0.4649445174634457; Train Acc = 0.8551166650652885, Test Acc = 0.8345999979972839\n",
      "Training at step=67, batch=0, train loss = 0.37546077370643616, train acc = 0.8899999856948853, time = 0.7169296741485596\n",
      "Training at step=67, batch=120, train loss = 0.36965781450271606, train acc = 0.8500000238418579, time = 0.7064533233642578\n",
      "Training at step=67, batch=240, train loss = 0.4721923768520355, train acc = 0.8500000238418579, time = 0.7029554843902588\n",
      "Training at step=67, batch=360, train loss = 0.36207106709480286, train acc = 0.8700000047683716, time = 0.7062101364135742\n",
      "Training at step=67, batch=480, train loss = 0.46152567863464355, train acc = 0.8899999856948853, time = 0.7046303749084473\n",
      "Testing at step=67, batch=0, test loss = 0.49832919239997864, test acc = 0.8399999737739563, time = 0.41970157623291016\n",
      "Testing at step=67, batch=20, test loss = 0.49411213397979736, test acc = 0.8399999737739563, time = 0.418109655380249\n",
      "Testing at step=67, batch=40, test loss = 0.3269790709018707, test acc = 0.8999999761581421, time = 0.4180927276611328\n",
      "Testing at step=67, batch=60, test loss = 0.5473687648773193, test acc = 0.8199999928474426, time = 0.41795969009399414\n",
      "Testing at step=67, batch=80, test loss = 0.4543344974517822, test acc = 0.8100000023841858, time = 0.13911700248718262\n",
      "Step 67 finished in 456.8057906627655, Train loss = 0.4117485935489337, Test loss = 0.45870948433876035; Train Acc = 0.8566500008106231, Test Acc = 0.8398999989032745\n",
      "Training at step=68, batch=0, train loss = 0.40013259649276733, train acc = 0.8600000143051147, time = 0.7196879386901855\n",
      "Training at step=68, batch=120, train loss = 0.36920061707496643, train acc = 0.8999999761581421, time = 0.7040190696716309\n",
      "Training at step=68, batch=240, train loss = 0.35286521911621094, train acc = 0.8500000238418579, time = 0.7054433822631836\n",
      "Training at step=68, batch=360, train loss = 0.37673959136009216, train acc = 0.8799999952316284, time = 0.7057392597198486\n",
      "Training at step=68, batch=480, train loss = 0.43235236406326294, train acc = 0.8399999737739563, time = 0.705955982208252\n",
      "Testing at step=68, batch=0, test loss = 0.20599719882011414, test acc = 0.9399999976158142, time = 0.2097184658050537\n",
      "Testing at step=68, batch=20, test loss = 0.43272921442985535, test acc = 0.8299999833106995, time = 0.1384291648864746\n",
      "Testing at step=68, batch=40, test loss = 0.32823070883750916, test acc = 0.8700000047683716, time = 0.13740134239196777\n",
      "Testing at step=68, batch=60, test loss = 0.4700281023979187, test acc = 0.8399999737739563, time = 0.14024066925048828\n",
      "Testing at step=68, batch=80, test loss = 0.48350754380226135, test acc = 0.8399999737739563, time = 0.13968777656555176\n",
      "Step 68 finished in 456.3023717403412, Train loss = 0.41105815544724467, Test loss = 0.4529582780599594; Train Acc = 0.8562666659553846, Test Acc = 0.8379999971389771\n",
      "Training at step=69, batch=0, train loss = 0.3015602231025696, train acc = 0.8899999856948853, time = 0.7141509056091309\n",
      "Training at step=69, batch=120, train loss = 0.38163793087005615, train acc = 0.8500000238418579, time = 0.7044031620025635\n",
      "Training at step=69, batch=240, train loss = 0.4576885998249054, train acc = 0.8100000023841858, time = 0.7057108879089355\n",
      "Training at step=69, batch=360, train loss = 0.46418869495391846, train acc = 0.8399999737739563, time = 0.7076201438903809\n",
      "Training at step=69, batch=480, train loss = 0.7412933111190796, train acc = 0.8199999928474426, time = 0.7049858570098877\n",
      "Testing at step=69, batch=0, test loss = 0.3350466191768646, test acc = 0.8999999761581421, time = 0.42487144470214844\n",
      "Testing at step=69, batch=20, test loss = 0.48097702860832214, test acc = 0.8299999833106995, time = 0.4173424243927002\n",
      "Testing at step=69, batch=40, test loss = 0.6518434882164001, test acc = 0.7900000214576721, time = 0.41742920875549316\n",
      "Testing at step=69, batch=60, test loss = 0.4211285412311554, test acc = 0.8700000047683716, time = 0.4180576801300049\n",
      "Testing at step=69, batch=80, test loss = 0.41494882106781006, test acc = 0.8700000047683716, time = 0.4171268939971924\n",
      "Step 69 finished in 456.11131739616394, Train loss = 0.41267337039113045, Test loss = 0.4525375905632973; Train Acc = 0.8553833320736886, Test Acc = 0.8401999980211258\n",
      "Training at step=70, batch=0, train loss = 0.4816006124019623, train acc = 0.800000011920929, time = 0.7270336151123047\n",
      "Training at step=70, batch=120, train loss = 0.4055997431278229, train acc = 0.8600000143051147, time = 0.6990423202514648\n",
      "Training at step=70, batch=240, train loss = 0.3783596158027649, train acc = 0.8500000238418579, time = 0.705303430557251\n",
      "Training at step=70, batch=360, train loss = 0.42044055461883545, train acc = 0.8899999856948853, time = 0.7060036659240723\n",
      "Training at step=70, batch=480, train loss = 0.43003979325294495, train acc = 0.8199999928474426, time = 0.705254316329956\n",
      "Testing at step=70, batch=0, test loss = 0.4284754693508148, test acc = 0.8600000143051147, time = 0.2172088623046875\n",
      "Testing at step=70, batch=20, test loss = 0.3928173780441284, test acc = 0.8299999833106995, time = 0.1493523120880127\n",
      "Testing at step=70, batch=40, test loss = 0.42585116624832153, test acc = 0.8700000047683716, time = 0.14206528663635254\n",
      "Testing at step=70, batch=60, test loss = 0.6507149338722229, test acc = 0.800000011920929, time = 0.1431877613067627\n",
      "Testing at step=70, batch=80, test loss = 0.3683113753795624, test acc = 0.8799999952316284, time = 0.13546442985534668\n",
      "Step 70 finished in 456.56749153137207, Train loss = 0.4108141760528088, Test loss = 0.4546611312031746; Train Acc = 0.8563666661580404, Test Acc = 0.8409999990463257\n",
      "Training at step=71, batch=0, train loss = 0.4774530827999115, train acc = 0.8399999737739563, time = 0.7016432285308838\n",
      "Training at step=71, batch=120, train loss = 0.33418768644332886, train acc = 0.8799999952316284, time = 0.7040402889251709\n",
      "Training at step=71, batch=240, train loss = 0.36642104387283325, train acc = 0.8799999952316284, time = 0.7155666351318359\n",
      "Training at step=71, batch=360, train loss = 0.4026555120944977, train acc = 0.8600000143051147, time = 0.7094757556915283\n",
      "Training at step=71, batch=480, train loss = 0.3796836733818054, train acc = 0.8299999833106995, time = 0.7033371925354004\n",
      "Testing at step=71, batch=0, test loss = 0.366255521774292, test acc = 0.8600000143051147, time = 0.2079629898071289\n",
      "Testing at step=71, batch=20, test loss = 0.4084964096546173, test acc = 0.8299999833106995, time = 0.14378905296325684\n",
      "Testing at step=71, batch=40, test loss = 0.6539677381515503, test acc = 0.7900000214576721, time = 0.14193367958068848\n",
      "Testing at step=71, batch=60, test loss = 0.38175421953201294, test acc = 0.8899999856948853, time = 0.1417255401611328\n",
      "Testing at step=71, batch=80, test loss = 0.3651403486728668, test acc = 0.8899999856948853, time = 0.1441516876220703\n",
      "Step 71 finished in 455.6853530406952, Train loss = 0.4114833326389392, Test loss = 0.4551356315612793; Train Acc = 0.8562166655063629, Test Acc = 0.8398000001907349\n",
      "Training at step=72, batch=0, train loss = 0.41907092928886414, train acc = 0.8600000143051147, time = 0.7307186126708984\n",
      "Training at step=72, batch=120, train loss = 0.6763365268707275, train acc = 0.7900000214576721, time = 0.7054662704467773\n",
      "Training at step=72, batch=240, train loss = 0.40891003608703613, train acc = 0.8899999856948853, time = 0.7034420967102051\n",
      "Training at step=72, batch=360, train loss = 0.3404219448566437, train acc = 0.8899999856948853, time = 0.7039012908935547\n",
      "Training at step=72, batch=480, train loss = 0.3138323724269867, train acc = 0.8899999856948853, time = 0.7050867080688477\n",
      "Testing at step=72, batch=0, test loss = 0.5705214142799377, test acc = 0.7900000214576721, time = 0.2086658477783203\n",
      "Testing at step=72, batch=20, test loss = 0.4868473410606384, test acc = 0.8399999737739563, time = 0.1427309513092041\n",
      "Testing at step=72, batch=40, test loss = 0.31380537152290344, test acc = 0.8799999952316284, time = 0.1449439525604248\n",
      "Testing at step=72, batch=60, test loss = 0.38294100761413574, test acc = 0.8600000143051147, time = 0.14890003204345703\n",
      "Testing at step=72, batch=80, test loss = 0.5137670040130615, test acc = 0.8100000023841858, time = 0.14803266525268555\n",
      "Step 72 finished in 457.5171911716461, Train loss = 0.4105360484868288, Test loss = 0.44620468497276305; Train Acc = 0.8566833317279816, Test Acc = 0.8433999967575073\n",
      "Training at step=73, batch=0, train loss = 0.4112168252468109, train acc = 0.8600000143051147, time = 0.7063999176025391\n",
      "Training at step=73, batch=120, train loss = 0.4812562167644501, train acc = 0.8199999928474426, time = 0.7055037021636963\n",
      "Training at step=73, batch=240, train loss = 0.4248286783695221, train acc = 0.8799999952316284, time = 0.7033498287200928\n",
      "Training at step=73, batch=360, train loss = 0.4310143291950226, train acc = 0.8199999928474426, time = 0.7042598724365234\n",
      "Training at step=73, batch=480, train loss = 0.6248842477798462, train acc = 0.7799999713897705, time = 0.7030503749847412\n",
      "Testing at step=73, batch=0, test loss = 0.3649936318397522, test acc = 0.8399999737739563, time = 0.42598772048950195\n",
      "Testing at step=73, batch=20, test loss = 0.3990020751953125, test acc = 0.8399999737739563, time = 0.4176352024078369\n",
      "Testing at step=73, batch=40, test loss = 0.5544935464859009, test acc = 0.8299999833106995, time = 0.4180161952972412\n",
      "Testing at step=73, batch=60, test loss = 0.529379665851593, test acc = 0.8399999737739563, time = 0.4179699420928955\n",
      "Testing at step=73, batch=80, test loss = 0.4528864026069641, test acc = 0.800000011920929, time = 0.41785740852355957\n",
      "Step 73 finished in 456.70536828041077, Train loss = 0.41055410693089167, Test loss = 0.46468299329280854; Train Acc = 0.8569166645407676, Test Acc = 0.8357999986410141\n",
      "Training at step=74, batch=0, train loss = 0.377014696598053, train acc = 0.8500000238418579, time = 0.7101030349731445\n",
      "Training at step=74, batch=120, train loss = 0.4491160213947296, train acc = 0.8700000047683716, time = 0.7072958946228027\n",
      "Training at step=74, batch=240, train loss = 0.334479421377182, train acc = 0.9200000166893005, time = 0.7414736747741699\n",
      "Training at step=74, batch=360, train loss = 0.4024103879928589, train acc = 0.8299999833106995, time = 0.7048778533935547\n",
      "Training at step=74, batch=480, train loss = 0.41555899381637573, train acc = 0.800000011920929, time = 0.7050895690917969\n",
      "Testing at step=74, batch=0, test loss = 0.4121435284614563, test acc = 0.8500000238418579, time = 0.23669219017028809\n",
      "Testing at step=74, batch=20, test loss = 0.5525560975074768, test acc = 0.7900000214576721, time = 0.14766669273376465\n",
      "Testing at step=74, batch=40, test loss = 0.5155565142631531, test acc = 0.8299999833106995, time = 0.14659738540649414\n",
      "Testing at step=74, batch=60, test loss = 0.5028343200683594, test acc = 0.8199999928474426, time = 0.1423172950744629\n",
      "Testing at step=74, batch=80, test loss = 0.5163321495056152, test acc = 0.800000011920929, time = 0.14705133438110352\n",
      "Step 74 finished in 456.39384269714355, Train loss = 0.4104496337473392, Test loss = 0.4479941691458225; Train Acc = 0.8565499997138977, Test Acc = 0.8443999981880188\n",
      "Training at step=75, batch=0, train loss = 0.376731276512146, train acc = 0.8500000238418579, time = 0.7168123722076416\n",
      "Training at step=75, batch=120, train loss = 0.31117045879364014, train acc = 0.8700000047683716, time = 0.7065331935882568\n",
      "Training at step=75, batch=240, train loss = 0.5162559747695923, train acc = 0.8600000143051147, time = 0.7050652503967285\n",
      "Training at step=75, batch=360, train loss = 0.5163322687149048, train acc = 0.8399999737739563, time = 0.7045366764068604\n",
      "Training at step=75, batch=480, train loss = 0.5122724175453186, train acc = 0.7900000214576721, time = 0.7048864364624023\n",
      "Testing at step=75, batch=0, test loss = 0.5395433306694031, test acc = 0.8100000023841858, time = 0.21903777122497559\n",
      "Testing at step=75, batch=20, test loss = 0.5811939835548401, test acc = 0.800000011920929, time = 0.4161851406097412\n",
      "Testing at step=75, batch=40, test loss = 0.4865652024745941, test acc = 0.8100000023841858, time = 0.4181194305419922\n",
      "Testing at step=75, batch=60, test loss = 0.3720378875732422, test acc = 0.8399999737739563, time = 0.41672515869140625\n",
      "Testing at step=75, batch=80, test loss = 0.40771493315696716, test acc = 0.8199999928474426, time = 0.41744446754455566\n",
      "Step 75 finished in 454.7564790248871, Train loss = 0.4093212833752235, Test loss = 0.4553976121544838; Train Acc = 0.855616665482521, Test Acc = 0.8416000020503998\n",
      "Training at step=76, batch=0, train loss = 0.5648730397224426, train acc = 0.7400000095367432, time = 0.7214601039886475\n",
      "Training at step=76, batch=120, train loss = 0.30933961272239685, train acc = 0.8799999952316284, time = 0.7041099071502686\n",
      "Training at step=76, batch=240, train loss = 0.3686276972293854, train acc = 0.8399999737739563, time = 0.7051019668579102\n",
      "Training at step=76, batch=360, train loss = 0.4051174521446228, train acc = 0.8399999737739563, time = 0.7054405212402344\n",
      "Training at step=76, batch=480, train loss = 0.37355339527130127, train acc = 0.8999999761581421, time = 0.7055613994598389\n",
      "Testing at step=76, batch=0, test loss = 0.4307962656021118, test acc = 0.8100000023841858, time = 0.2201237678527832\n",
      "Testing at step=76, batch=20, test loss = 0.3026719093322754, test acc = 0.8700000047683716, time = 0.41895627975463867\n",
      "Testing at step=76, batch=40, test loss = 0.3867161273956299, test acc = 0.9200000166893005, time = 0.41471076011657715\n",
      "Testing at step=76, batch=60, test loss = 0.575850248336792, test acc = 0.7799999713897705, time = 0.4167473316192627\n",
      "Testing at step=76, batch=80, test loss = 0.3931131362915039, test acc = 0.8700000047683716, time = 0.4172513484954834\n",
      "Step 76 finished in 456.2750005722046, Train loss = 0.4067164907852809, Test loss = 0.4476360176503658; Train Acc = 0.8581666661302249, Test Acc = 0.8458999979496002\n",
      "Training at step=77, batch=0, train loss = 0.31846463680267334, train acc = 0.9100000262260437, time = 0.7200877666473389\n",
      "Training at step=77, batch=120, train loss = 0.4055976867675781, train acc = 0.8500000238418579, time = 0.7055337429046631\n",
      "Training at step=77, batch=240, train loss = 0.32211658358573914, train acc = 0.8899999856948853, time = 0.7037253379821777\n",
      "Training at step=77, batch=360, train loss = 0.3489713966846466, train acc = 0.8899999856948853, time = 0.7030117511749268\n",
      "Training at step=77, batch=480, train loss = 0.42009103298187256, train acc = 0.8199999928474426, time = 0.7054636478424072\n",
      "Testing at step=77, batch=0, test loss = 0.34628674387931824, test acc = 0.8999999761581421, time = 0.21690082550048828\n",
      "Testing at step=77, batch=20, test loss = 0.5567502975463867, test acc = 0.8299999833106995, time = 0.13850617408752441\n",
      "Testing at step=77, batch=40, test loss = 0.49723705649375916, test acc = 0.8100000023841858, time = 0.1431434154510498\n",
      "Testing at step=77, batch=60, test loss = 0.5009078979492188, test acc = 0.8700000047683716, time = 0.1433107852935791\n",
      "Testing at step=77, batch=80, test loss = 0.4803335666656494, test acc = 0.8100000023841858, time = 0.13898873329162598\n",
      "Step 77 finished in 455.931405544281, Train loss = 0.40534105504552526, Test loss = 0.4620223268866539; Train Acc = 0.8586166656017303, Test Acc = 0.8368000000715256\n",
      "Training at step=78, batch=0, train loss = 0.4448864459991455, train acc = 0.8399999737739563, time = 0.7240450382232666\n",
      "Training at step=78, batch=120, train loss = 0.31355372071266174, train acc = 0.8600000143051147, time = 0.7039854526519775\n",
      "Training at step=78, batch=240, train loss = 0.37657836079597473, train acc = 0.8600000143051147, time = 0.7041425704956055\n",
      "Training at step=78, batch=360, train loss = 0.43402397632598877, train acc = 0.8899999856948853, time = 0.7054016590118408\n",
      "Training at step=78, batch=480, train loss = 0.3597491383552551, train acc = 0.8899999856948853, time = 0.704599142074585\n",
      "Testing at step=78, batch=0, test loss = 0.3782123327255249, test acc = 0.8500000238418579, time = 0.21917295455932617\n",
      "Testing at step=78, batch=20, test loss = 0.6127023100852966, test acc = 0.7699999809265137, time = 0.4177060127258301\n",
      "Testing at step=78, batch=40, test loss = 0.3777778744697571, test acc = 0.8899999856948853, time = 0.418001651763916\n",
      "Testing at step=78, batch=60, test loss = 0.4498721957206726, test acc = 0.8199999928474426, time = 0.41776490211486816\n",
      "Testing at step=78, batch=80, test loss = 0.5561808347702026, test acc = 0.8199999928474426, time = 0.41778087615966797\n",
      "Step 78 finished in 458.19422459602356, Train loss = 0.40850780146817367, Test loss = 0.45556107103824617; Train Acc = 0.8579166660706202, Test Acc = 0.8393999975919724\n",
      "Training at step=79, batch=0, train loss = 0.2827380299568176, train acc = 0.8999999761581421, time = 0.7228548526763916\n",
      "Training at step=79, batch=120, train loss = 0.4777539074420929, train acc = 0.8600000143051147, time = 0.7045965194702148\n",
      "Training at step=79, batch=240, train loss = 0.3909273147583008, train acc = 0.8500000238418579, time = 0.7034015655517578\n",
      "Training at step=79, batch=360, train loss = 0.40469929575920105, train acc = 0.8500000238418579, time = 0.7049667835235596\n",
      "Training at step=79, batch=480, train loss = 0.23186467587947845, train acc = 0.949999988079071, time = 0.705322265625\n",
      "Testing at step=79, batch=0, test loss = 0.4474983215332031, test acc = 0.8600000143051147, time = 0.2053985595703125\n",
      "Testing at step=79, batch=20, test loss = 0.5150184631347656, test acc = 0.8100000023841858, time = 0.14355683326721191\n",
      "Testing at step=79, batch=40, test loss = 0.31739577651023865, test acc = 0.8899999856948853, time = 0.13734197616577148\n",
      "Testing at step=79, batch=60, test loss = 0.3746418058872223, test acc = 0.8500000238418579, time = 0.13859009742736816\n",
      "Testing at step=79, batch=80, test loss = 0.344614177942276, test acc = 0.8999999761581421, time = 0.1454484462738037\n",
      "Step 79 finished in 456.96900963783264, Train loss = 0.4069939703742663, Test loss = 0.45023645788431166; Train Acc = 0.8571499998370806, Test Acc = 0.8417999988794327\n",
      "Training at step=80, batch=0, train loss = 0.423307329416275, train acc = 0.8700000047683716, time = 0.7167754173278809\n",
      "Training at step=80, batch=120, train loss = 0.6647084951400757, train acc = 0.7799999713897705, time = 0.7052600383758545\n",
      "Training at step=80, batch=240, train loss = 0.527485728263855, train acc = 0.8399999737739563, time = 0.7066729068756104\n",
      "Training at step=80, batch=360, train loss = 0.42769181728363037, train acc = 0.800000011920929, time = 0.7056822776794434\n",
      "Training at step=80, batch=480, train loss = 0.2788441479206085, train acc = 0.8999999761581421, time = 0.7034595012664795\n",
      "Testing at step=80, batch=0, test loss = 0.39627334475517273, test acc = 0.8799999952316284, time = 0.21158623695373535\n",
      "Testing at step=80, batch=20, test loss = 0.4321432113647461, test acc = 0.8500000238418579, time = 0.4173281192779541\n",
      "Testing at step=80, batch=40, test loss = 0.4427988827228546, test acc = 0.8799999952316284, time = 0.41796445846557617\n",
      "Testing at step=80, batch=60, test loss = 0.4620722532272339, test acc = 0.8100000023841858, time = 0.4191610813140869\n",
      "Testing at step=80, batch=80, test loss = 0.6945670247077942, test acc = 0.7400000095367432, time = 0.41731858253479004\n",
      "Step 80 finished in 455.47961258888245, Train loss = 0.4047831722597281, Test loss = 0.46195772469043733; Train Acc = 0.8591333323717117, Test Acc = 0.8415000009536743\n",
      "Training at step=81, batch=0, train loss = 0.5264338850975037, train acc = 0.8100000023841858, time = 0.7173895835876465\n",
      "Training at step=81, batch=120, train loss = 0.5449997782707214, train acc = 0.8100000023841858, time = 0.7089023590087891\n",
      "Training at step=81, batch=240, train loss = 0.35915932059288025, train acc = 0.8999999761581421, time = 0.7074480056762695\n",
      "Training at step=81, batch=360, train loss = 0.3969595730304718, train acc = 0.8299999833106995, time = 0.7059004306793213\n",
      "Training at step=81, batch=480, train loss = 0.4813616871833801, train acc = 0.8100000023841858, time = 0.7050433158874512\n",
      "Testing at step=81, batch=0, test loss = 0.48331591486930847, test acc = 0.8299999833106995, time = 0.21774911880493164\n",
      "Testing at step=81, batch=20, test loss = 0.5916534066200256, test acc = 0.7900000214576721, time = 0.14330458641052246\n",
      "Testing at step=81, batch=40, test loss = 0.3171129822731018, test acc = 0.8899999856948853, time = 0.14464235305786133\n",
      "Testing at step=81, batch=60, test loss = 0.6687918901443481, test acc = 0.75, time = 0.14374971389770508\n",
      "Testing at step=81, batch=80, test loss = 0.5109312534332275, test acc = 0.800000011920929, time = 0.14139747619628906\n",
      "Step 81 finished in 456.46737599372864, Train loss = 0.40453574910759926, Test loss = 0.4629940861463547; Train Acc = 0.8586499993006388, Test Acc = 0.8355000001192093\n",
      "Training at step=82, batch=0, train loss = 0.37098854780197144, train acc = 0.8700000047683716, time = 0.7266793251037598\n",
      "Training at step=82, batch=120, train loss = 0.3606165945529938, train acc = 0.8999999761581421, time = 0.6409311294555664\n",
      "Training at step=82, batch=240, train loss = 0.3590048849582672, train acc = 0.8399999737739563, time = 0.7055337429046631\n",
      "Training at step=82, batch=360, train loss = 0.4383726119995117, train acc = 0.8199999928474426, time = 0.7015378475189209\n",
      "Training at step=82, batch=480, train loss = 0.5113574862480164, train acc = 0.8399999737739563, time = 0.7057468891143799\n",
      "Testing at step=82, batch=0, test loss = 0.426748663187027, test acc = 0.8700000047683716, time = 0.2298145294189453\n",
      "Testing at step=82, batch=20, test loss = 0.3422723412513733, test acc = 0.8899999856948853, time = 0.4173543453216553\n",
      "Testing at step=82, batch=40, test loss = 0.31355488300323486, test acc = 0.8899999856948853, time = 0.4171273708343506\n",
      "Testing at step=82, batch=60, test loss = 0.34912925958633423, test acc = 0.8500000238418579, time = 0.4173092842102051\n",
      "Testing at step=82, batch=80, test loss = 0.559587836265564, test acc = 0.8100000023841858, time = 0.41753721237182617\n",
      "Step 82 finished in 457.08044505119324, Train loss = 0.40486581208805245, Test loss = 0.4561864909529686; Train Acc = 0.8581499991814295, Test Acc = 0.8398999983072281\n",
      "Training at step=83, batch=0, train loss = 0.30100294947624207, train acc = 0.9200000166893005, time = 0.7184867858886719\n",
      "Training at step=83, batch=120, train loss = 0.48972877860069275, train acc = 0.8399999737739563, time = 0.7040090560913086\n",
      "Training at step=83, batch=240, train loss = 0.21989500522613525, train acc = 0.949999988079071, time = 0.7040038108825684\n",
      "Training at step=83, batch=360, train loss = 0.3159085512161255, train acc = 0.8899999856948853, time = 0.7015960216522217\n",
      "Training at step=83, batch=480, train loss = 0.34353217482566833, train acc = 0.8600000143051147, time = 0.7054243087768555\n",
      "Testing at step=83, batch=0, test loss = 0.46252062916755676, test acc = 0.8600000143051147, time = 0.20295476913452148\n",
      "Testing at step=83, batch=20, test loss = 0.45032748579978943, test acc = 0.8299999833106995, time = 0.4183156490325928\n",
      "Testing at step=83, batch=40, test loss = 0.3624005615711212, test acc = 0.8799999952316284, time = 0.14018511772155762\n",
      "Testing at step=83, batch=60, test loss = 0.4827801585197449, test acc = 0.8600000143051147, time = 0.14232134819030762\n",
      "Testing at step=83, batch=80, test loss = 0.5413067936897278, test acc = 0.8199999928474426, time = 0.14638304710388184\n",
      "Step 83 finished in 456.84581208229065, Train loss = 0.40550997997323673, Test loss = 0.4574761036038399; Train Acc = 0.8585833344856898, Test Acc = 0.8429000008106232\n",
      "Training at step=84, batch=0, train loss = 0.3518610894680023, train acc = 0.8799999952316284, time = 0.7141222953796387\n",
      "Training at step=84, batch=120, train loss = 0.5260847806930542, train acc = 0.8500000238418579, time = 0.7051973342895508\n",
      "Training at step=84, batch=240, train loss = 0.3766542077064514, train acc = 0.8600000143051147, time = 0.7049531936645508\n",
      "Training at step=84, batch=360, train loss = 0.368871808052063, train acc = 0.8700000047683716, time = 0.7024471759796143\n",
      "Training at step=84, batch=480, train loss = 0.4197235107421875, train acc = 0.8899999856948853, time = 0.7044684886932373\n",
      "Testing at step=84, batch=0, test loss = 0.3658484220504761, test acc = 0.8500000238418579, time = 0.2136402130126953\n",
      "Testing at step=84, batch=20, test loss = 0.5782912969589233, test acc = 0.7699999809265137, time = 0.41753292083740234\n",
      "Testing at step=84, batch=40, test loss = 0.4884699583053589, test acc = 0.800000011920929, time = 0.4171111583709717\n",
      "Testing at step=84, batch=60, test loss = 0.33681708574295044, test acc = 0.8600000143051147, time = 0.4183382987976074\n",
      "Testing at step=84, batch=80, test loss = 0.29627302289009094, test acc = 0.8899999856948853, time = 0.41837239265441895\n",
      "Step 84 finished in 455.3649661540985, Train loss = 0.40192064409454664, Test loss = 0.4473357906937599; Train Acc = 0.8591500008106232, Test Acc = 0.8435999965667724\n",
      "Training at step=85, batch=0, train loss = 0.585939884185791, train acc = 0.8299999833106995, time = 0.7231900691986084\n",
      "Training at step=85, batch=120, train loss = 0.31148722767829895, train acc = 0.8799999952316284, time = 0.6978228092193604\n",
      "Training at step=85, batch=240, train loss = 0.26308196783065796, train acc = 0.9200000166893005, time = 0.7050957679748535\n",
      "Training at step=85, batch=360, train loss = 0.36902159452438354, train acc = 0.8799999952316284, time = 0.7059271335601807\n",
      "Training at step=85, batch=480, train loss = 0.44699332118034363, train acc = 0.7900000214576721, time = 0.7043921947479248\n",
      "Testing at step=85, batch=0, test loss = 0.44261524081230164, test acc = 0.8600000143051147, time = 0.20926213264465332\n",
      "Testing at step=85, batch=20, test loss = 0.301947683095932, test acc = 0.8700000047683716, time = 0.4178466796875\n",
      "Testing at step=85, batch=40, test loss = 0.5972697138786316, test acc = 0.8299999833106995, time = 0.4168994426727295\n",
      "Testing at step=85, batch=60, test loss = 0.4895287752151489, test acc = 0.7699999809265137, time = 0.4177212715148926\n",
      "Testing at step=85, batch=80, test loss = 0.5536222457885742, test acc = 0.800000011920929, time = 0.41729259490966797\n",
      "Step 85 finished in 457.82830834388733, Train loss = 0.40216087793310484, Test loss = 0.4464960990846157; Train Acc = 0.860433332324028, Test Acc = 0.8432999956607818\n",
      "Training at step=86, batch=0, train loss = 0.31286460161209106, train acc = 0.8899999856948853, time = 0.719702959060669\n",
      "Training at step=86, batch=120, train loss = 0.3437073230743408, train acc = 0.8799999952316284, time = 0.7055685520172119\n",
      "Training at step=86, batch=240, train loss = 0.4459308683872223, train acc = 0.8700000047683716, time = 0.7055521011352539\n",
      "Training at step=86, batch=360, train loss = 0.34586262702941895, train acc = 0.8500000238418579, time = 0.7219886779785156\n",
      "Training at step=86, batch=480, train loss = 0.3458680212497711, train acc = 0.8799999952316284, time = 0.7060046195983887\n",
      "Testing at step=86, batch=0, test loss = 0.45185795426368713, test acc = 0.8299999833106995, time = 0.21697568893432617\n",
      "Testing at step=86, batch=20, test loss = 0.4413454830646515, test acc = 0.7699999809265137, time = 0.41697239875793457\n",
      "Testing at step=86, batch=40, test loss = 0.41483741998672485, test acc = 0.8399999737739563, time = 0.41771364212036133\n",
      "Testing at step=86, batch=60, test loss = 0.4406236708164215, test acc = 0.8799999952316284, time = 0.41803932189941406\n",
      "Testing at step=86, batch=80, test loss = 0.5955378413200378, test acc = 0.8100000023841858, time = 0.4178471565246582\n",
      "Step 86 finished in 456.3165304660797, Train loss = 0.40551022338370485, Test loss = 0.4612126064300537; Train Acc = 0.8576333310206731, Test Acc = 0.8401999980211258\n",
      "Training at step=87, batch=0, train loss = 0.23479919135570526, train acc = 0.8899999856948853, time = 0.737647533416748\n",
      "Training at step=87, batch=120, train loss = 0.39075732231140137, train acc = 0.8999999761581421, time = 0.7037596702575684\n",
      "Training at step=87, batch=240, train loss = 0.19271202385425568, train acc = 0.9300000071525574, time = 0.7051331996917725\n",
      "Training at step=87, batch=360, train loss = 0.46284061670303345, train acc = 0.8299999833106995, time = 0.7098100185394287\n",
      "Training at step=87, batch=480, train loss = 0.5737110376358032, train acc = 0.7900000214576721, time = 0.7037947177886963\n",
      "Testing at step=87, batch=0, test loss = 0.35505688190460205, test acc = 0.8700000047683716, time = 0.21601176261901855\n",
      "Testing at step=87, batch=20, test loss = 0.4387069642543793, test acc = 0.8600000143051147, time = 0.14489459991455078\n",
      "Testing at step=87, batch=40, test loss = 0.4693560302257538, test acc = 0.8399999737739563, time = 0.14441728591918945\n",
      "Testing at step=87, batch=60, test loss = 0.5648678541183472, test acc = 0.800000011920929, time = 0.14378118515014648\n",
      "Testing at step=87, batch=80, test loss = 0.6639193892478943, test acc = 0.800000011920929, time = 0.14738202095031738\n",
      "Step 87 finished in 456.17537927627563, Train loss = 0.4031130577127139, Test loss = 0.45741962775588035; Train Acc = 0.8586000000437101, Test Acc = 0.8376999992132187\n",
      "Training at step=88, batch=0, train loss = 0.42829880118370056, train acc = 0.8299999833106995, time = 0.6965010166168213\n",
      "Training at step=88, batch=120, train loss = 0.3152727484703064, train acc = 0.8799999952316284, time = 0.7045176029205322\n",
      "Training at step=88, batch=240, train loss = 0.4625864326953888, train acc = 0.8299999833106995, time = 0.7053968906402588\n",
      "Training at step=88, batch=360, train loss = 0.44370177388191223, train acc = 0.8999999761581421, time = 0.7038099765777588\n",
      "Training at step=88, batch=480, train loss = 0.4370560050010681, train acc = 0.8899999856948853, time = 0.7078924179077148\n",
      "Testing at step=88, batch=0, test loss = 0.42604386806488037, test acc = 0.8600000143051147, time = 0.22311043739318848\n",
      "Testing at step=88, batch=20, test loss = 0.40039798617362976, test acc = 0.9200000166893005, time = 0.14172816276550293\n",
      "Testing at step=88, batch=40, test loss = 0.4826323688030243, test acc = 0.7900000214576721, time = 0.14296197891235352\n",
      "Testing at step=88, batch=60, test loss = 0.4879736602306366, test acc = 0.8299999833106995, time = 0.13662290573120117\n",
      "Testing at step=88, batch=80, test loss = 0.4654485583305359, test acc = 0.7799999713897705, time = 0.13600969314575195\n",
      "Step 88 finished in 455.2740135192871, Train loss = 0.4038478137801091, Test loss = 0.44921051293611525; Train Acc = 0.8591999997695287, Test Acc = 0.8419999951124191\n",
      "Training at step=89, batch=0, train loss = 0.3414011001586914, train acc = 0.8799999952316284, time = 0.7174632549285889\n",
      "Training at step=89, batch=120, train loss = 0.505074679851532, train acc = 0.8299999833106995, time = 0.7043614387512207\n",
      "Training at step=89, batch=240, train loss = 0.4437614381313324, train acc = 0.8100000023841858, time = 0.7038586139678955\n",
      "Training at step=89, batch=360, train loss = 0.4715149998664856, train acc = 0.8299999833106995, time = 0.7043638229370117\n",
      "Training at step=89, batch=480, train loss = 0.5163670182228088, train acc = 0.7900000214576721, time = 0.7041161060333252\n",
      "Testing at step=89, batch=0, test loss = 0.44790443778038025, test acc = 0.800000011920929, time = 0.2106480598449707\n",
      "Testing at step=89, batch=20, test loss = 0.40090247988700867, test acc = 0.8899999856948853, time = 0.41790127754211426\n",
      "Testing at step=89, batch=40, test loss = 0.5538102984428406, test acc = 0.8100000023841858, time = 0.41794514656066895\n",
      "Testing at step=89, batch=60, test loss = 0.3864195644855499, test acc = 0.8600000143051147, time = 0.42071533203125\n",
      "Testing at step=89, batch=80, test loss = 0.5298781991004944, test acc = 0.7799999713897705, time = 0.4171609878540039\n",
      "Step 89 finished in 455.3912932872772, Train loss = 0.40347081169486043, Test loss = 0.4518518334627151; Train Acc = 0.8591666660706202, Test Acc = 0.8412999993562699\n",
      "Training at step=90, batch=0, train loss = 0.37031471729278564, train acc = 0.8500000238418579, time = 0.715782880783081\n",
      "Training at step=90, batch=120, train loss = 0.480863481760025, train acc = 0.7900000214576721, time = 0.7038609981536865\n",
      "Training at step=90, batch=240, train loss = 0.4060131311416626, train acc = 0.8399999737739563, time = 0.7054688930511475\n",
      "Training at step=90, batch=360, train loss = 0.36841824650764465, train acc = 0.8199999928474426, time = 0.703948974609375\n",
      "Training at step=90, batch=480, train loss = 0.3899870216846466, train acc = 0.8399999737739563, time = 0.7051713466644287\n",
      "Testing at step=90, batch=0, test loss = 0.3456798493862152, test acc = 0.8700000047683716, time = 0.21721148490905762\n",
      "Testing at step=90, batch=20, test loss = 0.288364976644516, test acc = 0.8899999856948853, time = 0.13722610473632812\n",
      "Testing at step=90, batch=40, test loss = 0.39587271213531494, test acc = 0.8700000047683716, time = 0.13861417770385742\n",
      "Testing at step=90, batch=60, test loss = 0.424961656332016, test acc = 0.800000011920929, time = 0.13447213172912598\n",
      "Testing at step=90, batch=80, test loss = 0.44522780179977417, test acc = 0.8799999952316284, time = 0.13815999031066895\n",
      "Step 90 finished in 456.2098286151886, Train loss = 0.40135321420927844, Test loss = 0.4483211971819401; Train Acc = 0.8600833328564962, Test Acc = 0.8434999972581864\n",
      "Training at step=91, batch=0, train loss = 0.40528762340545654, train acc = 0.8399999737739563, time = 0.7113950252532959\n",
      "Training at step=91, batch=120, train loss = 0.4357213079929352, train acc = 0.8500000238418579, time = 0.6952872276306152\n",
      "Training at step=91, batch=240, train loss = 0.38884297013282776, train acc = 0.8600000143051147, time = 0.7040863037109375\n",
      "Training at step=91, batch=360, train loss = 0.45629799365997314, train acc = 0.8299999833106995, time = 0.7055370807647705\n",
      "Training at step=91, batch=480, train loss = 0.44831687211990356, train acc = 0.8299999833106995, time = 0.7052907943725586\n",
      "Testing at step=91, batch=0, test loss = 0.4485687017440796, test acc = 0.8500000238418579, time = 0.22074413299560547\n",
      "Testing at step=91, batch=20, test loss = 0.5031744837760925, test acc = 0.800000011920929, time = 0.1324625015258789\n",
      "Testing at step=91, batch=40, test loss = 0.4782024323940277, test acc = 0.8399999737739563, time = 0.14532256126403809\n",
      "Testing at step=91, batch=60, test loss = 0.3440149426460266, test acc = 0.8999999761581421, time = 0.14507007598876953\n",
      "Testing at step=91, batch=80, test loss = 0.5259413123130798, test acc = 0.8199999928474426, time = 0.14243221282958984\n",
      "Step 91 finished in 456.48640155792236, Train loss = 0.4011041677991549, Test loss = 0.4425097543001175; Train Acc = 0.8597833326458931, Test Acc = 0.8474999982118606\n",
      "Training at step=92, batch=0, train loss = 0.284879595041275, train acc = 0.8600000143051147, time = 0.7098910808563232\n",
      "Training at step=92, batch=120, train loss = 0.3590541183948517, train acc = 0.8700000047683716, time = 0.7039391994476318\n",
      "Training at step=92, batch=240, train loss = 0.41882047057151794, train acc = 0.8799999952316284, time = 0.705127477645874\n",
      "Training at step=92, batch=360, train loss = 0.4252760708332062, train acc = 0.8399999737739563, time = 0.7039096355438232\n",
      "Training at step=92, batch=480, train loss = 0.3051685392856598, train acc = 0.8600000143051147, time = 0.706007719039917\n",
      "Testing at step=92, batch=0, test loss = 0.433354914188385, test acc = 0.8299999833106995, time = 0.2066338062286377\n",
      "Testing at step=92, batch=20, test loss = 0.5279329419136047, test acc = 0.8600000143051147, time = 0.14159584045410156\n",
      "Testing at step=92, batch=40, test loss = 0.43157675862312317, test acc = 0.8500000238418579, time = 0.14392995834350586\n",
      "Testing at step=92, batch=60, test loss = 0.4339345693588257, test acc = 0.8700000047683716, time = 0.14390993118286133\n",
      "Testing at step=92, batch=80, test loss = 0.32023224234580994, test acc = 0.8799999952316284, time = 0.13647174835205078\n",
      "Step 92 finished in 456.1901876926422, Train loss = 0.400379533196489, Test loss = 0.45512345105409624; Train Acc = 0.861250000099341, Test Acc = 0.8434000027179718\n",
      "Training at step=93, batch=0, train loss = 0.516116201877594, train acc = 0.8100000023841858, time = 0.7077925205230713\n",
      "Training at step=93, batch=120, train loss = 0.41210511326789856, train acc = 0.8399999737739563, time = 0.7055368423461914\n",
      "Training at step=93, batch=240, train loss = 0.5107344388961792, train acc = 0.800000011920929, time = 0.7095193862915039\n",
      "Training at step=93, batch=360, train loss = 0.5998661518096924, train acc = 0.8100000023841858, time = 0.7049081325531006\n",
      "Training at step=93, batch=480, train loss = 0.4212377071380615, train acc = 0.8700000047683716, time = 0.7036614418029785\n",
      "Testing at step=93, batch=0, test loss = 0.37034690380096436, test acc = 0.8700000047683716, time = 0.2184751033782959\n",
      "Testing at step=93, batch=20, test loss = 0.7158380746841431, test acc = 0.7799999713897705, time = 0.13885831832885742\n",
      "Testing at step=93, batch=40, test loss = 0.5268049836158752, test acc = 0.8100000023841858, time = 0.14209222793579102\n",
      "Testing at step=93, batch=60, test loss = 0.2797514796257019, test acc = 0.9100000262260437, time = 0.14144086837768555\n",
      "Testing at step=93, batch=80, test loss = 0.546546995639801, test acc = 0.7799999713897705, time = 0.1392822265625\n",
      "Step 93 finished in 455.71030163764954, Train loss = 0.40030312264959017, Test loss = 0.44865405052900315; Train Acc = 0.8609166653951009, Test Acc = 0.8444999969005584\n",
      "Training at step=94, batch=0, train loss = 0.5416877269744873, train acc = 0.8600000143051147, time = 0.699315071105957\n",
      "Training at step=94, batch=120, train loss = 0.19122636318206787, train acc = 0.9399999976158142, time = 0.704758882522583\n",
      "Training at step=94, batch=240, train loss = 0.3007465898990631, train acc = 0.8999999761581421, time = 0.7033438682556152\n",
      "Training at step=94, batch=360, train loss = 0.37167415022850037, train acc = 0.8899999856948853, time = 0.7053368091583252\n",
      "Training at step=94, batch=480, train loss = 0.5422024726867676, train acc = 0.8299999833106995, time = 0.7045857906341553\n",
      "Testing at step=94, batch=0, test loss = 0.4736805260181427, test acc = 0.8799999952316284, time = 0.22182941436767578\n",
      "Testing at step=94, batch=20, test loss = 0.33597445487976074, test acc = 0.8999999761581421, time = 0.14247345924377441\n",
      "Testing at step=94, batch=40, test loss = 0.578900933265686, test acc = 0.75, time = 0.1477372646331787\n",
      "Testing at step=94, batch=60, test loss = 0.4173639714717865, test acc = 0.8899999856948853, time = 0.14761829376220703\n",
      "Testing at step=94, batch=80, test loss = 0.5139132738113403, test acc = 0.8199999928474426, time = 0.14068388938903809\n",
      "Step 94 finished in 456.15380215644836, Train loss = 0.3984538797785838, Test loss = 0.4439170196652412; Train Acc = 0.8610333328445753, Test Acc = 0.8431999987363815\n",
      "Training at step=95, batch=0, train loss = 0.3394426107406616, train acc = 0.8999999761581421, time = 0.7013752460479736\n",
      "Training at step=95, batch=120, train loss = 0.4262845516204834, train acc = 0.8199999928474426, time = 0.705547571182251\n",
      "Training at step=95, batch=240, train loss = 0.560936450958252, train acc = 0.7900000214576721, time = 0.7062797546386719\n",
      "Training at step=95, batch=360, train loss = 0.3875240087509155, train acc = 0.8799999952316284, time = 0.7040033340454102\n",
      "Training at step=95, batch=480, train loss = 0.39054861664772034, train acc = 0.8799999952316284, time = 0.7055873870849609\n",
      "Testing at step=95, batch=0, test loss = 0.6182115077972412, test acc = 0.7699999809265137, time = 0.22368240356445312\n",
      "Testing at step=95, batch=20, test loss = 0.31172698736190796, test acc = 0.8899999856948853, time = 0.14069533348083496\n",
      "Testing at step=95, batch=40, test loss = 0.346211701631546, test acc = 0.8700000047683716, time = 0.14317727088928223\n",
      "Testing at step=95, batch=60, test loss = 0.5699288845062256, test acc = 0.8299999833106995, time = 0.14269590377807617\n",
      "Testing at step=95, batch=80, test loss = 0.36703255772590637, test acc = 0.8799999952316284, time = 0.14609074592590332\n",
      "Step 95 finished in 455.96316623687744, Train loss = 0.40005150710542997, Test loss = 0.4522365614771843; Train Acc = 0.8599333331982295, Test Acc = 0.8437999963760376\n",
      "Training at step=96, batch=0, train loss = 0.4818389415740967, train acc = 0.8500000238418579, time = 0.7213423252105713\n",
      "Training at step=96, batch=120, train loss = 0.4321437180042267, train acc = 0.8500000238418579, time = 0.7321796417236328\n",
      "Training at step=96, batch=240, train loss = 0.3318652808666229, train acc = 0.8799999952316284, time = 0.7326633930206299\n",
      "Training at step=96, batch=360, train loss = 0.3535928428173065, train acc = 0.8600000143051147, time = 0.7174134254455566\n",
      "Training at step=96, batch=480, train loss = 0.5236465334892273, train acc = 0.8399999737739563, time = 0.7056665420532227\n",
      "Testing at step=96, batch=0, test loss = 0.3776244819164276, test acc = 0.9100000262260437, time = 0.20974016189575195\n",
      "Testing at step=96, batch=20, test loss = 0.39647120237350464, test acc = 0.8899999856948853, time = 0.14310932159423828\n",
      "Testing at step=96, batch=40, test loss = 0.5816124081611633, test acc = 0.800000011920929, time = 0.14727115631103516\n",
      "Testing at step=96, batch=60, test loss = 0.24677996337413788, test acc = 0.8899999856948853, time = 0.1318988800048828\n",
      "Testing at step=96, batch=80, test loss = 0.4386757016181946, test acc = 0.8500000238418579, time = 0.13567614555358887\n",
      "Step 96 finished in 462.88142919540405, Train loss = 0.39830091687540214, Test loss = 0.4459877546131611; Train Acc = 0.8608499987920125, Test Acc = 0.8443999987840652\n",
      "Training at step=97, batch=0, train loss = 0.19449804723262787, train acc = 0.949999988079071, time = 0.7060403823852539\n",
      "Training at step=97, batch=120, train loss = 0.40828973054885864, train acc = 0.8399999737739563, time = 0.7059669494628906\n",
      "Training at step=97, batch=240, train loss = 0.3268206715583801, train acc = 0.9100000262260437, time = 0.7022933959960938\n",
      "Training at step=97, batch=360, train loss = 0.46423280239105225, train acc = 0.8299999833106995, time = 0.7029991149902344\n",
      "Training at step=97, batch=480, train loss = 0.465539813041687, train acc = 0.8399999737739563, time = 0.7039241790771484\n",
      "Testing at step=97, batch=0, test loss = 0.47716477513313293, test acc = 0.800000011920929, time = 0.21728157997131348\n",
      "Testing at step=97, batch=20, test loss = 0.4370310604572296, test acc = 0.8399999737739563, time = 0.4174368381500244\n",
      "Testing at step=97, batch=40, test loss = 0.40423205494880676, test acc = 0.8600000143051147, time = 0.41765260696411133\n",
      "Testing at step=97, batch=60, test loss = 0.553187906742096, test acc = 0.800000011920929, time = 0.41732287406921387\n",
      "Testing at step=97, batch=80, test loss = 0.461615651845932, test acc = 0.8600000143051147, time = 0.4167509078979492\n",
      "Step 97 finished in 456.02069425582886, Train loss = 0.3978819907953342, Test loss = 0.45160328537225725; Train Acc = 0.8612499996026357, Test Acc = 0.8458999991416931\n",
      "Training at step=98, batch=0, train loss = 0.4005139470100403, train acc = 0.8799999952316284, time = 0.7191550731658936\n",
      "Training at step=98, batch=120, train loss = 0.3485286235809326, train acc = 0.8700000047683716, time = 0.6452648639678955\n",
      "Training at step=98, batch=240, train loss = 0.5647349953651428, train acc = 0.800000011920929, time = 0.7059240341186523\n",
      "Training at step=98, batch=360, train loss = 0.381343275308609, train acc = 0.8799999952316284, time = 0.7039210796356201\n",
      "Training at step=98, batch=480, train loss = 0.43012917041778564, train acc = 0.8799999952316284, time = 0.7037224769592285\n",
      "Testing at step=98, batch=0, test loss = 0.4569667875766754, test acc = 0.8100000023841858, time = 0.21538043022155762\n",
      "Testing at step=98, batch=20, test loss = 0.47952592372894287, test acc = 0.8500000238418579, time = 0.1432814598083496\n",
      "Testing at step=98, batch=40, test loss = 0.3518347442150116, test acc = 0.8399999737739563, time = 0.14159297943115234\n",
      "Testing at step=98, batch=60, test loss = 0.3269357979297638, test acc = 0.8799999952316284, time = 0.13599681854248047\n",
      "Testing at step=98, batch=80, test loss = 0.34303587675094604, test acc = 0.8600000143051147, time = 0.14068150520324707\n",
      "Step 98 finished in 456.49219489097595, Train loss = 0.39837473509212334, Test loss = 0.4457643856108189; Train Acc = 0.8616166651248932, Test Acc = 0.8428999966382981\n",
      "Training at step=99, batch=0, train loss = 0.44642239809036255, train acc = 0.8500000238418579, time = 0.6929521560668945\n",
      "Training at step=99, batch=120, train loss = 0.41589897871017456, train acc = 0.8500000238418579, time = 0.7057502269744873\n",
      "Training at step=99, batch=240, train loss = 0.35658225417137146, train acc = 0.8799999952316284, time = 0.7049527168273926\n",
      "Training at step=99, batch=360, train loss = 0.5011764764785767, train acc = 0.8600000143051147, time = 0.7044312953948975\n",
      "Training at step=99, batch=480, train loss = 0.42071861028671265, train acc = 0.8500000238418579, time = 0.7053549289703369\n",
      "Testing at step=99, batch=0, test loss = 0.43246185779571533, test acc = 0.8399999737739563, time = 0.22261762619018555\n",
      "Testing at step=99, batch=20, test loss = 0.37966883182525635, test acc = 0.8700000047683716, time = 0.10610437393188477\n",
      "Testing at step=99, batch=40, test loss = 0.4046209752559662, test acc = 0.8600000143051147, time = 0.1059269905090332\n",
      "Testing at step=99, batch=60, test loss = 0.5835041403770447, test acc = 0.800000011920929, time = 0.10589933395385742\n",
      "Testing at step=99, batch=80, test loss = 0.3539413809776306, test acc = 0.8500000238418579, time = 0.10722494125366211\n",
      "Step 99 finished in 439.5447177886963, Train loss = 0.3982000322888295, Test loss = 0.44812895357608795; Train Acc = 0.8609833334883054, Test Acc = 0.8436999994516373\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"1-fashionmnist_quanv_classical_linear.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "U0Q0vFm7B6cg",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711488462684,
     "user_tz": -660,
     "elapsed": 1244,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "a805910a-6214-4218-d940-429f1bae5d2e",
    "ExecuteTime": {
     "end_time": "2024-04-07T09:49:02.821336Z",
     "start_time": "2024-04-07T09:49:02.274958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T09:49:02.834617Z",
     "start_time": "2024-04-07T09:49:02.822697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"1-fashionmnist_quanv_classical_linear.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T09:49:02.266487Z",
     "start_time": "2024-04-07T09:49:02.264496Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 13
  }
 ]
}
