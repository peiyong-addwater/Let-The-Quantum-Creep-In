{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "authorship_tag": "ABX9TyO9ycf2GLNwAh4wkdecvhq8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ebnFsErY0EM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462301946,
     "user_tz": -660,
     "elapsed": 73092,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "710b1d81-a854-4328-959c-884f3b89fa61",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:07.320372Z",
     "start_time": "2024-04-06T21:07:03.773132Z"
    }
   },
   "source": [
    "# Install packages\n",
    "!pip install torch torchvision torchaudio pennylane pennylane-lightning pennylane-lightning[gpu] cotengra quimb torchmetrics --upgrade"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.17.2)\r\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: pennylane in /usr/local/lib/python3.9/dist-packages (0.35.1)\r\n",
      "Requirement already satisfied: pennylane-lightning in /usr/local/lib/python3.9/dist-packages (0.35.1)\r\n",
      "Requirement already satisfied: cotengra in /usr/local/lib/python3.9/dist-packages (0.5.6)\r\n",
      "Requirement already satisfied: quimb in /usr/local/lib/python3.9/dist-packages (1.7.3)\r\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (1.3.2)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from torch) (4.11.0)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.0.106)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch) (2.19.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.2.0)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\r\n",
      "Requirement already satisfied: rustworkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.14.2)\r\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.10.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\r\n",
      "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.6.9)\r\n",
      "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.10.0)\r\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\r\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.4.4)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\r\n",
      "Requirement already satisfied: autograd in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.6.2)\r\n",
      "Requirement already satisfied: pennylane-lightning-gpu in /usr/local/lib/python3.9/dist-packages (from pennylane-lightning) (0.35.1)\r\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\r\n",
      "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.59.1)\r\n",
      "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\r\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.12.3)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (0.11.2)\r\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\r\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.39->quimb) (0.42.0)\r\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import pennylane as qml\n",
    "import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "#prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# torch.cfloat won't pass the unitary check, but faster\n",
    "COMPLEX_DTYPE = torch.cfloat\n",
    "REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VN2wD-U7bGse",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462309702,
     "user_tz": -660,
     "elapsed": 7760,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "5cf6575c-ec02-4d3d-b222-1464aa8c1914",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:11.191285Z",
     "start_time": "2024-04-06T21:07:07.322167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data"
   ],
   "metadata": {
    "id": "9WmQPQuLbSFw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUhQUP2dbTja",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462317565,
     "user_tz": -660,
     "elapsed": 7868,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "5be0db17-8633-45dd-cde0-19eaec483b24",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:11.308714Z",
     "start_time": "2024-04-06T21:07:11.192556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n",
      "tensor([4, 1, 7, 3, 5, 8, 2, 6, 4, 6, 6, 3, 6, 9, 9, 7, 8, 7, 7, 7, 2, 7, 3, 8,\n",
      "        8, 8, 2, 2, 8, 2, 1, 5, 8, 7, 8, 2, 1, 4, 8, 7, 0, 0, 8, 9, 7, 5, 3, 6,\n",
      "        9, 3, 9, 8, 7, 1, 8, 8, 0, 7, 3, 0, 6, 4, 5, 8])\n",
      "tensor([-1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j,  0.7176+0.j,  0.5686+0.j,  0.8588+0.j,  0.2235+0.j,  0.0902+0.j,\n",
      "         0.9059+0.j,  0.5529+0.j,  0.6000+0.j,  0.6078+0.j,  0.6392+0.j,  0.6314+0.j,\n",
      "         0.6157+0.j,  0.8039+0.j,  0.4039+0.j, -0.0039+0.j,  0.9922+0.j,  0.6314+0.j,\n",
      "         0.7490+0.j, -0.6235+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j], dtype=torch.complex64)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Some Utilities"
   ],
   "metadata": {
    "id": "kQI8WWY6byQq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ket = {\n",
    "    '0':torch.tensor([1.,0.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '1':torch.tensor([0.,1.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '+':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) + torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device)),\n",
    "    '-':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) - torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device))\n",
    "}\n",
    "\n",
    "pauli = {\n",
    "    'I':torch.tensor([[1.,0.],[0.,1.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'X':torch.tensor([[0.,1.],[1.,0.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Y':torch.tensor([[0., -1.j],[1.j, 0]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Z':torch.tensor([[1.,0.],[0.,-1.]], dtype = COMPLEX_DTYPE, device=device)\n",
    "}\n",
    "\n",
    "def tensor_product(*args):\n",
    "  input_list = [a for a in args]\n",
    "  return functools.reduce(torch.kron, input_list)\n",
    "\n",
    "def multi_qubit_identity(n_qubits:int):\n",
    "  assert n_qubits>0\n",
    "  if n_qubits == 1:\n",
    "    return pauli['I']\n",
    "  else:\n",
    "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
    "\n",
    "pauli_words_su4 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
    "      #print(key1, key2)\n",
    "      #print(pauli[key1])\n",
    "      #print(pauli[key2])\n",
    "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
    "      #print(pauli_words_su4[key1+key2])\n",
    "\n",
    "pauli_words_su8 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      if not key1+key2+key3 == 'III':\n",
    "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
    "\n",
    "pauli_words_su16 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        if not key1+key2+key3+key4 == 'IIII':\n",
    "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
    "              pauli[key1],\n",
    "              pauli[key2],\n",
    "              pauli[key3],\n",
    "              pauli[key4]\n",
    "          )\n",
    "\n",
    "pauli_words_su32 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        for key5 in pauli.keys():\n",
    "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
    "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
    "                pauli[key1],\n",
    "                pauli[key2],\n",
    "                pauli[key3],\n",
    "                pauli[key4],\n",
    "                pauli[key5]\n",
    "            )\n",
    "\n",
    "def su32_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = list(pauli_words_su32.values())\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, torch.stack(dict_values))\n",
    "  #print(generator.shape)\n",
    "  return torch.matrix_exp(1j*generator)\n",
    "\n",
    "def su4_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = torch.stack(list(pauli_words_su4.values()))\n",
    "  #print(dict_values.shape)\n",
    "  generator = torch.einsum(\"i, ijk -> jk\", params, dict_values)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7B4DTncWbwQl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320391,
     "user_tz": -660,
     "elapsed": 2829,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "72ab9fd9-1a03-484b-e92a-147788cf05aa",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.368132Z",
     "start_time": "2024-04-06T21:07:11.310586Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "def measure_sv(\n",
    "    state,\n",
    "    observable\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Measure a statevector with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not\n",
    "  \"\"\"\n",
    "  expectation_value = torch.conj(state)@observable@state\n",
    "  return torch.real(expectation_value)\n",
    "\n",
    "def measure_dm(\n",
    "    rho,\n",
    "    observable\n",
    "):\n",
    "  \"\"\"\n",
    "  Measure a density matrix with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not.\n",
    "  \"\"\"\n",
    "  product = torch.matmul(rho, observable)\n",
    "\n",
    "  # Calculate the trace, which is the sum of diagonal elements\n",
    "  trace = torch.trace(product)\n",
    "\n",
    "  # The expectation value should be real for physical observables\n",
    "  return torch.real(trace)\n",
    "\n",
    "vmap_measure_sv = torch.vmap(measure_sv, in_dims=(None, 0), out_dims=0)\n",
    "vmap_measure_dm = torch.vmap(measure_dm, in_dims=(None, 0), out_dims=0)\n",
    "\n",
    "# assuming the input patch observables (hermitianized) has shape (batchsize,n_patches, c, h, w)\n",
    "# assuming the input set statevectors has shape (c, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_sv_batched_ob = torch.vmap(measure_sv, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "# assuming the input set desnity matrices has shape (batchsize, n_patches, c, 2**n, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_dm_batched_ob = torch.vmap(measure_dm, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "def bitstring_to_state(bitstring:str):\n",
    "  \"\"\"\n",
    "  Convert a bit string, like '0101001' or '+-+-101'\n",
    "  to a statevector. Each character in the bitstring must be among\n",
    "  0, 1, + and -\n",
    "  \"\"\"\n",
    "  assert len(bitstring)>0\n",
    "  for c in bitstring:\n",
    "    assert c in ['0', '1', '+', '-']\n",
    "  single_qubit_states = [ket[c] for c in bitstring]\n",
    "  return tensor_product(*single_qubit_states)\n",
    "\n",
    "\n",
    "#test_patch = torch.randn(size=[5, 2, 3, 4,4], dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_herm_patch = (torch.einsum(\"...jk->...kj\", test_patch)+test_patch)/2\n",
    "#print(test_herm_patch)\n",
    "#print(\"all patches shape\", test_herm_patch.shape)\n",
    "#test_sv = torch.stack([bitstring_to_state('++')]*3, axis = 0)\n",
    "#print(test_sv)\n",
    "#print(\"all sv shape\", test_sv.shape)\n",
    "#res = vmap_measure_channel_sv_batched_ob(test_sv, test_herm_patch)\n",
    "#print(res)\n",
    "#print(\"res shape\",res.shape)\n",
    "\n",
    "#for batchid in range(test_patch.shape[0]):\n",
    "#  batchitem = test_patch[batchid]\n",
    "#  for patch_idx in range(batchitem.shape[0]):\n",
    "#    patch = batchitem[patch_idx]\n",
    "#    for ch_idx in range(patch.shape[0]):\n",
    "#      print(f\"batchid={batchid}; patchid = {patch_idx}; chid = {ch_idx}\")\n",
    "#      ch = patch[ch_idx]\n",
    "#      #print(ch.shape)\n",
    "#      sv_ch = test_sv[ch_idx]\n",
    "#      #print(sv_ch.shape)\n",
    "#      print(measure_sv(sv_ch, ch))\n"
   ],
   "metadata": {
    "id": "Xs0c2F1eBnGc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320391,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.375546Z",
     "start_time": "2024-04-06T21:07:13.369704Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flipped Quanvolution Kernel\n",
    "\n",
    "First, let's define the function to extract patches for a single kernel."
   ],
   "metadata": {
    "id": "fFZqT6bpElaL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_patches(image, patch_size, stride, padding=None):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image with multiple input channels and optional custom padding.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (in_channels, height, width).\n",
    "        patch_size (int): Size of the square patches to extract.\n",
    "        stride (int): Stride between patches.\n",
    "        padding (tuple): Padding value(s) for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n",
    "    pad_l, pad_r, pad_t, pad_b = padding if padding is not None else (0,0,0,0)\n",
    "\n",
    "    if padding is not None:\n",
    "        image = torch.nn.functional.pad(image, (pad_l, pad_r, pad_t, pad_b), mode='constant')\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    height, width = image.shape[-2],  image.shape[-1]\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = image[..., i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    patches = torch.stack(patches, dim=-4)\n",
    "    return patches # has shape (batchsize, n_patches, channel, h, w)\n",
    "\n",
    "\n",
    "#single_img = torch.stack([torch.arange(32*32*1,dtype = torch.double, device=device).reshape((32,32)).type(torch.cdouble)]*3)\n",
    "\n",
    "#test_img = torch.stack([single_img]*2)\n",
    "#print(test_img[0][...,:4,:4])\n",
    "#patches = extract_patches(test_img, patch_size=4, stride=2,padding=(0,0,0,0))\n",
    "#print(patches.shape)\n",
    "#print(patches[0].shape)\n",
    "#print(patches[0][0])\n"
   ],
   "metadata": {
    "id": "He4HdMRHC7T6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320391,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.382085Z",
     "start_time": "2024-04-06T21:07:13.376557Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Three-by-Three Quanv Kernel Function\n",
    "\n",
    "For 3 by 3 patches, we only need a 2-qubit parameterised state, generated by the SU4 gate applied to the $|00\\rangle$ state. The first thing to do is to pad the 3 by 3 patches to 4 by 4. However, for input with size\n",
    "\n",
    "$$\n",
    "D_1\\times H_1 \\times W_1\n",
    "$$\n",
    "\n",
    "where $D_1$ is the number of input channels, we will need $D_1$ different two-qubit states, generated by $D_1$ different parameterised circuits. These circuits will produce $D_1$ expectation values. Suppose each circuit is parameterised by $\\theta_i, i\\in \\{1,2,\\cdots, D_1\\}$, the parameterised (pure) state can be written as\n",
    "\n",
    "$$\n",
    "|\\psi (\\theta_i)⟩ = SU4(\\theta_i)|00⟩\n",
    "$$\n",
    "\n",
    "For a channel $M_i$ of patch $M$ with shape $4\\times 4$, the expectation value is calculated by\n",
    "\n",
    "$$\n",
    "⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$\n",
    "\n",
    "Then, a single pixel value of the output channel corresponding this single kernel is\n",
    "\n",
    "$$\n",
    "\\frac{1}{D_1}\\sum_{i=1}^{D_1}⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$"
   ],
   "metadata": {
    "id": "9hRqflooEqKN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_2q_param_state(theta):\n",
    "  state = bitstring_to_state('00')\n",
    "  state = torch.matmul(su4_op(theta), state)\n",
    "  return state\n",
    "\n",
    "vmap_generate_2q_param_state = torch.vmap(generate_2q_param_state, in_dims=0, out_dims = 0)\n",
    "\n",
    "def single_kernel_op_over_batched_patches(thetas, patch):\n",
    "  # patch has shape (c_in, h, w)\n",
    "  # thetas has shape (c_in, 4^2-1) for SU4 gates\n",
    "  n_theta = thetas.shape[-2]\n",
    "  n_channel = patch.shape[-3]\n",
    "  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n",
    "  states = vmap_generate_2q_param_state(thetas)\n",
    "  #print(\"States shape\", states.shape)\n",
    "  patch = torch.nn.functional.pad(patch, (0, 1, 0, 1), mode='constant')\n",
    "  patch_t = torch.einsum(\"...jk->...kj\", patch)\n",
    "  herm_patch = (patch_t+patch)/2 # has dim (batchsize, num_patches, c, h, w)\n",
    "  #print(\"Herm patch shape\", herm_patch.shape)\n",
    "  channel_out = vmap_measure_channel_sv_batched_ob(states, herm_patch) # has dim (batchsize,n_patches, c)\n",
    "  return torch.sum(channel_out, axis = -1)/n_theta # has dim (batchszie, n_patches)\n",
    "\n",
    "\n",
    "#test_params = torch.randn((1,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_img = dummy_x.to(device)#.type(torch.cdouble)\n",
    "#print(test_img.shape)\n",
    "#test_patches = extract_patches(test_img, patch_size=3, stride=1,padding=(1,1,1,1))\n",
    "#print(test_patches.shape)\n",
    "#print(test_params.shape)\n",
    "#print(vmap_generate_2q_param_state(test_params))\n",
    "#test_out = single_kernel_op_over_batched_patches(test_params, test_patches)\n",
    "#print(test_out.shape)\n",
    "#h = (32-3+1*2)//1 +1\n",
    "#h**2\n"
   ],
   "metadata": {
    "id": "Yzn4KEt5ErG7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320391,
     "user_tz": -660,
     "elapsed": 5,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.387764Z",
     "start_time": "2024-04-06T21:07:13.383020Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multiple Output Channels"
   ],
   "metadata": {
    "id": "A4BXEKd8I67_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# For multiple channel output\n",
    "# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n",
    "vmap_vmap_single_kernel_op_through_extracted_patches = torch.vmap(single_kernel_op_over_batched_patches, in_dims=(0, None), out_dims=-2) # output has dim (batchsize, c_out, n_patches)\n",
    "\n",
    "#c_in = 1\n",
    "#c_out = 2\n",
    "\n",
    "#test_params2 = torch.randn((c_out, c_in,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_out2 = vmap_vmap_single_kernel_op_through_extracted_patches(test_params2, test_patches)\n",
    "#print(test_out2.shape)\n",
    "#test_out_2_features = test_out2.reshape((-1,c_out, 32, 32))\n",
    "#print(test_out_2_features.shape)\n"
   ],
   "metadata": {
    "id": "72vkHV_BI80l",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320392,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.391567Z",
     "start_time": "2024-04-06T21:07:13.388733Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch Module for Three-by-Three Flipped Quanv"
   ],
   "metadata": {
    "id": "NuZ6G5-sKK_J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FlippedQuanv3x3(torch.nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, padding):\n",
    "    super(FlippedQuanv3x3, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.pad_l, self.pad_r, self.pad_t, self.pad_b = padding if padding is not None else (0,0,0,0)\n",
    "    self.pad = (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n",
    "    self.weight= torch.nn.Parameter(torch.randn((out_channels, in_channels, 4**2-1)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn((out_channels, 1)).type(COMPLEX_DTYPE))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batchsize ,c_in, h, w)\n",
    "    # weight has shape (c_out, c_in, 15)\n",
    "    # bias has shape (c_out, 1)\n",
    "    x = x.type(COMPLEX_DTYPE)\n",
    "    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n",
    "    h_out = (h_in-3+(self.pad_t+self.pad_b))//self.stride +1\n",
    "    w_out = (w_in-3+(self.pad_l+self.pad_r))//self.stride +1\n",
    "\n",
    "    #print(self.weight.shape)\n",
    "    #print(patches.shape)\n",
    "\n",
    "    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n",
    "\n",
    "\n",
    "    out = out + self.bias\n",
    "    return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n",
    "\n",
    "#test_module = FlippedQuanv3x3(in_channels=1, out_channels=2, stride=1, padding=(1,1,1,1)).to(device)\n",
    "#print(dummy_x.shape)\n",
    "#test_out = test_module(dummy_x.to(device))\n",
    "#print(test_out.shape)"
   ],
   "metadata": {
    "id": "Gww_XdJ5KPJt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320392,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:13.398665Z",
     "start_time": "2024-04-06T21:07:13.392811Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flipped Quanvolution Replacing Conv2d\n",
    "\n",
    "First, let's just replace `Conv2d` with `FlippedQuanv3x3`."
   ],
   "metadata": {
    "id": "1yumZTjDVu63"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class HybridNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HybridNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(16*28*28, 10).type(REAL_DTYPE)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "net = HybridNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4RC5ou-VzbE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320392,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "f2e92dd2-63f4-4ed1-d191-6b016304911e",
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:16.031580Z",
     "start_time": "2024-04-06T21:07:13.400651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "HybridNet(\n",
      "  (layers): Sequential(\n",
      "    (0): FlippedQuanv3x3()\n",
      "    (1): FlippedQuanv3x3()\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=12544, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_257/2687080126.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n",
      "/tmp/ipykernel_257/2683249851.py:30: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:301.)\n",
      "  return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "import torchmetrics\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.2\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4ZY59q1WS4x",
    "outputId": "0e7cbfcd-1078-4c56-c675-1afbfa385c85",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711488461440,
     "user_tz": -660,
     "elapsed": 18075888,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-06T21:07:16.032868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 600, Number of test batches = 100\n",
      "Print every train batch = 120, Print every test batch = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_257/2687080126.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at step=0, batch=0, train loss = 2.5836989879608154, train acc = 0.07999999821186066, time = 0.6942799091339111\n",
      "Training at step=0, batch=120, train loss = 0.9371321797370911, train acc = 0.6299999952316284, time = 0.7056727409362793\n",
      "Training at step=0, batch=240, train loss = 0.8880257606506348, train acc = 0.6899999976158142, time = 0.7041270732879639\n",
      "Training at step=0, batch=360, train loss = 0.6269816160202026, train acc = 0.7699999809265137, time = 0.7041285037994385\n",
      "Training at step=0, batch=480, train loss = 0.7035717964172363, train acc = 0.7200000286102295, time = 0.7034604549407959\n",
      "Testing at step=0, batch=0, test loss = 0.581850528717041, test acc = 0.7799999713897705, time = 0.2047109603881836\n",
      "Testing at step=0, batch=20, test loss = 0.5622889995574951, test acc = 0.7900000214576721, time = 0.14134502410888672\n",
      "Testing at step=0, batch=40, test loss = 0.6838259100914001, test acc = 0.7799999713897705, time = 0.14039039611816406\n",
      "Testing at step=0, batch=60, test loss = 0.4995146691799164, test acc = 0.8500000238418579, time = 0.142592191696167\n",
      "Testing at step=0, batch=80, test loss = 0.8044507503509521, test acc = 0.7200000286102295, time = 0.14116120338439941\n",
      "Step 0 finished in 456.00109338760376, Train loss = 0.764598686248064, Test loss = 0.6363971829414368; Train Acc = 0.7235833340883255, Test Acc = 0.7703999978303909\n",
      "Training at step=1, batch=0, train loss = 0.7110718488693237, train acc = 0.7400000095367432, time = 0.7150058746337891\n",
      "Training at step=1, batch=120, train loss = 0.5455096364021301, train acc = 0.8199999928474426, time = 0.7054343223571777\n",
      "Training at step=1, batch=240, train loss = 0.5716806650161743, train acc = 0.7699999809265137, time = 0.7043781280517578\n",
      "Training at step=1, batch=360, train loss = 0.47319844365119934, train acc = 0.8100000023841858, time = 0.7042665481567383\n",
      "Training at step=1, batch=480, train loss = 0.5869431495666504, train acc = 0.7900000214576721, time = 0.7045483589172363\n",
      "Testing at step=1, batch=0, test loss = 0.4256950318813324, test acc = 0.8600000143051147, time = 0.21527576446533203\n",
      "Testing at step=1, batch=20, test loss = 0.6368618011474609, test acc = 0.7400000095367432, time = 0.1482100486755371\n",
      "Testing at step=1, batch=40, test loss = 0.6805402636528015, test acc = 0.7599999904632568, time = 0.4181978702545166\n",
      "Testing at step=1, batch=60, test loss = 0.6507700085639954, test acc = 0.800000011920929, time = 0.41753172874450684\n",
      "Testing at step=1, batch=80, test loss = 0.6572731733322144, test acc = 0.75, time = 0.4173121452331543\n",
      "Step 1 finished in 456.12537479400635, Train loss = 0.5784350056449572, Test loss = 0.634054321050644; Train Acc = 0.7925999960303307, Test Acc = 0.7668999999761581\n",
      "Training at step=2, batch=0, train loss = 0.5403960943222046, train acc = 0.7900000214576721, time = 0.7250816822052002\n",
      "Training at step=2, batch=120, train loss = 0.6074568033218384, train acc = 0.8199999928474426, time = 0.7037386894226074\n",
      "Training at step=2, batch=240, train loss = 0.49753284454345703, train acc = 0.8399999737739563, time = 0.7046875953674316\n",
      "Training at step=2, batch=360, train loss = 0.5856130123138428, train acc = 0.7900000214576721, time = 0.7039542198181152\n",
      "Training at step=2, batch=480, train loss = 0.5448890924453735, train acc = 0.800000011920929, time = 0.7046308517456055\n",
      "Testing at step=2, batch=0, test loss = 0.6612063050270081, test acc = 0.7599999904632568, time = 0.1929945945739746\n",
      "Testing at step=2, batch=20, test loss = 0.6898283362388611, test acc = 0.7699999809265137, time = 0.14329242706298828\n",
      "Testing at step=2, batch=40, test loss = 0.4943191111087799, test acc = 0.8600000143051147, time = 0.14353728294372559\n",
      "Testing at step=2, batch=60, test loss = 0.6529456973075867, test acc = 0.7300000190734863, time = 0.14299368858337402\n",
      "Testing at step=2, batch=80, test loss = 0.6678145527839661, test acc = 0.800000011920929, time = 0.14440178871154785\n",
      "Step 2 finished in 456.793842792511, Train loss = 0.5453186816970508, Test loss = 0.5951967200636864; Train Acc = 0.8037833310166995, Test Acc = 0.7811999994516373\n",
      "Training at step=3, batch=0, train loss = 0.5006687641143799, train acc = 0.8199999928474426, time = 0.7103610038757324\n",
      "Training at step=3, batch=120, train loss = 0.6498001217842102, train acc = 0.8100000023841858, time = 0.7330482006072998\n",
      "Training at step=3, batch=240, train loss = 0.5991846323013306, train acc = 0.7200000286102295, time = 0.7061505317687988\n",
      "Training at step=3, batch=360, train loss = 0.3979871869087219, train acc = 0.8600000143051147, time = 0.7036490440368652\n",
      "Training at step=3, batch=480, train loss = 0.40067240595817566, train acc = 0.8399999737739563, time = 0.701552152633667\n",
      "Testing at step=3, batch=0, test loss = 0.5248273611068726, test acc = 0.8500000238418579, time = 0.21711182594299316\n",
      "Testing at step=3, batch=20, test loss = 0.5039570331573486, test acc = 0.7900000214576721, time = 0.14200925827026367\n",
      "Testing at step=3, batch=40, test loss = 0.5787867903709412, test acc = 0.7900000214576721, time = 0.14683985710144043\n",
      "Testing at step=3, batch=60, test loss = 0.34330254793167114, test acc = 0.8799999952316284, time = 0.1477656364440918\n",
      "Testing at step=3, batch=80, test loss = 0.5878751873970032, test acc = 0.7599999904632568, time = 0.14446187019348145\n",
      "Step 3 finished in 458.2344596385956, Train loss = 0.5272795108457407, Test loss = 0.5386574950814247; Train Acc = 0.8111833307147026, Test Acc = 0.8038999962806702\n",
      "Training at step=4, batch=0, train loss = 0.4156323969364166, train acc = 0.8600000143051147, time = 0.7294206619262695\n",
      "Training at step=4, batch=120, train loss = 0.519503116607666, train acc = 0.8299999833106995, time = 0.703869104385376\n",
      "Training at step=4, batch=240, train loss = 0.46468907594680786, train acc = 0.8199999928474426, time = 0.7041590213775635\n",
      "Training at step=4, batch=360, train loss = 0.4092407524585724, train acc = 0.8799999952316284, time = 0.7021641731262207\n",
      "Training at step=4, batch=480, train loss = 0.3036113977432251, train acc = 0.8700000047683716, time = 0.705237865447998\n",
      "Testing at step=4, batch=0, test loss = 0.5691523551940918, test acc = 0.7900000214576721, time = 0.21664142608642578\n",
      "Testing at step=4, batch=20, test loss = 0.34584859013557434, test acc = 0.8600000143051147, time = 0.16241788864135742\n",
      "Testing at step=4, batch=40, test loss = 0.5068615674972534, test acc = 0.800000011920929, time = 0.1520707607269287\n",
      "Testing at step=4, batch=60, test loss = 0.510859489440918, test acc = 0.8299999833106995, time = 0.14288949966430664\n",
      "Testing at step=4, batch=80, test loss = 0.519633948802948, test acc = 0.8399999737739563, time = 0.14717674255371094\n",
      "Step 4 finished in 458.07330298423767, Train loss = 0.508265282958746, Test loss = 0.5419282969832421; Train Acc = 0.8192499990264575, Test Acc = 0.8068999963998794\n",
      "Training at step=5, batch=0, train loss = 0.7953063249588013, train acc = 0.8199999928474426, time = 0.7035589218139648\n",
      "Training at step=5, batch=120, train loss = 0.563596248626709, train acc = 0.7900000214576721, time = 0.703160285949707\n",
      "Training at step=5, batch=240, train loss = 0.48696380853652954, train acc = 0.8500000238418579, time = 0.7036678791046143\n",
      "Training at step=5, batch=360, train loss = 0.6045854091644287, train acc = 0.7799999713897705, time = 0.7051546573638916\n",
      "Training at step=5, batch=480, train loss = 0.4593188762664795, train acc = 0.8600000143051147, time = 0.7048430442810059\n",
      "Testing at step=5, batch=0, test loss = 0.5588992834091187, test acc = 0.7900000214576721, time = 0.21746611595153809\n",
      "Testing at step=5, batch=20, test loss = 0.596833348274231, test acc = 0.800000011920929, time = 0.14188385009765625\n",
      "Testing at step=5, batch=40, test loss = 0.5523189306259155, test acc = 0.75, time = 0.14325332641601562\n",
      "Testing at step=5, batch=60, test loss = 0.510686457157135, test acc = 0.8399999737739563, time = 0.14117813110351562\n",
      "Testing at step=5, batch=80, test loss = 0.5925377607345581, test acc = 0.7900000214576721, time = 0.14351296424865723\n",
      "Step 5 finished in 456.49703431129456, Train loss = 0.5001073582470417, Test loss = 0.5477593141794205; Train Acc = 0.821699997484684, Test Acc = 0.8024999982118607\n",
      "Training at step=6, batch=0, train loss = 0.5131503939628601, train acc = 0.8199999928474426, time = 0.6998727321624756\n",
      "Training at step=6, batch=120, train loss = 0.3779446482658386, train acc = 0.8799999952316284, time = 0.7057740688323975\n",
      "Training at step=6, batch=240, train loss = 0.45099353790283203, train acc = 0.8299999833106995, time = 0.7056541442871094\n",
      "Training at step=6, batch=360, train loss = 0.48642033338546753, train acc = 0.8700000047683716, time = 0.704491376876831\n",
      "Training at step=6, batch=480, train loss = 0.5854506492614746, train acc = 0.8399999737739563, time = 0.7058923244476318\n",
      "Testing at step=6, batch=0, test loss = 0.5053102374076843, test acc = 0.800000011920929, time = 0.20370697975158691\n",
      "Testing at step=6, batch=20, test loss = 0.5549144744873047, test acc = 0.800000011920929, time = 0.15019893646240234\n",
      "Testing at step=6, batch=40, test loss = 0.45852988958358765, test acc = 0.8199999928474426, time = 0.14861106872558594\n",
      "Testing at step=6, batch=60, test loss = 0.5003421902656555, test acc = 0.8199999928474426, time = 0.14943408966064453\n",
      "Testing at step=6, batch=80, test loss = 0.47083571553230286, test acc = 0.8299999833106995, time = 0.1493053436279297\n",
      "Step 6 finished in 456.4067442417145, Train loss = 0.49528150300184887, Test loss = 0.4986539569497108; Train Acc = 0.8241333312789599, Test Acc = 0.8197999978065491\n",
      "Training at step=7, batch=0, train loss = 0.3079851269721985, train acc = 0.9100000262260437, time = 0.7338972091674805\n",
      "Training at step=7, batch=120, train loss = 0.4281565845012665, train acc = 0.8199999928474426, time = 0.7051863670349121\n",
      "Training at step=7, batch=240, train loss = 0.4037320613861084, train acc = 0.8199999928474426, time = 0.7180707454681396\n",
      "Training at step=7, batch=360, train loss = 0.4286593496799469, train acc = 0.8500000238418579, time = 0.7065448760986328\n",
      "Training at step=7, batch=480, train loss = 0.3068290948867798, train acc = 0.9399999976158142, time = 0.7048852443695068\n",
      "Testing at step=7, batch=0, test loss = 0.5811710953712463, test acc = 0.800000011920929, time = 0.19431138038635254\n",
      "Testing at step=7, batch=20, test loss = 0.4622206389904022, test acc = 0.8299999833106995, time = 0.14078903198242188\n",
      "Testing at step=7, batch=40, test loss = 0.5795673727989197, test acc = 0.7699999809265137, time = 0.13730692863464355\n",
      "Testing at step=7, batch=60, test loss = 0.41157257556915283, test acc = 0.800000011920929, time = 0.41759228706359863\n",
      "Testing at step=7, batch=80, test loss = 0.6922125816345215, test acc = 0.7900000214576721, time = 0.41677355766296387\n",
      "Step 7 finished in 458.04687094688416, Train loss = 0.4879501762986183, Test loss = 0.5101201692223549; Train Acc = 0.827483332157135, Test Acc = 0.8208999967575074\n",
      "Training at step=8, batch=0, train loss = 0.41865235567092896, train acc = 0.8899999856948853, time = 0.7163324356079102\n",
      "Training at step=8, batch=120, train loss = 0.38789480924606323, train acc = 0.8399999737739563, time = 0.704984188079834\n",
      "Training at step=8, batch=240, train loss = 0.3257369101047516, train acc = 0.8799999952316284, time = 0.7033274173736572\n",
      "Training at step=8, batch=360, train loss = 0.459367036819458, train acc = 0.8600000143051147, time = 0.7055008411407471\n",
      "Training at step=8, batch=480, train loss = 0.666461706161499, train acc = 0.7300000190734863, time = 0.7065105438232422\n",
      "Testing at step=8, batch=0, test loss = 0.6766992211341858, test acc = 0.7799999713897705, time = 0.21277928352355957\n",
      "Testing at step=8, batch=20, test loss = 0.5956601500511169, test acc = 0.7900000214576721, time = 0.14673829078674316\n",
      "Testing at step=8, batch=40, test loss = 0.4126332402229309, test acc = 0.8500000238418579, time = 0.14756011962890625\n",
      "Testing at step=8, batch=60, test loss = 0.4091559648513794, test acc = 0.8600000143051147, time = 0.14821267127990723\n",
      "Testing at step=8, batch=80, test loss = 0.552273154258728, test acc = 0.8299999833106995, time = 0.14606356620788574\n",
      "Step 8 finished in 457.40761160850525, Train loss = 0.48341333483656246, Test loss = 0.49373015254735947; Train Acc = 0.8296333308021228, Test Acc = 0.8252999985218048\n",
      "Training at step=9, batch=0, train loss = 0.42179977893829346, train acc = 0.8700000047683716, time = 0.7268786430358887\n",
      "Training at step=9, batch=120, train loss = 0.431244432926178, train acc = 0.8399999737739563, time = 0.6929588317871094\n",
      "Training at step=9, batch=240, train loss = 0.44564080238342285, train acc = 0.8299999833106995, time = 0.7064242362976074\n",
      "Training at step=9, batch=360, train loss = 0.4100096523761749, train acc = 0.8399999737739563, time = 0.7048161029815674\n",
      "Training at step=9, batch=480, train loss = 0.4805448651313782, train acc = 0.8100000023841858, time = 0.7045340538024902\n",
      "Testing at step=9, batch=0, test loss = 0.9530547857284546, test acc = 0.7200000286102295, time = 0.3545069694519043\n",
      "Testing at step=9, batch=20, test loss = 0.6226334571838379, test acc = 0.7699999809265137, time = 0.41589856147766113\n",
      "Testing at step=9, batch=40, test loss = 0.5935570001602173, test acc = 0.7900000214576721, time = 0.41712045669555664\n",
      "Testing at step=9, batch=60, test loss = 0.5904956459999084, test acc = 0.8399999737739563, time = 0.41799283027648926\n",
      "Testing at step=9, batch=80, test loss = 0.5456560254096985, test acc = 0.800000011920929, time = 0.41851186752319336\n",
      "Step 9 finished in 456.6105628013611, Train loss = 0.4754742250839869, Test loss = 0.5512288343906403; Train Acc = 0.8317999982833862, Test Acc = 0.806099995970726\n",
      "Training at step=10, batch=0, train loss = 0.40707316994667053, train acc = 0.8199999928474426, time = 0.7327320575714111\n",
      "Training at step=10, batch=120, train loss = 0.3782639801502228, train acc = 0.8600000143051147, time = 0.7340168952941895\n",
      "Training at step=10, batch=240, train loss = 0.43855324387550354, train acc = 0.8899999856948853, time = 0.7046501636505127\n",
      "Training at step=10, batch=360, train loss = 0.624163031578064, train acc = 0.8100000023841858, time = 0.6888973712921143\n",
      "Training at step=10, batch=480, train loss = 0.5831882953643799, train acc = 0.800000011920929, time = 0.705087423324585\n",
      "Testing at step=10, batch=0, test loss = 0.32842883467674255, test acc = 0.8700000047683716, time = 0.2169208526611328\n",
      "Testing at step=10, batch=20, test loss = 0.5066608190536499, test acc = 0.800000011920929, time = 0.14801549911499023\n",
      "Testing at step=10, batch=40, test loss = 0.3708866834640503, test acc = 0.8600000143051147, time = 0.14791297912597656\n",
      "Testing at step=10, batch=60, test loss = 0.5163568258285522, test acc = 0.8100000023841858, time = 0.1421947479248047\n",
      "Testing at step=10, batch=80, test loss = 0.4993497133255005, test acc = 0.8299999833106995, time = 0.1422128677368164\n",
      "Step 10 finished in 461.5884690284729, Train loss = 0.47249131185313065, Test loss = 0.5193875503540039; Train Acc = 0.8323499976595243, Test Acc = 0.8175999963283539\n",
      "Training at step=11, batch=0, train loss = 0.3766229748725891, train acc = 0.8199999928474426, time = 0.6956281661987305\n",
      "Training at step=11, batch=120, train loss = 0.5130823254585266, train acc = 0.7599999904632568, time = 0.7048144340515137\n",
      "Training at step=11, batch=240, train loss = 0.50436931848526, train acc = 0.8299999833106995, time = 0.7029895782470703\n",
      "Training at step=11, batch=360, train loss = 0.38892197608947754, train acc = 0.8299999833106995, time = 0.7037022113800049\n",
      "Training at step=11, batch=480, train loss = 0.7053138613700867, train acc = 0.7900000214576721, time = 0.7063784599304199\n",
      "Testing at step=11, batch=0, test loss = 0.4709154963493347, test acc = 0.8600000143051147, time = 0.21624016761779785\n",
      "Testing at step=11, batch=20, test loss = 0.5053666234016418, test acc = 0.8199999928474426, time = 0.14736008644104004\n",
      "Testing at step=11, batch=40, test loss = 0.5918402075767517, test acc = 0.7799999713897705, time = 0.41739344596862793\n",
      "Testing at step=11, batch=60, test loss = 0.29736843705177307, test acc = 0.8999999761581421, time = 0.4178652763366699\n",
      "Testing at step=11, batch=80, test loss = 0.4542364478111267, test acc = 0.8299999833106995, time = 0.41774463653564453\n",
      "Step 11 finished in 456.159277677536, Train loss = 0.47389601051807406, Test loss = 0.47949127554893495; Train Acc = 0.8329999993244807, Test Acc = 0.8329999989271164\n",
      "Training at step=12, batch=0, train loss = 0.402303010225296, train acc = 0.8500000238418579, time = 0.7219505310058594\n",
      "Training at step=12, batch=120, train loss = 0.3268115520477295, train acc = 0.9100000262260437, time = 0.7046942710876465\n",
      "Training at step=12, batch=240, train loss = 0.5390752553939819, train acc = 0.8100000023841858, time = 0.7047328948974609\n",
      "Training at step=12, batch=360, train loss = 0.6665087938308716, train acc = 0.7900000214576721, time = 0.7054693698883057\n",
      "Training at step=12, batch=480, train loss = 0.48503878712654114, train acc = 0.8500000238418579, time = 0.7060391902923584\n",
      "Testing at step=12, batch=0, test loss = 0.4109826385974884, test acc = 0.8600000143051147, time = 0.21619009971618652\n",
      "Testing at step=12, batch=20, test loss = 0.46577098965644836, test acc = 0.8100000023841858, time = 0.41928625106811523\n",
      "Testing at step=12, batch=40, test loss = 0.5229445099830627, test acc = 0.800000011920929, time = 0.41727614402770996\n",
      "Testing at step=12, batch=60, test loss = 0.4304868280887604, test acc = 0.8100000023841858, time = 0.4176809787750244\n",
      "Testing at step=12, batch=80, test loss = 0.6124879717826843, test acc = 0.8199999928474426, time = 0.41805458068847656\n",
      "Step 12 finished in 456.2666447162628, Train loss = 0.46878914105395475, Test loss = 0.4999197354912758; Train Acc = 0.8355166663726171, Test Acc = 0.8252999973297119\n",
      "Training at step=13, batch=0, train loss = 0.43874961137771606, train acc = 0.8199999928474426, time = 0.7260279655456543\n",
      "Training at step=13, batch=120, train loss = 0.6594992280006409, train acc = 0.8100000023841858, time = 0.7047092914581299\n",
      "Training at step=13, batch=240, train loss = 0.6099026203155518, train acc = 0.8399999737739563, time = 0.7041568756103516\n",
      "Training at step=13, batch=360, train loss = 0.30135735869407654, train acc = 0.9100000262260437, time = 0.7030730247497559\n",
      "Training at step=13, batch=480, train loss = 0.5906194448471069, train acc = 0.7599999904632568, time = 0.7044522762298584\n",
      "Testing at step=13, batch=0, test loss = 0.49416035413742065, test acc = 0.7799999713897705, time = 0.2055361270904541\n",
      "Testing at step=13, batch=20, test loss = 0.4301818609237671, test acc = 0.8399999737739563, time = 0.1457984447479248\n",
      "Testing at step=13, batch=40, test loss = 0.5736342668533325, test acc = 0.7699999809265137, time = 0.41745615005493164\n",
      "Testing at step=13, batch=60, test loss = 0.3847528398036957, test acc = 0.8399999737739563, time = 0.41748738288879395\n",
      "Testing at step=13, batch=80, test loss = 0.523226797580719, test acc = 0.8199999928474426, time = 0.4170217514038086\n",
      "Step 13 finished in 456.856507062912, Train loss = 0.4662287653982639, Test loss = 0.5067566525936127; Train Acc = 0.8343833317359288, Test Acc = 0.8251999974250793\n",
      "Training at step=14, batch=0, train loss = 0.4817822277545929, train acc = 0.8399999737739563, time = 0.7227199077606201\n",
      "Training at step=14, batch=120, train loss = 0.303949236869812, train acc = 0.8899999856948853, time = 0.7052016258239746\n",
      "Training at step=14, batch=240, train loss = 0.4212009906768799, train acc = 0.8799999952316284, time = 0.7056341171264648\n",
      "Training at step=14, batch=360, train loss = 0.3398321866989136, train acc = 0.8700000047683716, time = 0.704207181930542\n",
      "Training at step=14, batch=480, train loss = 0.38202956318855286, train acc = 0.8700000047683716, time = 0.6993482112884521\n",
      "Testing at step=14, batch=0, test loss = 0.45321136713027954, test acc = 0.8399999737739563, time = 0.1739037036895752\n",
      "Testing at step=14, batch=20, test loss = 0.35754722356796265, test acc = 0.8700000047683716, time = 0.14027070999145508\n",
      "Testing at step=14, batch=40, test loss = 0.5431442856788635, test acc = 0.8199999928474426, time = 0.1404104232788086\n",
      "Testing at step=14, batch=60, test loss = 0.4609263241291046, test acc = 0.8399999737739563, time = 0.14225983619689941\n",
      "Testing at step=14, batch=80, test loss = 0.5839819312095642, test acc = 0.8500000238418579, time = 0.14323139190673828\n",
      "Step 14 finished in 457.27523159980774, Train loss = 0.46439996575315795, Test loss = 0.5031320914626122; Train Acc = 0.8353499990701675, Test Acc = 0.8194999969005585\n",
      "Training at step=15, batch=0, train loss = 0.38016387820243835, train acc = 0.8700000047683716, time = 0.70013427734375\n",
      "Training at step=15, batch=120, train loss = 0.5252819657325745, train acc = 0.8100000023841858, time = 0.7056193351745605\n",
      "Training at step=15, batch=240, train loss = 0.5489832162857056, train acc = 0.8299999833106995, time = 0.7049953937530518\n",
      "Training at step=15, batch=360, train loss = 0.3122580647468567, train acc = 0.8700000047683716, time = 0.7002999782562256\n",
      "Training at step=15, batch=480, train loss = 0.3799818158149719, train acc = 0.8700000047683716, time = 0.7027554512023926\n",
      "Testing at step=15, batch=0, test loss = 0.4944506883621216, test acc = 0.8299999833106995, time = 0.4248948097229004\n",
      "Testing at step=15, batch=20, test loss = 0.5267869234085083, test acc = 0.7900000214576721, time = 0.4175150394439697\n",
      "Testing at step=15, batch=40, test loss = 0.5221590995788574, test acc = 0.8199999928474426, time = 0.4173140525817871\n",
      "Testing at step=15, batch=60, test loss = 0.3368505835533142, test acc = 0.8799999952316284, time = 0.4177992343902588\n",
      "Testing at step=15, batch=80, test loss = 0.4475051462650299, test acc = 0.8600000143051147, time = 0.42204833030700684\n",
      "Step 15 finished in 455.6875171661377, Train loss = 0.4609855676690737, Test loss = 0.4897383800148964; Train Acc = 0.8377333325147629, Test Acc = 0.8253999972343444\n",
      "Training at step=16, batch=0, train loss = 0.36863213777542114, train acc = 0.9100000262260437, time = 0.7311935424804688\n",
      "Training at step=16, batch=120, train loss = 0.4382559061050415, train acc = 0.8199999928474426, time = 0.7052912712097168\n",
      "Training at step=16, batch=240, train loss = 0.46391046047210693, train acc = 0.8199999928474426, time = 0.7051424980163574\n",
      "Training at step=16, batch=360, train loss = 0.3935889005661011, train acc = 0.8899999856948853, time = 0.7060692310333252\n",
      "Training at step=16, batch=480, train loss = 0.5795872807502747, train acc = 0.800000011920929, time = 0.7043871879577637\n",
      "Testing at step=16, batch=0, test loss = 0.37092846632003784, test acc = 0.8199999928474426, time = 0.2132551670074463\n",
      "Testing at step=16, batch=20, test loss = 0.5681855082511902, test acc = 0.800000011920929, time = 0.15068554878234863\n",
      "Testing at step=16, batch=40, test loss = 0.6460704207420349, test acc = 0.75, time = 0.14250683784484863\n",
      "Testing at step=16, batch=60, test loss = 0.5046252012252808, test acc = 0.8399999737739563, time = 0.1551070213317871\n",
      "Testing at step=16, batch=80, test loss = 0.47124701738357544, test acc = 0.8100000023841858, time = 0.14240384101867676\n",
      "Step 16 finished in 456.2136297225952, Train loss = 0.4617140822360913, Test loss = 0.4911623142659664; Train Acc = 0.8365999985734621, Test Acc = 0.8263999962806702\n",
      "Training at step=17, batch=0, train loss = 0.42390045523643494, train acc = 0.8999999761581421, time = 0.7275311946868896\n",
      "Training at step=17, batch=120, train loss = 0.4261901080608368, train acc = 0.8500000238418579, time = 0.731903076171875\n",
      "Training at step=17, batch=240, train loss = 0.4052235782146454, train acc = 0.8299999833106995, time = 0.7031280994415283\n",
      "Training at step=17, batch=360, train loss = 0.49176475405693054, train acc = 0.8399999737739563, time = 0.7046117782592773\n",
      "Training at step=17, batch=480, train loss = 0.48318395018577576, train acc = 0.8299999833106995, time = 0.7036502361297607\n",
      "Testing at step=17, batch=0, test loss = 0.40681731700897217, test acc = 0.8299999833106995, time = 0.20215845108032227\n",
      "Testing at step=17, batch=20, test loss = 0.5045398473739624, test acc = 0.8100000023841858, time = 0.141737699508667\n",
      "Testing at step=17, batch=40, test loss = 0.5470139980316162, test acc = 0.8399999737739563, time = 0.14345026016235352\n",
      "Testing at step=17, batch=60, test loss = 0.4775587022304535, test acc = 0.8199999928474426, time = 0.14284372329711914\n",
      "Testing at step=17, batch=80, test loss = 0.6183226108551025, test acc = 0.7699999809265137, time = 0.14017248153686523\n",
      "Step 17 finished in 460.1881334781647, Train loss = 0.4567331436028083, Test loss = 0.5266998848319053; Train Acc = 0.8401499995589257, Test Acc = 0.8142999988794327\n",
      "Training at step=18, batch=0, train loss = 0.43261730670928955, train acc = 0.8600000143051147, time = 0.7169637680053711\n",
      "Training at step=18, batch=120, train loss = 0.4167031943798065, train acc = 0.8500000238418579, time = 0.7063252925872803\n",
      "Training at step=18, batch=240, train loss = 0.5004487633705139, train acc = 0.7699999809265137, time = 0.7054078578948975\n",
      "Training at step=18, batch=360, train loss = 0.6306230425834656, train acc = 0.8199999928474426, time = 0.7059130668640137\n",
      "Training at step=18, batch=480, train loss = 0.3969259560108185, train acc = 0.8500000238418579, time = 0.7071328163146973\n",
      "Testing at step=18, batch=0, test loss = 0.317615270614624, test acc = 0.8799999952316284, time = 0.21545648574829102\n",
      "Testing at step=18, batch=20, test loss = 0.5479483604431152, test acc = 0.7799999713897705, time = 0.14492154121398926\n",
      "Testing at step=18, batch=40, test loss = 0.36460959911346436, test acc = 0.8500000238418579, time = 0.1477818489074707\n",
      "Testing at step=18, batch=60, test loss = 0.4522160589694977, test acc = 0.8100000023841858, time = 0.14759206771850586\n",
      "Testing at step=18, batch=80, test loss = 0.49114975333213806, test acc = 0.8299999833106995, time = 0.14682412147521973\n",
      "Step 18 finished in 458.51345229148865, Train loss = 0.4554745694249868, Test loss = 0.48436667650938037; Train Acc = 0.8384833318988482, Test Acc = 0.8265999960899353\n",
      "Training at step=19, batch=0, train loss = 0.5536739230155945, train acc = 0.7699999809265137, time = 0.7107799053192139\n",
      "Training at step=19, batch=120, train loss = 0.3789885640144348, train acc = 0.8600000143051147, time = 0.6914527416229248\n",
      "Training at step=19, batch=240, train loss = 0.5599356293678284, train acc = 0.8299999833106995, time = 0.6991264820098877\n",
      "Training at step=19, batch=360, train loss = 0.4348159730434418, train acc = 0.8199999928474426, time = 0.6991987228393555\n",
      "Training at step=19, batch=480, train loss = 0.43253958225250244, train acc = 0.8799999952316284, time = 0.7052955627441406\n",
      "Testing at step=19, batch=0, test loss = 0.6611605882644653, test acc = 0.8199999928474426, time = 0.21474337577819824\n",
      "Testing at step=19, batch=20, test loss = 0.4191944897174835, test acc = 0.8299999833106995, time = 0.14136171340942383\n",
      "Testing at step=19, batch=40, test loss = 0.661003589630127, test acc = 0.7900000214576721, time = 0.1415700912475586\n",
      "Testing at step=19, batch=60, test loss = 0.41524994373321533, test acc = 0.8100000023841858, time = 0.4188981056213379\n",
      "Testing at step=19, batch=80, test loss = 0.4027870297431946, test acc = 0.8600000143051147, time = 0.4168236255645752\n",
      "Step 19 finished in 458.5296814441681, Train loss = 0.45384931708375614, Test loss = 0.5046708369255066; Train Acc = 0.8406499988834063, Test Acc = 0.816099995970726\n",
      "Training at step=20, batch=0, train loss = 0.5167936086654663, train acc = 0.7799999713897705, time = 0.7205643653869629\n",
      "Training at step=20, batch=120, train loss = 0.4453178644180298, train acc = 0.8399999737739563, time = 0.7056465148925781\n",
      "Training at step=20, batch=240, train loss = 0.3049720525741577, train acc = 0.8899999856948853, time = 0.7051610946655273\n",
      "Training at step=20, batch=360, train loss = 0.7020414471626282, train acc = 0.800000011920929, time = 0.6993224620819092\n",
      "Training at step=20, batch=480, train loss = 0.3691588342189789, train acc = 0.8100000023841858, time = 0.7050933837890625\n",
      "Testing at step=20, batch=0, test loss = 0.4294397830963135, test acc = 0.7900000214576721, time = 0.22243142127990723\n",
      "Testing at step=20, batch=20, test loss = 0.4061073362827301, test acc = 0.8600000143051147, time = 0.14498186111450195\n",
      "Testing at step=20, batch=40, test loss = 0.5187147855758667, test acc = 0.8299999833106995, time = 0.1479802131652832\n",
      "Testing at step=20, batch=60, test loss = 0.47814705967903137, test acc = 0.8299999833106995, time = 0.14211750030517578\n",
      "Testing at step=20, batch=80, test loss = 0.4875328540802002, test acc = 0.800000011920929, time = 0.14731788635253906\n",
      "Step 20 finished in 456.6799235343933, Train loss = 0.4490342014282942, Test loss = 0.4853870651125908; Train Acc = 0.8432333317399024, Test Acc = 0.8273999977111817\n",
      "Training at step=21, batch=0, train loss = 0.45767566561698914, train acc = 0.8100000023841858, time = 0.700080394744873\n",
      "Training at step=21, batch=120, train loss = 0.409812331199646, train acc = 0.8600000143051147, time = 0.6851084232330322\n",
      "Training at step=21, batch=240, train loss = 0.48901766538619995, train acc = 0.8399999737739563, time = 0.704493522644043\n",
      "Training at step=21, batch=360, train loss = 0.4023078978061676, train acc = 0.8600000143051147, time = 0.705888032913208\n",
      "Training at step=21, batch=480, train loss = 0.3996330499649048, train acc = 0.8799999952316284, time = 0.70278000831604\n",
      "Testing at step=21, batch=0, test loss = 0.4382367432117462, test acc = 0.8700000047683716, time = 0.21632075309753418\n",
      "Testing at step=21, batch=20, test loss = 0.5659179091453552, test acc = 0.800000011920929, time = 0.13734197616577148\n",
      "Testing at step=21, batch=40, test loss = 0.49220508337020874, test acc = 0.8100000023841858, time = 0.1375565528869629\n",
      "Testing at step=21, batch=60, test loss = 0.5118113160133362, test acc = 0.8199999928474426, time = 0.14119267463684082\n",
      "Testing at step=21, batch=80, test loss = 0.5524092316627502, test acc = 0.8199999928474426, time = 0.1406419277191162\n",
      "Step 21 finished in 455.83018016815186, Train loss = 0.4486587506781022, Test loss = 0.47772032260894776; Train Acc = 0.8429333332180977, Test Acc = 0.8350000011920929\n",
      "Training at step=22, batch=0, train loss = 0.3313891291618347, train acc = 0.8700000047683716, time = 0.7077314853668213\n",
      "Training at step=22, batch=120, train loss = 0.44395145773887634, train acc = 0.8700000047683716, time = 0.7059378623962402\n",
      "Training at step=22, batch=240, train loss = 0.34258899092674255, train acc = 0.8500000238418579, time = 0.7048676013946533\n",
      "Training at step=22, batch=360, train loss = 0.5430681705474854, train acc = 0.800000011920929, time = 0.732072114944458\n",
      "Training at step=22, batch=480, train loss = 0.4465436637401581, train acc = 0.8299999833106995, time = 0.7332925796508789\n",
      "Testing at step=22, batch=0, test loss = 0.5940528512001038, test acc = 0.7599999904632568, time = 0.22232341766357422\n",
      "Testing at step=22, batch=20, test loss = 0.3863440454006195, test acc = 0.8799999952316284, time = 0.1502392292022705\n",
      "Testing at step=22, batch=40, test loss = 0.4899102747440338, test acc = 0.8299999833106995, time = 0.14190983772277832\n",
      "Testing at step=22, batch=60, test loss = 0.5265815854072571, test acc = 0.8100000023841858, time = 0.1501779556274414\n",
      "Testing at step=22, batch=80, test loss = 0.6827204823493958, test acc = 0.8100000023841858, time = 0.1512453556060791\n",
      "Step 22 finished in 463.50418758392334, Train loss = 0.4491105587035418, Test loss = 0.5562531456351281; Train Acc = 0.8425999995072683, Test Acc = 0.8115999990701676\n",
      "Training at step=23, batch=0, train loss = 0.373381644487381, train acc = 0.8700000047683716, time = 0.7258889675140381\n",
      "Training at step=23, batch=120, train loss = 0.49694663286209106, train acc = 0.8600000143051147, time = 0.7041523456573486\n",
      "Training at step=23, batch=240, train loss = 0.5928541421890259, train acc = 0.7799999713897705, time = 0.7054927349090576\n",
      "Training at step=23, batch=360, train loss = 0.3851701319217682, train acc = 0.8700000047683716, time = 0.7057764530181885\n",
      "Training at step=23, batch=480, train loss = 0.5011603832244873, train acc = 0.8399999737739563, time = 0.7033603191375732\n",
      "Testing at step=23, batch=0, test loss = 0.3915104269981384, test acc = 0.8700000047683716, time = 0.2162337303161621\n",
      "Testing at step=23, batch=20, test loss = 0.43997547030448914, test acc = 0.8899999856948853, time = 0.14214348793029785\n",
      "Testing at step=23, batch=40, test loss = 0.45554888248443604, test acc = 0.8199999928474426, time = 0.14019393920898438\n",
      "Testing at step=23, batch=60, test loss = 0.5792999863624573, test acc = 0.8399999737739563, time = 0.14200067520141602\n",
      "Testing at step=23, batch=80, test loss = 0.4933737516403198, test acc = 0.8600000143051147, time = 0.14777302742004395\n",
      "Step 23 finished in 456.847975730896, Train loss = 0.44988846031328045, Test loss = 0.4779954707622528; Train Acc = 0.8422333320975304, Test Acc = 0.8309999984502793\n",
      "Training at step=24, batch=0, train loss = 0.38782593607902527, train acc = 0.8700000047683716, time = 0.7065892219543457\n",
      "Training at step=24, batch=120, train loss = 0.7070816159248352, train acc = 0.7799999713897705, time = 0.7053380012512207\n",
      "Training at step=24, batch=240, train loss = 0.44507163763046265, train acc = 0.8500000238418579, time = 0.7053661346435547\n",
      "Training at step=24, batch=360, train loss = 0.44560471177101135, train acc = 0.8100000023841858, time = 0.7039766311645508\n",
      "Training at step=24, batch=480, train loss = 0.42132118344306946, train acc = 0.8799999952316284, time = 0.7039170265197754\n",
      "Testing at step=24, batch=0, test loss = 0.5291650295257568, test acc = 0.8600000143051147, time = 0.20711994171142578\n",
      "Testing at step=24, batch=20, test loss = 0.5094763040542603, test acc = 0.7799999713897705, time = 0.1418313980102539\n",
      "Testing at step=24, batch=40, test loss = 0.49438396096229553, test acc = 0.8500000238418579, time = 0.14365744590759277\n",
      "Testing at step=24, batch=60, test loss = 0.6199904680252075, test acc = 0.7799999713897705, time = 0.14188504219055176\n",
      "Testing at step=24, batch=80, test loss = 0.4084780216217041, test acc = 0.8600000143051147, time = 0.1419672966003418\n",
      "Step 24 finished in 456.1104245185852, Train loss = 0.4473826499531666, Test loss = 0.46263786673545837; Train Acc = 0.843166664938132, Test Acc = 0.8373999965190887\n",
      "Training at step=25, batch=0, train loss = 0.529517650604248, train acc = 0.7699999809265137, time = 0.7115342617034912\n",
      "Training at step=25, batch=120, train loss = 0.5825998783111572, train acc = 0.8100000023841858, time = 0.7047317028045654\n",
      "Training at step=25, batch=240, train loss = 0.35570746660232544, train acc = 0.8999999761581421, time = 0.70570969581604\n",
      "Training at step=25, batch=360, train loss = 0.4182649552822113, train acc = 0.8700000047683716, time = 0.6976146697998047\n",
      "Training at step=25, batch=480, train loss = 0.45445698499679565, train acc = 0.8799999952316284, time = 0.7019665241241455\n",
      "Testing at step=25, batch=0, test loss = 0.4641473889350891, test acc = 0.8299999833106995, time = 0.21703600883483887\n",
      "Testing at step=25, batch=20, test loss = 0.4268237352371216, test acc = 0.8299999833106995, time = 0.14723825454711914\n",
      "Testing at step=25, batch=40, test loss = 0.5594601631164551, test acc = 0.8399999737739563, time = 0.14480018615722656\n",
      "Testing at step=25, batch=60, test loss = 0.5305129885673523, test acc = 0.8500000238418579, time = 0.1324901580810547\n",
      "Testing at step=25, batch=80, test loss = 0.370148241519928, test acc = 0.8700000047683716, time = 0.14181041717529297\n",
      "Step 25 finished in 456.12342858314514, Train loss = 0.4451936722298463, Test loss = 0.467105772793293; Train Acc = 0.843499998251597, Test Acc = 0.8337999969720841\n",
      "Training at step=26, batch=0, train loss = 0.25615912675857544, train acc = 0.9399999976158142, time = 0.7046604156494141\n",
      "Training at step=26, batch=120, train loss = 0.5983080267906189, train acc = 0.8199999928474426, time = 0.7036929130554199\n",
      "Training at step=26, batch=240, train loss = 0.46842700242996216, train acc = 0.8700000047683716, time = 0.710961103439331\n",
      "Training at step=26, batch=360, train loss = 0.26218193769454956, train acc = 0.9399999976158142, time = 0.7045762538909912\n",
      "Training at step=26, batch=480, train loss = 0.3243862986564636, train acc = 0.8899999856948853, time = 0.7048490047454834\n",
      "Testing at step=26, batch=0, test loss = 0.3783702850341797, test acc = 0.8399999737739563, time = 0.24723172187805176\n",
      "Testing at step=26, batch=20, test loss = 0.5155704617500305, test acc = 0.8199999928474426, time = 0.14186882972717285\n",
      "Testing at step=26, batch=40, test loss = 0.38912665843963623, test acc = 0.8700000047683716, time = 0.13950657844543457\n",
      "Testing at step=26, batch=60, test loss = 0.46592071652412415, test acc = 0.8299999833106995, time = 0.1418929100036621\n",
      "Testing at step=26, batch=80, test loss = 0.364628404378891, test acc = 0.8999999761581421, time = 0.14484190940856934\n",
      "Step 26 finished in 456.26277780532837, Train loss = 0.4429330095897118, Test loss = 0.484149377644062; Train Acc = 0.8444333323836326, Test Acc = 0.8311000001430512\n",
      "Training at step=27, batch=0, train loss = 0.3683125376701355, train acc = 0.8500000238418579, time = 0.7139670848846436\n",
      "Training at step=27, batch=120, train loss = 0.4876323938369751, train acc = 0.8199999928474426, time = 0.7127468585968018\n",
      "Training at step=27, batch=240, train loss = 0.5204612612724304, train acc = 0.8100000023841858, time = 0.7033767700195312\n",
      "Training at step=27, batch=360, train loss = 0.42435309290885925, train acc = 0.8799999952316284, time = 0.7042324542999268\n",
      "Training at step=27, batch=480, train loss = 0.3133281469345093, train acc = 0.8700000047683716, time = 0.7045350074768066\n",
      "Testing at step=27, batch=0, test loss = 0.5098335146903992, test acc = 0.8299999833106995, time = 0.3033723831176758\n",
      "Testing at step=27, batch=20, test loss = 0.3995729088783264, test acc = 0.8299999833106995, time = 0.14331316947937012\n",
      "Testing at step=27, batch=40, test loss = 0.5187602043151855, test acc = 0.7900000214576721, time = 0.13831782341003418\n",
      "Testing at step=27, batch=60, test loss = 0.35218706727027893, test acc = 0.8299999833106995, time = 0.13960719108581543\n",
      "Testing at step=27, batch=80, test loss = 0.713105320930481, test acc = 0.7200000286102295, time = 0.1415398120880127\n",
      "Step 27 finished in 454.5793948173523, Train loss = 0.43912770253916583, Test loss = 0.49261617958545684; Train Acc = 0.846066665649414, Test Acc = 0.8242999970912933\n",
      "Training at step=28, batch=0, train loss = 0.4008989632129669, train acc = 0.8500000238418579, time = 0.689687967300415\n",
      "Training at step=28, batch=120, train loss = 0.5200506448745728, train acc = 0.8399999737739563, time = 0.7039716243743896\n",
      "Training at step=28, batch=240, train loss = 0.40780922770500183, train acc = 0.8299999833106995, time = 0.708855152130127\n",
      "Training at step=28, batch=360, train loss = 0.5118582248687744, train acc = 0.8500000238418579, time = 0.7043006420135498\n",
      "Training at step=28, batch=480, train loss = 0.495276540517807, train acc = 0.8600000143051147, time = 0.7053260803222656\n",
      "Testing at step=28, batch=0, test loss = 0.36517632007598877, test acc = 0.8799999952316284, time = 0.21582508087158203\n",
      "Testing at step=28, batch=20, test loss = 0.513538122177124, test acc = 0.8500000238418579, time = 0.14725828170776367\n",
      "Testing at step=28, batch=40, test loss = 0.5758606195449829, test acc = 0.800000011920929, time = 0.14636921882629395\n",
      "Testing at step=28, batch=60, test loss = 0.6866488456726074, test acc = 0.7699999809265137, time = 0.14620304107666016\n",
      "Testing at step=28, batch=80, test loss = 0.30865803360939026, test acc = 0.8700000047683716, time = 0.13512611389160156\n",
      "Step 28 finished in 456.38975739479065, Train loss = 0.44342311975856624, Test loss = 0.4731666254997253; Train Acc = 0.8432833329836528, Test Acc = 0.8357000005245209\n",
      "Training at step=29, batch=0, train loss = 0.4678434729576111, train acc = 0.8399999737739563, time = 0.7145318984985352\n",
      "Training at step=29, batch=120, train loss = 0.42941582202911377, train acc = 0.8500000238418579, time = 0.7318761348724365\n",
      "Training at step=29, batch=240, train loss = 0.5496862530708313, train acc = 0.8100000023841858, time = 0.73223876953125\n",
      "Training at step=29, batch=360, train loss = 0.41414016485214233, train acc = 0.8299999833106995, time = 0.7322630882263184\n",
      "Training at step=29, batch=480, train loss = 0.42386317253112793, train acc = 0.8100000023841858, time = 0.7324192523956299\n",
      "Testing at step=29, batch=0, test loss = 0.31632164120674133, test acc = 0.8899999856948853, time = 0.35666966438293457\n",
      "Testing at step=29, batch=20, test loss = 0.5689898133277893, test acc = 0.7799999713897705, time = 0.4168057441711426\n",
      "Testing at step=29, batch=40, test loss = 0.4202044606208801, test acc = 0.8500000238418579, time = 0.4168989658355713\n",
      "Testing at step=29, batch=60, test loss = 0.37404167652130127, test acc = 0.8399999737739563, time = 0.4175252914428711\n",
      "Testing at step=29, batch=80, test loss = 0.3408221900463104, test acc = 0.8799999952316284, time = 0.41686034202575684\n",
      "Step 29 finished in 466.72936630249023, Train loss = 0.4376265976577997, Test loss = 0.48335735231637955; Train Acc = 0.8463166654109955, Test Acc = 0.828999997973442\n",
      "Training at step=30, batch=0, train loss = 0.2806750237941742, train acc = 0.9100000262260437, time = 0.7215571403503418\n",
      "Training at step=30, batch=120, train loss = 0.26405447721481323, train acc = 0.9100000262260437, time = 0.7045116424560547\n",
      "Training at step=30, batch=240, train loss = 0.31332796812057495, train acc = 0.8700000047683716, time = 0.7022838592529297\n",
      "Training at step=30, batch=360, train loss = 0.7367913126945496, train acc = 0.7900000214576721, time = 0.7043204307556152\n",
      "Training at step=30, batch=480, train loss = 0.4506540298461914, train acc = 0.8199999928474426, time = 0.7045409679412842\n",
      "Testing at step=30, batch=0, test loss = 0.45435887575149536, test acc = 0.8399999737739563, time = 0.4223935604095459\n",
      "Testing at step=30, batch=20, test loss = 0.5460830926895142, test acc = 0.7900000214576721, time = 0.4174511432647705\n",
      "Testing at step=30, batch=40, test loss = 0.5668935179710388, test acc = 0.7699999809265137, time = 0.4180276393890381\n",
      "Testing at step=30, batch=60, test loss = 0.4520436227321625, test acc = 0.8500000238418579, time = 0.41798901557922363\n",
      "Testing at step=30, batch=80, test loss = 0.5555583834648132, test acc = 0.8100000023841858, time = 0.4177565574645996\n",
      "Step 30 finished in 456.4439055919647, Train loss = 0.43776731145878633, Test loss = 0.46058255940675735; Train Acc = 0.8467166646321614, Test Acc = 0.8374999994039536\n",
      "Training at step=31, batch=0, train loss = 0.30470213294029236, train acc = 0.8999999761581421, time = 0.7223272323608398\n",
      "Training at step=31, batch=120, train loss = 0.31679224967956543, train acc = 0.9200000166893005, time = 0.7065556049346924\n",
      "Training at step=31, batch=240, train loss = 0.43138325214385986, train acc = 0.8600000143051147, time = 0.70418381690979\n",
      "Training at step=31, batch=360, train loss = 0.43316590785980225, train acc = 0.8500000238418579, time = 0.7127463817596436\n",
      "Training at step=31, batch=480, train loss = 0.5291745662689209, train acc = 0.800000011920929, time = 0.704218864440918\n",
      "Testing at step=31, batch=0, test loss = 0.5924100279808044, test acc = 0.8199999928474426, time = 0.21728920936584473\n",
      "Testing at step=31, batch=20, test loss = 0.3225539028644562, test acc = 0.8999999761581421, time = 0.14713025093078613\n",
      "Testing at step=31, batch=40, test loss = 0.5708610415458679, test acc = 0.7900000214576721, time = 0.14899516105651855\n",
      "Testing at step=31, batch=60, test loss = 0.5635886192321777, test acc = 0.8500000238418579, time = 0.15101838111877441\n",
      "Testing at step=31, batch=80, test loss = 0.4290068447589874, test acc = 0.8799999952316284, time = 0.4174938201904297\n",
      "Step 31 finished in 456.5207221508026, Train loss = 0.4363722864041726, Test loss = 0.488566555082798; Train Acc = 0.8473333326975504, Test Acc = 0.8291999977827073\n",
      "Training at step=32, batch=0, train loss = 0.3290497958660126, train acc = 0.8999999761581421, time = 0.7216629981994629\n",
      "Training at step=32, batch=120, train loss = 0.5213116407394409, train acc = 0.8500000238418579, time = 0.7035036087036133\n",
      "Training at step=32, batch=240, train loss = 0.4175311028957367, train acc = 0.8700000047683716, time = 0.7055585384368896\n",
      "Training at step=32, batch=360, train loss = 0.3808581531047821, train acc = 0.8999999761581421, time = 0.6913549900054932\n",
      "Training at step=32, batch=480, train loss = 0.6284018754959106, train acc = 0.7599999904632568, time = 0.7057979106903076\n",
      "Testing at step=32, batch=0, test loss = 0.7304959297180176, test acc = 0.7799999713897705, time = 0.21592235565185547\n",
      "Testing at step=32, batch=20, test loss = 0.5167315006256104, test acc = 0.8500000238418579, time = 0.14731907844543457\n",
      "Testing at step=32, batch=40, test loss = 0.38931483030319214, test acc = 0.8899999856948853, time = 0.1466658115386963\n",
      "Testing at step=32, batch=60, test loss = 0.4637361764907837, test acc = 0.8100000023841858, time = 0.14542770385742188\n",
      "Testing at step=32, batch=80, test loss = 0.3017677068710327, test acc = 0.9200000166893005, time = 0.1440582275390625\n",
      "Step 32 finished in 455.9878327846527, Train loss = 0.436791727344195, Test loss = 0.4662397062778473; Train Acc = 0.8470499994357427, Test Acc = 0.8399999982118607\n",
      "Training at step=33, batch=0, train loss = 0.29713204503059387, train acc = 0.8799999952316284, time = 0.7083184719085693\n",
      "Training at step=33, batch=120, train loss = 0.422793984413147, train acc = 0.8500000238418579, time = 0.6574835777282715\n",
      "Training at step=33, batch=240, train loss = 0.33550944924354553, train acc = 0.8899999856948853, time = 0.7053887844085693\n",
      "Training at step=33, batch=360, train loss = 0.4398644268512726, train acc = 0.8399999737739563, time = 0.7036423683166504\n",
      "Training at step=33, batch=480, train loss = 0.3523654043674469, train acc = 0.8600000143051147, time = 0.7045023441314697\n",
      "Testing at step=33, batch=0, test loss = 0.6963317394256592, test acc = 0.7599999904632568, time = 0.21713590621948242\n",
      "Testing at step=33, batch=20, test loss = 0.3382833003997803, test acc = 0.8799999952316284, time = 0.1420912742614746\n",
      "Testing at step=33, batch=40, test loss = 0.4500652253627777, test acc = 0.8700000047683716, time = 0.1427013874053955\n",
      "Testing at step=33, batch=60, test loss = 0.5443277955055237, test acc = 0.7200000286102295, time = 0.149308443069458\n",
      "Testing at step=33, batch=80, test loss = 0.5303916335105896, test acc = 0.8199999928474426, time = 0.1485428810119629\n",
      "Step 33 finished in 458.26359391212463, Train loss = 0.4349016021688779, Test loss = 0.4644082233309746; Train Acc = 0.8468166659275691, Test Acc = 0.8320000004768372\n",
      "Training at step=34, batch=0, train loss = 0.3782455325126648, train acc = 0.8500000238418579, time = 0.7124452590942383\n",
      "Training at step=34, batch=120, train loss = 0.32433560490608215, train acc = 0.9200000166893005, time = 0.7048468589782715\n",
      "Training at step=34, batch=240, train loss = 0.464836984872818, train acc = 0.7900000214576721, time = 0.7049236297607422\n",
      "Training at step=34, batch=360, train loss = 0.3293117582798004, train acc = 0.8899999856948853, time = 0.7039375305175781\n",
      "Training at step=34, batch=480, train loss = 0.36566588282585144, train acc = 0.8500000238418579, time = 0.7053661346435547\n",
      "Testing at step=34, batch=0, test loss = 0.550486147403717, test acc = 0.8299999833106995, time = 0.41751527786254883\n",
      "Testing at step=34, batch=20, test loss = 0.49235087633132935, test acc = 0.8199999928474426, time = 0.4173104763031006\n",
      "Testing at step=34, batch=40, test loss = 0.4693759083747864, test acc = 0.8600000143051147, time = 0.41739535331726074\n",
      "Testing at step=34, batch=60, test loss = 0.3332579731941223, test acc = 0.8799999952316284, time = 0.41719508171081543\n",
      "Testing at step=34, batch=80, test loss = 0.42439815402030945, test acc = 0.8299999833106995, time = 0.4182271957397461\n",
      "Step 34 finished in 455.67069363594055, Train loss = 0.43478305655221144, Test loss = 0.48284188866615296; Train Acc = 0.8479499991734822, Test Acc = 0.8305999946594238\n",
      "Training at step=35, batch=0, train loss = 0.4655101001262665, train acc = 0.8199999928474426, time = 0.722346305847168\n",
      "Training at step=35, batch=120, train loss = 0.4858068823814392, train acc = 0.8199999928474426, time = 0.7052502632141113\n",
      "Training at step=35, batch=240, train loss = 0.45450690388679504, train acc = 0.8799999952316284, time = 0.7047193050384521\n",
      "Training at step=35, batch=360, train loss = 0.383757084608078, train acc = 0.8700000047683716, time = 0.705322265625\n",
      "Training at step=35, batch=480, train loss = 0.4667462110519409, train acc = 0.8199999928474426, time = 0.7045083045959473\n",
      "Testing at step=35, batch=0, test loss = 0.6748159527778625, test acc = 0.8100000023841858, time = 0.20837950706481934\n",
      "Testing at step=35, batch=20, test loss = 0.5571056008338928, test acc = 0.800000011920929, time = 0.14805126190185547\n",
      "Testing at step=35, batch=40, test loss = 0.37650200724601746, test acc = 0.8700000047683716, time = 0.1461653709411621\n",
      "Testing at step=35, batch=60, test loss = 0.5162479877471924, test acc = 0.8199999928474426, time = 0.14205408096313477\n",
      "Testing at step=35, batch=80, test loss = 0.4442530870437622, test acc = 0.8299999833106995, time = 0.14736604690551758\n",
      "Step 35 finished in 456.40640354156494, Train loss = 0.43624265380203725, Test loss = 0.4675870920717716; Train Acc = 0.8477166652679443, Test Acc = 0.8357999992370605\n",
      "Training at step=36, batch=0, train loss = 0.366542249917984, train acc = 0.8700000047683716, time = 0.7280638217926025\n",
      "Training at step=36, batch=120, train loss = 0.342003732919693, train acc = 0.8999999761581421, time = 0.7055497169494629\n",
      "Training at step=36, batch=240, train loss = 0.3357591927051544, train acc = 0.8799999952316284, time = 0.7060413360595703\n",
      "Training at step=36, batch=360, train loss = 0.41206809878349304, train acc = 0.8700000047683716, time = 0.7032546997070312\n",
      "Training at step=36, batch=480, train loss = 0.43420010805130005, train acc = 0.8299999833106995, time = 0.7042891979217529\n",
      "Testing at step=36, batch=0, test loss = 0.4706283509731293, test acc = 0.8500000238418579, time = 0.21592164039611816\n",
      "Testing at step=36, batch=20, test loss = 0.4008530080318451, test acc = 0.8799999952316284, time = 0.1410682201385498\n",
      "Testing at step=36, batch=40, test loss = 0.6133649349212646, test acc = 0.8100000023841858, time = 0.14916276931762695\n",
      "Testing at step=36, batch=60, test loss = 0.416767954826355, test acc = 0.8399999737739563, time = 0.1431276798248291\n",
      "Testing at step=36, batch=80, test loss = 0.5687324404716492, test acc = 0.7900000214576721, time = 0.1405348777770996\n",
      "Step 36 finished in 457.53054213523865, Train loss = 0.43390074945986273, Test loss = 0.45893566340208053; Train Acc = 0.846633332769076, Test Acc = 0.838999999165535\n",
      "Training at step=37, batch=0, train loss = 0.3674776554107666, train acc = 0.8799999952316284, time = 0.721806526184082\n",
      "Training at step=37, batch=120, train loss = 0.4540349543094635, train acc = 0.8899999856948853, time = 0.7039871215820312\n",
      "Training at step=37, batch=240, train loss = 0.5403060913085938, train acc = 0.8100000023841858, time = 0.7033710479736328\n",
      "Training at step=37, batch=360, train loss = 0.4153043031692505, train acc = 0.8600000143051147, time = 0.7055912017822266\n",
      "Training at step=37, batch=480, train loss = 0.36377134919166565, train acc = 0.8899999856948853, time = 0.7049770355224609\n",
      "Testing at step=37, batch=0, test loss = 0.6297474503517151, test acc = 0.8199999928474426, time = 0.25806641578674316\n",
      "Testing at step=37, batch=20, test loss = 0.4905925691127777, test acc = 0.8399999737739563, time = 0.14615702629089355\n",
      "Testing at step=37, batch=40, test loss = 0.3473035395145416, test acc = 0.8999999761581421, time = 0.14468073844909668\n",
      "Testing at step=37, batch=60, test loss = 0.2944945991039276, test acc = 0.8899999856948853, time = 0.14145946502685547\n",
      "Testing at step=37, batch=80, test loss = 0.5803104639053345, test acc = 0.8299999833106995, time = 0.41776180267333984\n",
      "Step 37 finished in 456.12613224983215, Train loss = 0.4306685869395733, Test loss = 0.4903254386782646; Train Acc = 0.84849999944369, Test Acc = 0.8284999984502792\n",
      "Training at step=38, batch=0, train loss = 0.5274378657341003, train acc = 0.8100000023841858, time = 0.7159297466278076\n",
      "Training at step=38, batch=120, train loss = 0.46400538086891174, train acc = 0.8299999833106995, time = 0.7072865962982178\n",
      "Training at step=38, batch=240, train loss = 0.4528145492076874, train acc = 0.8199999928474426, time = 0.7066514492034912\n",
      "Training at step=38, batch=360, train loss = 0.31429415941238403, train acc = 0.8999999761581421, time = 0.705878496170044\n",
      "Training at step=38, batch=480, train loss = 0.47686442732810974, train acc = 0.8600000143051147, time = 0.7027935981750488\n",
      "Testing at step=38, batch=0, test loss = 0.6599875092506409, test acc = 0.7799999713897705, time = 0.4243454933166504\n",
      "Testing at step=38, batch=20, test loss = 0.4511227309703827, test acc = 0.8700000047683716, time = 0.4171600341796875\n",
      "Testing at step=38, batch=40, test loss = 0.3654552698135376, test acc = 0.8700000047683716, time = 0.4163095951080322\n",
      "Testing at step=38, batch=60, test loss = 0.3334263861179352, test acc = 0.8700000047683716, time = 0.4163784980773926\n",
      "Testing at step=38, batch=80, test loss = 0.3991096019744873, test acc = 0.8500000238418579, time = 0.13445329666137695\n",
      "Step 38 finished in 456.6781828403473, Train loss = 0.4334530645608902, Test loss = 0.46400021731853486; Train Acc = 0.8468999988834063, Test Acc = 0.8374999988079072\n",
      "Training at step=39, batch=0, train loss = 0.2972504794597626, train acc = 0.8799999952316284, time = 0.6963362693786621\n",
      "Training at step=39, batch=120, train loss = 0.36793527007102966, train acc = 0.8500000238418579, time = 0.7057380676269531\n",
      "Training at step=39, batch=240, train loss = 0.36453086137771606, train acc = 0.8799999952316284, time = 0.702164888381958\n",
      "Training at step=39, batch=360, train loss = 0.4128932058811188, train acc = 0.800000011920929, time = 0.7059056758880615\n",
      "Training at step=39, batch=480, train loss = 0.4283325970172882, train acc = 0.8399999737739563, time = 0.7066864967346191\n",
      "Testing at step=39, batch=0, test loss = 0.5703340172767639, test acc = 0.7900000214576721, time = 0.18444395065307617\n",
      "Testing at step=39, batch=20, test loss = 0.40763285756111145, test acc = 0.8100000023841858, time = 0.14302420616149902\n",
      "Testing at step=39, batch=40, test loss = 0.6846544742584229, test acc = 0.8199999928474426, time = 0.13930296897888184\n",
      "Testing at step=39, batch=60, test loss = 0.47387078404426575, test acc = 0.8299999833106995, time = 0.1482830047607422\n",
      "Testing at step=39, batch=80, test loss = 0.6608421802520752, test acc = 0.7799999713897705, time = 0.13835954666137695\n",
      "Step 39 finished in 455.9381182193756, Train loss = 0.433488116239508, Test loss = 0.48353770494461057; Train Acc = 0.8469333307941754, Test Acc = 0.831700000166893\n",
      "Training at step=40, batch=0, train loss = 0.40194323658943176, train acc = 0.8500000238418579, time = 0.7112433910369873\n",
      "Training at step=40, batch=120, train loss = 0.5346889495849609, train acc = 0.8100000023841858, time = 0.7061128616333008\n",
      "Training at step=40, batch=240, train loss = 0.2687378525733948, train acc = 0.8999999761581421, time = 0.7052726745605469\n",
      "Training at step=40, batch=360, train loss = 0.4573705196380615, train acc = 0.8299999833106995, time = 0.7037060260772705\n",
      "Training at step=40, batch=480, train loss = 0.46227872371673584, train acc = 0.8299999833106995, time = 0.7045295238494873\n",
      "Testing at step=40, batch=0, test loss = 0.4948413372039795, test acc = 0.8199999928474426, time = 0.422121524810791\n",
      "Testing at step=40, batch=20, test loss = 0.3787209987640381, test acc = 0.8700000047683716, time = 0.4178340435028076\n",
      "Testing at step=40, batch=40, test loss = 0.5098829865455627, test acc = 0.8399999737739563, time = 0.41754770278930664\n",
      "Testing at step=40, batch=60, test loss = 0.4022981524467468, test acc = 0.8500000238418579, time = 0.4177687168121338\n",
      "Testing at step=40, batch=80, test loss = 0.4343763589859009, test acc = 0.8399999737739563, time = 0.416858434677124\n",
      "Step 40 finished in 456.3316595554352, Train loss = 0.43029877682526907, Test loss = 0.46263609200716016; Train Acc = 0.8493499987324079, Test Acc = 0.8343000000715256\n",
      "Training at step=41, batch=0, train loss = 0.337619811296463, train acc = 0.9399999976158142, time = 0.7092075347900391\n",
      "Training at step=41, batch=120, train loss = 0.25367432832717896, train acc = 0.9100000262260437, time = 0.7034029960632324\n",
      "Training at step=41, batch=240, train loss = 0.6024587750434875, train acc = 0.7900000214576721, time = 0.7041199207305908\n",
      "Training at step=41, batch=360, train loss = 0.45332372188568115, train acc = 0.8600000143051147, time = 0.7039239406585693\n",
      "Training at step=41, batch=480, train loss = 0.4911735951900482, train acc = 0.8600000143051147, time = 0.7048568725585938\n",
      "Testing at step=41, batch=0, test loss = 0.6149259805679321, test acc = 0.8199999928474426, time = 0.203660249710083\n",
      "Testing at step=41, batch=20, test loss = 0.490897536277771, test acc = 0.8100000023841858, time = 0.4171781539916992\n",
      "Testing at step=41, batch=40, test loss = 0.4095081388950348, test acc = 0.8500000238418579, time = 0.41709089279174805\n",
      "Testing at step=41, batch=60, test loss = 0.47291773557662964, test acc = 0.8899999856948853, time = 0.4178123474121094\n",
      "Testing at step=41, batch=80, test loss = 0.445402592420578, test acc = 0.8399999737739563, time = 0.41736364364624023\n",
      "Step 41 finished in 455.9295358657837, Train loss = 0.4281548057993253, Test loss = 0.4600663931667805; Train Acc = 0.8499333326021831, Test Acc = 0.8375999999046325\n",
      "Training at step=42, batch=0, train loss = 0.24382337927818298, train acc = 0.8999999761581421, time = 0.698408842086792\n",
      "Training at step=42, batch=120, train loss = 0.31576067209243774, train acc = 0.8799999952316284, time = 0.7038853168487549\n",
      "Training at step=42, batch=240, train loss = 0.5698180794715881, train acc = 0.800000011920929, time = 0.706226110458374\n",
      "Training at step=42, batch=360, train loss = 0.4311457574367523, train acc = 0.8600000143051147, time = 0.7101585865020752\n",
      "Training at step=42, batch=480, train loss = 0.3724265396595001, train acc = 0.8399999737739563, time = 0.7062528133392334\n",
      "Testing at step=42, batch=0, test loss = 0.4287187159061432, test acc = 0.8500000238418579, time = 0.21024179458618164\n",
      "Testing at step=42, batch=20, test loss = 0.40872469544410706, test acc = 0.8600000143051147, time = 0.14246511459350586\n",
      "Testing at step=42, batch=40, test loss = 0.42774349451065063, test acc = 0.8500000238418579, time = 0.141890287399292\n",
      "Testing at step=42, batch=60, test loss = 0.5405309200286865, test acc = 0.8299999833106995, time = 0.14438247680664062\n",
      "Testing at step=42, batch=80, test loss = 0.6246796250343323, test acc = 0.8199999928474426, time = 0.14612078666687012\n",
      "Step 42 finished in 455.71827149391174, Train loss = 0.43051576539874076, Test loss = 0.4785565859079361; Train Acc = 0.8488999993602435, Test Acc = 0.8348000001907349\n",
      "Training at step=43, batch=0, train loss = 0.3025488555431366, train acc = 0.8899999856948853, time = 0.7251451015472412\n",
      "Training at step=43, batch=120, train loss = 0.4151259958744049, train acc = 0.8299999833106995, time = 0.7328641414642334\n",
      "Training at step=43, batch=240, train loss = 0.4470871388912201, train acc = 0.8700000047683716, time = 0.6928164958953857\n",
      "Training at step=43, batch=360, train loss = 0.38235417008399963, train acc = 0.8700000047683716, time = 0.7050909996032715\n",
      "Training at step=43, batch=480, train loss = 0.5555179119110107, train acc = 0.8299999833106995, time = 0.7062692642211914\n",
      "Testing at step=43, batch=0, test loss = 0.48568665981292725, test acc = 0.8500000238418579, time = 0.2160351276397705\n",
      "Testing at step=43, batch=20, test loss = 0.47226840257644653, test acc = 0.8399999737739563, time = 0.13725543022155762\n",
      "Testing at step=43, batch=40, test loss = 0.3671504259109497, test acc = 0.8299999833106995, time = 0.1374211311340332\n",
      "Testing at step=43, batch=60, test loss = 0.3077840209007263, test acc = 0.9200000166893005, time = 0.1404421329498291\n",
      "Testing at step=43, batch=80, test loss = 0.4406183660030365, test acc = 0.8399999737739563, time = 0.33241963386535645\n",
      "Step 43 finished in 462.0984880924225, Train loss = 0.4288333298265934, Test loss = 0.49576325207948685; Train Acc = 0.8496833310524623, Test Acc = 0.8174999970197677\n",
      "Training at step=44, batch=0, train loss = 0.5980778336524963, train acc = 0.75, time = 0.7252092361450195\n",
      "Training at step=44, batch=120, train loss = 0.3042448163032532, train acc = 0.8600000143051147, time = 0.705265998840332\n",
      "Training at step=44, batch=240, train loss = 0.4502311944961548, train acc = 0.8700000047683716, time = 0.7054810523986816\n",
      "Training at step=44, batch=360, train loss = 0.23223252594470978, train acc = 0.9300000071525574, time = 0.7057473659515381\n",
      "Training at step=44, batch=480, train loss = 0.4185255169868469, train acc = 0.8199999928474426, time = 0.7048287391662598\n",
      "Testing at step=44, batch=0, test loss = 0.38968193531036377, test acc = 0.8600000143051147, time = 0.42361927032470703\n",
      "Testing at step=44, batch=20, test loss = 0.49860209226608276, test acc = 0.8299999833106995, time = 0.418612003326416\n",
      "Testing at step=44, batch=40, test loss = 0.46868419647216797, test acc = 0.8199999928474426, time = 0.4186704158782959\n",
      "Testing at step=44, batch=60, test loss = 0.49678799510002136, test acc = 0.8100000023841858, time = 0.41764211654663086\n",
      "Testing at step=44, batch=80, test loss = 0.42802292108535767, test acc = 0.8700000047683716, time = 0.4181396961212158\n",
      "Step 44 finished in 456.83735251426697, Train loss = 0.4247341588387887, Test loss = 0.4739836919307709; Train Acc = 0.8514833305279413, Test Acc = 0.8312999987602234\n",
      "Training at step=45, batch=0, train loss = 0.5127807855606079, train acc = 0.7900000214576721, time = 0.7249114513397217\n",
      "Training at step=45, batch=120, train loss = 0.5004675388336182, train acc = 0.8100000023841858, time = 0.7046763896942139\n",
      "Training at step=45, batch=240, train loss = 0.3923271596431732, train acc = 0.8199999928474426, time = 0.7053420543670654\n",
      "Training at step=45, batch=360, train loss = 0.35949480533599854, train acc = 0.8799999952316284, time = 0.7053003311157227\n",
      "Training at step=45, batch=480, train loss = 0.34469902515411377, train acc = 0.8899999856948853, time = 0.7017459869384766\n",
      "Testing at step=45, batch=0, test loss = 0.4249993562698364, test acc = 0.8399999737739563, time = 0.35656094551086426\n",
      "Testing at step=45, batch=20, test loss = 0.49592551589012146, test acc = 0.8500000238418579, time = 0.4163837432861328\n",
      "Testing at step=45, batch=40, test loss = 0.32569485902786255, test acc = 0.8899999856948853, time = 0.4173922538757324\n",
      "Testing at step=45, batch=60, test loss = 0.5685765743255615, test acc = 0.8100000023841858, time = 0.4178764820098877\n",
      "Testing at step=45, batch=80, test loss = 0.5733910202980042, test acc = 0.8299999833106995, time = 0.4177839756011963\n",
      "Step 45 finished in 455.778240442276, Train loss = 0.42462757356464864, Test loss = 0.46522341966629027; Train Acc = 0.8510333318511645, Test Acc = 0.8382000011205674\n",
      "Training at step=46, batch=0, train loss = 0.49039000272750854, train acc = 0.800000011920929, time = 0.7190148830413818\n",
      "Training at step=46, batch=120, train loss = 0.36420178413391113, train acc = 0.9100000262260437, time = 0.7059831619262695\n",
      "Training at step=46, batch=240, train loss = 0.3536510169506073, train acc = 0.8899999856948853, time = 0.7060337066650391\n",
      "Training at step=46, batch=360, train loss = 0.34290122985839844, train acc = 0.8799999952316284, time = 0.7038729190826416\n",
      "Training at step=46, batch=480, train loss = 0.5139844417572021, train acc = 0.7699999809265137, time = 0.7031993865966797\n",
      "Testing at step=46, batch=0, test loss = 0.3639072775840759, test acc = 0.8799999952316284, time = 0.2224414348602295\n",
      "Testing at step=46, batch=20, test loss = 0.42108994722366333, test acc = 0.8799999952316284, time = 0.14202284812927246\n",
      "Testing at step=46, batch=40, test loss = 0.39602547883987427, test acc = 0.8700000047683716, time = 0.14444684982299805\n",
      "Testing at step=46, batch=60, test loss = 0.5940569043159485, test acc = 0.8299999833106995, time = 0.13994383811950684\n",
      "Testing at step=46, batch=80, test loss = 0.5786557197570801, test acc = 0.7900000214576721, time = 0.14160418510437012\n",
      "Step 46 finished in 456.24431133270264, Train loss = 0.4252054889748494, Test loss = 0.4581734947860241; Train Acc = 0.8506166663765907, Test Acc = 0.8405999994277954\n",
      "Training at step=47, batch=0, train loss = 0.3661458492279053, train acc = 0.8600000143051147, time = 0.7248363494873047\n",
      "Training at step=47, batch=120, train loss = 0.6422917246818542, train acc = 0.7699999809265137, time = 0.7346303462982178\n",
      "Training at step=47, batch=240, train loss = 0.48622196912765503, train acc = 0.8100000023841858, time = 0.7320451736450195\n",
      "Training at step=47, batch=360, train loss = 0.5493990778923035, train acc = 0.8600000143051147, time = 0.7329905033111572\n",
      "Training at step=47, batch=480, train loss = 0.504666805267334, train acc = 0.8600000143051147, time = 0.6917893886566162\n",
      "Testing at step=47, batch=0, test loss = 0.3834351599216461, test acc = 0.8399999737739563, time = 0.22276687622070312\n",
      "Testing at step=47, batch=20, test loss = 0.5133270621299744, test acc = 0.8199999928474426, time = 0.1401059627532959\n",
      "Testing at step=47, batch=40, test loss = 0.5248197317123413, test acc = 0.8600000143051147, time = 0.14036178588867188\n",
      "Testing at step=47, batch=60, test loss = 0.4288289248943329, test acc = 0.8500000238418579, time = 0.14338088035583496\n",
      "Testing at step=47, batch=80, test loss = 0.5705165266990662, test acc = 0.8100000023841858, time = 0.14171171188354492\n",
      "Step 47 finished in 467.7352657318115, Train loss = 0.42447901556889217, Test loss = 0.46407617419958114; Train Acc = 0.8517333330710729, Test Acc = 0.8361999988555908\n",
      "Training at step=48, batch=0, train loss = 0.6242622137069702, train acc = 0.8399999737739563, time = 0.6960422992706299\n",
      "Training at step=48, batch=120, train loss = 0.49843543767929077, train acc = 0.8399999737739563, time = 0.7049784660339355\n",
      "Training at step=48, batch=240, train loss = 0.40513846278190613, train acc = 0.8600000143051147, time = 0.7047789096832275\n",
      "Training at step=48, batch=360, train loss = 0.45307186245918274, train acc = 0.8600000143051147, time = 0.7046470642089844\n",
      "Training at step=48, batch=480, train loss = 0.384185791015625, train acc = 0.8399999737739563, time = 0.7047398090362549\n",
      "Testing at step=48, batch=0, test loss = 0.4066540598869324, test acc = 0.8700000047683716, time = 0.2080531120300293\n",
      "Testing at step=48, batch=20, test loss = 0.4801803231239319, test acc = 0.8100000023841858, time = 0.14252448081970215\n",
      "Testing at step=48, batch=40, test loss = 0.5305193066596985, test acc = 0.7599999904632568, time = 0.1510932445526123\n",
      "Testing at step=48, batch=60, test loss = 0.4791277050971985, test acc = 0.7799999713897705, time = 0.1475210189819336\n",
      "Testing at step=48, batch=80, test loss = 0.5489516258239746, test acc = 0.75, time = 0.14949727058410645\n",
      "Step 48 finished in 456.03391456604004, Train loss = 0.4230356240272522, Test loss = 0.4627136406302452; Train Acc = 0.8516999994715054, Test Acc = 0.8320999991893768\n",
      "Training at step=49, batch=0, train loss = 0.4987051486968994, train acc = 0.8299999833106995, time = 0.7028119564056396\n",
      "Training at step=49, batch=120, train loss = 0.5869541764259338, train acc = 0.8100000023841858, time = 0.7063558101654053\n",
      "Training at step=49, batch=240, train loss = 0.43462321162223816, train acc = 0.8399999737739563, time = 0.7046108245849609\n",
      "Training at step=49, batch=360, train loss = 0.6140182018280029, train acc = 0.7799999713897705, time = 0.7041440010070801\n",
      "Training at step=49, batch=480, train loss = 0.3097210228443146, train acc = 0.8899999856948853, time = 0.7066788673400879\n",
      "Testing at step=49, batch=0, test loss = 0.44902098178863525, test acc = 0.8199999928474426, time = 0.26941585540771484\n",
      "Testing at step=49, batch=20, test loss = 0.36644408106803894, test acc = 0.8500000238418579, time = 0.137315034866333\n",
      "Testing at step=49, batch=40, test loss = 0.5323834419250488, test acc = 0.7799999713897705, time = 0.14262986183166504\n",
      "Testing at step=49, batch=60, test loss = 0.6696960926055908, test acc = 0.7699999809265137, time = 0.1434619426727295\n",
      "Testing at step=49, batch=80, test loss = 0.6231759786605835, test acc = 0.8100000023841858, time = 0.4175455570220947\n",
      "Step 49 finished in 455.9414179325104, Train loss = 0.4234515568117301, Test loss = 0.46379565328359607; Train Acc = 0.8524833324551583, Test Acc = 0.836800001859665\n",
      "Training at step=50, batch=0, train loss = 0.47538816928863525, train acc = 0.8199999928474426, time = 0.7245886325836182\n",
      "Training at step=50, batch=120, train loss = 0.7322688102722168, train acc = 0.8199999928474426, time = 0.7050366401672363\n",
      "Training at step=50, batch=240, train loss = 0.4385502338409424, train acc = 0.8600000143051147, time = 0.7044548988342285\n",
      "Training at step=50, batch=360, train loss = 0.2556154429912567, train acc = 0.9300000071525574, time = 0.7033114433288574\n",
      "Training at step=50, batch=480, train loss = 0.285334974527359, train acc = 0.9200000166893005, time = 0.7049882411956787\n",
      "Testing at step=50, batch=0, test loss = 0.4604271352291107, test acc = 0.7900000214576721, time = 0.1764659881591797\n",
      "Testing at step=50, batch=20, test loss = 0.4427977502346039, test acc = 0.8100000023841858, time = 0.14206790924072266\n",
      "Testing at step=50, batch=40, test loss = 0.5247276425361633, test acc = 0.8299999833106995, time = 0.14217686653137207\n",
      "Testing at step=50, batch=60, test loss = 0.44319820404052734, test acc = 0.8600000143051147, time = 0.1421647071838379\n",
      "Testing at step=50, batch=80, test loss = 0.5334923267364502, test acc = 0.7699999809265137, time = 0.13909268379211426\n",
      "Step 50 finished in 456.8969216346741, Train loss = 0.4218338900059462, Test loss = 0.4680379718542099; Train Acc = 0.852666666607062, Test Acc = 0.8328999954462052\n",
      "Training at step=51, batch=0, train loss = 0.45085984468460083, train acc = 0.8399999737739563, time = 0.7145600318908691\n",
      "Training at step=51, batch=120, train loss = 0.359986811876297, train acc = 0.8700000047683716, time = 0.7040107250213623\n",
      "Training at step=51, batch=240, train loss = 0.40591561794281006, train acc = 0.8500000238418579, time = 0.7044527530670166\n",
      "Training at step=51, batch=360, train loss = 0.5581965446472168, train acc = 0.8700000047683716, time = 0.7044961452484131\n",
      "Training at step=51, batch=480, train loss = 0.38013848662376404, train acc = 0.8899999856948853, time = 0.7044832706451416\n",
      "Testing at step=51, batch=0, test loss = 0.497714638710022, test acc = 0.8299999833106995, time = 0.19951868057250977\n",
      "Testing at step=51, batch=20, test loss = 0.41785064339637756, test acc = 0.8199999928474426, time = 0.14609670639038086\n",
      "Testing at step=51, batch=40, test loss = 0.5861868262290955, test acc = 0.7599999904632568, time = 0.14986729621887207\n",
      "Testing at step=51, batch=60, test loss = 0.685778021812439, test acc = 0.7699999809265137, time = 0.14437150955200195\n",
      "Testing at step=51, batch=80, test loss = 0.40887510776519775, test acc = 0.8500000238418579, time = 0.14195585250854492\n",
      "Step 51 finished in 455.93957328796387, Train loss = 0.42378160516421, Test loss = 0.4796094611287117; Train Acc = 0.8521499984463056, Test Acc = 0.8260999965667725\n",
      "Training at step=52, batch=0, train loss = 0.7177407145500183, train acc = 0.7300000190734863, time = 0.7096381187438965\n",
      "Training at step=52, batch=120, train loss = 0.30105260014533997, train acc = 0.8999999761581421, time = 0.7035248279571533\n",
      "Training at step=52, batch=240, train loss = 0.4571724236011505, train acc = 0.8600000143051147, time = 0.7036585807800293\n",
      "Training at step=52, batch=360, train loss = 0.21701852977275848, train acc = 0.9300000071525574, time = 0.7038257122039795\n",
      "Training at step=52, batch=480, train loss = 0.3846551775932312, train acc = 0.8700000047683716, time = 0.7045021057128906\n",
      "Testing at step=52, batch=0, test loss = 0.4668736159801483, test acc = 0.8199999928474426, time = 0.17519307136535645\n",
      "Testing at step=52, batch=20, test loss = 0.5306933522224426, test acc = 0.800000011920929, time = 0.14927124977111816\n",
      "Testing at step=52, batch=40, test loss = 0.43627825379371643, test acc = 0.8500000238418579, time = 0.1478571891784668\n",
      "Testing at step=52, batch=60, test loss = 0.5000165104866028, test acc = 0.8100000023841858, time = 0.15207386016845703\n",
      "Testing at step=52, batch=80, test loss = 0.2278645932674408, test acc = 0.8999999761581421, time = 0.14755535125732422\n",
      "Step 52 finished in 456.42340898513794, Train loss = 0.42141741221149764, Test loss = 0.46706772699952126; Train Acc = 0.8522999983032544, Test Acc = 0.8349999982118607\n",
      "Training at step=53, batch=0, train loss = 0.3188960552215576, train acc = 0.8999999761581421, time = 0.6985118389129639\n",
      "Training at step=53, batch=120, train loss = 0.320830374956131, train acc = 0.8799999952316284, time = 0.7020370960235596\n",
      "Training at step=53, batch=240, train loss = 0.48154354095458984, train acc = 0.8399999737739563, time = 0.7059564590454102\n",
      "Training at step=53, batch=360, train loss = 0.2685995101928711, train acc = 0.8799999952316284, time = 0.6948654651641846\n",
      "Training at step=53, batch=480, train loss = 0.5975061058998108, train acc = 0.8199999928474426, time = 0.7043650150299072\n",
      "Testing at step=53, batch=0, test loss = 0.4126715362071991, test acc = 0.8299999833106995, time = 0.21088361740112305\n",
      "Testing at step=53, batch=20, test loss = 0.562249481678009, test acc = 0.7900000214576721, time = 0.14993667602539062\n",
      "Testing at step=53, batch=40, test loss = 0.5624715089797974, test acc = 0.8299999833106995, time = 0.14899730682373047\n",
      "Testing at step=53, batch=60, test loss = 0.4962204396724701, test acc = 0.800000011920929, time = 0.14827775955200195\n",
      "Testing at step=53, batch=80, test loss = 0.6309691667556763, test acc = 0.8500000238418579, time = 0.15058493614196777\n",
      "Step 53 finished in 455.78870725631714, Train loss = 0.4194472496459882, Test loss = 0.4694538661837578; Train Acc = 0.8531999991337458, Test Acc = 0.8380999994277955\n",
      "Training at step=54, batch=0, train loss = 0.44458019733428955, train acc = 0.8299999833106995, time = 0.7302958965301514\n",
      "Training at step=54, batch=120, train loss = 0.5690861940383911, train acc = 0.8199999928474426, time = 0.7323400974273682\n",
      "Training at step=54, batch=240, train loss = 0.502577543258667, train acc = 0.800000011920929, time = 0.7334258556365967\n",
      "Training at step=54, batch=360, train loss = 0.4795036017894745, train acc = 0.8299999833106995, time = 0.7288503646850586\n",
      "Training at step=54, batch=480, train loss = 0.41843685507774353, train acc = 0.8700000047683716, time = 0.7018494606018066\n",
      "Testing at step=54, batch=0, test loss = 0.43181660771369934, test acc = 0.8799999952316284, time = 0.4245777130126953\n",
      "Testing at step=54, batch=20, test loss = 0.41272246837615967, test acc = 0.8500000238418579, time = 0.4179983139038086\n",
      "Testing at step=54, batch=40, test loss = 0.501396894454956, test acc = 0.8100000023841858, time = 0.14705753326416016\n",
      "Testing at step=54, batch=60, test loss = 0.4556628465652466, test acc = 0.8600000143051147, time = 0.14719629287719727\n",
      "Testing at step=54, batch=80, test loss = 0.5165034532546997, test acc = 0.8500000238418579, time = 0.14878320693969727\n",
      "Step 54 finished in 468.9777133464813, Train loss = 0.4198978802065055, Test loss = 0.4863953399658203; Train Acc = 0.8526666646202405, Test Acc = 0.8332999974489212\n",
      "Training at step=55, batch=0, train loss = 0.44547221064567566, train acc = 0.8399999737739563, time = 0.714118480682373\n",
      "Training at step=55, batch=120, train loss = 0.33278995752334595, train acc = 0.8700000047683716, time = 0.7311711311340332\n",
      "Training at step=55, batch=240, train loss = 0.511610746383667, train acc = 0.8600000143051147, time = 0.7023768424987793\n",
      "Training at step=55, batch=360, train loss = 0.35992538928985596, train acc = 0.8600000143051147, time = 0.7056455612182617\n",
      "Training at step=55, batch=480, train loss = 0.37541690468788147, train acc = 0.8799999952316284, time = 0.7051866054534912\n",
      "Testing at step=55, batch=0, test loss = 0.566192090511322, test acc = 0.7699999809265137, time = 0.19284701347351074\n",
      "Testing at step=55, batch=20, test loss = 0.4313715696334839, test acc = 0.9200000166893005, time = 0.14192962646484375\n",
      "Testing at step=55, batch=40, test loss = 0.4486232399940491, test acc = 0.8700000047683716, time = 0.142380952835083\n",
      "Testing at step=55, batch=60, test loss = 0.30594202876091003, test acc = 0.9300000071525574, time = 0.1422281265258789\n",
      "Testing at step=55, batch=80, test loss = 0.3477114140987396, test acc = 0.8700000047683716, time = 0.14246535301208496\n",
      "Step 55 finished in 461.7671947479248, Train loss = 0.4223589408149322, Test loss = 0.4477321594953537; Train Acc = 0.8517666666706403, Test Acc = 0.8437999987602234\n",
      "Training at step=56, batch=0, train loss = 0.41018134355545044, train acc = 0.8100000023841858, time = 0.7231359481811523\n",
      "Training at step=56, batch=120, train loss = 0.3284321129322052, train acc = 0.8899999856948853, time = 0.7042696475982666\n",
      "Training at step=56, batch=240, train loss = 0.2367236465215683, train acc = 0.9100000262260437, time = 0.7044692039489746\n",
      "Training at step=56, batch=360, train loss = 0.3188905715942383, train acc = 0.8600000143051147, time = 0.7053327560424805\n",
      "Training at step=56, batch=480, train loss = 0.4376630485057831, train acc = 0.8600000143051147, time = 0.7099945545196533\n",
      "Testing at step=56, batch=0, test loss = 0.45546597242355347, test acc = 0.8500000238418579, time = 0.3815906047821045\n",
      "Testing at step=56, batch=20, test loss = 0.5205065011978149, test acc = 0.7799999713897705, time = 0.4172031879425049\n",
      "Testing at step=56, batch=40, test loss = 0.3866475820541382, test acc = 0.8399999737739563, time = 0.4178471565246582\n",
      "Testing at step=56, batch=60, test loss = 0.37440788745880127, test acc = 0.8600000143051147, time = 0.41725611686706543\n",
      "Testing at step=56, batch=80, test loss = 0.49757319688796997, test acc = 0.8299999833106995, time = 0.41794514656066895\n",
      "Step 56 finished in 456.31082224845886, Train loss = 0.4175121996303399, Test loss = 0.48097401529550554; Train Acc = 0.8537333330512047, Test Acc = 0.8224999994039536\n",
      "Training at step=57, batch=0, train loss = 0.43159282207489014, train acc = 0.8199999928474426, time = 0.7089798450469971\n",
      "Training at step=57, batch=120, train loss = 0.3782888352870941, train acc = 0.8600000143051147, time = 0.7037463188171387\n",
      "Training at step=57, batch=240, train loss = 0.44100695848464966, train acc = 0.8700000047683716, time = 0.7053899765014648\n",
      "Training at step=57, batch=360, train loss = 0.48834431171417236, train acc = 0.800000011920929, time = 0.7044098377227783\n",
      "Training at step=57, batch=480, train loss = 0.4893416464328766, train acc = 0.8299999833106995, time = 0.7051293849945068\n",
      "Testing at step=57, batch=0, test loss = 0.37995830178260803, test acc = 0.8999999761581421, time = 0.2048356533050537\n",
      "Testing at step=57, batch=20, test loss = 0.5044615864753723, test acc = 0.7900000214576721, time = 0.13582444190979004\n",
      "Testing at step=57, batch=40, test loss = 0.6134921908378601, test acc = 0.7900000214576721, time = 0.13570475578308105\n",
      "Testing at step=57, batch=60, test loss = 0.35620152950286865, test acc = 0.8500000238418579, time = 0.13463997840881348\n",
      "Testing at step=57, batch=80, test loss = 0.3757152259349823, test acc = 0.8500000238418579, time = 0.13709139823913574\n",
      "Step 57 finished in 455.950208902359, Train loss = 0.41742238618433475, Test loss = 0.4655445069074631; Train Acc = 0.8543333341677983, Test Acc = 0.838700001835823\n",
      "Training at step=58, batch=0, train loss = 0.4881598651409149, train acc = 0.800000011920929, time = 0.7092423439025879\n",
      "Training at step=58, batch=120, train loss = 0.37944942712783813, train acc = 0.8799999952316284, time = 0.7071568965911865\n",
      "Training at step=58, batch=240, train loss = 0.33118656277656555, train acc = 0.8999999761581421, time = 0.7054219245910645\n",
      "Training at step=58, batch=360, train loss = 0.432092547416687, train acc = 0.8100000023841858, time = 0.7047185897827148\n",
      "Training at step=58, batch=480, train loss = 0.6458247303962708, train acc = 0.7699999809265137, time = 0.7033016681671143\n",
      "Testing at step=58, batch=0, test loss = 0.36789053678512573, test acc = 0.8799999952316284, time = 0.22260022163391113\n",
      "Testing at step=58, batch=20, test loss = 0.48857536911964417, test acc = 0.8299999833106995, time = 0.14607787132263184\n",
      "Testing at step=58, batch=40, test loss = 0.35649871826171875, test acc = 0.8600000143051147, time = 0.14299583435058594\n",
      "Testing at step=58, batch=60, test loss = 0.3666932284832001, test acc = 0.8799999952316284, time = 0.14409470558166504\n",
      "Testing at step=58, batch=80, test loss = 0.37385645508766174, test acc = 0.8500000238418579, time = 0.14522266387939453\n",
      "Step 58 finished in 455.7496111392975, Train loss = 0.4172552980730931, Test loss = 0.45710100799798964; Train Acc = 0.8530499986807505, Test Acc = 0.8406999957561493\n",
      "Training at step=59, batch=0, train loss = 0.30384817719459534, train acc = 0.9200000166893005, time = 0.7276272773742676\n",
      "Training at step=59, batch=120, train loss = 0.4848605990409851, train acc = 0.8199999928474426, time = 0.7054805755615234\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"1-fashionmnist_quanv_classical_linear.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "U0Q0vFm7B6cg",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711488462684,
     "user_tz": -660,
     "elapsed": 1244,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "a805910a-6214-4218-d940-429f1bae5d2e",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"1-fashionmnist_quanv_classical_linear.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ]
}
