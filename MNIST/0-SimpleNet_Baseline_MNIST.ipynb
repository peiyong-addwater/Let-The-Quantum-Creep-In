{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "authorship_tag": "ABX9TyMZ79VIcBqatCN3QMKuyuE4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acHJk38WrBOi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712091561992,
     "user_tz": -660,
     "elapsed": 641011,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "85a0290a-e46d-4caf-93ed-a7e0fdc53d3e",
    "ExecuteTime": {
     "end_time": "2024-04-06T20:10:31.018547Z",
     "start_time": "2024-04-06T20:10:15.922283Z"
    }
   },
   "source": [
    "#!pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install torch torchvision torchaudio pennylane cotengra quimb torchmetrics --upgrade"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\r\n",
      "Requirement already satisfied: jax[cuda12_pip] in /usr/local/lib/python3.9/dist-packages (0.4.1)\r\n",
      "Collecting jax[cuda12_pip]\r\n",
      "  Downloading jax-0.4.26-py3-none-any.whl (1.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m71.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: opt-einsum in /usr/local/lib/python3.9/dist-packages (from jax[cuda12_pip]) (3.3.0)\r\n",
      "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.9/dist-packages (from jax[cuda12_pip]) (1.9.2)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /usr/local/lib/python3.9/dist-packages (from jax[cuda12_pip]) (6.0.0)\r\n",
      "Collecting ml-dtypes>=0.2.0\r\n",
      "  Downloading ml_dtypes-0.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.2/2.2 MB\u001B[0m \u001B[31m27.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.9/dist-packages (from jax[cuda12_pip]) (1.23.4)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.9/dist-packages (from jax[cuda12_pip]) (12.1.105)\r\n",
      "Collecting nvidia-cuda-nvcc-cu12>=12.1.105\r\n",
      "  Downloading nvidia_cuda_nvcc_cu12-12.4.131-py3-none-manylinux2014_x86_64.whl (22.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m22.3/22.3 MB\u001B[0m \u001B[31m53.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.9/dist-packages (from jax[cuda12_pip]) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from jax[cuda12_pip]) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12<9.0,>=8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from jax[cuda12_pip]) (8.9.2.26)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.9/dist-packages (from jax[cuda12_pip]) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from jax[cuda12_pip]) (12.1.3.1)\r\n",
      "Collecting jaxlib==0.4.26+cuda12.cudnn89\r\n",
      "  Downloading https://storage.googleapis.com/jax-releases/cuda12/jaxlib-0.4.26%2Bcuda12.cudnn89-cp39-cp39-manylinux2014_x86_64.whl (144.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m144.2/144.2 MB\u001B[0m \u001B[31m9.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from jax[cuda12_pip]) (11.4.5.107)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /usr/local/lib/python3.9/dist-packages (from jax[cuda12_pip]) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from jax[cuda12_pip]) (12.1.0.106)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.6->jax[cuda12_pip]) (3.11.0)\r\n",
      "Installing collected packages: nvidia-cuda-nvcc-cu12, ml-dtypes, jaxlib, jax\r\n",
      "  Attempting uninstall: jaxlib\r\n",
      "    Found existing installation: jaxlib 0.4.1+cuda11.cudnn82\r\n",
      "    Uninstalling jaxlib-0.4.1+cuda11.cudnn82:\r\n",
      "      Successfully uninstalled jaxlib-0.4.1+cuda11.cudnn82\r\n",
      "  Attempting uninstall: jax\r\n",
      "    Found existing installation: jax 0.4.1\r\n",
      "    Uninstalling jax-0.4.1:\r\n",
      "      Successfully uninstalled jax-0.4.1\r\n",
      "Successfully installed jax-0.4.26 jaxlib-0.4.26+cuda12.cudnn89 ml-dtypes-0.4.0 nvidia-cuda-nvcc-cu12-12.4.131\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.17.2)\r\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (2.2.2)\r\n",
      "Requirement already satisfied: pennylane in /usr/local/lib/python3.9/dist-packages (0.35.1)\r\n",
      "Requirement already satisfied: cotengra in /usr/local/lib/python3.9/dist-packages (0.5.6)\r\n",
      "Requirement already satisfied: quimb in /usr/local/lib/python3.9/dist-packages (1.7.3)\r\n",
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.9/dist-packages (1.3.2)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.0.106)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.3.1)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch) (10.3.2.106)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch) (11.4.5.107)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch) (2.19.3)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch) (11.0.2.54)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch) (12.1.105)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch) (8.9.2.26)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from torch) (4.11.0)\r\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.2.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\r\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\r\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.10.2)\r\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.4.4)\r\n",
      "Requirement already satisfied: autoray>=0.6.1 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.6.9)\r\n",
      "Requirement already satisfied: autograd in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.6.2)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\r\n",
      "Requirement already satisfied: pennylane-lightning>=0.35 in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.35.1)\r\n",
      "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.10.0)\r\n",
      "Requirement already satisfied: rustworkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (0.14.2)\r\n",
      "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\r\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\r\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.12.3)\r\n",
      "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.9/dist-packages (from quimb) (0.59.1)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (0.11.2)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\r\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\r\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.39->quimb) (0.42.0)\r\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "#import pennylane as qml\n",
    "#import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "#prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "#REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FB_BuVzCrMNk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712091570174,
     "user_tz": -660,
     "elapsed": 8191,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "9efa8c3e-cdcc-46df-95ab-f4f1006c4ba3",
    "ExecuteTime": {
     "end_time": "2024-04-06T20:11:01.952170Z",
     "start_time": "2024-04-06T20:11:00.795443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"MNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K7wJnOiEralR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712091571140,
     "user_tz": -660,
     "elapsed": 977,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "aeaff2d6-e1d2-4a3b-db6c-918faa8f8746",
    "ExecuteTime": {
     "end_time": "2024-04-06T20:11:11.764998Z",
     "start_time": "2024-04-06T20:11:10.796222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 301920868.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/train-images-idx3-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 115808502.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 184291081.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 33363447.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n",
      "\n",
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n",
      "tensor([7, 7, 5, 7, 4, 7, 7, 7, 5, 4, 7, 2, 7, 8, 1, 1, 1, 6, 4, 1, 5, 1, 2, 7,\n",
      "        8, 0, 8, 6, 1, 8, 8, 7, 4, 4, 7, 9, 5, 4, 3, 9, 6, 5, 6, 0, 3, 9, 5, 3,\n",
      "        5, 1, 0, 4, 2, 3, 2, 0, 1, 5, 6, 4, 8, 6, 3, 9])\n",
      "tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  0.2863,  0.7098, -0.7333, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "class SimpleNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SimpleNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=3),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Conv2d(32, 16, kernel_size=3),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(16*28*28, 10),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "net = SimpleNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6wDzOaJrmZ4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712091571799,
     "user_tz": -660,
     "elapsed": 661,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "53fc9e5e-21f5-46b1-9274-a493223a8d56",
    "ExecuteTime": {
     "end_time": "2024-04-06T20:11:18.305655Z",
     "start_time": "2024-04-06T20:11:14.678137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "SimpleNet(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=12544, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.2\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDNHjKy4sSWZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712093853656,
     "user_tz": -660,
     "elapsed": 2281859,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "eed38404-a80c-4311-ba2b-c191172a7133",
    "ExecuteTime": {
     "end_time": "2024-04-06T20:52:10.334987Z",
     "start_time": "2024-04-06T20:11:23.929165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 600, Number of test batches = 100\n",
      "Print every train batch = 120, Print every test batch = 20\n",
      "Training at step=0, batch=0, train loss = 2.30913119184226, train acc = 0.10000000149011612, time = 0.16901254653930664\n",
      "Training at step=0, batch=120, train loss = 0.6894455833445946, train acc = 0.8199999928474426, time = 0.020582914352416992\n",
      "Training at step=0, batch=240, train loss = 0.44537607037991733, train acc = 0.8799999952316284, time = 0.020616769790649414\n",
      "Training at step=0, batch=360, train loss = 0.4238521498207151, train acc = 0.8600000143051147, time = 0.0206143856048584\n",
      "Training at step=0, batch=480, train loss = 0.3411368893037979, train acc = 0.8999999761581421, time = 0.02077174186706543\n",
      "Testing at step=0, batch=0, test loss = 0.2716785533017093, test acc = 0.9300000071525574, time = 0.004769563674926758\n",
      "Testing at step=0, batch=20, test loss = 0.2800219567198676, test acc = 0.9399999976158142, time = 0.004729270935058594\n",
      "Testing at step=0, batch=40, test loss = 0.3252035169837458, test acc = 0.8799999952316284, time = 0.0047245025634765625\n",
      "Testing at step=0, batch=60, test loss = 0.3635226098537159, test acc = 0.8999999761581421, time = 0.004754066467285156\n",
      "Testing at step=0, batch=80, test loss = 0.4023799988086055, test acc = 0.8999999761581421, time = 0.004727363586425781\n",
      "Step 0 finished in 24.566010236740112, Train loss = 0.629183675616407, Test loss = 0.349028008582764; Train Acc = 0.8228833331974844, Test Acc = 0.8960999983549118\n",
      "Training at step=1, batch=0, train loss = 0.44450048268444226, train acc = 0.8399999737739563, time = 0.020503997802734375\n",
      "Training at step=1, batch=120, train loss = 0.366866429416344, train acc = 0.9100000262260437, time = 0.020418882369995117\n",
      "Training at step=1, batch=240, train loss = 0.33282565287987337, train acc = 0.8899999856948853, time = 0.020640850067138672\n",
      "Training at step=1, batch=360, train loss = 0.31348835569794453, train acc = 0.9200000166893005, time = 0.020361900329589844\n",
      "Training at step=1, batch=480, train loss = 0.5275333363102758, train acc = 0.8999999761581421, time = 0.020441770553588867\n",
      "Testing at step=1, batch=0, test loss = 0.2917365260201864, test acc = 0.8899999856948853, time = 0.004826545715332031\n",
      "Testing at step=1, batch=20, test loss = 0.3692703472021789, test acc = 0.9100000262260437, time = 0.004730224609375\n",
      "Testing at step=1, batch=40, test loss = 0.36983970705959524, test acc = 0.8600000143051147, time = 0.004755735397338867\n",
      "Testing at step=1, batch=60, test loss = 0.3045837550191411, test acc = 0.9399999976158142, time = 0.004728078842163086\n",
      "Testing at step=1, batch=80, test loss = 0.4774512631774581, test acc = 0.8299999833106995, time = 0.004767179489135742\n",
      "Step 1 finished in 24.517164707183838, Train loss = 0.344105106574508, Test loss = 0.32849819414892245; Train Acc = 0.8987833345929782, Test Acc = 0.902200003862381\n",
      "Training at step=2, batch=0, train loss = 0.2569306496790489, train acc = 0.9300000071525574, time = 0.02058720588684082\n",
      "Training at step=2, batch=120, train loss = 0.41934714593077227, train acc = 0.8700000047683716, time = 0.020611286163330078\n",
      "Training at step=2, batch=240, train loss = 0.2878348523106126, train acc = 0.8999999761581421, time = 0.020667314529418945\n",
      "Training at step=2, batch=360, train loss = 0.19980511551629093, train acc = 0.9399999976158142, time = 0.020501375198364258\n",
      "Training at step=2, batch=480, train loss = 0.5028864620504081, train acc = 0.8600000143051147, time = 0.020491361618041992\n",
      "Testing at step=2, batch=0, test loss = 0.3079983583688572, test acc = 0.8999999761581421, time = 0.004833698272705078\n",
      "Testing at step=2, batch=20, test loss = 0.39032051056493394, test acc = 0.8799999952316284, time = 0.004741668701171875\n",
      "Testing at step=2, batch=40, test loss = 0.3303193988376596, test acc = 0.9200000166893005, time = 0.0047719478607177734\n",
      "Testing at step=2, batch=60, test loss = 0.23973993768157506, test acc = 0.9100000262260437, time = 0.004748106002807617\n",
      "Testing at step=2, batch=80, test loss = 0.32732424664150755, test acc = 0.9399999976158142, time = 0.0047454833984375\n",
      "Step 2 finished in 24.490870237350464, Train loss = 0.32006430406907016, Test loss = 0.2920148433699177; Train Acc = 0.9067500022053718, Test Acc = 0.9175000023841858\n",
      "Training at step=3, batch=0, train loss = 0.379557162132746, train acc = 0.8799999952316284, time = 0.02054452896118164\n",
      "Training at step=3, batch=120, train loss = 0.4151243621931073, train acc = 0.8999999761581421, time = 0.02049112319946289\n",
      "Training at step=3, batch=240, train loss = 0.25816929338326333, train acc = 0.9300000071525574, time = 0.020521879196166992\n",
      "Training at step=3, batch=360, train loss = 0.15770904894295856, train acc = 0.9599999785423279, time = 0.02051568031311035\n",
      "Training at step=3, batch=480, train loss = 0.28129739075426985, train acc = 0.9100000262260437, time = 0.020475149154663086\n",
      "Testing at step=3, batch=0, test loss = 0.26710278946585886, test acc = 0.9399999976158142, time = 0.004795551300048828\n",
      "Testing at step=3, batch=20, test loss = 0.1430122691787511, test acc = 0.949999988079071, time = 0.004794120788574219\n",
      "Testing at step=3, batch=40, test loss = 0.27403611567446584, test acc = 0.8799999952316284, time = 0.004743099212646484\n",
      "Testing at step=3, batch=60, test loss = 0.20792658107534553, test acc = 0.9100000262260437, time = 0.0047228336334228516\n",
      "Testing at step=3, batch=80, test loss = 0.307985765266961, test acc = 0.9300000071525574, time = 0.00472712516784668\n",
      "Step 3 finished in 24.454712629318237, Train loss = 0.30970632199584297, Test loss = 0.2929057426267734; Train Acc = 0.9107000022133191, Test Acc = 0.9162000030279159\n",
      "Training at step=4, batch=0, train loss = 0.23157585593914792, train acc = 0.9200000166893005, time = 0.020478487014770508\n",
      "Training at step=4, batch=120, train loss = 0.390374812987259, train acc = 0.9200000166893005, time = 0.02067279815673828\n",
      "Training at step=4, batch=240, train loss = 0.20677470134435666, train acc = 0.9399999976158142, time = 0.020401954650878906\n",
      "Training at step=4, batch=360, train loss = 0.31458909379543404, train acc = 0.9399999976158142, time = 0.020593643188476562\n",
      "Training at step=4, batch=480, train loss = 0.3316348534047673, train acc = 0.8600000143051147, time = 0.020632505416870117\n",
      "Testing at step=4, batch=0, test loss = 0.29439316933078435, test acc = 0.949999988079071, time = 0.004763603210449219\n",
      "Testing at step=4, batch=20, test loss = 0.29649469219633046, test acc = 0.9399999976158142, time = 0.004717111587524414\n",
      "Testing at step=4, batch=40, test loss = 0.4132046837087425, test acc = 0.8500000238418579, time = 0.00474238395690918\n",
      "Testing at step=4, batch=60, test loss = 0.31199952487467614, test acc = 0.8999999761581421, time = 0.0047261714935302734\n",
      "Testing at step=4, batch=80, test loss = 0.4774248739167938, test acc = 0.8999999761581421, time = 0.004714250564575195\n",
      "Step 4 finished in 24.403308868408203, Train loss = 0.29905504256997884, Test loss = 0.2838882369238973; Train Acc = 0.9143000010649364, Test Acc = 0.9196999996900559\n",
      "Training at step=5, batch=0, train loss = 0.33162642159303596, train acc = 0.9100000262260437, time = 0.020457029342651367\n",
      "Training at step=5, batch=120, train loss = 0.27153089471811115, train acc = 0.8999999761581421, time = 0.020380020141601562\n",
      "Training at step=5, batch=240, train loss = 0.1976414991870764, train acc = 0.949999988079071, time = 0.020437955856323242\n",
      "Training at step=5, batch=360, train loss = 0.20733652715449935, train acc = 0.9300000071525574, time = 0.020463943481445312\n",
      "Training at step=5, batch=480, train loss = 0.17638327458822345, train acc = 0.949999988079071, time = 0.020465373992919922\n",
      "Testing at step=5, batch=0, test loss = 0.22502749920403875, test acc = 0.9300000071525574, time = 0.005052804946899414\n",
      "Testing at step=5, batch=20, test loss = 0.334828422041027, test acc = 0.9200000166893005, time = 0.004762411117553711\n",
      "Testing at step=5, batch=40, test loss = 0.2972406311909605, test acc = 0.9399999976158142, time = 0.004761695861816406\n",
      "Testing at step=5, batch=60, test loss = 0.3969790439748819, test acc = 0.8999999761581421, time = 0.0047457218170166016\n",
      "Testing at step=5, batch=80, test loss = 0.41583654709226864, test acc = 0.9100000262260437, time = 0.00484919548034668\n",
      "Step 5 finished in 24.387444257736206, Train loss = 0.29471320485489905, Test loss = 0.2858067732541939; Train Acc = 0.9163666686415672, Test Acc = 0.9178000038862228\n",
      "Training at step=6, batch=0, train loss = 0.3251150667558248, train acc = 0.9399999976158142, time = 0.02063918113708496\n",
      "Training at step=6, batch=120, train loss = 0.30139020903264485, train acc = 0.9100000262260437, time = 0.020583391189575195\n",
      "Training at step=6, batch=240, train loss = 0.21343235897400642, train acc = 0.949999988079071, time = 0.020528554916381836\n",
      "Training at step=6, batch=360, train loss = 0.13954230529718697, train acc = 0.9700000286102295, time = 0.020415782928466797\n",
      "Training at step=6, batch=480, train loss = 0.6597896668713141, train acc = 0.8600000143051147, time = 0.020779132843017578\n",
      "Testing at step=6, batch=0, test loss = 0.31240308278449275, test acc = 0.9200000166893005, time = 0.004911184310913086\n",
      "Testing at step=6, batch=20, test loss = 0.3459604911846856, test acc = 0.9300000071525574, time = 0.004781007766723633\n",
      "Testing at step=6, batch=40, test loss = 0.25925118922579116, test acc = 0.9399999976158142, time = 0.004728794097900391\n",
      "Testing at step=6, batch=60, test loss = 0.3116058339675077, test acc = 0.9100000262260437, time = 0.004817962646484375\n",
      "Testing at step=6, batch=80, test loss = 0.17395019382242657, test acc = 0.9599999785423279, time = 0.004815101623535156\n",
      "Step 6 finished in 24.492868900299072, Train loss = 0.2904220152346709, Test loss = 0.284183644370473; Train Acc = 0.9171166692177455, Test Acc = 0.9199000006914139\n",
      "Training at step=7, batch=0, train loss = 0.3009895701596308, train acc = 0.9300000071525574, time = 0.02059030532836914\n",
      "Training at step=7, batch=120, train loss = 0.47424324203268514, train acc = 0.8399999737739563, time = 0.020538330078125\n",
      "Training at step=7, batch=240, train loss = 0.16588626839795384, train acc = 0.9700000286102295, time = 0.020397663116455078\n",
      "Training at step=7, batch=360, train loss = 0.17394152195258705, train acc = 0.9599999785423279, time = 0.020492076873779297\n",
      "Training at step=7, batch=480, train loss = 0.2554361062761167, train acc = 0.8799999952316284, time = 0.020877838134765625\n",
      "Testing at step=7, batch=0, test loss = 0.28302667299628603, test acc = 0.9200000166893005, time = 0.004906654357910156\n",
      "Testing at step=7, batch=20, test loss = 0.3612180028108395, test acc = 0.8799999952316284, time = 0.004832029342651367\n",
      "Testing at step=7, batch=40, test loss = 0.3513343814558926, test acc = 0.8899999856948853, time = 0.004732608795166016\n",
      "Testing at step=7, batch=60, test loss = 0.27981649686944715, test acc = 0.9100000262260437, time = 0.0047414302825927734\n",
      "Testing at step=7, batch=80, test loss = 0.2775827606591353, test acc = 0.8999999761581421, time = 0.004721164703369141\n",
      "Step 7 finished in 24.418084383010864, Train loss = 0.28522112153915835, Test loss = 0.2774642261997198; Train Acc = 0.919716669122378, Test Acc = 0.920500003695488\n",
      "Training at step=8, batch=0, train loss = 0.2873922258597965, train acc = 0.8700000047683716, time = 0.02057623863220215\n",
      "Training at step=8, batch=120, train loss = 0.2635823318652117, train acc = 0.9300000071525574, time = 0.020629167556762695\n",
      "Training at step=8, batch=240, train loss = 0.3867201536547338, train acc = 0.9100000262260437, time = 0.02033519744873047\n",
      "Training at step=8, batch=360, train loss = 0.4543459755499881, train acc = 0.8799999952316284, time = 0.020612478256225586\n",
      "Training at step=8, batch=480, train loss = 0.3705787311480782, train acc = 0.9100000262260437, time = 0.020560741424560547\n",
      "Testing at step=8, batch=0, test loss = 0.19971533465927432, test acc = 0.9100000262260437, time = 0.004889249801635742\n",
      "Testing at step=8, batch=20, test loss = 0.2263655816722588, test acc = 0.949999988079071, time = 0.004759073257446289\n",
      "Testing at step=8, batch=40, test loss = 0.18603171562303938, test acc = 0.9200000166893005, time = 0.0047724246978759766\n",
      "Testing at step=8, batch=60, test loss = 0.2532974553931167, test acc = 0.9300000071525574, time = 0.0047533512115478516\n",
      "Testing at step=8, batch=80, test loss = 0.31567704420778164, test acc = 0.9200000166893005, time = 0.00479888916015625\n",
      "Step 8 finished in 24.408812522888184, Train loss = 0.2821509030620618, Test loss = 0.27696062662445015; Train Acc = 0.9196833357214927, Test Acc = 0.9189000028371811\n",
      "Training at step=9, batch=0, train loss = 0.3304743403018868, train acc = 0.9399999976158142, time = 0.020619630813598633\n",
      "Training at step=9, batch=120, train loss = 0.23134189834208307, train acc = 0.9300000071525574, time = 0.020719528198242188\n",
      "Training at step=9, batch=240, train loss = 0.2521577055644849, train acc = 0.949999988079071, time = 0.020565509796142578\n",
      "Training at step=9, batch=360, train loss = 0.42470524150857164, train acc = 0.8899999856948853, time = 0.02049422264099121\n",
      "Training at step=9, batch=480, train loss = 0.161032503260056, train acc = 0.949999988079071, time = 0.02046370506286621\n",
      "Testing at step=9, batch=0, test loss = 0.33077869455585396, test acc = 0.9100000262260437, time = 0.004762887954711914\n",
      "Testing at step=9, batch=20, test loss = 0.2445883674238633, test acc = 0.9200000166893005, time = 0.004800319671630859\n",
      "Testing at step=9, batch=40, test loss = 0.22479641697511657, test acc = 0.9300000071525574, time = 0.004769325256347656\n",
      "Testing at step=9, batch=60, test loss = 0.3874515833141924, test acc = 0.9200000166893005, time = 0.004759311676025391\n",
      "Testing at step=9, batch=80, test loss = 0.4052500493820252, test acc = 0.8799999952316284, time = 0.004729747772216797\n",
      "Step 9 finished in 24.45286536216736, Train loss = 0.2800589650091036, Test loss = 0.27600014548584967; Train Acc = 0.9211833350857099, Test Acc = 0.9239000004529953\n",
      "Training at step=10, batch=0, train loss = 0.20980414759450677, train acc = 0.9399999976158142, time = 0.020474910736083984\n",
      "Training at step=10, batch=120, train loss = 0.17482484966789794, train acc = 0.9200000166893005, time = 0.020482301712036133\n",
      "Training at step=10, batch=240, train loss = 0.2128288354779483, train acc = 0.9200000166893005, time = 0.02048015594482422\n",
      "Training at step=10, batch=360, train loss = 0.1724727277826898, train acc = 0.9599999785423279, time = 0.020435333251953125\n",
      "Training at step=10, batch=480, train loss = 0.243210592539686, train acc = 0.949999988079071, time = 0.020410776138305664\n",
      "Testing at step=10, batch=0, test loss = 0.20729003922317848, test acc = 0.9399999976158142, time = 0.004804849624633789\n",
      "Testing at step=10, batch=20, test loss = 0.1242572847338445, test acc = 0.9700000286102295, time = 0.0047149658203125\n",
      "Testing at step=10, batch=40, test loss = 0.27739245813480146, test acc = 0.9200000166893005, time = 0.004739999771118164\n",
      "Testing at step=10, batch=60, test loss = 0.2861125694994572, test acc = 0.8999999761581421, time = 0.00473475456237793\n",
      "Testing at step=10, batch=80, test loss = 0.3447827020781531, test acc = 0.9200000166893005, time = 0.004731893539428711\n",
      "Step 10 finished in 24.444915771484375, Train loss = 0.27755107824182185, Test loss = 0.2732731814108977; Train Acc = 0.9222000013788542, Test Acc = 0.9232000023126602\n",
      "Training at step=11, batch=0, train loss = 0.3739610799621938, train acc = 0.8999999761581421, time = 0.020563125610351562\n",
      "Training at step=11, batch=120, train loss = 0.27023116428531596, train acc = 0.9100000262260437, time = 0.020479202270507812\n",
      "Training at step=11, batch=240, train loss = 0.32008476695516835, train acc = 0.8899999856948853, time = 0.020371198654174805\n",
      "Training at step=11, batch=360, train loss = 0.1844935571132402, train acc = 0.9700000286102295, time = 0.020455598831176758\n",
      "Training at step=11, batch=480, train loss = 0.30864273370142087, train acc = 0.9300000071525574, time = 0.0203702449798584\n",
      "Testing at step=11, batch=0, test loss = 0.33443017228088684, test acc = 0.9200000166893005, time = 0.0048444271087646484\n",
      "Testing at step=11, batch=20, test loss = 0.3083171956813888, test acc = 0.9100000262260437, time = 0.004736661911010742\n",
      "Testing at step=11, batch=40, test loss = 0.2917206103472544, test acc = 0.9100000262260437, time = 0.004727363586425781\n",
      "Testing at step=11, batch=60, test loss = 0.449665644020585, test acc = 0.8700000047683716, time = 0.004742145538330078\n",
      "Testing at step=11, batch=80, test loss = 0.2502999111772069, test acc = 0.9100000262260437, time = 0.004738569259643555\n",
      "Step 11 finished in 24.476930618286133, Train loss = 0.2758953998895812, Test loss = 0.275472610865732; Train Acc = 0.9225833349426588, Test Acc = 0.9208000028133392\n",
      "Training at step=12, batch=0, train loss = 0.371280928668831, train acc = 0.8899999856948853, time = 0.020565509796142578\n",
      "Training at step=12, batch=120, train loss = 0.15896091457558695, train acc = 0.949999988079071, time = 0.02038288116455078\n",
      "Training at step=12, batch=240, train loss = 0.21936009977898596, train acc = 0.949999988079071, time = 0.02053070068359375\n",
      "Training at step=12, batch=360, train loss = 0.48217944822574266, train acc = 0.8799999952316284, time = 0.020373821258544922\n",
      "Training at step=12, batch=480, train loss = 0.26740891522352844, train acc = 0.9100000262260437, time = 0.02046990394592285\n",
      "Testing at step=12, batch=0, test loss = 0.19156123470619377, test acc = 0.9200000166893005, time = 0.0048100948333740234\n",
      "Testing at step=12, batch=20, test loss = 0.29534722931276974, test acc = 0.9300000071525574, time = 0.004729747772216797\n",
      "Testing at step=12, batch=40, test loss = 0.16669581644995424, test acc = 0.9599999785423279, time = 0.004729509353637695\n",
      "Testing at step=12, batch=60, test loss = 0.16733294135420898, test acc = 0.949999988079071, time = 0.0047419071197509766\n",
      "Testing at step=12, batch=80, test loss = 0.13545372017803278, test acc = 0.949999988079071, time = 0.004773139953613281\n",
      "Step 12 finished in 24.450565099716187, Train loss = 0.2745830815228811, Test loss = 0.269955880246911; Train Acc = 0.9232166687647502, Test Acc = 0.9240000009536743\n",
      "Training at step=13, batch=0, train loss = 0.38036958795087733, train acc = 0.8799999952316284, time = 0.0206298828125\n",
      "Training at step=13, batch=120, train loss = 0.3331247397305641, train acc = 0.8899999856948853, time = 0.020572662353515625\n",
      "Training at step=13, batch=240, train loss = 0.24975610959712072, train acc = 0.9300000071525574, time = 0.020879507064819336\n",
      "Training at step=13, batch=360, train loss = 0.17624009969882182, train acc = 0.9599999785423279, time = 0.020443439483642578\n",
      "Training at step=13, batch=480, train loss = 0.12077643214608484, train acc = 0.9700000286102295, time = 0.02077937126159668\n",
      "Testing at step=13, batch=0, test loss = 0.24981250553106563, test acc = 0.9599999785423279, time = 0.004792451858520508\n",
      "Testing at step=13, batch=20, test loss = 0.4616393916110255, test acc = 0.8999999761581421, time = 0.004746913909912109\n",
      "Testing at step=13, batch=40, test loss = 0.20975596078095593, test acc = 0.9200000166893005, time = 0.004771709442138672\n",
      "Testing at step=13, batch=60, test loss = 0.21409376058692597, test acc = 0.9200000166893005, time = 0.004756927490234375\n",
      "Testing at step=13, batch=80, test loss = 0.20320869674556996, test acc = 0.9399999976158142, time = 0.004731416702270508\n",
      "Step 13 finished in 24.524795055389404, Train loss = 0.2732834116154994, Test loss = 0.27142460138086855; Train Acc = 0.9236166675885519, Test Acc = 0.9238000017404556\n",
      "Training at step=14, batch=0, train loss = 0.33096106819011695, train acc = 0.8999999761581421, time = 0.020555973052978516\n",
      "Training at step=14, batch=120, train loss = 0.23154471729910853, train acc = 0.9300000071525574, time = 0.020493268966674805\n",
      "Training at step=14, batch=240, train loss = 0.3715284682788653, train acc = 0.8999999761581421, time = 0.020598649978637695\n",
      "Training at step=14, batch=360, train loss = 0.28633802382962814, train acc = 0.9200000166893005, time = 0.020407438278198242\n",
      "Training at step=14, batch=480, train loss = 0.32697464308895113, train acc = 0.9100000262260437, time = 0.020405054092407227\n",
      "Testing at step=14, batch=0, test loss = 0.29696368583924293, test acc = 0.9100000262260437, time = 0.004848957061767578\n",
      "Testing at step=14, batch=20, test loss = 0.4044957483590139, test acc = 0.9100000262260437, time = 0.004753828048706055\n",
      "Testing at step=14, batch=40, test loss = 0.23399705882328278, test acc = 0.9300000071525574, time = 0.004740476608276367\n",
      "Testing at step=14, batch=60, test loss = 0.3081693671315802, test acc = 0.9300000071525574, time = 0.0047342777252197266\n",
      "Testing at step=14, batch=80, test loss = 0.20817849095809657, test acc = 0.9200000166893005, time = 0.0048100948333740234\n",
      "Step 14 finished in 24.468681812286377, Train loss = 0.27097589492463886, Test loss = 0.27217538360924004; Train Acc = 0.9237500016887983, Test Acc = 0.9244000017642975\n",
      "Training at step=15, batch=0, train loss = 0.2862158413040466, train acc = 0.9300000071525574, time = 0.02067852020263672\n",
      "Training at step=15, batch=120, train loss = 0.3049724362776042, train acc = 0.9300000071525574, time = 0.0205843448638916\n",
      "Training at step=15, batch=240, train loss = 0.37532177829230373, train acc = 0.8899999856948853, time = 0.020527124404907227\n",
      "Training at step=15, batch=360, train loss = 0.2549219611640455, train acc = 0.8899999856948853, time = 0.02048516273498535\n",
      "Training at step=15, batch=480, train loss = 0.330219248132586, train acc = 0.9300000071525574, time = 0.020522594451904297\n",
      "Testing at step=15, batch=0, test loss = 0.15039119489051203, test acc = 0.9700000286102295, time = 0.00484013557434082\n",
      "Testing at step=15, batch=20, test loss = 0.3496018675341611, test acc = 0.8600000143051147, time = 0.004818916320800781\n",
      "Testing at step=15, batch=40, test loss = 0.16063704635962858, test acc = 0.9399999976158142, time = 0.0048520565032958984\n",
      "Testing at step=15, batch=60, test loss = 0.182549124639107, test acc = 0.9200000166893005, time = 0.0048029422760009766\n",
      "Testing at step=15, batch=80, test loss = 0.27813327816617367, test acc = 0.9399999976158142, time = 0.004873514175415039\n",
      "Step 15 finished in 24.525824785232544, Train loss = 0.2695319614683529, Test loss = 0.2700689257193095; Train Acc = 0.925650001168251, Test Acc = 0.9241000032424926\n",
      "Training at step=16, batch=0, train loss = 0.28701713897121856, train acc = 0.9100000262260437, time = 0.020639419555664062\n",
      "Training at step=16, batch=120, train loss = 0.19997779915535166, train acc = 0.9399999976158142, time = 0.020565509796142578\n",
      "Training at step=16, batch=240, train loss = 0.34721860756991324, train acc = 0.8899999856948853, time = 0.020494937896728516\n",
      "Training at step=16, batch=360, train loss = 0.2687800162105476, train acc = 0.949999988079071, time = 0.020502328872680664\n",
      "Training at step=16, batch=480, train loss = 0.21682253906020685, train acc = 0.9300000071525574, time = 0.020627498626708984\n",
      "Testing at step=16, batch=0, test loss = 0.1724229597473711, test acc = 0.9300000071525574, time = 0.004846811294555664\n",
      "Testing at step=16, batch=20, test loss = 0.12431139002208315, test acc = 0.9599999785423279, time = 0.004803180694580078\n",
      "Testing at step=16, batch=40, test loss = 0.21185840975512713, test acc = 0.9200000166893005, time = 0.0047609806060791016\n",
      "Testing at step=16, batch=60, test loss = 0.20406486218791497, test acc = 0.949999988079071, time = 0.0048062801361083984\n",
      "Testing at step=16, batch=80, test loss = 0.25820215480524156, test acc = 0.9100000262260437, time = 0.004741668701171875\n",
      "Step 16 finished in 24.391269207000732, Train loss = 0.268901085172789, Test loss = 0.26870935558881826; Train Acc = 0.924950001736482, Test Acc = 0.9239000004529953\n",
      "Training at step=17, batch=0, train loss = 0.2631217063566069, train acc = 0.9200000166893005, time = 0.02067708969116211\n",
      "Training at step=17, batch=120, train loss = 0.15046858421589582, train acc = 0.949999988079071, time = 0.0203549861907959\n",
      "Training at step=17, batch=240, train loss = 0.1544098705113239, train acc = 0.9300000071525574, time = 0.020528554916381836\n",
      "Training at step=17, batch=360, train loss = 0.2362661648948022, train acc = 0.9300000071525574, time = 0.020397186279296875\n",
      "Training at step=17, batch=480, train loss = 0.317449511745244, train acc = 0.9300000071525574, time = 0.020576000213623047\n",
      "Testing at step=17, batch=0, test loss = 0.22380887204698297, test acc = 0.9300000071525574, time = 0.0048100948333740234\n",
      "Testing at step=17, batch=20, test loss = 0.4992179287715702, test acc = 0.8799999952316284, time = 0.004807949066162109\n",
      "Testing at step=17, batch=40, test loss = 0.2276358359976091, test acc = 0.9100000262260437, time = 0.004788398742675781\n",
      "Testing at step=17, batch=60, test loss = 0.37029533134671083, test acc = 0.9100000262260437, time = 0.004755258560180664\n",
      "Testing at step=17, batch=80, test loss = 0.37524362696187213, test acc = 0.9200000166893005, time = 0.004790782928466797\n",
      "Step 17 finished in 24.442296743392944, Train loss = 0.2671123268025921, Test loss = 0.27007205193782374; Train Acc = 0.9259833351771036, Test Acc = 0.9227000010013581\n",
      "Training at step=18, batch=0, train loss = 0.2956007582722138, train acc = 0.9100000262260437, time = 0.020618438720703125\n",
      "Training at step=18, batch=120, train loss = 0.37647642581509927, train acc = 0.8799999952316284, time = 0.02066826820373535\n",
      "Training at step=18, batch=240, train loss = 0.19113524135090457, train acc = 0.9599999785423279, time = 0.020449161529541016\n",
      "Training at step=18, batch=360, train loss = 0.2193770330800215, train acc = 0.949999988079071, time = 0.020555496215820312\n",
      "Training at step=18, batch=480, train loss = 0.18562826564958043, train acc = 0.9399999976158142, time = 0.0205080509185791\n",
      "Testing at step=18, batch=0, test loss = 0.2398312328452395, test acc = 0.9399999976158142, time = 0.004838705062866211\n",
      "Testing at step=18, batch=20, test loss = 0.23891490547266606, test acc = 0.949999988079071, time = 0.0047206878662109375\n",
      "Testing at step=18, batch=40, test loss = 0.14541060582852827, test acc = 0.949999988079071, time = 0.004738330841064453\n",
      "Testing at step=18, batch=60, test loss = 0.18511736835873976, test acc = 0.9200000166893005, time = 0.004744291305541992\n",
      "Testing at step=18, batch=80, test loss = 0.27956821266226095, test acc = 0.8999999761581421, time = 0.004819631576538086\n",
      "Step 18 finished in 24.49567699432373, Train loss = 0.2663131263038273, Test loss = 0.26642963351844295; Train Acc = 0.9262000040213267, Test Acc = 0.9243000012636184\n",
      "Training at step=19, batch=0, train loss = 0.3517556918037068, train acc = 0.8999999761581421, time = 0.020658016204833984\n",
      "Training at step=19, batch=120, train loss = 0.16392839708847715, train acc = 0.949999988079071, time = 0.02045297622680664\n",
      "Training at step=19, batch=240, train loss = 0.3054267195251697, train acc = 0.8999999761581421, time = 0.020634889602661133\n",
      "Training at step=19, batch=360, train loss = 0.28342157470329477, train acc = 0.9399999976158142, time = 0.020676612854003906\n",
      "Training at step=19, batch=480, train loss = 0.2813408955038892, train acc = 0.9200000166893005, time = 0.020565032958984375\n",
      "Testing at step=19, batch=0, test loss = 0.2898858775215698, test acc = 0.9599999785423279, time = 0.004850864410400391\n",
      "Testing at step=19, batch=20, test loss = 0.30878870093354993, test acc = 0.9200000166893005, time = 0.0047376155853271484\n",
      "Testing at step=19, batch=40, test loss = 0.24083458949292386, test acc = 0.8899999856948853, time = 0.00479435920715332\n",
      "Testing at step=19, batch=60, test loss = 0.27850990162385203, test acc = 0.9300000071525574, time = 0.004759311676025391\n",
      "Testing at step=19, batch=80, test loss = 0.25765664253030635, test acc = 0.9399999976158142, time = 0.0048618316650390625\n",
      "Step 19 finished in 24.472853899002075, Train loss = 0.26585298654844963, Test loss = 0.26931856960788997; Train Acc = 0.9260333344340325, Test Acc = 0.923700003027916\n",
      "Training at step=20, batch=0, train loss = 0.21585115798075713, train acc = 0.9300000071525574, time = 0.020525693893432617\n",
      "Training at step=20, batch=120, train loss = 0.30546959633537557, train acc = 0.9200000166893005, time = 0.0204010009765625\n",
      "Training at step=20, batch=240, train loss = 0.23744205801270715, train acc = 0.9300000071525574, time = 0.020685434341430664\n",
      "Training at step=20, batch=360, train loss = 0.29098622823975445, train acc = 0.9200000166893005, time = 0.020487308502197266\n",
      "Training at step=20, batch=480, train loss = 0.31776508705559153, train acc = 0.8600000143051147, time = 0.020590543746948242\n",
      "Testing at step=20, batch=0, test loss = 0.24335790967270227, test acc = 0.9300000071525574, time = 0.0047512054443359375\n",
      "Testing at step=20, batch=20, test loss = 0.3709986847160172, test acc = 0.8899999856948853, time = 0.004784345626831055\n",
      "Testing at step=20, batch=40, test loss = 0.2527142473958103, test acc = 0.9100000262260437, time = 0.0047817230224609375\n",
      "Testing at step=20, batch=60, test loss = 0.18664258413603954, test acc = 0.9300000071525574, time = 0.004738330841064453\n",
      "Testing at step=20, batch=80, test loss = 0.15814379979793924, test acc = 0.9700000286102295, time = 0.0047702789306640625\n",
      "Step 20 finished in 24.528797149658203, Train loss = 0.26482815129520637, Test loss = 0.2661712241712168; Train Acc = 0.9261166694760322, Test Acc = 0.9245000010728837\n",
      "Training at step=21, batch=0, train loss = 0.1896446332723762, train acc = 0.9700000286102295, time = 0.02049541473388672\n",
      "Training at step=21, batch=120, train loss = 0.20753669576762085, train acc = 0.9100000262260437, time = 0.020469188690185547\n",
      "Training at step=21, batch=240, train loss = 0.4267592477438985, train acc = 0.8899999856948853, time = 0.02059793472290039\n",
      "Training at step=21, batch=360, train loss = 0.321910559057261, train acc = 0.9300000071525574, time = 0.020496368408203125\n",
      "Training at step=21, batch=480, train loss = 0.22784745814593316, train acc = 0.9300000071525574, time = 0.020674467086791992\n",
      "Testing at step=21, batch=0, test loss = 0.1532003625443305, test acc = 0.9599999785423279, time = 0.00474095344543457\n",
      "Testing at step=21, batch=20, test loss = 0.24207671179891727, test acc = 0.9399999976158142, time = 0.004754543304443359\n",
      "Testing at step=21, batch=40, test loss = 0.2398339032532692, test acc = 0.949999988079071, time = 0.00476527214050293\n",
      "Testing at step=21, batch=60, test loss = 0.25258981907343003, test acc = 0.9300000071525574, time = 0.004723548889160156\n",
      "Testing at step=21, batch=80, test loss = 0.13579875989132703, test acc = 0.9599999785423279, time = 0.004729509353637695\n",
      "Step 21 finished in 24.30355215072632, Train loss = 0.26395299999772853, Test loss = 0.26676624393714127; Train Acc = 0.9271500017245611, Test Acc = 0.9249000012874603\n",
      "Training at step=22, batch=0, train loss = 0.16947188506525243, train acc = 0.9800000190734863, time = 0.020643234252929688\n",
      "Training at step=22, batch=120, train loss = 0.21845978277046313, train acc = 0.9100000262260437, time = 0.02040410041809082\n",
      "Training at step=22, batch=240, train loss = 0.18409567454297052, train acc = 0.949999988079071, time = 0.0205233097076416\n",
      "Training at step=22, batch=360, train loss = 0.2040887481405257, train acc = 0.949999988079071, time = 0.02068018913269043\n",
      "Training at step=22, batch=480, train loss = 0.2023578073525844, train acc = 0.949999988079071, time = 0.020470142364501953\n",
      "Testing at step=22, batch=0, test loss = 0.09806356236900406, test acc = 0.9900000095367432, time = 0.004833698272705078\n",
      "Testing at step=22, batch=20, test loss = 0.09129663131371622, test acc = 0.9700000286102295, time = 0.004749774932861328\n",
      "Testing at step=22, batch=40, test loss = 0.2967407917807911, test acc = 0.9399999976158142, time = 0.004738807678222656\n",
      "Testing at step=22, batch=60, test loss = 0.22382476412012856, test acc = 0.8999999761581421, time = 0.004771709442138672\n",
      "Testing at step=22, batch=80, test loss = 0.33620081935915974, test acc = 0.8999999761581421, time = 0.004748344421386719\n",
      "Step 22 finished in 24.41673970222473, Train loss = 0.26285757391452297, Test loss = 0.26761747283318643; Train Acc = 0.9271666692694028, Test Acc = 0.9232000023126602\n",
      "Training at step=23, batch=0, train loss = 0.28891764742801285, train acc = 0.9100000262260437, time = 0.02064657211303711\n",
      "Training at step=23, batch=120, train loss = 0.24843843340563423, train acc = 0.9300000071525574, time = 0.020444154739379883\n",
      "Training at step=23, batch=240, train loss = 0.15461079357474825, train acc = 0.9399999976158142, time = 0.02041172981262207\n",
      "Training at step=23, batch=360, train loss = 0.14608732231356095, train acc = 0.949999988079071, time = 0.020493268966674805\n",
      "Training at step=23, batch=480, train loss = 0.2076668327027893, train acc = 0.949999988079071, time = 0.02057933807373047\n",
      "Testing at step=23, batch=0, test loss = 0.28187775079748517, test acc = 0.9399999976158142, time = 0.004910945892333984\n",
      "Testing at step=23, batch=20, test loss = 0.4223423992075922, test acc = 0.9200000166893005, time = 0.0047397613525390625\n",
      "Testing at step=23, batch=40, test loss = 0.1451124152241092, test acc = 0.9599999785423279, time = 0.004738807678222656\n",
      "Testing at step=23, batch=60, test loss = 0.32512835347766783, test acc = 0.9100000262260437, time = 0.004744768142700195\n",
      "Testing at step=23, batch=80, test loss = 0.2711816743257059, test acc = 0.9200000166893005, time = 0.004817008972167969\n",
      "Step 23 finished in 24.462400913238525, Train loss = 0.26241549910230266, Test loss = 0.2679392685675514; Train Acc = 0.9274833358327548, Test Acc = 0.9246000039577484\n",
      "Training at step=24, batch=0, train loss = 0.19965239264880658, train acc = 0.9399999976158142, time = 0.0205991268157959\n",
      "Training at step=24, batch=120, train loss = 0.23577510871685362, train acc = 0.9200000166893005, time = 0.020442724227905273\n",
      "Training at step=24, batch=240, train loss = 0.14942509731490403, train acc = 0.9599999785423279, time = 0.020409107208251953\n",
      "Training at step=24, batch=360, train loss = 0.28419149351549283, train acc = 0.9399999976158142, time = 0.02043914794921875\n",
      "Training at step=24, batch=480, train loss = 0.20826252125952618, train acc = 0.9100000262260437, time = 0.020531654357910156\n",
      "Testing at step=24, batch=0, test loss = 0.24600778584913147, test acc = 0.9300000071525574, time = 0.004754781723022461\n",
      "Testing at step=24, batch=20, test loss = 0.19286403408274197, test acc = 0.9399999976158142, time = 0.0047245025634765625\n",
      "Testing at step=24, batch=40, test loss = 0.18976311694410952, test acc = 0.9599999785423279, time = 0.004727840423583984\n",
      "Testing at step=24, batch=60, test loss = 0.23866190150449593, test acc = 0.9200000166893005, time = 0.004730939865112305\n",
      "Testing at step=24, batch=80, test loss = 0.35376674473271774, test acc = 0.9200000166893005, time = 0.004740715026855469\n",
      "Step 24 finished in 24.33119297027588, Train loss = 0.2620794126828676, Test loss = 0.2688304655274656; Train Acc = 0.9272833348313967, Test Acc = 0.9237000000476837\n",
      "Training at step=25, batch=0, train loss = 0.1973998952755549, train acc = 0.9599999785423279, time = 0.020499467849731445\n",
      "Training at step=25, batch=120, train loss = 0.19683995202169652, train acc = 0.9200000166893005, time = 0.020432233810424805\n",
      "Training at step=25, batch=240, train loss = 0.3880066667850695, train acc = 0.9300000071525574, time = 0.02047133445739746\n",
      "Training at step=25, batch=360, train loss = 0.1927417996438382, train acc = 0.9399999976158142, time = 0.020554304122924805\n",
      "Training at step=25, batch=480, train loss = 0.2120156292575767, train acc = 0.9700000286102295, time = 0.02059650421142578\n",
      "Testing at step=25, batch=0, test loss = 0.2583184677452944, test acc = 0.8799999952316284, time = 0.00475311279296875\n",
      "Testing at step=25, batch=20, test loss = 0.19737523505908008, test acc = 0.9300000071525574, time = 0.004832267761230469\n",
      "Testing at step=25, batch=40, test loss = 0.30000879010642667, test acc = 0.9300000071525574, time = 0.004770755767822266\n",
      "Testing at step=25, batch=60, test loss = 0.2673350134101888, test acc = 0.8999999761581421, time = 0.004723787307739258\n",
      "Testing at step=25, batch=80, test loss = 0.3306746979423629, test acc = 0.9100000262260437, time = 0.004724264144897461\n",
      "Step 25 finished in 24.28699564933777, Train loss = 0.2608633202063117, Test loss = 0.26677697755756646; Train Acc = 0.9279000014066696, Test Acc = 0.9233000010251999\n",
      "Training at step=26, batch=0, train loss = 0.28156581884637577, train acc = 0.8999999761581421, time = 0.020454883575439453\n",
      "Training at step=26, batch=120, train loss = 0.1661944762995779, train acc = 0.9399999976158142, time = 0.020497798919677734\n",
      "Training at step=26, batch=240, train loss = 0.2293803095707564, train acc = 0.9200000166893005, time = 0.02049541473388672\n",
      "Training at step=26, batch=360, train loss = 0.21874877449046004, train acc = 0.9300000071525574, time = 0.020342588424682617\n",
      "Training at step=26, batch=480, train loss = 0.21939151238974902, train acc = 0.9200000166893005, time = 0.020670652389526367\n",
      "Testing at step=26, batch=0, test loss = 0.4480981793717198, test acc = 0.8999999761581421, time = 0.00478816032409668\n",
      "Testing at step=26, batch=20, test loss = 0.164046880133702, test acc = 0.9700000286102295, time = 0.00472259521484375\n",
      "Testing at step=26, batch=40, test loss = 0.2846073641328238, test acc = 0.9200000166893005, time = 0.004744529724121094\n",
      "Testing at step=26, batch=60, test loss = 0.23865626778199617, test acc = 0.9399999976158142, time = 0.004777193069458008\n",
      "Testing at step=26, batch=80, test loss = 0.12317619099209733, test acc = 0.949999988079071, time = 0.00478672981262207\n",
      "Step 26 finished in 24.41857385635376, Train loss = 0.26086375092872405, Test loss = 0.26762444634924537; Train Acc = 0.9272833350300789, Test Acc = 0.9253000020980835\n",
      "Training at step=27, batch=0, train loss = 0.13143249561047038, train acc = 0.9700000286102295, time = 0.020527124404907227\n",
      "Training at step=27, batch=120, train loss = 0.20098668550810153, train acc = 0.9599999785423279, time = 0.020470142364501953\n",
      "Training at step=27, batch=240, train loss = 0.19704054431312795, train acc = 0.9399999976158142, time = 0.020662784576416016\n",
      "Training at step=27, batch=360, train loss = 0.25442299080951297, train acc = 0.9100000262260437, time = 0.020493745803833008\n",
      "Training at step=27, batch=480, train loss = 0.36400478985023904, train acc = 0.9399999976158142, time = 0.02064228057861328\n",
      "Testing at step=27, batch=0, test loss = 0.16396075260855425, test acc = 0.949999988079071, time = 0.00480341911315918\n",
      "Testing at step=27, batch=20, test loss = 0.19094941137450036, test acc = 0.9599999785423279, time = 0.004730701446533203\n",
      "Testing at step=27, batch=40, test loss = 0.3032892074704236, test acc = 0.9100000262260437, time = 0.00473332405090332\n",
      "Testing at step=27, batch=60, test loss = 0.29488180874054226, test acc = 0.8899999856948853, time = 0.004716396331787109\n",
      "Testing at step=27, batch=80, test loss = 0.3461499039932535, test acc = 0.9599999785423279, time = 0.004765510559082031\n",
      "Step 27 finished in 24.363844394683838, Train loss = 0.26015907997065674, Test loss = 0.2654984728280189; Train Acc = 0.9280666692058245, Test Acc = 0.9245000004768371\n",
      "Training at step=28, batch=0, train loss = 0.18089681780088662, train acc = 0.949999988079071, time = 0.020534276962280273\n",
      "Training at step=28, batch=120, train loss = 0.28156567498184554, train acc = 0.9200000166893005, time = 0.02051854133605957\n",
      "Training at step=28, batch=240, train loss = 0.3722154854313222, train acc = 0.8799999952316284, time = 0.020578622817993164\n",
      "Training at step=28, batch=360, train loss = 0.18921977532071452, train acc = 0.9599999785423279, time = 0.020482778549194336\n",
      "Training at step=28, batch=480, train loss = 0.31201663227492654, train acc = 0.8999999761581421, time = 0.020380020141601562\n",
      "Testing at step=28, batch=0, test loss = 0.1813851220974918, test acc = 0.9399999976158142, time = 0.0048601627349853516\n",
      "Testing at step=28, batch=20, test loss = 0.13127492534216473, test acc = 0.9700000286102295, time = 0.004896640777587891\n",
      "Testing at step=28, batch=40, test loss = 0.2037440354645688, test acc = 0.9399999976158142, time = 0.0048198699951171875\n",
      "Testing at step=28, batch=60, test loss = 0.14528907219870352, test acc = 0.9399999976158142, time = 0.004723787307739258\n",
      "Testing at step=28, batch=80, test loss = 0.29374883846131133, test acc = 0.9200000166893005, time = 0.004736661911010742\n",
      "Step 28 finished in 24.401682376861572, Train loss = 0.2594326637453798, Test loss = 0.26494726582442746; Train Acc = 0.9276166683435441, Test Acc = 0.9252000027894973\n",
      "Training at step=29, batch=0, train loss = 0.23224086291408227, train acc = 0.9300000071525574, time = 0.02057361602783203\n",
      "Training at step=29, batch=120, train loss = 0.24598988314778217, train acc = 0.8999999761581421, time = 0.020671844482421875\n",
      "Training at step=29, batch=240, train loss = 0.2603371342014514, train acc = 0.9399999976158142, time = 0.020470857620239258\n",
      "Training at step=29, batch=360, train loss = 0.21765295412115537, train acc = 0.9300000071525574, time = 0.020442485809326172\n",
      "Training at step=29, batch=480, train loss = 0.30479950157280084, train acc = 0.8799999952316284, time = 0.020465373992919922\n",
      "Testing at step=29, batch=0, test loss = 0.2375019777496473, test acc = 0.9100000262260437, time = 0.004785060882568359\n",
      "Testing at step=29, batch=20, test loss = 0.20462206105554265, test acc = 0.949999988079071, time = 0.004852294921875\n",
      "Testing at step=29, batch=40, test loss = 0.2515920450691521, test acc = 0.9399999976158142, time = 0.0047550201416015625\n",
      "Testing at step=29, batch=60, test loss = 0.2218727768200587, test acc = 0.9300000071525574, time = 0.004772186279296875\n",
      "Testing at step=29, batch=80, test loss = 0.2736047018066931, test acc = 0.9200000166893005, time = 0.004723548889160156\n",
      "Step 29 finished in 24.546605348587036, Train loss = 0.2585935668907364, Test loss = 0.27070597536523067; Train Acc = 0.9283500010768573, Test Acc = 0.9224000018835068\n",
      "Training at step=30, batch=0, train loss = 0.298653545792923, train acc = 0.8999999761581421, time = 0.02058863639831543\n",
      "Training at step=30, batch=120, train loss = 0.2577341042495082, train acc = 0.8999999761581421, time = 0.020648479461669922\n",
      "Training at step=30, batch=240, train loss = 0.25616934064515323, train acc = 0.9399999976158142, time = 0.02051830291748047\n",
      "Training at step=30, batch=360, train loss = 0.2716674568480712, train acc = 0.9300000071525574, time = 0.02046513557434082\n",
      "Training at step=30, batch=480, train loss = 0.2835568649831215, train acc = 0.9100000262260437, time = 0.0204317569732666\n",
      "Testing at step=30, batch=0, test loss = 0.3806328533802263, test acc = 0.8899999856948853, time = 0.004820823669433594\n",
      "Testing at step=30, batch=20, test loss = 0.2523106801475881, test acc = 0.9200000166893005, time = 0.004725456237792969\n",
      "Testing at step=30, batch=40, test loss = 0.22979993317959926, test acc = 0.9200000166893005, time = 0.004792928695678711\n",
      "Testing at step=30, batch=60, test loss = 0.26414289466135804, test acc = 0.9399999976158142, time = 0.004778623580932617\n",
      "Testing at step=30, batch=80, test loss = 0.20866221254416575, test acc = 0.949999988079071, time = 0.004833221435546875\n",
      "Step 30 finished in 24.44144034385681, Train loss = 0.25859451324322774, Test loss = 0.26697389262595167; Train Acc = 0.9285166694720586, Test Acc = 0.9238000023365021\n",
      "Training at step=31, batch=0, train loss = 0.14957114974894914, train acc = 0.949999988079071, time = 0.020565271377563477\n",
      "Training at step=31, batch=120, train loss = 0.237840753067123, train acc = 0.9300000071525574, time = 0.02060866355895996\n",
      "Training at step=31, batch=240, train loss = 0.313782966689646, train acc = 0.9200000166893005, time = 0.020311832427978516\n",
      "Training at step=31, batch=360, train loss = 0.20328332217952233, train acc = 0.9399999976158142, time = 0.020564794540405273\n",
      "Training at step=31, batch=480, train loss = 0.24120951737938945, train acc = 0.9200000166893005, time = 0.020672321319580078\n",
      "Testing at step=31, batch=0, test loss = 0.34129478835940824, test acc = 0.9200000166893005, time = 0.004765987396240234\n",
      "Testing at step=31, batch=20, test loss = 0.2751074837845907, test acc = 0.9100000262260437, time = 0.004773378372192383\n",
      "Testing at step=31, batch=40, test loss = 0.35028663743846705, test acc = 0.9200000166893005, time = 0.004748344421386719\n",
      "Testing at step=31, batch=60, test loss = 0.22047300229127903, test acc = 0.949999988079071, time = 0.0047342777252197266\n",
      "Testing at step=31, batch=80, test loss = 0.20988907416049504, test acc = 0.9399999976158142, time = 0.004781484603881836\n",
      "Step 31 finished in 24.477076530456543, Train loss = 0.2575565375318612, Test loss = 0.26647351723395507; Train Acc = 0.9287500015894572, Test Acc = 0.9249000000953674\n",
      "Training at step=32, batch=0, train loss = 0.19086160547446848, train acc = 0.9800000190734863, time = 0.02065300941467285\n",
      "Training at step=32, batch=120, train loss = 0.24351979593875633, train acc = 0.8999999761581421, time = 0.020535707473754883\n",
      "Training at step=32, batch=240, train loss = 0.26544937202873925, train acc = 0.9399999976158142, time = 0.020445823669433594\n",
      "Training at step=32, batch=360, train loss = 0.25100835989230236, train acc = 0.9399999976158142, time = 0.020650625228881836\n",
      "Training at step=32, batch=480, train loss = 0.23282386771573244, train acc = 0.9399999976158142, time = 0.020615339279174805\n",
      "Testing at step=32, batch=0, test loss = 0.2975150980667729, test acc = 0.949999988079071, time = 0.004765987396240234\n",
      "Testing at step=32, batch=20, test loss = 0.31736296620909227, test acc = 0.9100000262260437, time = 0.004736423492431641\n",
      "Testing at step=32, batch=40, test loss = 0.3138641742774946, test acc = 0.9100000262260437, time = 0.0047376155853271484\n",
      "Testing at step=32, batch=60, test loss = 0.2659932926115392, test acc = 0.9100000262260437, time = 0.004734516143798828\n",
      "Testing at step=32, batch=80, test loss = 0.15989508914409212, test acc = 0.949999988079071, time = 0.004727602005004883\n",
      "Step 32 finished in 24.418139696121216, Train loss = 0.2576090404816504, Test loss = 0.26695659126814186; Train Acc = 0.9286166696747145, Test Acc = 0.9242000031471252\n",
      "Training at step=33, batch=0, train loss = 0.17127829834634323, train acc = 0.9399999976158142, time = 0.020592927932739258\n",
      "Training at step=33, batch=120, train loss = 0.20569206860702793, train acc = 0.9399999976158142, time = 0.020534276962280273\n",
      "Training at step=33, batch=240, train loss = 0.13274763257668423, train acc = 0.9599999785423279, time = 0.020511865615844727\n",
      "Training at step=33, batch=360, train loss = 0.3131955349111613, train acc = 0.9300000071525574, time = 0.020416736602783203\n",
      "Training at step=33, batch=480, train loss = 0.19589217465336645, train acc = 0.9300000071525574, time = 0.020550251007080078\n",
      "Testing at step=33, batch=0, test loss = 0.43538298168641254, test acc = 0.8799999952316284, time = 0.004830360412597656\n",
      "Testing at step=33, batch=20, test loss = 0.36222157155453794, test acc = 0.9200000166893005, time = 0.004729270935058594\n",
      "Testing at step=33, batch=40, test loss = 0.15750848575911883, test acc = 0.949999988079071, time = 0.0047359466552734375\n",
      "Testing at step=33, batch=60, test loss = 0.45604064410309975, test acc = 0.9200000166893005, time = 0.004725217819213867\n",
      "Testing at step=33, batch=80, test loss = 0.28026027750313753, test acc = 0.9399999976158142, time = 0.004715442657470703\n",
      "Step 33 finished in 24.453492641448975, Train loss = 0.2567336418622036, Test loss = 0.2644061306412169; Train Acc = 0.9289166677991549, Test Acc = 0.9255999994277954\n",
      "Training at step=34, batch=0, train loss = 0.29056376510875953, train acc = 0.9100000262260437, time = 0.020483016967773438\n",
      "Training at step=34, batch=120, train loss = 0.3454526832083553, train acc = 0.9100000262260437, time = 0.02042531967163086\n",
      "Training at step=34, batch=240, train loss = 0.05358732411036032, train acc = 1.0, time = 0.020415067672729492\n",
      "Training at step=34, batch=360, train loss = 0.23437856776698113, train acc = 0.9300000071525574, time = 0.020594120025634766\n",
      "Training at step=34, batch=480, train loss = 0.34364774510930457, train acc = 0.8700000047683716, time = 0.0205991268157959\n",
      "Testing at step=34, batch=0, test loss = 0.1940309130846462, test acc = 0.9200000166893005, time = 0.0048558712005615234\n",
      "Testing at step=34, batch=20, test loss = 0.14364027653998857, test acc = 0.9300000071525574, time = 0.0047702789306640625\n",
      "Testing at step=34, batch=40, test loss = 0.430660422355008, test acc = 0.9200000166893005, time = 0.004765748977661133\n",
      "Testing at step=34, batch=60, test loss = 0.2640502315925868, test acc = 0.9100000262260437, time = 0.004739522933959961\n",
      "Testing at step=34, batch=80, test loss = 0.1293972546680372, test acc = 0.9700000286102295, time = 0.004730224609375\n",
      "Step 34 finished in 24.44971013069153, Train loss = 0.2566224083451453, Test loss = 0.2684023360742621; Train Acc = 0.9290833355983098, Test Acc = 0.9253999996185303\n",
      "Training at step=35, batch=0, train loss = 0.36394263681571615, train acc = 0.9399999976158142, time = 0.02094864845275879\n",
      "Training at step=35, batch=120, train loss = 0.2571005917811338, train acc = 0.9200000166893005, time = 0.020554542541503906\n",
      "Training at step=35, batch=240, train loss = 0.17918370245292084, train acc = 0.9599999785423279, time = 0.02056097984313965\n",
      "Training at step=35, batch=360, train loss = 0.4806963517200839, train acc = 0.8999999761581421, time = 0.02045273780822754\n",
      "Training at step=35, batch=480, train loss = 0.2189666705934238, train acc = 0.949999988079071, time = 0.020457744598388672\n",
      "Testing at step=35, batch=0, test loss = 0.2130777732027246, test acc = 0.9300000071525574, time = 0.004764556884765625\n",
      "Testing at step=35, batch=20, test loss = 0.1308104916027369, test acc = 0.9599999785423279, time = 0.004758358001708984\n",
      "Testing at step=35, batch=40, test loss = 0.29492527877597413, test acc = 0.9200000166893005, time = 0.004758596420288086\n",
      "Testing at step=35, batch=60, test loss = 0.2660442492458723, test acc = 0.9399999976158142, time = 0.004746437072753906\n",
      "Testing at step=35, batch=80, test loss = 0.3115746102807673, test acc = 0.9300000071525574, time = 0.00475001335144043\n",
      "Step 35 finished in 24.53442144393921, Train loss = 0.2558132658432632, Test loss = 0.26495627845975006; Train Acc = 0.9296666691700618, Test Acc = 0.9259000021219254\n",
      "Training at step=36, batch=0, train loss = 0.20804980542073165, train acc = 0.9300000071525574, time = 0.02057170867919922\n",
      "Training at step=36, batch=120, train loss = 0.3168101468958283, train acc = 0.8899999856948853, time = 0.020584821701049805\n",
      "Training at step=36, batch=240, train loss = 0.2061516357236361, train acc = 0.9300000071525574, time = 0.02073812484741211\n",
      "Training at step=36, batch=360, train loss = 0.4638122465211579, train acc = 0.8999999761581421, time = 0.020422935485839844\n",
      "Training at step=36, batch=480, train loss = 0.24097434391518938, train acc = 0.8999999761581421, time = 0.020604848861694336\n",
      "Testing at step=36, batch=0, test loss = 0.18057393561678992, test acc = 0.9200000166893005, time = 0.004745006561279297\n",
      "Testing at step=36, batch=20, test loss = 0.376520669700073, test acc = 0.9200000166893005, time = 0.004824399948120117\n",
      "Testing at step=36, batch=40, test loss = 0.16471175700908913, test acc = 0.949999988079071, time = 0.004813432693481445\n",
      "Testing at step=36, batch=60, test loss = 0.1493751034439763, test acc = 0.949999988079071, time = 0.0048673152923583984\n",
      "Testing at step=36, batch=80, test loss = 0.4751684777699904, test acc = 0.9300000071525574, time = 0.004877567291259766\n",
      "Step 36 finished in 24.41001534461975, Train loss = 0.25553085035458784, Test loss = 0.2672432173181219; Train Acc = 0.9298666684826216, Test Acc = 0.9253000015020371\n",
      "Training at step=37, batch=0, train loss = 0.2420733906694516, train acc = 0.9200000166893005, time = 0.020609378814697266\n",
      "Training at step=37, batch=120, train loss = 0.31199534080847846, train acc = 0.9300000071525574, time = 0.02063775062561035\n",
      "Training at step=37, batch=240, train loss = 0.2702806274950494, train acc = 0.8999999761581421, time = 0.020400524139404297\n",
      "Training at step=37, batch=360, train loss = 0.5508762136695897, train acc = 0.8500000238418579, time = 0.02064347267150879\n",
      "Training at step=37, batch=480, train loss = 0.2643958863393441, train acc = 0.949999988079071, time = 0.02051997184753418\n",
      "Testing at step=37, batch=0, test loss = 0.31607734398675974, test acc = 0.8999999761581421, time = 0.004903078079223633\n",
      "Testing at step=37, batch=20, test loss = 0.16947620939999447, test acc = 0.949999988079071, time = 0.004741191864013672\n",
      "Testing at step=37, batch=40, test loss = 0.24245684071352827, test acc = 0.9200000166893005, time = 0.0047607421875\n",
      "Testing at step=37, batch=60, test loss = 0.2543657431582175, test acc = 0.9599999785423279, time = 0.004805803298950195\n",
      "Testing at step=37, batch=80, test loss = 0.24161455762429437, test acc = 0.949999988079071, time = 0.004765033721923828\n",
      "Step 37 finished in 24.523566246032715, Train loss = 0.2552990797577261, Test loss = 0.2651860359126042; Train Acc = 0.9293666687607766, Test Acc = 0.9264000004529953\n",
      "Training at step=38, batch=0, train loss = 0.20741304005391928, train acc = 0.9399999976158142, time = 0.020690202713012695\n",
      "Training at step=38, batch=120, train loss = 0.1624218776177329, train acc = 0.949999988079071, time = 0.02057814598083496\n",
      "Training at step=38, batch=240, train loss = 0.44574360752425357, train acc = 0.8799999952316284, time = 0.021309375762939453\n",
      "Training at step=38, batch=360, train loss = 0.144404983830839, train acc = 0.9599999785423279, time = 0.02071237564086914\n",
      "Training at step=38, batch=480, train loss = 0.20466451711143782, train acc = 0.9200000166893005, time = 0.02045297622680664\n",
      "Testing at step=38, batch=0, test loss = 0.38844147923341554, test acc = 0.9300000071525574, time = 0.004758119583129883\n",
      "Testing at step=38, batch=20, test loss = 0.2558692804304743, test acc = 0.9399999976158142, time = 0.004725217819213867\n",
      "Testing at step=38, batch=40, test loss = 0.1505669410522698, test acc = 0.9399999976158142, time = 0.004832267761230469\n",
      "Testing at step=38, batch=60, test loss = 0.20800281479476918, test acc = 0.949999988079071, time = 0.004756450653076172\n",
      "Testing at step=38, batch=80, test loss = 0.12603962277111624, test acc = 0.9700000286102295, time = 0.004750490188598633\n",
      "Step 38 finished in 24.536615133285522, Train loss = 0.25466209716640503, Test loss = 0.2680523077859647; Train Acc = 0.9294500013192495, Test Acc = 0.9255000054836273\n",
      "Training at step=39, batch=0, train loss = 0.2922371153169627, train acc = 0.9100000262260437, time = 0.020511627197265625\n",
      "Training at step=39, batch=120, train loss = 0.30280937658724605, train acc = 0.8700000047683716, time = 0.020706653594970703\n",
      "Training at step=39, batch=240, train loss = 0.25996918695077925, train acc = 0.9300000071525574, time = 0.02051544189453125\n",
      "Training at step=39, batch=360, train loss = 0.2956120977120917, train acc = 0.9100000262260437, time = 0.020529747009277344\n",
      "Training at step=39, batch=480, train loss = 0.5311876067494586, train acc = 0.8999999761581421, time = 0.020474910736083984\n",
      "Testing at step=39, batch=0, test loss = 0.3804008745555346, test acc = 0.8799999952316284, time = 0.004922151565551758\n",
      "Testing at step=39, batch=20, test loss = 0.29188819656961235, test acc = 0.8999999761581421, time = 0.00473475456237793\n",
      "Testing at step=39, batch=40, test loss = 0.3046804944539571, test acc = 0.9100000262260437, time = 0.004751682281494141\n",
      "Testing at step=39, batch=60, test loss = 0.2671358516659872, test acc = 0.9300000071525574, time = 0.0047190189361572266\n",
      "Testing at step=39, batch=80, test loss = 0.1827936449755338, test acc = 0.9599999785423279, time = 0.0047245025634765625\n",
      "Step 39 finished in 24.46641445159912, Train loss = 0.254331965566098, Test loss = 0.2651971514041863; Train Acc = 0.9295000018676122, Test Acc = 0.9252000021934509\n",
      "Training at step=40, batch=0, train loss = 0.35456126055943776, train acc = 0.9399999976158142, time = 0.020638227462768555\n",
      "Training at step=40, batch=120, train loss = 0.31688993258822895, train acc = 0.9300000071525574, time = 0.020544767379760742\n",
      "Training at step=40, batch=240, train loss = 0.3130441787912625, train acc = 0.8899999856948853, time = 0.02063727378845215\n",
      "Training at step=40, batch=360, train loss = 0.20522246843068237, train acc = 0.9599999785423279, time = 0.020587682723999023\n",
      "Training at step=40, batch=480, train loss = 0.4656434154917168, train acc = 0.8899999856948853, time = 0.020411252975463867\n",
      "Testing at step=40, batch=0, test loss = 0.1856146090478599, test acc = 0.9700000286102295, time = 0.004825592041015625\n",
      "Testing at step=40, batch=20, test loss = 0.48128220183079257, test acc = 0.8600000143051147, time = 0.004736661911010742\n",
      "Testing at step=40, batch=40, test loss = 0.38413253189586805, test acc = 0.8899999856948853, time = 0.0047647953033447266\n",
      "Testing at step=40, batch=60, test loss = 0.20936704761020639, test acc = 0.949999988079071, time = 0.0047435760498046875\n",
      "Testing at step=40, batch=80, test loss = 0.3287487195160656, test acc = 0.8999999761581421, time = 0.004803180694580078\n",
      "Step 40 finished in 24.482608795166016, Train loss = 0.25398052231132584, Test loss = 0.26420313865350814; Train Acc = 0.9299666676918665, Test Acc = 0.925100001692772\n",
      "Training at step=41, batch=0, train loss = 0.29227986876696166, train acc = 0.9300000071525574, time = 0.02071356773376465\n",
      "Training at step=41, batch=120, train loss = 0.11120333835317094, train acc = 0.9700000286102295, time = 0.020552396774291992\n",
      "Training at step=41, batch=240, train loss = 0.13674826065596382, train acc = 0.949999988079071, time = 0.0204317569732666\n",
      "Training at step=41, batch=360, train loss = 0.2616992431033567, train acc = 0.9300000071525574, time = 0.020650386810302734\n",
      "Training at step=41, batch=480, train loss = 0.28094173143473694, train acc = 0.9200000166893005, time = 0.02041149139404297\n",
      "Testing at step=41, batch=0, test loss = 0.2347287178137363, test acc = 0.9100000262260437, time = 0.00483393669128418\n",
      "Testing at step=41, batch=20, test loss = 0.3313617874784343, test acc = 0.9100000262260437, time = 0.004763126373291016\n",
      "Testing at step=41, batch=40, test loss = 0.18216207178304056, test acc = 0.9300000071525574, time = 0.004753828048706055\n",
      "Testing at step=41, batch=60, test loss = 0.24825099760264724, test acc = 0.8799999952316284, time = 0.004775524139404297\n",
      "Testing at step=41, batch=80, test loss = 0.24772635004422283, test acc = 0.949999988079071, time = 0.004790544509887695\n",
      "Step 41 finished in 24.553921222686768, Train loss = 0.25390219883335396, Test loss = 0.26229418267494525; Train Acc = 0.9298000005880992, Test Acc = 0.9261000025272369\n",
      "Training at step=42, batch=0, train loss = 0.2357803730669603, train acc = 0.9300000071525574, time = 0.02065587043762207\n",
      "Training at step=42, batch=120, train loss = 0.448881361347089, train acc = 0.8999999761581421, time = 0.020405054092407227\n",
      "Training at step=42, batch=240, train loss = 0.3188287850540124, train acc = 0.9300000071525574, time = 0.02038741111755371\n",
      "Training at step=42, batch=360, train loss = 0.5730228804517279, train acc = 0.8999999761581421, time = 0.02064061164855957\n",
      "Training at step=42, batch=480, train loss = 0.22954214581589713, train acc = 0.9300000071525574, time = 0.020496129989624023\n",
      "Testing at step=42, batch=0, test loss = 0.24255808272268328, test acc = 0.9399999976158142, time = 0.004788637161254883\n",
      "Testing at step=42, batch=20, test loss = 0.45606213978354637, test acc = 0.8700000047683716, time = 0.004731893539428711\n",
      "Testing at step=42, batch=40, test loss = 0.31605742339439497, test acc = 0.9100000262260437, time = 0.004725456237792969\n",
      "Testing at step=42, batch=60, test loss = 0.3354970224927453, test acc = 0.8899999856948853, time = 0.004725456237792969\n",
      "Testing at step=42, batch=80, test loss = 0.37001512579883944, test acc = 0.8799999952316284, time = 0.0047380924224853516\n",
      "Step 42 finished in 24.461106777191162, Train loss = 0.2532640744529608, Test loss = 0.26371891473033615; Train Acc = 0.930166667898496, Test Acc = 0.9261000031232833\n",
      "Training at step=43, batch=0, train loss = 0.2215027570382518, train acc = 0.9700000286102295, time = 0.02067852020263672\n",
      "Training at step=43, batch=120, train loss = 0.23375300854557404, train acc = 0.9399999976158142, time = 0.020533084869384766\n",
      "Training at step=43, batch=240, train loss = 0.3048766166611476, train acc = 0.9200000166893005, time = 0.020587444305419922\n",
      "Training at step=43, batch=360, train loss = 0.29968458130132225, train acc = 0.9300000071525574, time = 0.020427703857421875\n",
      "Training at step=43, batch=480, train loss = 0.33795760728815283, train acc = 0.8999999761581421, time = 0.020483016967773438\n",
      "Testing at step=43, batch=0, test loss = 0.32115324944302925, test acc = 0.8799999952316284, time = 0.004833221435546875\n",
      "Testing at step=43, batch=20, test loss = 0.2305758977193484, test acc = 0.9399999976158142, time = 0.0048236846923828125\n",
      "Testing at step=43, batch=40, test loss = 0.2277977568928614, test acc = 0.9100000262260437, time = 0.0047757625579833984\n",
      "Testing at step=43, batch=60, test loss = 0.5251513619400116, test acc = 0.9399999976158142, time = 0.004742622375488281\n",
      "Testing at step=43, batch=80, test loss = 0.21570030565647524, test acc = 0.949999988079071, time = 0.004721403121948242\n",
      "Step 43 finished in 24.43679904937744, Train loss = 0.2527323210937841, Test loss = 0.2690877956767913; Train Acc = 0.9303500019510587, Test Acc = 0.9258000022172928\n",
      "Training at step=44, batch=0, train loss = 0.27229781848620727, train acc = 0.949999988079071, time = 0.020535707473754883\n",
      "Training at step=44, batch=120, train loss = 0.3179065382866473, train acc = 0.9399999976158142, time = 0.020548105239868164\n",
      "Training at step=44, batch=240, train loss = 0.2926907445340547, train acc = 0.8999999761581421, time = 0.02067255973815918\n",
      "Training at step=44, batch=360, train loss = 0.2796881165492398, train acc = 0.9300000071525574, time = 0.020479679107666016\n",
      "Training at step=44, batch=480, train loss = 0.3935006776981036, train acc = 0.9300000071525574, time = 0.020474910736083984\n",
      "Testing at step=44, batch=0, test loss = 0.16997879700034393, test acc = 0.9100000262260437, time = 0.004816532135009766\n",
      "Testing at step=44, batch=20, test loss = 0.2326832050884696, test acc = 0.9200000166893005, time = 0.004788398742675781\n",
      "Testing at step=44, batch=40, test loss = 0.2851513421113874, test acc = 0.9200000166893005, time = 0.004745960235595703\n",
      "Testing at step=44, batch=60, test loss = 0.2628948817011061, test acc = 0.9399999976158142, time = 0.004746675491333008\n",
      "Testing at step=44, batch=80, test loss = 0.182331124328248, test acc = 0.9399999976158142, time = 0.004757404327392578\n",
      "Step 44 finished in 24.480573654174805, Train loss = 0.25279938752193687, Test loss = 0.2676220620144905; Train Acc = 0.9299666688839594, Test Acc = 0.9241000026464462\n",
      "Training at step=45, batch=0, train loss = 0.19062321994238698, train acc = 0.9599999785423279, time = 0.02075338363647461\n",
      "Training at step=45, batch=120, train loss = 0.23055008336427224, train acc = 0.9100000262260437, time = 0.020447969436645508\n",
      "Training at step=45, batch=240, train loss = 0.29455149815843934, train acc = 0.9399999976158142, time = 0.020421981811523438\n",
      "Training at step=45, batch=360, train loss = 0.21972152726550165, train acc = 0.949999988079071, time = 0.02072906494140625\n",
      "Training at step=45, batch=480, train loss = 0.2974661656746639, train acc = 0.9100000262260437, time = 0.020668506622314453\n",
      "Testing at step=45, batch=0, test loss = 0.25605715030772613, test acc = 0.9300000071525574, time = 0.0049211978912353516\n",
      "Testing at step=45, batch=20, test loss = 0.15964243192565075, test acc = 0.9700000286102295, time = 0.004744529724121094\n",
      "Testing at step=45, batch=40, test loss = 0.31316529097594575, test acc = 0.8899999856948853, time = 0.00473475456237793\n",
      "Testing at step=45, batch=60, test loss = 0.2431021678846842, test acc = 0.9200000166893005, time = 0.004740476608276367\n",
      "Testing at step=45, batch=80, test loss = 0.3226601019011943, test acc = 0.8899999856948853, time = 0.004775047302246094\n",
      "Step 45 finished in 24.549811363220215, Train loss = 0.25217095146904445, Test loss = 0.2677445189961178; Train Acc = 0.9305833365519841, Test Acc = 0.9232000005245209\n",
      "Training at step=46, batch=0, train loss = 0.37734122327925, train acc = 0.9300000071525574, time = 0.020618677139282227\n",
      "Training at step=46, batch=120, train loss = 0.1663144757075634, train acc = 0.9800000190734863, time = 0.020555973052978516\n",
      "Training at step=46, batch=240, train loss = 0.23638161130284557, train acc = 0.949999988079071, time = 0.020457983016967773\n",
      "Training at step=46, batch=360, train loss = 0.2511448809785797, train acc = 0.9300000071525574, time = 0.020606517791748047\n",
      "Training at step=46, batch=480, train loss = 0.22618298959959074, train acc = 0.9700000286102295, time = 0.020639419555664062\n",
      "Testing at step=46, batch=0, test loss = 0.16731537599776874, test acc = 0.9599999785423279, time = 0.0049741268157958984\n",
      "Testing at step=46, batch=20, test loss = 0.29584615350530535, test acc = 0.9300000071525574, time = 0.004757881164550781\n",
      "Testing at step=46, batch=40, test loss = 0.21937807582780913, test acc = 0.9200000166893005, time = 0.0047283172607421875\n",
      "Testing at step=46, batch=60, test loss = 0.34735262466852745, test acc = 0.8799999952316284, time = 0.004740715026855469\n",
      "Testing at step=46, batch=80, test loss = 0.3011089605400485, test acc = 0.8899999856948853, time = 0.004734516143798828\n",
      "Step 46 finished in 24.471511840820312, Train loss = 0.2518863069598945, Test loss = 0.26697595038008953; Train Acc = 0.9307666688164076, Test Acc = 0.9251999998092652\n",
      "Training at step=47, batch=0, train loss = 0.14690712376507464, train acc = 0.9599999785423279, time = 0.020667314529418945\n",
      "Training at step=47, batch=120, train loss = 0.23744584318161654, train acc = 0.8999999761581421, time = 0.020570039749145508\n",
      "Training at step=47, batch=240, train loss = 0.2869123310931269, train acc = 0.9200000166893005, time = 0.020724058151245117\n",
      "Training at step=47, batch=360, train loss = 0.30559321279004314, train acc = 0.8999999761581421, time = 0.020531415939331055\n",
      "Training at step=47, batch=480, train loss = 0.16871635794355616, train acc = 0.9399999976158142, time = 0.020323753356933594\n",
      "Testing at step=47, batch=0, test loss = 0.3575234299471419, test acc = 0.9200000166893005, time = 0.004800081253051758\n",
      "Testing at step=47, batch=20, test loss = 0.20926143885547013, test acc = 0.9399999976158142, time = 0.004805564880371094\n",
      "Testing at step=47, batch=40, test loss = 0.1714379997251056, test acc = 0.9399999976158142, time = 0.004767894744873047\n",
      "Testing at step=47, batch=60, test loss = 0.13715410364731326, test acc = 0.949999988079071, time = 0.004735231399536133\n",
      "Testing at step=47, batch=80, test loss = 0.0911640255433219, test acc = 0.9700000286102295, time = 0.0047397613525390625\n",
      "Step 47 finished in 24.61842703819275, Train loss = 0.2515530367722015, Test loss = 0.2671886043431673; Train Acc = 0.9307500017682712, Test Acc = 0.9267000031471252\n",
      "Training at step=48, batch=0, train loss = 0.21993300989082784, train acc = 0.9399999976158142, time = 0.02059483528137207\n",
      "Training at step=48, batch=120, train loss = 0.24590089468131882, train acc = 0.9399999976158142, time = 0.02074885368347168\n",
      "Training at step=48, batch=240, train loss = 0.1776850853294018, train acc = 0.949999988079071, time = 0.020692110061645508\n",
      "Training at step=48, batch=360, train loss = 0.19933697451717416, train acc = 0.949999988079071, time = 0.020502090454101562\n",
      "Training at step=48, batch=480, train loss = 0.31077101393328177, train acc = 0.9200000166893005, time = 0.020413637161254883\n",
      "Testing at step=48, batch=0, test loss = 0.21451926303817406, test acc = 0.9700000286102295, time = 0.004797458648681641\n",
      "Testing at step=48, batch=20, test loss = 0.32612140016808744, test acc = 0.9300000071525574, time = 0.004820108413696289\n",
      "Testing at step=48, batch=40, test loss = 0.27778665468907127, test acc = 0.8899999856948853, time = 0.004808187484741211\n",
      "Testing at step=48, batch=60, test loss = 0.21574623488259673, test acc = 0.9399999976158142, time = 0.004803180694580078\n",
      "Testing at step=48, batch=80, test loss = 0.2006026154931356, test acc = 0.9100000262260437, time = 0.004771232604980469\n",
      "Step 48 finished in 24.414729356765747, Train loss = 0.25129099939094357, Test loss = 0.26850379945929975; Train Acc = 0.9301000012954076, Test Acc = 0.9234999978542328\n",
      "Training at step=49, batch=0, train loss = 0.17649320425967727, train acc = 0.9399999976158142, time = 0.020743608474731445\n",
      "Training at step=49, batch=120, train loss = 0.2552827137330568, train acc = 0.9100000262260437, time = 0.020565271377563477\n",
      "Training at step=49, batch=240, train loss = 0.2544525212180102, train acc = 0.9300000071525574, time = 0.02066969871520996\n",
      "Training at step=49, batch=360, train loss = 0.2815830459653432, train acc = 0.9200000166893005, time = 0.02057814598083496\n",
      "Training at step=49, batch=480, train loss = 0.37242849206765294, train acc = 0.8700000047683716, time = 0.020606040954589844\n",
      "Testing at step=49, batch=0, test loss = 0.2182681685552565, test acc = 0.9300000071525574, time = 0.0047702789306640625\n",
      "Testing at step=49, batch=20, test loss = 0.11507934916896602, test acc = 0.9599999785423279, time = 0.0047550201416015625\n",
      "Testing at step=49, batch=40, test loss = 0.15933061741835228, test acc = 0.9599999785423279, time = 0.004769563674926758\n",
      "Testing at step=49, batch=60, test loss = 0.23494673791736492, test acc = 0.9300000071525574, time = 0.004748821258544922\n",
      "Testing at step=49, batch=80, test loss = 0.4137566666571256, test acc = 0.8600000143051147, time = 0.004740238189697266\n",
      "Step 49 finished in 24.508071899414062, Train loss = 0.2510714680100715, Test loss = 0.26666733848183183; Train Acc = 0.9310166673858961, Test Acc = 0.9259000021219254\n",
      "Training at step=50, batch=0, train loss = 0.14993192532807392, train acc = 0.949999988079071, time = 0.020694971084594727\n",
      "Training at step=50, batch=120, train loss = 0.2194190613307158, train acc = 0.9300000071525574, time = 0.020435094833374023\n",
      "Training at step=50, batch=240, train loss = 0.23320183007760534, train acc = 0.949999988079071, time = 0.020888090133666992\n",
      "Training at step=50, batch=360, train loss = 0.14640781288766325, train acc = 0.9599999785423279, time = 0.020639657974243164\n",
      "Training at step=50, batch=480, train loss = 0.35938690678189356, train acc = 0.8600000143051147, time = 0.02081155776977539\n",
      "Testing at step=50, batch=0, test loss = 0.20731441073809986, test acc = 0.9399999976158142, time = 0.0048406124114990234\n",
      "Testing at step=50, batch=20, test loss = 0.3106657970506466, test acc = 0.8999999761581421, time = 0.004725217819213867\n",
      "Testing at step=50, batch=40, test loss = 0.24919662962516065, test acc = 0.9100000262260437, time = 0.004717826843261719\n",
      "Testing at step=50, batch=60, test loss = 0.1376451945933121, test acc = 0.9700000286102295, time = 0.00471186637878418\n",
      "Testing at step=50, batch=80, test loss = 0.33436876657856657, test acc = 0.8899999856948853, time = 0.00471806526184082\n",
      "Step 50 finished in 24.54062294960022, Train loss = 0.2504141116173554, Test loss = 0.26611094670918667; Train Acc = 0.9308666701118151, Test Acc = 0.9266000014543533\n",
      "Training at step=51, batch=0, train loss = 0.16706723997646616, train acc = 0.9599999785423279, time = 0.02060222625732422\n",
      "Training at step=51, batch=120, train loss = 0.3627657182966446, train acc = 0.9200000166893005, time = 0.020505666732788086\n",
      "Training at step=51, batch=240, train loss = 0.38952622571825357, train acc = 0.8700000047683716, time = 0.02050304412841797\n",
      "Training at step=51, batch=360, train loss = 0.3454588602460531, train acc = 0.9399999976158142, time = 0.02056264877319336\n",
      "Training at step=51, batch=480, train loss = 0.18453640865988913, train acc = 0.9599999785423279, time = 0.02052927017211914\n",
      "Testing at step=51, batch=0, test loss = 0.28738811442911166, test acc = 0.9200000166893005, time = 0.0048825740814208984\n",
      "Testing at step=51, batch=20, test loss = 0.16763181182003745, test acc = 0.9599999785423279, time = 0.004781961441040039\n",
      "Testing at step=51, batch=40, test loss = 0.16297968915745845, test acc = 0.9200000166893005, time = 0.004774808883666992\n",
      "Testing at step=51, batch=60, test loss = 0.20109212406816035, test acc = 0.9700000286102295, time = 0.004727363586425781\n",
      "Testing at step=51, batch=80, test loss = 0.3702029284661335, test acc = 0.8799999952316284, time = 0.004751920700073242\n",
      "Step 51 finished in 24.448508739471436, Train loss = 0.2502169530789506, Test loss = 0.265353566121373; Train Acc = 0.9306500009695688, Test Acc = 0.9262000012397766\n",
      "Training at step=52, batch=0, train loss = 0.13984867425515765, train acc = 0.9599999785423279, time = 0.020646333694458008\n",
      "Training at step=52, batch=120, train loss = 0.14930621344401313, train acc = 0.9399999976158142, time = 0.02046990394592285\n",
      "Training at step=52, batch=240, train loss = 0.2248907385656361, train acc = 0.9300000071525574, time = 0.02048325538635254\n",
      "Training at step=52, batch=360, train loss = 0.26721451640101657, train acc = 0.9300000071525574, time = 0.02045464515686035\n",
      "Training at step=52, batch=480, train loss = 0.30120739446751404, train acc = 0.8999999761581421, time = 0.02082204818725586\n",
      "Testing at step=52, batch=0, test loss = 0.259948975304742, test acc = 0.9200000166893005, time = 0.004825115203857422\n",
      "Testing at step=52, batch=20, test loss = 0.20108143164320638, test acc = 0.949999988079071, time = 0.004760026931762695\n",
      "Testing at step=52, batch=40, test loss = 0.27592348555537166, test acc = 0.9399999976158142, time = 0.004742860794067383\n",
      "Testing at step=52, batch=60, test loss = 0.3386945944956538, test acc = 0.8799999952316284, time = 0.004745006561279297\n",
      "Testing at step=52, batch=80, test loss = 0.1877331333126694, test acc = 0.949999988079071, time = 0.004719972610473633\n",
      "Step 52 finished in 24.47325873374939, Train loss = 0.24984758330173415, Test loss = 0.2689798886100563; Train Acc = 0.9308500018715858, Test Acc = 0.9236000043153763\n",
      "Training at step=53, batch=0, train loss = 0.1921943638317542, train acc = 0.9100000262260437, time = 0.020752429962158203\n",
      "Training at step=53, batch=120, train loss = 0.22179741001618072, train acc = 0.9399999976158142, time = 0.02048468589782715\n",
      "Training at step=53, batch=240, train loss = 0.2511669608939243, train acc = 0.9100000262260437, time = 0.020771503448486328\n",
      "Training at step=53, batch=360, train loss = 0.20086565144804308, train acc = 0.9399999976158142, time = 0.020526885986328125\n",
      "Training at step=53, batch=480, train loss = 0.2749866325951792, train acc = 0.8899999856948853, time = 0.020414352416992188\n",
      "Testing at step=53, batch=0, test loss = 0.12836803830245724, test acc = 0.9599999785423279, time = 0.004782199859619141\n",
      "Testing at step=53, batch=20, test loss = 0.17799307841139622, test acc = 0.9599999785423279, time = 0.00476384162902832\n",
      "Testing at step=53, batch=40, test loss = 0.1789277484278822, test acc = 0.9399999976158142, time = 0.004749774932861328\n",
      "Testing at step=53, batch=60, test loss = 0.14281005201554703, test acc = 0.9399999976158142, time = 0.004792451858520508\n",
      "Testing at step=53, batch=80, test loss = 0.3181796612367108, test acc = 0.9100000262260437, time = 0.004775285720825195\n",
      "Step 53 finished in 24.516082048416138, Train loss = 0.24985132592433734, Test loss = 0.26557706421757843; Train Acc = 0.9314166679978371, Test Acc = 0.9248000025749207\n",
      "Training at step=54, batch=0, train loss = 0.20285544899248614, train acc = 0.949999988079071, time = 0.020758867263793945\n",
      "Training at step=54, batch=120, train loss = 0.2539436423121542, train acc = 0.9300000071525574, time = 0.020415544509887695\n",
      "Training at step=54, batch=240, train loss = 0.3478513926188648, train acc = 0.9100000262260437, time = 0.020510435104370117\n",
      "Training at step=54, batch=360, train loss = 0.33808767342398455, train acc = 0.8799999952316284, time = 0.020600557327270508\n",
      "Training at step=54, batch=480, train loss = 0.22103080723442742, train acc = 0.9399999976158142, time = 0.020569562911987305\n",
      "Testing at step=54, batch=0, test loss = 0.20024457687897115, test acc = 0.9200000166893005, time = 0.004897356033325195\n",
      "Testing at step=54, batch=20, test loss = 0.23268881742801273, test acc = 0.9300000071525574, time = 0.004728555679321289\n",
      "Testing at step=54, batch=40, test loss = 0.2760781548888262, test acc = 0.9100000262260437, time = 0.004785299301147461\n",
      "Testing at step=54, batch=60, test loss = 0.2570128408579626, test acc = 0.9399999976158142, time = 0.004772663116455078\n",
      "Testing at step=54, batch=80, test loss = 0.3890265879190008, test acc = 0.8899999856948853, time = 0.004740476608276367\n",
      "Step 54 finished in 24.367872714996338, Train loss = 0.24934672528035187, Test loss = 0.26850389730761215; Train Acc = 0.9313833355903626, Test Acc = 0.9232000017166138\n",
      "Training at step=55, batch=0, train loss = 0.1803795146359935, train acc = 0.949999988079071, time = 0.02057480812072754\n",
      "Training at step=55, batch=120, train loss = 0.29425512113655744, train acc = 0.9200000166893005, time = 0.020450592041015625\n",
      "Training at step=55, batch=240, train loss = 0.3761191671604418, train acc = 0.9200000166893005, time = 0.020423173904418945\n",
      "Training at step=55, batch=360, train loss = 0.30116063955875916, train acc = 0.9100000262260437, time = 0.02045154571533203\n",
      "Training at step=55, batch=480, train loss = 0.17562893910301106, train acc = 0.949999988079071, time = 0.020570039749145508\n",
      "Testing at step=55, batch=0, test loss = 0.29636665672120716, test acc = 0.9200000166893005, time = 0.004827737808227539\n",
      "Testing at step=55, batch=20, test loss = 0.1779765541129761, test acc = 0.9399999976158142, time = 0.004739046096801758\n",
      "Testing at step=55, batch=40, test loss = 0.3172969593969735, test acc = 0.9100000262260437, time = 0.0047321319580078125\n",
      "Testing at step=55, batch=60, test loss = 0.23470008986058738, test acc = 0.9200000166893005, time = 0.004792213439941406\n",
      "Testing at step=55, batch=80, test loss = 0.1821849850491661, test acc = 0.9399999976158142, time = 0.004754781723022461\n",
      "Step 55 finished in 24.417412042617798, Train loss = 0.24890342077024827, Test loss = 0.2666879898442945; Train Acc = 0.9307333354155223, Test Acc = 0.9247000044584275\n",
      "Training at step=56, batch=0, train loss = 0.23378290614754582, train acc = 0.9200000166893005, time = 0.020536422729492188\n",
      "Training at step=56, batch=120, train loss = 0.21395082680876848, train acc = 0.949999988079071, time = 0.020407915115356445\n",
      "Training at step=56, batch=240, train loss = 0.3096361981062492, train acc = 0.9200000166893005, time = 0.02073073387145996\n",
      "Training at step=56, batch=360, train loss = 0.2466200384079231, train acc = 0.9300000071525574, time = 0.020442962646484375\n",
      "Training at step=56, batch=480, train loss = 0.18674804844677662, train acc = 0.9599999785423279, time = 0.020531177520751953\n",
      "Testing at step=56, batch=0, test loss = 0.20972915289460922, test acc = 0.9599999785423279, time = 0.0047914981842041016\n",
      "Testing at step=56, batch=20, test loss = 0.29713990551692954, test acc = 0.8999999761581421, time = 0.004716157913208008\n",
      "Testing at step=56, batch=40, test loss = 0.12780966827893997, test acc = 0.949999988079071, time = 0.004748106002807617\n",
      "Testing at step=56, batch=60, test loss = 0.3406519624693399, test acc = 0.9200000166893005, time = 0.004719734191894531\n",
      "Testing at step=56, batch=80, test loss = 0.36938558918285536, test acc = 0.8299999833106995, time = 0.004853010177612305\n",
      "Step 56 finished in 24.447556972503662, Train loss = 0.248513146696488, Test loss = 0.26605176740690795; Train Acc = 0.931783335407575, Test Acc = 0.9248999989032746\n",
      "Training at step=57, batch=0, train loss = 0.16446633582008754, train acc = 0.9300000071525574, time = 0.02066826820373535\n",
      "Training at step=57, batch=120, train loss = 0.40154720393088206, train acc = 0.8999999761581421, time = 0.02040266990661621\n",
      "Training at step=57, batch=240, train loss = 0.24410501078896277, train acc = 0.9599999785423279, time = 0.020542383193969727\n",
      "Training at step=57, batch=360, train loss = 0.2301076886590139, train acc = 0.9300000071525574, time = 0.02073073387145996\n",
      "Training at step=57, batch=480, train loss = 0.1886737799301779, train acc = 0.949999988079071, time = 0.020461320877075195\n",
      "Testing at step=57, batch=0, test loss = 0.1760826338831665, test acc = 0.9100000262260437, time = 0.004767179489135742\n",
      "Testing at step=57, batch=20, test loss = 0.21849575473713462, test acc = 0.9399999976158142, time = 0.004767179489135742\n",
      "Testing at step=57, batch=40, test loss = 0.1851732027233471, test acc = 0.9300000071525574, time = 0.004723548889160156\n",
      "Testing at step=57, batch=60, test loss = 0.47335377704899734, test acc = 0.8500000238418579, time = 0.004723310470581055\n",
      "Testing at step=57, batch=80, test loss = 0.28978367650184067, test acc = 0.9100000262260437, time = 0.004711627960205078\n",
      "Step 57 finished in 24.503718852996826, Train loss = 0.24847662636368298, Test loss = 0.26563675905567413; Train Acc = 0.9308666683236758, Test Acc = 0.9259000051021576\n",
      "Training at step=58, batch=0, train loss = 0.13856076768691702, train acc = 0.9700000286102295, time = 0.020661115646362305\n",
      "Training at step=58, batch=120, train loss = 0.223554876635055, train acc = 0.9300000071525574, time = 0.0203249454498291\n",
      "Training at step=58, batch=240, train loss = 0.22578989444609285, train acc = 0.8899999856948853, time = 0.020690202713012695\n",
      "Training at step=58, batch=360, train loss = 0.1355966620893484, train acc = 0.9599999785423279, time = 0.020492076873779297\n",
      "Training at step=58, batch=480, train loss = 0.19542684700543803, train acc = 0.9700000286102295, time = 0.020479917526245117\n",
      "Testing at step=58, batch=0, test loss = 0.1873368407348076, test acc = 0.9300000071525574, time = 0.00492548942565918\n",
      "Testing at step=58, batch=20, test loss = 0.25010637623524856, test acc = 0.9200000166893005, time = 0.004753589630126953\n",
      "Testing at step=58, batch=40, test loss = 0.3263918817996585, test acc = 0.8899999856948853, time = 0.004789590835571289\n",
      "Testing at step=58, batch=60, test loss = 0.1710170914021683, test acc = 0.9300000071525574, time = 0.004754781723022461\n",
      "Testing at step=58, batch=80, test loss = 0.47783554345873547, test acc = 0.8700000047683716, time = 0.004815816879272461\n",
      "Step 58 finished in 24.53069758415222, Train loss = 0.24824322738874202, Test loss = 0.268628546191982; Train Acc = 0.9318333355585734, Test Acc = 0.9252000015974045\n",
      "Training at step=59, batch=0, train loss = 0.2524371106168442, train acc = 0.9599999785423279, time = 0.020498275756835938\n",
      "Training at step=59, batch=120, train loss = 0.27514357575836873, train acc = 0.8899999856948853, time = 0.0204010009765625\n",
      "Training at step=59, batch=240, train loss = 0.30102355178716456, train acc = 0.9300000071525574, time = 0.020573139190673828\n",
      "Training at step=59, batch=360, train loss = 0.26087645508082313, train acc = 0.9300000071525574, time = 0.020600318908691406\n",
      "Training at step=59, batch=480, train loss = 0.25586910313447836, train acc = 0.9300000071525574, time = 0.020713329315185547\n",
      "Testing at step=59, batch=0, test loss = 0.2970451842332966, test acc = 0.9399999976158142, time = 0.004753589630126953\n",
      "Testing at step=59, batch=20, test loss = 0.42080450224590854, test acc = 0.8899999856948853, time = 0.00477290153503418\n",
      "Testing at step=59, batch=40, test loss = 0.27762578648665576, test acc = 0.9300000071525574, time = 0.004726886749267578\n",
      "Testing at step=59, batch=60, test loss = 0.2187340660798002, test acc = 0.9300000071525574, time = 0.004729032516479492\n",
      "Testing at step=59, batch=80, test loss = 0.17610574883058455, test acc = 0.9599999785423279, time = 0.0047321319580078125\n",
      "Step 59 finished in 24.451268434524536, Train loss = 0.24794089691601082, Test loss = 0.269423894674921; Train Acc = 0.9317000012596448, Test Acc = 0.9247000026702881\n",
      "Training at step=60, batch=0, train loss = 0.2429407484398163, train acc = 0.9300000071525574, time = 0.020478248596191406\n",
      "Training at step=60, batch=120, train loss = 0.11897955127622124, train acc = 0.949999988079071, time = 0.020580530166625977\n",
      "Training at step=60, batch=240, train loss = 0.40875688779126873, train acc = 0.9100000262260437, time = 0.02048778533935547\n",
      "Training at step=60, batch=360, train loss = 0.33871564809978233, train acc = 0.9300000071525574, time = 0.020555496215820312\n",
      "Training at step=60, batch=480, train loss = 0.3101542352389671, train acc = 0.9200000166893005, time = 0.02048182487487793\n",
      "Testing at step=60, batch=0, test loss = 0.3242116995053742, test acc = 0.8799999952316284, time = 0.00482940673828125\n",
      "Testing at step=60, batch=20, test loss = 0.29698688823720376, test acc = 0.9300000071525574, time = 0.004766702651977539\n",
      "Testing at step=60, batch=40, test loss = 0.2596897455346578, test acc = 0.9200000166893005, time = 0.0047338008880615234\n",
      "Testing at step=60, batch=60, test loss = 0.22724506949165632, test acc = 0.9100000262260437, time = 0.004741668701171875\n",
      "Testing at step=60, batch=80, test loss = 0.21285492903390252, test acc = 0.949999988079071, time = 0.004720449447631836\n",
      "Step 60 finished in 24.443084955215454, Train loss = 0.24749490143303443, Test loss = 0.26494229719708196; Train Acc = 0.9321166679263115, Test Acc = 0.9271000003814698\n",
      "Training at step=61, batch=0, train loss = 0.3082791802528089, train acc = 0.8999999761581421, time = 0.02061152458190918\n",
      "Training at step=61, batch=120, train loss = 0.12818899405010092, train acc = 0.9599999785423279, time = 0.020369768142700195\n",
      "Training at step=61, batch=240, train loss = 0.35628038156026776, train acc = 0.9300000071525574, time = 0.020623445510864258\n",
      "Training at step=61, batch=360, train loss = 0.3308014937521488, train acc = 0.9200000166893005, time = 0.02054905891418457\n",
      "Training at step=61, batch=480, train loss = 0.1511611252787036, train acc = 0.949999988079071, time = 0.020447969436645508\n",
      "Testing at step=61, batch=0, test loss = 0.19922623527567368, test acc = 0.949999988079071, time = 0.004923582077026367\n",
      "Testing at step=61, batch=20, test loss = 0.4965669841155218, test acc = 0.8999999761581421, time = 0.004739522933959961\n",
      "Testing at step=61, batch=40, test loss = 0.17354912320721666, test acc = 0.9599999785423279, time = 0.0047299861907958984\n",
      "Testing at step=61, batch=60, test loss = 0.20431810762407202, test acc = 0.9399999976158142, time = 0.004739284515380859\n",
      "Testing at step=61, batch=80, test loss = 0.29350421737717225, test acc = 0.9200000166893005, time = 0.0047435760498046875\n",
      "Step 61 finished in 24.45044493675232, Train loss = 0.24745972588216147, Test loss = 0.26753574724148305; Train Acc = 0.9317333349585533, Test Acc = 0.9256000047922135\n",
      "Training at step=62, batch=0, train loss = 0.2979289827828997, train acc = 0.8799999952316284, time = 0.020618677139282227\n",
      "Training at step=62, batch=120, train loss = 0.20694753747382388, train acc = 0.9200000166893005, time = 0.020569562911987305\n",
      "Training at step=62, batch=240, train loss = 0.1937794459535396, train acc = 0.9399999976158142, time = 0.020598411560058594\n",
      "Training at step=62, batch=360, train loss = 0.28940496009213246, train acc = 0.9100000262260437, time = 0.020338058471679688\n",
      "Training at step=62, batch=480, train loss = 0.2355062543376945, train acc = 0.9200000166893005, time = 0.020494937896728516\n",
      "Testing at step=62, batch=0, test loss = 0.23270903062683893, test acc = 0.9399999976158142, time = 0.004799842834472656\n",
      "Testing at step=62, batch=20, test loss = 0.3876922397085034, test acc = 0.9200000166893005, time = 0.004782199859619141\n",
      "Testing at step=62, batch=40, test loss = 0.23289385316873637, test acc = 0.8999999761581421, time = 0.004767656326293945\n",
      "Testing at step=62, batch=60, test loss = 0.20627833267839754, test acc = 0.9100000262260437, time = 0.004872560501098633\n",
      "Testing at step=62, batch=80, test loss = 0.2470423300589664, test acc = 0.9200000166893005, time = 0.004781484603881836\n",
      "Step 62 finished in 24.529122829437256, Train loss = 0.2467991408223452, Test loss = 0.26473178357889293; Train Acc = 0.9325666693846385, Test Acc = 0.926000000834465\n",
      "Training at step=63, batch=0, train loss = 0.2510815785617544, train acc = 0.9200000166893005, time = 0.020490407943725586\n",
      "Training at step=63, batch=120, train loss = 0.25225665208320547, train acc = 0.9100000262260437, time = 0.0204770565032959\n",
      "Training at step=63, batch=240, train loss = 0.14973677477755973, train acc = 0.9599999785423279, time = 0.02041339874267578\n",
      "Training at step=63, batch=360, train loss = 0.22951413657387387, train acc = 0.9399999976158142, time = 0.02038288116455078\n",
      "Training at step=63, batch=480, train loss = 0.21364283573256973, train acc = 0.9100000262260437, time = 0.020634174346923828\n",
      "Testing at step=63, batch=0, test loss = 0.2637745342675356, test acc = 0.8999999761581421, time = 0.004761695861816406\n",
      "Testing at step=63, batch=20, test loss = 0.3026490646663364, test acc = 0.8600000143051147, time = 0.0047588348388671875\n",
      "Testing at step=63, batch=40, test loss = 0.2967161465651206, test acc = 0.9200000166893005, time = 0.004792213439941406\n",
      "Testing at step=63, batch=60, test loss = 0.3141316773029603, test acc = 0.8999999761581421, time = 0.004793882369995117\n",
      "Testing at step=63, batch=80, test loss = 0.15809940349142898, test acc = 0.9599999785423279, time = 0.004745006561279297\n",
      "Step 63 finished in 24.515021800994873, Train loss = 0.2467117224506446, Test loss = 0.26586269030127907; Train Acc = 0.9318333354592323, Test Acc = 0.9241999995708465\n",
      "Training at step=64, batch=0, train loss = 0.2778164308053554, train acc = 0.949999988079071, time = 0.020460128784179688\n",
      "Training at step=64, batch=120, train loss = 0.20569178925194403, train acc = 0.9200000166893005, time = 0.020626544952392578\n",
      "Training at step=64, batch=240, train loss = 0.15707234964686612, train acc = 0.949999988079071, time = 0.020548582077026367\n",
      "Training at step=64, batch=360, train loss = 0.2452875914700219, train acc = 0.9300000071525574, time = 0.020395755767822266\n",
      "Training at step=64, batch=480, train loss = 0.1592930060530361, train acc = 0.9599999785423279, time = 0.020521163940429688\n",
      "Testing at step=64, batch=0, test loss = 0.4265130849157639, test acc = 0.8799999952316284, time = 0.004845857620239258\n",
      "Testing at step=64, batch=20, test loss = 0.36856608867373014, test acc = 0.9100000262260437, time = 0.004757881164550781\n",
      "Testing at step=64, batch=40, test loss = 0.1970187749565394, test acc = 0.9200000166893005, time = 0.004773855209350586\n",
      "Testing at step=64, batch=60, test loss = 0.2553319673568027, test acc = 0.8999999761581421, time = 0.004774808883666992\n",
      "Testing at step=64, batch=80, test loss = 0.15674846643742898, test acc = 0.949999988079071, time = 0.004735231399536133\n",
      "Step 64 finished in 24.604933977127075, Train loss = 0.24680138024708537, Test loss = 0.2655506696689978; Train Acc = 0.9322333353757858, Test Acc = 0.9264000016450882\n",
      "Training at step=65, batch=0, train loss = 0.169828686478549, train acc = 0.9599999785423279, time = 0.020524978637695312\n",
      "Training at step=65, batch=120, train loss = 0.33097878329463315, train acc = 0.9300000071525574, time = 0.020365238189697266\n",
      "Training at step=65, batch=240, train loss = 0.18924692587414696, train acc = 0.949999988079071, time = 0.02047276496887207\n",
      "Training at step=65, batch=360, train loss = 0.24606334909046398, train acc = 0.9599999785423279, time = 0.020558834075927734\n",
      "Training at step=65, batch=480, train loss = 0.2974880078065682, train acc = 0.9300000071525574, time = 0.020526409149169922\n",
      "Testing at step=65, batch=0, test loss = 0.1729049760231793, test acc = 0.9100000262260437, time = 0.004803657531738281\n",
      "Testing at step=65, batch=20, test loss = 0.2765719740486555, test acc = 0.9300000071525574, time = 0.004769086837768555\n",
      "Testing at step=65, batch=40, test loss = 0.26719045389230967, test acc = 0.9300000071525574, time = 0.004891395568847656\n",
      "Testing at step=65, batch=60, test loss = 0.17996577736217378, test acc = 0.9399999976158142, time = 0.004829883575439453\n",
      "Testing at step=65, batch=80, test loss = 0.4064461531578709, test acc = 0.8700000047683716, time = 0.0047380924224853516\n",
      "Step 65 finished in 24.519261598587036, Train loss = 0.24625144151734144, Test loss = 0.26582579686553665; Train Acc = 0.9326000020901362, Test Acc = 0.9262000024318695\n",
      "Training at step=66, batch=0, train loss = 0.1863230182236736, train acc = 0.9399999976158142, time = 0.02053546905517578\n",
      "Training at step=66, batch=120, train loss = 0.2341443780766167, train acc = 0.949999988079071, time = 0.020632505416870117\n",
      "Training at step=66, batch=240, train loss = 0.22000547791486466, train acc = 0.9399999976158142, time = 0.02049851417541504\n",
      "Training at step=66, batch=360, train loss = 0.2041783651706063, train acc = 0.9300000071525574, time = 0.02049994468688965\n",
      "Training at step=66, batch=480, train loss = 0.21945737276378666, train acc = 0.949999988079071, time = 0.02061009407043457\n",
      "Testing at step=66, batch=0, test loss = 0.14554208963301524, test acc = 0.949999988079071, time = 0.004765510559082031\n",
      "Testing at step=66, batch=20, test loss = 0.4225135832226611, test acc = 0.9100000262260437, time = 0.004736423492431641\n",
      "Testing at step=66, batch=40, test loss = 0.21821014842498612, test acc = 0.9300000071525574, time = 0.004761457443237305\n",
      "Testing at step=66, batch=60, test loss = 0.1629017045933168, test acc = 0.949999988079071, time = 0.004791259765625\n",
      "Testing at step=66, batch=80, test loss = 0.2981407962368587, test acc = 0.8899999856948853, time = 0.004889726638793945\n",
      "Step 66 finished in 24.433849334716797, Train loss = 0.24619923741755695, Test loss = 0.26831353547557746; Train Acc = 0.9319000028570493, Test Acc = 0.9241000038385391\n",
      "Training at step=67, batch=0, train loss = 0.21658593153066935, train acc = 0.9300000071525574, time = 0.020721912384033203\n",
      "Training at step=67, batch=120, train loss = 0.23421525560700343, train acc = 0.9300000071525574, time = 0.020503520965576172\n",
      "Training at step=67, batch=240, train loss = 0.19704253545633132, train acc = 0.949999988079071, time = 0.02048039436340332\n",
      "Training at step=67, batch=360, train loss = 0.2524444147932437, train acc = 0.9200000166893005, time = 0.020456314086914062\n",
      "Training at step=67, batch=480, train loss = 0.2731624738377847, train acc = 0.9300000071525574, time = 0.020862579345703125\n",
      "Testing at step=67, batch=0, test loss = 0.29966145867205374, test acc = 0.9200000166893005, time = 0.004740476608276367\n",
      "Testing at step=67, batch=20, test loss = 0.26682037084944316, test acc = 0.9200000166893005, time = 0.004748344421386719\n",
      "Testing at step=67, batch=40, test loss = 0.1370433598279719, test acc = 0.949999988079071, time = 0.004830360412597656\n",
      "Testing at step=67, batch=60, test loss = 0.47351099432982097, test acc = 0.8999999761581421, time = 0.004724740982055664\n",
      "Testing at step=67, batch=80, test loss = 0.2420228598606015, test acc = 0.9200000166893005, time = 0.00476837158203125\n",
      "Step 67 finished in 24.4951069355011, Train loss = 0.24558866219179548, Test loss = 0.2665476757336162; Train Acc = 0.9328000014026959, Test Acc = 0.9257000011205673\n",
      "Training at step=68, batch=0, train loss = 0.2534717238918815, train acc = 0.9100000262260437, time = 0.020634174346923828\n",
      "Training at step=68, batch=120, train loss = 0.3605234965770864, train acc = 0.9200000166893005, time = 0.020545482635498047\n",
      "Training at step=68, batch=240, train loss = 0.18030728587146508, train acc = 0.9300000071525574, time = 0.020576000213623047\n",
      "Training at step=68, batch=360, train loss = 0.07577766855785305, train acc = 0.9800000190734863, time = 0.020569562911987305\n",
      "Training at step=68, batch=480, train loss = 0.17569183155999454, train acc = 0.9300000071525574, time = 0.020557403564453125\n",
      "Testing at step=68, batch=0, test loss = 0.225560404570997, test acc = 0.9300000071525574, time = 0.004775047302246094\n",
      "Testing at step=68, batch=20, test loss = 0.3742315971080461, test acc = 0.8899999856948853, time = 0.0047359466552734375\n",
      "Testing at step=68, batch=40, test loss = 0.21690540725919735, test acc = 0.9100000262260437, time = 0.0047872066497802734\n",
      "Testing at step=68, batch=60, test loss = 0.4175534492381685, test acc = 0.8899999856948853, time = 0.004729270935058594\n",
      "Testing at step=68, batch=80, test loss = 0.16397280419822396, test acc = 0.9399999976158142, time = 0.004723072052001953\n",
      "Step 68 finished in 24.370585203170776, Train loss = 0.24532188735014696, Test loss = 0.268386979342001; Train Acc = 0.9324666677912077, Test Acc = 0.9258000004291534\n",
      "Training at step=69, batch=0, train loss = 0.1947624018222011, train acc = 0.9599999785423279, time = 0.020565271377563477\n",
      "Training at step=69, batch=120, train loss = 0.11372458677476799, train acc = 0.9800000190734863, time = 0.020448684692382812\n",
      "Training at step=69, batch=240, train loss = 0.2217838449934946, train acc = 0.9300000071525574, time = 0.020501375198364258\n",
      "Training at step=69, batch=360, train loss = 0.41258726866877216, train acc = 0.9200000166893005, time = 0.02057933807373047\n",
      "Training at step=69, batch=480, train loss = 0.3593619371942164, train acc = 0.9100000262260437, time = 0.02072000503540039\n",
      "Testing at step=69, batch=0, test loss = 0.12481778091886779, test acc = 0.9599999785423279, time = 0.0047495365142822266\n",
      "Testing at step=69, batch=20, test loss = 0.2982288391386399, test acc = 0.8999999761581421, time = 0.004747629165649414\n",
      "Testing at step=69, batch=40, test loss = 0.24118005633666642, test acc = 0.8999999761581421, time = 0.0047528743743896484\n",
      "Testing at step=69, batch=60, test loss = 0.3417678876091497, test acc = 0.9100000262260437, time = 0.0047261714935302734\n",
      "Testing at step=69, batch=80, test loss = 0.23404507358781518, test acc = 0.8999999761581421, time = 0.004729270935058594\n",
      "Step 69 finished in 24.41620945930481, Train loss = 0.24528675549934764, Test loss = 0.26786975017967907; Train Acc = 0.9325166675448417, Test Acc = 0.9253000009059906\n",
      "Training at step=70, batch=0, train loss = 0.2057286484204157, train acc = 0.9399999976158142, time = 0.02072739601135254\n",
      "Training at step=70, batch=120, train loss = 0.10796011776670021, train acc = 0.9599999785423279, time = 0.02049088478088379\n",
      "Training at step=70, batch=240, train loss = 0.26653489715043216, train acc = 0.9200000166893005, time = 0.0206146240234375\n",
      "Training at step=70, batch=360, train loss = 0.27619966885090175, train acc = 0.9399999976158142, time = 0.020511865615844727\n",
      "Training at step=70, batch=480, train loss = 0.24855392513790028, train acc = 0.9599999785423279, time = 0.02030777931213379\n",
      "Testing at step=70, batch=0, test loss = 0.42037472273094, test acc = 0.9200000166893005, time = 0.004771232604980469\n",
      "Testing at step=70, batch=20, test loss = 0.18278774813651638, test acc = 0.9599999785423279, time = 0.004767179489135742\n",
      "Testing at step=70, batch=40, test loss = 0.18991912785199325, test acc = 0.949999988079071, time = 0.004765033721923828\n",
      "Testing at step=70, batch=60, test loss = 0.2376075549026556, test acc = 0.9100000262260437, time = 0.004723548889160156\n",
      "Testing at step=70, batch=80, test loss = 0.27238245289765717, test acc = 0.8999999761581421, time = 0.004781007766723633\n",
      "Step 70 finished in 24.40429139137268, Train loss = 0.24487161633529608, Test loss = 0.26727747900285764; Train Acc = 0.9329666675130526, Test Acc = 0.9237000000476837\n",
      "Training at step=71, batch=0, train loss = 0.20672035132417357, train acc = 0.949999988079071, time = 0.020601987838745117\n",
      "Training at step=71, batch=120, train loss = 0.25576200012478606, train acc = 0.949999988079071, time = 0.02048206329345703\n",
      "Training at step=71, batch=240, train loss = 0.16738342449922894, train acc = 0.9399999976158142, time = 0.0204927921295166\n",
      "Training at step=71, batch=360, train loss = 0.3973466280223861, train acc = 0.9399999976158142, time = 0.020472288131713867\n",
      "Training at step=71, batch=480, train loss = 0.3730092835562918, train acc = 0.8999999761581421, time = 0.020555734634399414\n",
      "Testing at step=71, batch=0, test loss = 0.3229472383068559, test acc = 0.9100000262260437, time = 0.004817008972167969\n",
      "Testing at step=71, batch=20, test loss = 0.32144013375287467, test acc = 0.8999999761581421, time = 0.004822969436645508\n",
      "Testing at step=71, batch=40, test loss = 0.2745262529168387, test acc = 0.9300000071525574, time = 0.004721879959106445\n",
      "Testing at step=71, batch=60, test loss = 0.09857815103864524, test acc = 0.9700000286102295, time = 0.004799365997314453\n",
      "Testing at step=71, batch=80, test loss = 0.2879660918759495, test acc = 0.9300000071525574, time = 0.004786968231201172\n",
      "Step 71 finished in 24.436161279678345, Train loss = 0.24488353529439053, Test loss = 0.2667069070078114; Train Acc = 0.9322833358248075, Test Acc = 0.926500004529953\n",
      "Training at step=72, batch=0, train loss = 0.27595804846956573, train acc = 0.9200000166893005, time = 0.020632266998291016\n",
      "Training at step=72, batch=120, train loss = 0.338371834952859, train acc = 0.9100000262260437, time = 0.02059197425842285\n",
      "Training at step=72, batch=240, train loss = 0.27234516161880185, train acc = 0.9200000166893005, time = 0.02050161361694336\n",
      "Training at step=72, batch=360, train loss = 0.22352645068854493, train acc = 0.9300000071525574, time = 0.020404338836669922\n",
      "Training at step=72, batch=480, train loss = 0.12806332707806625, train acc = 0.9599999785423279, time = 0.020616769790649414\n",
      "Testing at step=72, batch=0, test loss = 0.2174326007943862, test acc = 0.949999988079071, time = 0.004765033721923828\n",
      "Testing at step=72, batch=20, test loss = 0.19006125987231876, test acc = 0.9399999976158142, time = 0.004740476608276367\n",
      "Testing at step=72, batch=40, test loss = 0.2203366412729206, test acc = 0.9300000071525574, time = 0.004715681076049805\n",
      "Testing at step=72, batch=60, test loss = 0.1291275125322128, test acc = 0.949999988079071, time = 0.004771232604980469\n",
      "Testing at step=72, batch=80, test loss = 0.3638598432705333, test acc = 0.9200000166893005, time = 0.00471949577331543\n",
      "Step 72 finished in 24.376296281814575, Train loss = 0.24429310729750037, Test loss = 0.2668932198568619; Train Acc = 0.932533334394296, Test Acc = 0.9255999976396561\n",
      "Training at step=73, batch=0, train loss = 0.18595203018762899, train acc = 0.9399999976158142, time = 0.02054882049560547\n",
      "Training at step=73, batch=120, train loss = 0.12402782768115235, train acc = 0.9800000190734863, time = 0.020542621612548828\n",
      "Training at step=73, batch=240, train loss = 0.4640914383242504, train acc = 0.9200000166893005, time = 0.020601511001586914\n",
      "Training at step=73, batch=360, train loss = 0.21378490458252145, train acc = 0.9200000166893005, time = 0.020758390426635742\n",
      "Training at step=73, batch=480, train loss = 0.3272979806511889, train acc = 0.9300000071525574, time = 0.02047586441040039\n",
      "Testing at step=73, batch=0, test loss = 0.1878454343598239, test acc = 0.9300000071525574, time = 0.00482487678527832\n",
      "Testing at step=73, batch=20, test loss = 0.33408812077011113, test acc = 0.9300000071525574, time = 0.004814863204956055\n",
      "Testing at step=73, batch=40, test loss = 0.3357066679243503, test acc = 0.8799999952316284, time = 0.004735469818115234\n",
      "Testing at step=73, batch=60, test loss = 0.1659033186891206, test acc = 0.9399999976158142, time = 0.004767894744873047\n",
      "Testing at step=73, batch=80, test loss = 0.19326129050980298, test acc = 0.9300000071525574, time = 0.004771232604980469\n",
      "Step 73 finished in 24.50405263900757, Train loss = 0.24425587463825457, Test loss = 0.2673416188363438; Train Acc = 0.9324333354830742, Test Acc = 0.9255000001192093\n",
      "Training at step=74, batch=0, train loss = 0.41334114405461064, train acc = 0.8700000047683716, time = 0.020596981048583984\n",
      "Training at step=74, batch=120, train loss = 0.3198673179128205, train acc = 0.9399999976158142, time = 0.020444869995117188\n",
      "Training at step=74, batch=240, train loss = 0.19842696167998272, train acc = 0.949999988079071, time = 0.02044391632080078\n",
      "Training at step=74, batch=360, train loss = 0.23046456729029557, train acc = 0.9399999976158142, time = 0.02046489715576172\n",
      "Training at step=74, batch=480, train loss = 0.2562709754180664, train acc = 0.9200000166893005, time = 0.02053356170654297\n",
      "Testing at step=74, batch=0, test loss = 0.11343524105185555, test acc = 0.9599999785423279, time = 0.004909992218017578\n",
      "Testing at step=74, batch=20, test loss = 0.2553535485453994, test acc = 0.9599999785423279, time = 0.00473785400390625\n",
      "Testing at step=74, batch=40, test loss = 0.21141344551023805, test acc = 0.9399999976158142, time = 0.004744768142700195\n",
      "Testing at step=74, batch=60, test loss = 0.34607171778197887, test acc = 0.9100000262260437, time = 0.004731416702270508\n",
      "Testing at step=74, batch=80, test loss = 0.2965481929472406, test acc = 0.9100000262260437, time = 0.004731416702270508\n",
      "Step 74 finished in 24.42489767074585, Train loss = 0.24411959512506518, Test loss = 0.2674036415313032; Train Acc = 0.9326166685422261, Test Acc = 0.9269000035524368\n",
      "Training at step=75, batch=0, train loss = 0.23644331980730485, train acc = 0.9599999785423279, time = 0.020545244216918945\n",
      "Training at step=75, batch=120, train loss = 0.2216563314881947, train acc = 0.9399999976158142, time = 0.02052927017211914\n",
      "Training at step=75, batch=240, train loss = 0.22318960256313894, train acc = 0.9599999785423279, time = 0.020402908325195312\n",
      "Training at step=75, batch=360, train loss = 0.23887731289093353, train acc = 0.9300000071525574, time = 0.02055525779724121\n",
      "Training at step=75, batch=480, train loss = 0.44143162027068605, train acc = 0.9200000166893005, time = 0.020557403564453125\n",
      "Testing at step=75, batch=0, test loss = 0.2642506311942154, test acc = 0.9200000166893005, time = 0.00487518310546875\n",
      "Testing at step=75, batch=20, test loss = 0.24137029305224134, test acc = 0.9100000262260437, time = 0.004786968231201172\n",
      "Testing at step=75, batch=40, test loss = 0.28315912104378843, test acc = 0.9200000166893005, time = 0.004721164703369141\n",
      "Testing at step=75, batch=60, test loss = 0.13371933233750513, test acc = 0.949999988079071, time = 0.004734516143798828\n",
      "Testing at step=75, batch=80, test loss = 0.24282505797253473, test acc = 0.8999999761581421, time = 0.004827976226806641\n",
      "Step 75 finished in 24.46887731552124, Train loss = 0.24367483142532856, Test loss = 0.2662059117784931; Train Acc = 0.9329333348075549, Test Acc = 0.9265000015497208\n",
      "Training at step=76, batch=0, train loss = 0.13847647596253654, train acc = 0.949999988079071, time = 0.020635366439819336\n",
      "Training at step=76, batch=120, train loss = 0.20974120730607357, train acc = 0.9399999976158142, time = 0.02057814598083496\n",
      "Training at step=76, batch=240, train loss = 0.20736210343477254, train acc = 0.9300000071525574, time = 0.020560026168823242\n",
      "Training at step=76, batch=360, train loss = 0.1540026971473838, train acc = 0.9700000286102295, time = 0.020412206649780273\n",
      "Training at step=76, batch=480, train loss = 0.27987412601642453, train acc = 0.949999988079071, time = 0.02062702178955078\n",
      "Testing at step=76, batch=0, test loss = 0.2125321308529595, test acc = 0.9700000286102295, time = 0.00475001335144043\n",
      "Testing at step=76, batch=20, test loss = 0.1911288172837075, test acc = 0.9200000166893005, time = 0.0047647953033447266\n",
      "Testing at step=76, batch=40, test loss = 0.324486162003657, test acc = 0.8700000047683716, time = 0.00478363037109375\n",
      "Testing at step=76, batch=60, test loss = 0.11586505681276459, test acc = 0.949999988079071, time = 0.004739284515380859\n",
      "Testing at step=76, batch=80, test loss = 0.20065686037320737, test acc = 0.949999988079071, time = 0.004715919494628906\n",
      "Step 76 finished in 24.353363037109375, Train loss = 0.24378584415402296, Test loss = 0.2700760250179707; Train Acc = 0.9328000008066495, Test Acc = 0.9243000024557113\n",
      "Training at step=77, batch=0, train loss = 0.14752037731085144, train acc = 0.949999988079071, time = 0.020495176315307617\n",
      "Training at step=77, batch=120, train loss = 0.10317439677587512, train acc = 0.9700000286102295, time = 0.02042388916015625\n",
      "Training at step=77, batch=240, train loss = 0.25826172690545096, train acc = 0.9300000071525574, time = 0.02067112922668457\n",
      "Training at step=77, batch=360, train loss = 0.22256612014774976, train acc = 0.9399999976158142, time = 0.020479202270507812\n",
      "Training at step=77, batch=480, train loss = 0.28368315407492556, train acc = 0.9100000262260437, time = 0.020611047744750977\n",
      "Testing at step=77, batch=0, test loss = 0.25704215236077965, test acc = 0.9100000262260437, time = 0.004786491394042969\n",
      "Testing at step=77, batch=20, test loss = 0.28487363028141893, test acc = 0.9200000166893005, time = 0.00474238395690918\n",
      "Testing at step=77, batch=40, test loss = 0.25752850813857986, test acc = 0.9399999976158142, time = 0.004796028137207031\n",
      "Testing at step=77, batch=60, test loss = 0.2718663700967127, test acc = 0.8999999761581421, time = 0.004786968231201172\n",
      "Testing at step=77, batch=80, test loss = 0.4390614152874535, test acc = 0.8999999761581421, time = 0.004740476608276367\n",
      "Step 77 finished in 24.361409664154053, Train loss = 0.2429024667665693, Test loss = 0.2666391375601721; Train Acc = 0.9327333343029022, Test Acc = 0.9256000018119812\n",
      "Training at step=78, batch=0, train loss = 0.41687411419374354, train acc = 0.8899999856948853, time = 0.020694494247436523\n",
      "Training at step=78, batch=120, train loss = 0.38240295084657666, train acc = 0.9100000262260437, time = 0.020472288131713867\n",
      "Training at step=78, batch=240, train loss = 0.22118675932169893, train acc = 0.9200000166893005, time = 0.020534992218017578\n",
      "Training at step=78, batch=360, train loss = 0.2876060096395404, train acc = 0.9100000262260437, time = 0.020503759384155273\n",
      "Training at step=78, batch=480, train loss = 0.2654599155270032, train acc = 0.9599999785423279, time = 0.020495891571044922\n",
      "Testing at step=78, batch=0, test loss = 0.409533240831184, test acc = 0.8700000047683716, time = 0.004774332046508789\n",
      "Testing at step=78, batch=20, test loss = 0.21629831733814356, test acc = 0.9300000071525574, time = 0.004734516143798828\n",
      "Testing at step=78, batch=40, test loss = 0.2115819174898677, test acc = 0.9399999976158142, time = 0.0047452449798583984\n",
      "Testing at step=78, batch=60, test loss = 0.15569468945203194, test acc = 0.949999988079071, time = 0.004793882369995117\n",
      "Testing at step=78, batch=80, test loss = 0.3523203546315264, test acc = 0.8999999761581421, time = 0.004780769348144531\n",
      "Step 78 finished in 24.42326855659485, Train loss = 0.24313755609703, Test loss = 0.26741794954000564; Train Acc = 0.9333166681726773, Test Acc = 0.9253000003099442\n",
      "Training at step=79, batch=0, train loss = 0.17357943326015954, train acc = 0.9399999976158142, time = 0.02063298225402832\n",
      "Training at step=79, batch=120, train loss = 0.2898834057319266, train acc = 0.9300000071525574, time = 0.020398855209350586\n",
      "Training at step=79, batch=240, train loss = 0.3385705778436556, train acc = 0.8999999761581421, time = 0.02061915397644043\n",
      "Training at step=79, batch=360, train loss = 0.49812188297570315, train acc = 0.8600000143051147, time = 0.020359277725219727\n",
      "Training at step=79, batch=480, train loss = 0.1489716505890601, train acc = 0.9599999785423279, time = 0.02045154571533203\n",
      "Testing at step=79, batch=0, test loss = 0.21401695924959005, test acc = 0.9599999785423279, time = 0.004830121994018555\n",
      "Testing at step=79, batch=20, test loss = 0.4213319651543443, test acc = 0.8799999952316284, time = 0.004736185073852539\n",
      "Testing at step=79, batch=40, test loss = 0.22797828469848333, test acc = 0.949999988079071, time = 0.0047490596771240234\n",
      "Testing at step=79, batch=60, test loss = 0.28428342112940075, test acc = 0.9100000262260437, time = 0.004761934280395508\n",
      "Testing at step=79, batch=80, test loss = 0.1650757354538101, test acc = 0.9599999785423279, time = 0.004752159118652344\n",
      "Step 79 finished in 24.48853588104248, Train loss = 0.2429431581951489, Test loss = 0.2693604883938077; Train Acc = 0.9325833350419999, Test Acc = 0.9260999995470047\n",
      "Training at step=80, batch=0, train loss = 0.26593811466976364, train acc = 0.9300000071525574, time = 0.020483732223510742\n",
      "Training at step=80, batch=120, train loss = 0.28628823939044434, train acc = 0.8999999761581421, time = 0.020481586456298828\n",
      "Training at step=80, batch=240, train loss = 0.32034342542298666, train acc = 0.9100000262260437, time = 0.02046680450439453\n",
      "Training at step=80, batch=360, train loss = 0.2981043381119564, train acc = 0.949999988079071, time = 0.02057933807373047\n",
      "Training at step=80, batch=480, train loss = 0.07381571263222311, train acc = 0.9900000095367432, time = 0.02043318748474121\n",
      "Testing at step=80, batch=0, test loss = 0.19973449942924243, test acc = 0.9399999976158142, time = 0.004812479019165039\n",
      "Testing at step=80, batch=20, test loss = 0.16623752428992006, test acc = 0.949999988079071, time = 0.004799365997314453\n",
      "Testing at step=80, batch=40, test loss = 0.3284002196431554, test acc = 0.949999988079071, time = 0.004782438278198242\n",
      "Testing at step=80, batch=60, test loss = 0.11146441172987144, test acc = 0.9599999785423279, time = 0.004739522933959961\n",
      "Testing at step=80, batch=80, test loss = 0.23507912750395174, test acc = 0.9100000262260437, time = 0.0047359466552734375\n",
      "Step 80 finished in 24.51480484008789, Train loss = 0.24257473467921623, Test loss = 0.26980428829775616; Train Acc = 0.933333335618178, Test Acc = 0.9234000027179718\n",
      "Training at step=81, batch=0, train loss = 0.18796384838111752, train acc = 0.949999988079071, time = 0.020586490631103516\n",
      "Training at step=81, batch=120, train loss = 0.19942418212767665, train acc = 0.9300000071525574, time = 0.020593643188476562\n",
      "Training at step=81, batch=240, train loss = 0.24341293528581734, train acc = 0.9599999785423279, time = 0.020598173141479492\n",
      "Training at step=81, batch=360, train loss = 0.2635119489939933, train acc = 0.9399999976158142, time = 0.020570039749145508\n",
      "Training at step=81, batch=480, train loss = 0.260023119896843, train acc = 0.8999999761581421, time = 0.020470142364501953\n",
      "Testing at step=81, batch=0, test loss = 0.25270225770301646, test acc = 0.9399999976158142, time = 0.004845857620239258\n",
      "Testing at step=81, batch=20, test loss = 0.4327808206506294, test acc = 0.8899999856948853, time = 0.004716634750366211\n",
      "Testing at step=81, batch=40, test loss = 0.29099672655273207, test acc = 0.9200000166893005, time = 0.004791975021362305\n",
      "Testing at step=81, batch=60, test loss = 0.2610581339328115, test acc = 0.9399999976158142, time = 0.00473475456237793\n",
      "Testing at step=81, batch=80, test loss = 0.2015686101109231, test acc = 0.9300000071525574, time = 0.0047533512115478516\n",
      "Step 81 finished in 24.49248957633972, Train loss = 0.2426431480955575, Test loss = 0.2679765483052305; Train Acc = 0.9335000017285346, Test Acc = 0.9265000021457672\n",
      "Training at step=82, batch=0, train loss = 0.3967011011313632, train acc = 0.8899999856948853, time = 0.020576000213623047\n",
      "Training at step=82, batch=120, train loss = 0.20840937782238014, train acc = 0.9300000071525574, time = 0.020526647567749023\n",
      "Training at step=82, batch=240, train loss = 0.3781909484698109, train acc = 0.8899999856948853, time = 0.020417213439941406\n",
      "Training at step=82, batch=360, train loss = 0.2453107680672223, train acc = 0.9399999976158142, time = 0.02064204216003418\n",
      "Training at step=82, batch=480, train loss = 0.295411180971494, train acc = 0.8799999952316284, time = 0.020482778549194336\n",
      "Testing at step=82, batch=0, test loss = 0.39288633836577946, test acc = 0.8899999856948853, time = 0.0048792362213134766\n",
      "Testing at step=82, batch=20, test loss = 0.3773532378965786, test acc = 0.949999988079071, time = 0.004754781723022461\n",
      "Testing at step=82, batch=40, test loss = 0.29501790631452157, test acc = 0.9599999785423279, time = 0.0047533512115478516\n",
      "Testing at step=82, batch=60, test loss = 0.2895306627298225, test acc = 0.8899999856948853, time = 0.004717111587524414\n",
      "Testing at step=82, batch=80, test loss = 0.19891247921175156, test acc = 0.9200000166893005, time = 0.004761219024658203\n",
      "Step 82 finished in 24.427276849746704, Train loss = 0.2416863814866899, Test loss = 0.27114726292252356; Train Acc = 0.9341500011086464, Test Acc = 0.9225000035762787\n",
      "Training at step=83, batch=0, train loss = 0.29055641277429134, train acc = 0.9399999976158142, time = 0.02056431770324707\n",
      "Training at step=83, batch=120, train loss = 0.27689136468562914, train acc = 0.9399999976158142, time = 0.020621538162231445\n",
      "Training at step=83, batch=240, train loss = 0.23631967183637678, train acc = 0.9599999785423279, time = 0.020612239837646484\n",
      "Training at step=83, batch=360, train loss = 0.3336066944000111, train acc = 0.949999988079071, time = 0.020452499389648438\n",
      "Training at step=83, batch=480, train loss = 0.2820862865074845, train acc = 0.9100000262260437, time = 0.020665645599365234\n",
      "Testing at step=83, batch=0, test loss = 0.19662687308623938, test acc = 0.949999988079071, time = 0.0047872066497802734\n",
      "Testing at step=83, batch=20, test loss = 0.47799601367800937, test acc = 0.9200000166893005, time = 0.004853248596191406\n",
      "Testing at step=83, batch=40, test loss = 0.23874727345601848, test acc = 0.9200000166893005, time = 0.004849433898925781\n",
      "Testing at step=83, batch=60, test loss = 0.2844811139050054, test acc = 0.9100000262260437, time = 0.004815101623535156\n",
      "Testing at step=83, batch=80, test loss = 0.18790712910758278, test acc = 0.9399999976158142, time = 0.004854440689086914\n",
      "Step 83 finished in 24.40365195274353, Train loss = 0.24223602157117094, Test loss = 0.27166178705971444; Train Acc = 0.9332166682680448, Test Acc = 0.9249000018835067\n",
      "Training at step=84, batch=0, train loss = 0.20199264143014545, train acc = 0.949999988079071, time = 0.0208132266998291\n",
      "Training at step=84, batch=120, train loss = 0.19491782220813966, train acc = 0.9399999976158142, time = 0.020559072494506836\n",
      "Training at step=84, batch=240, train loss = 0.23122232501667703, train acc = 0.9300000071525574, time = 0.020520925521850586\n",
      "Training at step=84, batch=360, train loss = 0.20603717928285273, train acc = 0.949999988079071, time = 0.020473957061767578\n",
      "Training at step=84, batch=480, train loss = 0.15870962081479742, train acc = 0.9399999976158142, time = 0.020641088485717773\n",
      "Testing at step=84, batch=0, test loss = 0.37885904757670097, test acc = 0.9300000071525574, time = 0.004760026931762695\n",
      "Testing at step=84, batch=20, test loss = 0.2381513518619763, test acc = 0.9300000071525574, time = 0.004751682281494141\n",
      "Testing at step=84, batch=40, test loss = 0.26292201424025946, test acc = 0.9200000166893005, time = 0.004726886749267578\n",
      "Testing at step=84, batch=60, test loss = 0.34829178141250255, test acc = 0.8700000047683716, time = 0.0047686100006103516\n",
      "Testing at step=84, batch=80, test loss = 0.1726957711056236, test acc = 0.9399999976158142, time = 0.00474238395690918\n",
      "Step 84 finished in 24.565495491027832, Train loss = 0.24178700773251122, Test loss = 0.2696125914997908; Train Acc = 0.9334500013788541, Test Acc = 0.9239000010490418\n",
      "Training at step=85, batch=0, train loss = 0.4167443912644063, train acc = 0.9100000262260437, time = 0.0205841064453125\n",
      "Training at step=85, batch=120, train loss = 0.22233179731731972, train acc = 0.9200000166893005, time = 0.020557165145874023\n",
      "Training at step=85, batch=240, train loss = 0.15835158197924437, train acc = 0.949999988079071, time = 0.02057504653930664\n",
      "Training at step=85, batch=360, train loss = 0.21455183146774406, train acc = 0.9200000166893005, time = 0.02053666114807129\n",
      "Training at step=85, batch=480, train loss = 0.19060378992075278, train acc = 0.949999988079071, time = 0.020618677139282227\n",
      "Testing at step=85, batch=0, test loss = 0.23577047235736004, test acc = 0.9100000262260437, time = 0.004797220230102539\n",
      "Testing at step=85, batch=20, test loss = 0.14127144030639852, test acc = 0.9599999785423279, time = 0.004735231399536133\n",
      "Testing at step=85, batch=40, test loss = 0.3724342046983289, test acc = 0.8999999761581421, time = 0.004795551300048828\n",
      "Testing at step=85, batch=60, test loss = 0.2221789342091684, test acc = 0.9599999785423279, time = 0.0048465728759765625\n",
      "Testing at step=85, batch=80, test loss = 0.4235008686426785, test acc = 0.8799999952316284, time = 0.0048716068267822266\n",
      "Step 85 finished in 24.450281858444214, Train loss = 0.24198409800450718, Test loss = 0.26951648220834123; Train Acc = 0.9335833357771238, Test Acc = 0.924899999499321\n",
      "Training at step=86, batch=0, train loss = 0.12927925727780307, train acc = 0.949999988079071, time = 0.02064347267150879\n",
      "Training at step=86, batch=120, train loss = 0.14960017656209895, train acc = 0.9599999785423279, time = 0.020531654357910156\n",
      "Training at step=86, batch=240, train loss = 0.3337107887728796, train acc = 0.8700000047683716, time = 0.020439863204956055\n",
      "Training at step=86, batch=360, train loss = 0.1946774311390901, train acc = 0.9300000071525574, time = 0.020646095275878906\n",
      "Training at step=86, batch=480, train loss = 0.17344904157805807, train acc = 0.949999988079071, time = 0.02060985565185547\n",
      "Testing at step=86, batch=0, test loss = 0.2600009435749406, test acc = 0.9100000262260437, time = 0.004895210266113281\n",
      "Testing at step=86, batch=20, test loss = 0.3060332337235611, test acc = 0.949999988079071, time = 0.004716634750366211\n",
      "Testing at step=86, batch=40, test loss = 0.39683899403116746, test acc = 0.8899999856948853, time = 0.004724979400634766\n",
      "Testing at step=86, batch=60, test loss = 0.22544685480037102, test acc = 0.9300000071525574, time = 0.004743814468383789\n",
      "Testing at step=86, batch=80, test loss = 0.17142983809743037, test acc = 0.9399999976158142, time = 0.004743099212646484\n",
      "Step 86 finished in 24.46088933944702, Train loss = 0.24158966306437302, Test loss = 0.2676329623753659; Train Acc = 0.933433336019516, Test Acc = 0.9254000002145767\n",
      "Training at step=87, batch=0, train loss = 0.16794805492661408, train acc = 0.949999988079071, time = 0.02078866958618164\n",
      "Training at step=87, batch=120, train loss = 0.2608294811366768, train acc = 0.9300000071525574, time = 0.02048969268798828\n",
      "Training at step=87, batch=240, train loss = 0.18806689236342028, train acc = 0.9300000071525574, time = 0.02064204216003418\n",
      "Training at step=87, batch=360, train loss = 0.3245851693382725, train acc = 0.9399999976158142, time = 0.020749568939208984\n",
      "Training at step=87, batch=480, train loss = 0.16128894662149892, train acc = 0.949999988079071, time = 0.02045297622680664\n",
      "Testing at step=87, batch=0, test loss = 0.2954353290949551, test acc = 0.949999988079071, time = 0.004926204681396484\n",
      "Testing at step=87, batch=20, test loss = 0.1712472532116921, test acc = 0.949999988079071, time = 0.004763364791870117\n",
      "Testing at step=87, batch=40, test loss = 0.2265012315219784, test acc = 0.9300000071525574, time = 0.00474238395690918\n",
      "Testing at step=87, batch=60, test loss = 0.326687101692401, test acc = 0.9399999976158142, time = 0.004797935485839844\n",
      "Testing at step=87, batch=80, test loss = 0.26847657320602597, test acc = 0.9100000262260437, time = 0.004815578460693359\n",
      "Step 87 finished in 24.58055853843689, Train loss = 0.24105641100123867, Test loss = 0.26759277146186755; Train Acc = 0.9335833337903022, Test Acc = 0.9257000064849854\n",
      "Training at step=88, batch=0, train loss = 0.1697863797693428, train acc = 0.949999988079071, time = 0.020569562911987305\n",
      "Training at step=88, batch=120, train loss = 0.2521981585018056, train acc = 0.8999999761581421, time = 0.02066183090209961\n",
      "Training at step=88, batch=240, train loss = 0.2717537477103222, train acc = 0.9300000071525574, time = 0.020532846450805664\n",
      "Training at step=88, batch=360, train loss = 0.18631996193170292, train acc = 0.9399999976158142, time = 0.02042412757873535\n",
      "Training at step=88, batch=480, train loss = 0.23579028341929897, train acc = 0.9200000166893005, time = 0.020420551300048828\n",
      "Testing at step=88, batch=0, test loss = 0.2943238651061738, test acc = 0.8999999761581421, time = 0.00482630729675293\n",
      "Testing at step=88, batch=20, test loss = 0.13875159086652397, test acc = 0.9599999785423279, time = 0.004735469818115234\n",
      "Testing at step=88, batch=40, test loss = 0.21594513402258367, test acc = 0.9399999976158142, time = 0.004735708236694336\n",
      "Testing at step=88, batch=60, test loss = 0.2662909981466258, test acc = 0.9300000071525574, time = 0.004746437072753906\n",
      "Testing at step=88, batch=80, test loss = 0.2785705005159002, test acc = 0.9300000071525574, time = 0.0047397613525390625\n",
      "Step 88 finished in 24.456526279449463, Train loss = 0.2409461614225296, Test loss = 0.2697016252602438; Train Acc = 0.9333666678269704, Test Acc = 0.9250999993085861\n",
      "Training at step=89, batch=0, train loss = 0.19178440694547436, train acc = 0.9399999976158142, time = 0.02074718475341797\n",
      "Training at step=89, batch=120, train loss = 0.26126393568249007, train acc = 0.8999999761581421, time = 0.02046966552734375\n",
      "Training at step=89, batch=240, train loss = 0.29192659929976417, train acc = 0.9399999976158142, time = 0.020444154739379883\n",
      "Training at step=89, batch=360, train loss = 0.14846050200157407, train acc = 0.9399999976158142, time = 0.020612716674804688\n",
      "Training at step=89, batch=480, train loss = 0.4866508900544003, train acc = 0.8899999856948853, time = 0.02052903175354004\n",
      "Testing at step=89, batch=0, test loss = 0.4093451706173256, test acc = 0.8999999761581421, time = 0.004888057708740234\n",
      "Testing at step=89, batch=20, test loss = 0.2925704867725979, test acc = 0.9100000262260437, time = 0.0047457218170166016\n",
      "Testing at step=89, batch=40, test loss = 0.24968909166389644, test acc = 0.9200000166893005, time = 0.004732608795166016\n",
      "Testing at step=89, batch=60, test loss = 0.41132326346752807, test acc = 0.8700000047683716, time = 0.00478363037109375\n",
      "Testing at step=89, batch=80, test loss = 0.3805919555934997, test acc = 0.8999999761581421, time = 0.004748106002807617\n",
      "Step 89 finished in 24.408324241638184, Train loss = 0.24126212237820974, Test loss = 0.2669004956240771; Train Acc = 0.9329000007112821, Test Acc = 0.9259000027179718\n",
      "Training at step=90, batch=0, train loss = 0.19646664333430505, train acc = 0.9399999976158142, time = 0.020525455474853516\n",
      "Training at step=90, batch=120, train loss = 0.18204436786186728, train acc = 0.949999988079071, time = 0.02046680450439453\n",
      "Training at step=90, batch=240, train loss = 0.38200107738426403, train acc = 0.9200000166893005, time = 0.020502805709838867\n",
      "Training at step=90, batch=360, train loss = 0.440908536998465, train acc = 0.8500000238418579, time = 0.020680665969848633\n",
      "Training at step=90, batch=480, train loss = 0.23304903810092573, train acc = 0.9399999976158142, time = 0.02043914794921875\n",
      "Testing at step=90, batch=0, test loss = 0.13477568289615827, test acc = 0.949999988079071, time = 0.004897117614746094\n",
      "Testing at step=90, batch=20, test loss = 0.35359506461573775, test acc = 0.8899999856948853, time = 0.0047473907470703125\n",
      "Testing at step=90, batch=40, test loss = 0.24391526044734213, test acc = 0.9399999976158142, time = 0.004732370376586914\n",
      "Testing at step=90, batch=60, test loss = 0.3049001745785744, test acc = 0.9300000071525574, time = 0.004746437072753906\n",
      "Testing at step=90, batch=80, test loss = 0.17204791039420378, test acc = 0.9399999976158142, time = 0.0047304630279541016\n",
      "Step 90 finished in 24.459473133087158, Train loss = 0.24078260890384068, Test loss = 0.2685516740800815; Train Acc = 0.9335000017285346, Test Acc = 0.9251000022888184\n",
      "Training at step=91, batch=0, train loss = 0.20113758464967935, train acc = 0.949999988079071, time = 0.020453691482543945\n",
      "Training at step=91, batch=120, train loss = 0.14352239913468046, train acc = 0.9700000286102295, time = 0.020508766174316406\n",
      "Training at step=91, batch=240, train loss = 0.34259543247987717, train acc = 0.9300000071525574, time = 0.020459651947021484\n",
      "Training at step=91, batch=360, train loss = 0.31172496793673393, train acc = 0.949999988079071, time = 0.02063775062561035\n",
      "Training at step=91, batch=480, train loss = 0.1373441368493804, train acc = 0.949999988079071, time = 0.02058720588684082\n",
      "Testing at step=91, batch=0, test loss = 0.18217981021151697, test acc = 0.9399999976158142, time = 0.004832744598388672\n",
      "Testing at step=91, batch=20, test loss = 0.4083128782053204, test acc = 0.8600000143051147, time = 0.004775047302246094\n",
      "Testing at step=91, batch=40, test loss = 0.39171736722437683, test acc = 0.8999999761581421, time = 0.004739284515380859\n",
      "Testing at step=91, batch=60, test loss = 0.2132611097214365, test acc = 0.9399999976158142, time = 0.004755496978759766\n",
      "Testing at step=91, batch=80, test loss = 0.3951719476541599, test acc = 0.8899999856948853, time = 0.004742145538330078\n",
      "Step 91 finished in 24.47005820274353, Train loss = 0.24005840478883764, Test loss = 0.2688726438155218; Train Acc = 0.9334333341320356, Test Acc = 0.9250999993085861\n",
      "Training at step=92, batch=0, train loss = 0.0812780807688394, train acc = 0.9800000190734863, time = 0.02063608169555664\n",
      "Training at step=92, batch=120, train loss = 0.13925736177941336, train acc = 0.9700000286102295, time = 0.020704984664916992\n",
      "Training at step=92, batch=240, train loss = 0.18828282122911083, train acc = 0.9700000286102295, time = 0.02049851417541504\n",
      "Training at step=92, batch=360, train loss = 0.29783947298953256, train acc = 0.9300000071525574, time = 0.020483016967773438\n",
      "Training at step=92, batch=480, train loss = 0.21107958343091035, train acc = 0.9599999785423279, time = 0.02042675018310547\n",
      "Testing at step=92, batch=0, test loss = 0.45567825333039197, test acc = 0.9100000262260437, time = 0.004876613616943359\n",
      "Testing at step=92, batch=20, test loss = 0.17394482728478589, test acc = 0.949999988079071, time = 0.004791975021362305\n",
      "Testing at step=92, batch=40, test loss = 0.3994531661702779, test acc = 0.9200000166893005, time = 0.004799842834472656\n",
      "Testing at step=92, batch=60, test loss = 0.22422529080419654, test acc = 0.9300000071525574, time = 0.004721879959106445\n",
      "Testing at step=92, batch=80, test loss = 0.3360898799223006, test acc = 0.949999988079071, time = 0.0049533843994140625\n",
      "Step 92 finished in 24.465011596679688, Train loss = 0.24001599898861622, Test loss = 0.2706449591400246; Train Acc = 0.933533335228761, Test Acc = 0.924500002861023\n",
      "Training at step=93, batch=0, train loss = 0.3271242526399852, train acc = 0.9200000166893005, time = 0.020570755004882812\n",
      "Training at step=93, batch=120, train loss = 0.2331103242409519, train acc = 0.9399999976158142, time = 0.020516633987426758\n",
      "Training at step=93, batch=240, train loss = 0.27949750407734325, train acc = 0.9200000166893005, time = 0.02047562599182129\n",
      "Training at step=93, batch=360, train loss = 0.2604939894462383, train acc = 0.9399999976158142, time = 0.02042555809020996\n",
      "Training at step=93, batch=480, train loss = 0.21964284101574486, train acc = 0.9300000071525574, time = 0.020533323287963867\n",
      "Testing at step=93, batch=0, test loss = 0.2580602224797169, test acc = 0.949999988079071, time = 0.004777431488037109\n",
      "Testing at step=93, batch=20, test loss = 0.3421764775631572, test acc = 0.8799999952316284, time = 0.004792213439941406\n",
      "Testing at step=93, batch=40, test loss = 0.22077265274826757, test acc = 0.9599999785423279, time = 0.004769325256347656\n",
      "Testing at step=93, batch=60, test loss = 0.28420967098988903, test acc = 0.9200000166893005, time = 0.004780769348144531\n",
      "Testing at step=93, batch=80, test loss = 0.2605182025469843, test acc = 0.9300000071525574, time = 0.004749774932861328\n",
      "Step 93 finished in 24.583584547042847, Train loss = 0.24023468092458755, Test loss = 0.26815368603530404; Train Acc = 0.9341166673103968, Test Acc = 0.925500002503395\n",
      "Training at step=94, batch=0, train loss = 0.1525093408829724, train acc = 0.9599999785423279, time = 0.02048039436340332\n",
      "Training at step=94, batch=120, train loss = 0.2825840296975592, train acc = 0.8999999761581421, time = 0.02062845230102539\n",
      "Training at step=94, batch=240, train loss = 0.18030571162310843, train acc = 0.949999988079071, time = 0.02063131332397461\n",
      "Training at step=94, batch=360, train loss = 0.2576966468767327, train acc = 0.9300000071525574, time = 0.020483732223510742\n",
      "Training at step=94, batch=480, train loss = 0.4165993929157502, train acc = 0.8899999856948853, time = 0.020466089248657227\n",
      "Testing at step=94, batch=0, test loss = 0.2865796017056304, test acc = 0.9300000071525574, time = 0.004885673522949219\n",
      "Testing at step=94, batch=20, test loss = 0.4301273229898306, test acc = 0.8899999856948853, time = 0.0048236846923828125\n",
      "Testing at step=94, batch=40, test loss = 0.29020857680900447, test acc = 0.9200000166893005, time = 0.00484013557434082\n",
      "Testing at step=94, batch=60, test loss = 0.27307683523553294, test acc = 0.9300000071525574, time = 0.004847526550292969\n",
      "Testing at step=94, batch=80, test loss = 0.1986675531356995, test acc = 0.9300000071525574, time = 0.0048198699951171875\n",
      "Step 94 finished in 24.542112112045288, Train loss = 0.23983113784558066, Test loss = 0.27004397795889906; Train Acc = 0.9339000016450882, Test Acc = 0.9252000015974045\n",
      "Training at step=95, batch=0, train loss = 0.19193949656796627, train acc = 0.9399999976158142, time = 0.02070164680480957\n",
      "Training at step=95, batch=120, train loss = 0.21741684022689944, train acc = 0.9300000071525574, time = 0.020720243453979492\n",
      "Training at step=95, batch=240, train loss = 0.17006750845954294, train acc = 0.949999988079071, time = 0.020450353622436523\n",
      "Training at step=95, batch=360, train loss = 0.33035486355839006, train acc = 0.9100000262260437, time = 0.020539522171020508\n",
      "Training at step=95, batch=480, train loss = 0.3037994213405949, train acc = 0.9200000166893005, time = 0.020502567291259766\n",
      "Testing at step=95, batch=0, test loss = 0.42361435006964787, test acc = 0.8700000047683716, time = 0.004746198654174805\n",
      "Testing at step=95, batch=20, test loss = 0.30051087215679984, test acc = 0.949999988079071, time = 0.004748344421386719\n",
      "Testing at step=95, batch=40, test loss = 0.2020319287616778, test acc = 0.9700000286102295, time = 0.0047376155853271484\n",
      "Testing at step=95, batch=60, test loss = 0.3839579149817118, test acc = 0.8899999856948853, time = 0.004752159118652344\n",
      "Testing at step=95, batch=80, test loss = 0.2032446882147668, test acc = 0.9599999785423279, time = 0.004731893539428711\n",
      "Step 95 finished in 24.475090503692627, Train loss = 0.2398315389738786, Test loss = 0.26870224528858083; Train Acc = 0.9340500019987424, Test Acc = 0.9274000024795532\n",
      "Training at step=96, batch=0, train loss = 0.2605307601629007, train acc = 0.9300000071525574, time = 0.020618200302124023\n",
      "Training at step=96, batch=120, train loss = 0.2891738266674954, train acc = 0.9300000071525574, time = 0.020450115203857422\n",
      "Training at step=96, batch=240, train loss = 0.20782902566886424, train acc = 0.9399999976158142, time = 0.020577430725097656\n",
      "Training at step=96, batch=360, train loss = 0.27451188269468063, train acc = 0.9200000166893005, time = 0.020689725875854492\n",
      "Training at step=96, batch=480, train loss = 0.19035275259050638, train acc = 0.9200000166893005, time = 0.02067089080810547\n",
      "Testing at step=96, batch=0, test loss = 0.2855400482829362, test acc = 0.9200000166893005, time = 0.004935741424560547\n",
      "Testing at step=96, batch=20, test loss = 0.39901124118165393, test acc = 0.8799999952316284, time = 0.004749298095703125\n",
      "Testing at step=96, batch=40, test loss = 0.28138610379549944, test acc = 0.9100000262260437, time = 0.004757404327392578\n",
      "Testing at step=96, batch=60, test loss = 0.3443393640039315, test acc = 0.8899999856948853, time = 0.004732608795166016\n",
      "Testing at step=96, batch=80, test loss = 0.32939361826181845, test acc = 0.9100000262260437, time = 0.004777431488037109\n",
      "Step 96 finished in 24.436466693878174, Train loss = 0.23971625217606785, Test loss = 0.26934825686720304; Train Acc = 0.9340500017007192, Test Acc = 0.925699999332428\n",
      "Training at step=97, batch=0, train loss = 0.20920303160555292, train acc = 0.9399999976158142, time = 0.020654916763305664\n",
      "Training at step=97, batch=120, train loss = 0.2784675624889428, train acc = 0.9300000071525574, time = 0.02044224739074707\n",
      "Training at step=97, batch=240, train loss = 0.14900819140136248, train acc = 0.949999988079071, time = 0.020627737045288086\n",
      "Training at step=97, batch=360, train loss = 0.1256657250687188, train acc = 0.9700000286102295, time = 0.020490646362304688\n",
      "Training at step=97, batch=480, train loss = 0.200603974034295, train acc = 0.949999988079071, time = 0.020525693893432617\n",
      "Testing at step=97, batch=0, test loss = 0.3682111953123559, test acc = 0.8999999761581421, time = 0.004912614822387695\n",
      "Testing at step=97, batch=20, test loss = 0.16841281180692647, test acc = 0.949999988079071, time = 0.004909992218017578\n",
      "Testing at step=97, batch=40, test loss = 0.28818418995682626, test acc = 0.8899999856948853, time = 0.004762172698974609\n",
      "Testing at step=97, batch=60, test loss = 0.316119750201713, test acc = 0.8899999856948853, time = 0.004766225814819336\n",
      "Testing at step=97, batch=80, test loss = 0.21993359091485096, test acc = 0.9399999976158142, time = 0.004750490188598633\n",
      "Step 97 finished in 24.483882665634155, Train loss = 0.23959155901535278, Test loss = 0.2725238941164847; Train Acc = 0.9337666684389114, Test Acc = 0.9243000012636184\n",
      "Training at step=98, batch=0, train loss = 0.2742228194419394, train acc = 0.9100000262260437, time = 0.020588159561157227\n",
      "Training at step=98, batch=120, train loss = 0.4438834099644036, train acc = 0.9200000166893005, time = 0.020512104034423828\n",
      "Training at step=98, batch=240, train loss = 0.3228960305913749, train acc = 0.9399999976158142, time = 0.020421266555786133\n",
      "Training at step=98, batch=360, train loss = 0.25354858181949036, train acc = 0.9300000071525574, time = 0.02037215232849121\n",
      "Training at step=98, batch=480, train loss = 0.3699533614470773, train acc = 0.949999988079071, time = 0.02037644386291504\n",
      "Testing at step=98, batch=0, test loss = 0.35831887316922606, test acc = 0.9100000262260437, time = 0.004911661148071289\n",
      "Testing at step=98, batch=20, test loss = 0.23845156816500615, test acc = 0.9200000166893005, time = 0.004775524139404297\n",
      "Testing at step=98, batch=40, test loss = 0.33648348399804356, test acc = 0.9200000166893005, time = 0.004775524139404297\n",
      "Testing at step=98, batch=60, test loss = 0.320359161574203, test acc = 0.8999999761581421, time = 0.004774332046508789\n",
      "Testing at step=98, batch=80, test loss = 0.0668211776390727, test acc = 0.9900000095367432, time = 0.0047338008880615234\n",
      "Step 98 finished in 24.468114137649536, Train loss = 0.23929388328958442, Test loss = 0.2708436777915149; Train Acc = 0.9337333350380261, Test Acc = 0.9247000008821488\n",
      "Training at step=99, batch=0, train loss = 0.2553588066428347, train acc = 0.949999988079071, time = 0.020743131637573242\n",
      "Training at step=99, batch=120, train loss = 0.2808196693501155, train acc = 0.8999999761581421, time = 0.020384788513183594\n",
      "Training at step=99, batch=240, train loss = 0.4263687879899019, train acc = 0.8899999856948853, time = 0.020449399948120117\n",
      "Training at step=99, batch=360, train loss = 0.1903116783662312, train acc = 0.9300000071525574, time = 0.020579099655151367\n",
      "Training at step=99, batch=480, train loss = 0.15279259037409193, train acc = 0.949999988079071, time = 0.020545244216918945\n",
      "Testing at step=99, batch=0, test loss = 0.19731569752473146, test acc = 0.9200000166893005, time = 0.004769325256347656\n",
      "Testing at step=99, batch=20, test loss = 0.38423670061012216, test acc = 0.8600000143051147, time = 0.0048220157623291016\n",
      "Testing at step=99, batch=40, test loss = 0.1923853719143875, test acc = 0.949999988079071, time = 0.0047969818115234375\n",
      "Testing at step=99, batch=60, test loss = 0.25883412537134026, test acc = 0.9300000071525574, time = 0.004765510559082031\n",
      "Testing at step=99, batch=80, test loss = 0.25121894208952467, test acc = 0.949999988079071, time = 0.004735231399536133\n",
      "Step 99 finished in 24.484004974365234, Train loss = 0.23901725146388833, Test loss = 0.27157784563966963; Train Acc = 0.9344666687647502, Test Acc = 0.9254000025987625\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"mnist_simple_cnn.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "V7YYHs33saIx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712093854908,
     "user_tz": -660,
     "elapsed": 1263,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "d1f320d0-a339-46a4-fbb6-b9f4fc265cfa",
    "ExecuteTime": {
     "end_time": "2024-04-06T20:52:16.468908Z",
     "start_time": "2024-04-06T20:52:15.930695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:52:51.177096Z",
     "start_time": "2024-04-06T20:52:51.168767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"mnist_simple_cnn.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
