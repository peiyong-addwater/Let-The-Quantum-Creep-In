{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMZ79VIcBqatCN3QMKuyuE4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"acHJk38WrBOi","executionInfo":{"status":"ok","timestamp":1712091561992,"user_tz":-660,"elapsed":641011,"user":{"displayName":"P. W.","userId":"06457912707533471190"}},"outputId":"85a0290a-e46d-4caf-93ed-a7e0fdc53d3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pennylane\n","  Downloading PennyLane-0.35.1-py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cotengra\n","  Downloading cotengra-0.5.6-py3-none-any.whl (148 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.0/148.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting quimb\n","  Downloading quimb-1.7.3-py3-none-any.whl (500 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.7/500.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchmetrics\n","  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.11.4)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.2.1)\n","Collecting rustworkx (from pennylane)\n","  Downloading rustworkx-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n","Collecting semantic-version>=2.7 (from pennylane)\n","  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n","Collecting autoray>=0.6.1 (from pennylane)\n","  Downloading autoray-0.6.9-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.3)\n","Collecting pennylane-lightning>=0.35 (from pennylane)\n","  Downloading PennyLane_Lightning-0.35.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.10.0)\n","Collecting cytoolz>=0.8.0 (from quimb)\n","  Downloading cytoolz-0.12.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numba>=0.39 in /usr/local/lib/python3.10/dist-packages (from quimb) (0.58.1)\n","Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from quimb) (5.9.5)\n","Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.10/dist-packages (from quimb) (4.66.2)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.39->quimb) (0.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n","  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n","Installing collected packages: semantic-version, rustworkx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, cytoolz, autoray, nvidia-cusparse-cu12, nvidia-cudnn-cu12, cotengra, quimb, nvidia-cusolver-cu12, torchmetrics, pennylane-lightning, pennylane\n","Successfully installed autoray-0.6.9 cotengra-0.5.6 cytoolz-0.12.3 lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pennylane-0.35.1 pennylane-lightning-0.35.1 quimb-1.7.3 rustworkx-0.14.2 semantic-version-2.10.0 torchmetrics-1.3.2\n"]}],"source":["!pip install pennylane cotengra quimb torchmetrics --upgrade"]},{"cell_type":"code","source":["# Import packages\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import sys\n","import numpy as np\n","#np.set_printoptions(threshold=sys.maxsize)\n","import pandas as pd\n","from sklearn import datasets\n","import seaborn as sns\n","#import jax\n","import time\n","\n","import functools\n","\n","from typing import List, Union, Tuple, Dict, Optional, Any\n","from typing import Callable\n","\n","#jax.config.update(\"jax_enable_x64\", True)\n","#jax.config.update(\"jax_debug_nans\", True)\n","#import jax.numpy as jnp\n","\n","#import optax  # optimization using jax\n","\n","import torch  # https://pytorch.org\n","import torchvision  # https://pytorch.org\n","#torch.set_printoptions(profile=\"full\")\n","#import torch_xla\n","#import torch_xla.core.xla_model as xm\n","\n","\n","\n","import pennylane as qml\n","import pennylane.numpy as pnp\n","\n","import os, cv2, itertools # cv2 -- OpenCV\n","import shutil\n","import zipfile\n","%matplotlib inline\n","\n","#from jax.lib import xla_bridge\n","\n","\n","sns.set()\n","\n","seed = 1701\n","rng = np.random.default_rng(seed=seed)\n","prng = pnp.random.default_rng(seed=seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","\n","torch.set_default_dtype(torch.double)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n","#REAL_DTYPE = torch.float\n","\n","print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FB_BuVzCrMNk","executionInfo":{"status":"ok","timestamp":1712091570174,"user_tz":-660,"elapsed":8191,"user":{"displayName":"P. W.","userId":"06457912707533471190"}},"outputId":"9efa8c3e-cdcc-46df-95ab-f4f1006c4ba3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["preprocess = torchvision.transforms.Compose([\n","    torchvision.transforms.Pad(2),\n","    torchvision.transforms.ToTensor(),\n","    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n","    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n","    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n","    torchvision.transforms.Normalize((0.5,), (0.5,)),\n","    #torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n","])\n","\n","train_dataset = torchvision.datasets.MNIST(\n","    \"MNIST\",\n","    train=True,\n","    download=True,\n","    transform=preprocess,\n",")\n","test_dataset = torchvision.datasets.MNIST(\n","    \"MNIST\",\n","    train=False,\n","    download=True,\n","    transform=preprocess,\n",")\n","dummy_trainloader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=64, shuffle=True\n",")\n","dummy_testloader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=64, shuffle=True\n",")\n","\n","dummy_x, dummy_y = next(iter(dummy_trainloader))\n","\n","print(dummy_x.shape)  # 64x32x32\n","print(dummy_y.shape)  # 64\n","print(dummy_y)\n","print(dummy_x[0,0,16])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K7wJnOiEralR","executionInfo":{"status":"ok","timestamp":1712091571140,"user_tz":-660,"elapsed":977,"user":{"displayName":"P. W.","userId":"06457912707533471190"}},"outputId":"aeaff2d6-e1d2-4a3b-db6c-918faa8f8746"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 97874946.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST/MNIST/raw/train-images-idx3-ubyte.gz to MNIST/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 51306943.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 31439585.57it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 4393572.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST/MNIST/raw\n","\n","torch.Size([64, 1, 32, 32])\n","torch.Size([64])\n","tensor([7, 7, 5, 7, 4, 7, 7, 7, 5, 4, 7, 2, 7, 8, 1, 1, 1, 6, 4, 1, 5, 1, 2, 7,\n","        8, 0, 8, 6, 1, 8, 8, 7, 4, 4, 7, 9, 5, 4, 3, 9, 6, 5, 6, 0, 3, 9, 5, 3,\n","        5, 1, 0, 4, 2, 3, 2, 0, 1, 5, 6, 4, 8, 6, 3, 9])\n","tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n","        -1.0000, -1.0000, -1.0000,  0.2863,  0.7098, -0.7333, -1.0000, -1.0000,\n","        -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000])\n"]}]},{"cell_type":"code","source":["class SimpleNet(torch.nn.Module):\n","  def __init__(self):\n","    super(SimpleNet, self).__init__()\n","\n","    self.layers = torch.nn.Sequential(\n","        torch.nn.Conv2d(1, 32, kernel_size=3),\n","        #torch.nn.ReLU(),\n","        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n","        torch.nn.Conv2d(32, 16, kernel_size=3),\n","        #torch.nn.ReLU(),\n","        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n","        torch.nn.Flatten(),\n","        torch.nn.Linear(16*28*28, 10),\n","    )\n","\n","  def forward(self, x):\n","    return self.layers(x)\n","\n","net = SimpleNet().to(device)\n","test_img = dummy_x.to(device)\n","print(test_img.shape)\n","print(net)\n","test_out = net(test_img)\n","print(test_out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t6wDzOaJrmZ4","executionInfo":{"status":"ok","timestamp":1712091571799,"user_tz":-660,"elapsed":661,"user":{"displayName":"P. W.","userId":"06457912707533471190"}},"outputId":"53fc9e5e-21f5-46b1-9274-a493223a8d56"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 1, 32, 32])\n","SimpleNet(\n","  (layers): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n","    (1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n","    (2): Flatten(start_dim=1, end_dim=-1)\n","    (3): Linear(in_features=12544, out_features=10, bias=True)\n","  )\n",")\n","torch.Size([64, 10])\n"]}]},{"cell_type":"code","source":["import torchmetrics\n","#criterion = torch.nn.CrossEntropyLoss()\n","#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n","#accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n","\n","BATCH_SIZE = 100\n","LEARNING_RATE = 3e-4\n","STEPS = 100\n","PRINT_EVERY_PERCENT = 0.2\n","\n","def train(\n","    model,\n","    optim=torch.optim.SGD,\n","    criterion=torch.nn.CrossEntropyLoss,\n","    accuracy = torchmetrics.Accuracy,\n","    steps = 100,\n","    print_every_percent=0.1,\n","    batchsize = 100,\n","    lr = 0.001,\n","    device=torch.device(\"cpu\")\n","):\n","  trainloader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=batchsize, shuffle=True\n","  )\n","  testloader = torch.utils.data.DataLoader(\n","    test_dataset, batch_size=batchsize, shuffle=True\n","  )\n","\n","  n_train_batches = len(trainloader)\n","  n_test_batches = len(testloader)\n","  print_every_train_batch = int(n_train_batches*print_every_percent)\n","  print_every_test_batch = int(n_test_batches*print_every_percent)\n","\n","  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n","  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n","\n","  model.to(device)\n","  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n","  loss = criterion()\n","  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n","  step_train_losses = []\n","  step_test_losses = []\n","  step_train_accs = []\n","  step_test_accs = []\n","  for i in range(steps):\n","    step_start = time.time()\n","    batch_train_loss = []\n","    batch_train_acc = []\n","    batch_test_loss = []\n","    batch_test_acc = []\n","    # train\n","    model.train()\n","    for batchid, (images, labels) in enumerate(trainloader):\n","      batch_start = time.time()\n","      images, labels = images.to(device), labels.to(device)\n","      optimizer.zero_grad()\n","      outputs = model(images)\n","      train_loss = loss(outputs, labels)\n","      train_loss.backward()\n","      optimizer.step()\n","      train_acc = acc_func(outputs, labels)\n","      batch_train_loss.append(train_loss.item())\n","      batch_train_acc.append(train_acc.item())\n","      batch_finish = time.time()\n","\n","      if (batchid) % print_every_train_batch == 0:\n","        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n","\n","    # eval\n","    model.eval()\n","    with torch.no_grad():\n","      for batchid, (images, labels) in enumerate(testloader):\n","        batch_start = time.time()\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        test_loss = loss(outputs, labels)\n","        test_acc = acc_func(outputs, labels)\n","        batch_test_loss.append(test_loss.item())\n","        batch_test_acc.append(test_acc.item())\n","        batch_finish = time.time()\n","        if (batchid) % print_every_test_batch == 0:\n","          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n","\n","    step_train_losses.append(np.mean(batch_train_loss))\n","    step_test_losses.append(np.mean(batch_test_loss))\n","    step_train_accs.append(np.mean(batch_train_acc))\n","    step_test_accs.append(np.mean(batch_test_acc))\n","    step_finish = time.time()\n","    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n","\n","  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n","\n","train_losses, test_losses, train_accs, test_accs = train(net,\n","                                                        optim=torch.optim.SGD,\n","                                                        criterion=torch.nn.CrossEntropyLoss,\n","                                                        accuracy = torchmetrics.Accuracy,\n","                                                        steps = STEPS,\n","                                                        print_every_percent=PRINT_EVERY_PERCENT,\n","                                                        batchsize = BATCH_SIZE,\n","                                                        lr = LEARNING_RATE,\n","                                                        device=device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PDNHjKy4sSWZ","executionInfo":{"status":"ok","timestamp":1712093853656,"user_tz":-660,"elapsed":2281859,"user":{"displayName":"P. W.","userId":"06457912707533471190"}},"outputId":"eed38404-a80c-4311-ba2b-c191172a7133"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of train batches = 600, Number of test batches = 100\n","Print every train batch = 120, Print every test batch = 20\n","Training at step=0, batch=0, train loss = 2.30913119184226, train acc = 0.10000000149011612, time = 0.34546589851379395\n","Training at step=0, batch=120, train loss = 0.6894455833445946, train acc = 0.8199999928474426, time = 0.009140968322753906\n","Training at step=0, batch=240, train loss = 0.4453760703799171, train acc = 0.8799999952316284, time = 0.009415626525878906\n","Training at step=0, batch=360, train loss = 0.423852149820715, train acc = 0.8600000143051147, time = 0.009052515029907227\n","Training at step=0, batch=480, train loss = 0.3411368893037979, train acc = 0.8999999761581421, time = 0.009101629257202148\n","Testing at step=0, batch=0, test loss = 0.2716785533017093, test acc = 0.9300000071525574, time = 0.0020148754119873047\n","Testing at step=0, batch=20, test loss = 0.2800219567198676, test acc = 0.9399999976158142, time = 0.0019428730010986328\n","Testing at step=0, batch=40, test loss = 0.3252035169837456, test acc = 0.8799999952316284, time = 0.0019345283508300781\n","Testing at step=0, batch=60, test loss = 0.3635226098537159, test acc = 0.8999999761581421, time = 0.0019674301147460938\n","Testing at step=0, batch=80, test loss = 0.4023799988086056, test acc = 0.8999999761581421, time = 0.001958608627319336\n","Step 0 finished in 23.227052450180054, Train loss = 0.6291836756164069, Test loss = 0.349028008582764; Train Acc = 0.8228833331974844, Test Acc = 0.8960999983549118\n","Training at step=1, batch=0, train loss = 0.4445004826844425, train acc = 0.8399999737739563, time = 0.009023189544677734\n","Training at step=1, batch=120, train loss = 0.366866429416344, train acc = 0.9100000262260437, time = 0.009096145629882812\n","Training at step=1, batch=240, train loss = 0.3328256528798735, train acc = 0.8899999856948853, time = 0.009067773818969727\n","Training at step=1, batch=360, train loss = 0.31348835569794464, train acc = 0.9200000166893005, time = 0.009028911590576172\n","Training at step=1, batch=480, train loss = 0.527533336310276, train acc = 0.8999999761581421, time = 0.009024381637573242\n","Testing at step=1, batch=0, test loss = 0.2917365260201864, test acc = 0.8899999856948853, time = 0.001981496810913086\n","Testing at step=1, batch=20, test loss = 0.369270347202179, test acc = 0.9100000262260437, time = 0.0019860267639160156\n","Testing at step=1, batch=40, test loss = 0.3698397070595953, test acc = 0.8600000143051147, time = 0.0019636154174804688\n","Testing at step=1, batch=60, test loss = 0.3045837550191411, test acc = 0.9399999976158142, time = 0.0019390583038330078\n","Testing at step=1, batch=80, test loss = 0.4774512631774581, test acc = 0.8299999833106995, time = 0.0019643306732177734\n","Step 1 finished in 22.768288373947144, Train loss = 0.34410510657450805, Test loss = 0.3284981941489225; Train Acc = 0.8987833345929782, Test Acc = 0.902200003862381\n","Training at step=2, batch=0, train loss = 0.256930649679049, train acc = 0.9300000071525574, time = 0.00925755500793457\n","Training at step=2, batch=120, train loss = 0.4193471459307723, train acc = 0.8700000047683716, time = 0.009115934371948242\n","Training at step=2, batch=240, train loss = 0.2878348523106126, train acc = 0.8999999761581421, time = 0.00913548469543457\n","Training at step=2, batch=360, train loss = 0.19980511551629104, train acc = 0.9399999976158142, time = 0.009036064147949219\n","Training at step=2, batch=480, train loss = 0.5028864620504082, train acc = 0.8600000143051147, time = 0.009083747863769531\n","Testing at step=2, batch=0, test loss = 0.30799835836885725, test acc = 0.8999999761581421, time = 0.0020036697387695312\n","Testing at step=2, batch=20, test loss = 0.390320510564934, test acc = 0.8799999952316284, time = 0.0019381046295166016\n","Testing at step=2, batch=40, test loss = 0.33031939883765965, test acc = 0.9200000166893005, time = 0.002003192901611328\n","Testing at step=2, batch=60, test loss = 0.23973993768157487, test acc = 0.9100000262260437, time = 0.001943349838256836\n","Testing at step=2, batch=80, test loss = 0.3273242466415076, test acc = 0.9399999976158142, time = 0.0019347667694091797\n","Step 2 finished in 22.674288511276245, Train loss = 0.32006430406907016, Test loss = 0.2920148433699176; Train Acc = 0.9067500022053718, Test Acc = 0.9175000023841858\n","Training at step=3, batch=0, train loss = 0.3795571621327458, train acc = 0.8799999952316284, time = 0.009193658828735352\n","Training at step=3, batch=120, train loss = 0.41512436219310717, train acc = 0.8999999761581421, time = 0.009029626846313477\n","Training at step=3, batch=240, train loss = 0.25816929338326333, train acc = 0.9300000071525574, time = 0.00899362564086914\n","Training at step=3, batch=360, train loss = 0.1577090489429586, train acc = 0.9599999785423279, time = 0.00905609130859375\n","Training at step=3, batch=480, train loss = 0.2812973907542697, train acc = 0.9100000262260437, time = 0.009185314178466797\n","Testing at step=3, batch=0, test loss = 0.26710278946585886, test acc = 0.9399999976158142, time = 0.001947164535522461\n","Testing at step=3, batch=20, test loss = 0.14301226917875118, test acc = 0.949999988079071, time = 0.0019843578338623047\n","Testing at step=3, batch=40, test loss = 0.2740361156744659, test acc = 0.8799999952316284, time = 0.0019538402557373047\n","Testing at step=3, batch=60, test loss = 0.20792658107534542, test acc = 0.9100000262260437, time = 0.0019659996032714844\n","Testing at step=3, batch=80, test loss = 0.307985765266961, test acc = 0.9300000071525574, time = 0.0019428730010986328\n","Step 3 finished in 22.64921498298645, Train loss = 0.30970632199584297, Test loss = 0.2929057426267734; Train Acc = 0.9107000022133191, Test Acc = 0.9162000030279159\n","Training at step=4, batch=0, train loss = 0.2315758559391479, train acc = 0.9200000166893005, time = 0.009090185165405273\n","Training at step=4, batch=120, train loss = 0.39037481298725896, train acc = 0.9200000166893005, time = 0.009110212326049805\n","Training at step=4, batch=240, train loss = 0.20677470134435672, train acc = 0.9399999976158142, time = 0.008976936340332031\n","Training at step=4, batch=360, train loss = 0.31458909379543404, train acc = 0.9399999976158142, time = 0.009042501449584961\n","Training at step=4, batch=480, train loss = 0.33163485340476745, train acc = 0.8600000143051147, time = 0.008986711502075195\n","Testing at step=4, batch=0, test loss = 0.29439316933078435, test acc = 0.949999988079071, time = 0.0019409656524658203\n","Testing at step=4, batch=20, test loss = 0.29649469219633057, test acc = 0.9399999976158142, time = 0.0019805431365966797\n","Testing at step=4, batch=40, test loss = 0.4132046837087426, test acc = 0.8500000238418579, time = 0.0019423961639404297\n","Testing at step=4, batch=60, test loss = 0.3119995248746762, test acc = 0.8999999761581421, time = 0.0019664764404296875\n","Testing at step=4, batch=80, test loss = 0.47742487391679395, test acc = 0.8999999761581421, time = 0.0019621849060058594\n","Step 4 finished in 22.656960248947144, Train loss = 0.2990550425699789, Test loss = 0.2838882369238973; Train Acc = 0.9143000010649364, Test Acc = 0.9196999996900559\n","Training at step=5, batch=0, train loss = 0.33162642159303585, train acc = 0.9100000262260437, time = 0.009130001068115234\n","Training at step=5, batch=120, train loss = 0.27153089471811115, train acc = 0.8999999761581421, time = 0.009011983871459961\n","Training at step=5, batch=240, train loss = 0.1976414991870762, train acc = 0.949999988079071, time = 0.009076118469238281\n","Training at step=5, batch=360, train loss = 0.20733652715449938, train acc = 0.9300000071525574, time = 0.009006261825561523\n","Training at step=5, batch=480, train loss = 0.17638327458822353, train acc = 0.949999988079071, time = 0.009081602096557617\n","Testing at step=5, batch=0, test loss = 0.2250274992040387, test acc = 0.9300000071525574, time = 0.002009868621826172\n","Testing at step=5, batch=20, test loss = 0.33482842204102703, test acc = 0.9200000166893005, time = 0.0019750595092773438\n","Testing at step=5, batch=40, test loss = 0.2972406311909604, test acc = 0.9399999976158142, time = 0.0019655227661132812\n","Testing at step=5, batch=60, test loss = 0.39697904397488165, test acc = 0.8999999761581421, time = 0.0020835399627685547\n","Testing at step=5, batch=80, test loss = 0.41583654709226875, test acc = 0.9100000262260437, time = 0.001970052719116211\n","Step 5 finished in 22.65583848953247, Train loss = 0.29471320485489905, Test loss = 0.28580677325419396; Train Acc = 0.9163666686415672, Test Acc = 0.9178000038862228\n","Training at step=6, batch=0, train loss = 0.32511506675582474, train acc = 0.9399999976158142, time = 0.009133577346801758\n","Training at step=6, batch=120, train loss = 0.3013902090326448, train acc = 0.9100000262260437, time = 0.009061336517333984\n","Training at step=6, batch=240, train loss = 0.21343235897400653, train acc = 0.949999988079071, time = 0.009137868881225586\n","Training at step=6, batch=360, train loss = 0.13954230529718709, train acc = 0.9700000286102295, time = 0.008966922760009766\n","Training at step=6, batch=480, train loss = 0.659789666871314, train acc = 0.8600000143051147, time = 0.009106159210205078\n","Testing at step=6, batch=0, test loss = 0.3124030827844928, test acc = 0.9200000166893005, time = 0.001984119415283203\n","Testing at step=6, batch=20, test loss = 0.34596049118468564, test acc = 0.9300000071525574, time = 0.0019600391387939453\n","Testing at step=6, batch=40, test loss = 0.259251189225791, test acc = 0.9399999976158142, time = 0.0019457340240478516\n","Testing at step=6, batch=60, test loss = 0.3116058339675077, test acc = 0.9100000262260437, time = 0.001924753189086914\n","Testing at step=6, batch=80, test loss = 0.17395019382242655, test acc = 0.9599999785423279, time = 0.0019025802612304688\n","Step 6 finished in 22.674646615982056, Train loss = 0.2904220152346709, Test loss = 0.284183644370473; Train Acc = 0.9171166692177455, Test Acc = 0.9199000006914139\n","Training at step=7, batch=0, train loss = 0.3009895701596308, train acc = 0.9300000071525574, time = 0.009076356887817383\n","Training at step=7, batch=120, train loss = 0.47424324203268525, train acc = 0.8399999737739563, time = 0.008987188339233398\n","Training at step=7, batch=240, train loss = 0.1658862683979539, train acc = 0.9700000286102295, time = 0.008984088897705078\n","Training at step=7, batch=360, train loss = 0.17394152195258705, train acc = 0.9599999785423279, time = 0.009018421173095703\n","Training at step=7, batch=480, train loss = 0.2554361062761165, train acc = 0.8799999952316284, time = 0.009043455123901367\n","Testing at step=7, batch=0, test loss = 0.28302667299628614, test acc = 0.9200000166893005, time = 0.0021860599517822266\n","Testing at step=7, batch=20, test loss = 0.36121800281083943, test acc = 0.8799999952316284, time = 0.002008199691772461\n","Testing at step=7, batch=40, test loss = 0.3513343814558925, test acc = 0.8899999856948853, time = 0.0019729137420654297\n","Testing at step=7, batch=60, test loss = 0.27981649686944704, test acc = 0.9100000262260437, time = 0.0020029544830322266\n","Testing at step=7, batch=80, test loss = 0.2775827606591352, test acc = 0.8999999761581421, time = 0.0019893646240234375\n","Step 7 finished in 22.68677592277527, Train loss = 0.28522112153915835, Test loss = 0.2774642261997198; Train Acc = 0.919716669122378, Test Acc = 0.920500003695488\n","Training at step=8, batch=0, train loss = 0.2873922258597966, train acc = 0.8700000047683716, time = 0.009126901626586914\n","Training at step=8, batch=120, train loss = 0.26358233186521174, train acc = 0.9300000071525574, time = 0.009127140045166016\n","Training at step=8, batch=240, train loss = 0.3867201536547338, train acc = 0.9100000262260437, time = 0.008968114852905273\n","Training at step=8, batch=360, train loss = 0.454345975549988, train acc = 0.8799999952316284, time = 0.009096860885620117\n","Training at step=8, batch=480, train loss = 0.3705787311480782, train acc = 0.9100000262260437, time = 0.009045124053955078\n","Testing at step=8, batch=0, test loss = 0.19971533465927435, test acc = 0.9100000262260437, time = 0.001987457275390625\n","Testing at step=8, batch=20, test loss = 0.22636558167225881, test acc = 0.949999988079071, time = 0.001992940902709961\n","Testing at step=8, batch=40, test loss = 0.18603171562303944, test acc = 0.9200000166893005, time = 0.002013683319091797\n","Testing at step=8, batch=60, test loss = 0.2532974553931168, test acc = 0.9300000071525574, time = 0.0019922256469726562\n","Testing at step=8, batch=80, test loss = 0.31567704420778175, test acc = 0.9200000166893005, time = 0.0019593238830566406\n","Step 8 finished in 22.839566946029663, Train loss = 0.28215090306206175, Test loss = 0.27696062662445015; Train Acc = 0.9196833357214927, Test Acc = 0.9189000028371811\n","Training at step=9, batch=0, train loss = 0.33047434030188677, train acc = 0.9399999976158142, time = 0.009165525436401367\n","Training at step=9, batch=120, train loss = 0.2313418983420831, train acc = 0.9300000071525574, time = 0.009055137634277344\n","Training at step=9, batch=240, train loss = 0.25215770556448486, train acc = 0.949999988079071, time = 0.00897669792175293\n","Training at step=9, batch=360, train loss = 0.42470524150857153, train acc = 0.8899999856948853, time = 0.009047508239746094\n","Training at step=9, batch=480, train loss = 0.161032503260056, train acc = 0.949999988079071, time = 0.008926153182983398\n","Testing at step=9, batch=0, test loss = 0.330778694555854, test acc = 0.9100000262260437, time = 0.001977682113647461\n","Testing at step=9, batch=20, test loss = 0.24458836742386328, test acc = 0.9200000166893005, time = 0.001960277557373047\n","Testing at step=9, batch=40, test loss = 0.2247964169751166, test acc = 0.9300000071525574, time = 0.001996755599975586\n","Testing at step=9, batch=60, test loss = 0.38745158331419227, test acc = 0.9200000166893005, time = 0.0019686222076416016\n","Testing at step=9, batch=80, test loss = 0.4052500493820253, test acc = 0.8799999952316284, time = 0.0020513534545898438\n","Step 9 finished in 22.738693475723267, Train loss = 0.2800589650091036, Test loss = 0.2760001454858497; Train Acc = 0.9211833350857099, Test Acc = 0.9239000004529953\n","Training at step=10, batch=0, train loss = 0.20980414759450686, train acc = 0.9399999976158142, time = 0.009265661239624023\n","Training at step=10, batch=120, train loss = 0.174824849667898, train acc = 0.9200000166893005, time = 0.009085893630981445\n","Training at step=10, batch=240, train loss = 0.2128288354779483, train acc = 0.9200000166893005, time = 0.008973360061645508\n","Training at step=10, batch=360, train loss = 0.17247272778268968, train acc = 0.9599999785423279, time = 0.009020566940307617\n","Training at step=10, batch=480, train loss = 0.24321059253968602, train acc = 0.949999988079071, time = 0.009046792984008789\n","Testing at step=10, batch=0, test loss = 0.20729003922317843, test acc = 0.9399999976158142, time = 0.0019774436950683594\n","Testing at step=10, batch=20, test loss = 0.12425728473384462, test acc = 0.9700000286102295, time = 0.001935720443725586\n","Testing at step=10, batch=40, test loss = 0.2773924581348013, test acc = 0.9200000166893005, time = 0.0019490718841552734\n","Testing at step=10, batch=60, test loss = 0.28611256949945707, test acc = 0.8999999761581421, time = 0.0019104480743408203\n","Testing at step=10, batch=80, test loss = 0.344782702078153, test acc = 0.9200000166893005, time = 0.0019638538360595703\n","Step 10 finished in 22.70090675354004, Train loss = 0.27755107824182185, Test loss = 0.2732731814108977; Train Acc = 0.9222000013788542, Test Acc = 0.9232000023126602\n","Training at step=11, batch=0, train loss = 0.3739610799621938, train acc = 0.8999999761581421, time = 0.009241342544555664\n","Training at step=11, batch=120, train loss = 0.2702311642853161, train acc = 0.9100000262260437, time = 0.00903177261352539\n","Training at step=11, batch=240, train loss = 0.32008476695516824, train acc = 0.8899999856948853, time = 0.008954048156738281\n","Training at step=11, batch=360, train loss = 0.18449355711324006, train acc = 0.9700000286102295, time = 0.008986711502075195\n","Training at step=11, batch=480, train loss = 0.3086427337014208, train acc = 0.9300000071525574, time = 0.009003639221191406\n","Testing at step=11, batch=0, test loss = 0.334430172280887, test acc = 0.9200000166893005, time = 0.0021011829376220703\n","Testing at step=11, batch=20, test loss = 0.30831719568138866, test acc = 0.9100000262260437, time = 0.0019729137420654297\n","Testing at step=11, batch=40, test loss = 0.29172061034725444, test acc = 0.9100000262260437, time = 0.0019490718841552734\n","Testing at step=11, batch=60, test loss = 0.449665644020585, test acc = 0.8700000047683716, time = 0.0019626617431640625\n","Testing at step=11, batch=80, test loss = 0.25029991117720707, test acc = 0.9100000262260437, time = 0.0019750595092773438\n","Step 11 finished in 22.67141056060791, Train loss = 0.27589539988958117, Test loss = 0.275472610865732; Train Acc = 0.9225833349426588, Test Acc = 0.9208000028133392\n","Training at step=12, batch=0, train loss = 0.37128092866883095, train acc = 0.8899999856948853, time = 0.009127378463745117\n","Training at step=12, batch=120, train loss = 0.15896091457558692, train acc = 0.949999988079071, time = 0.00898885726928711\n","Training at step=12, batch=240, train loss = 0.21936009977898605, train acc = 0.949999988079071, time = 0.009015798568725586\n","Training at step=12, batch=360, train loss = 0.48217944822574255, train acc = 0.8799999952316284, time = 0.009044885635375977\n","Training at step=12, batch=480, train loss = 0.2674089152235284, train acc = 0.9100000262260437, time = 0.008948564529418945\n","Testing at step=12, batch=0, test loss = 0.1915612347061937, test acc = 0.9200000166893005, time = 0.0019807815551757812\n","Testing at step=12, batch=20, test loss = 0.29534722931276974, test acc = 0.9300000071525574, time = 0.0019459724426269531\n","Testing at step=12, batch=40, test loss = 0.1666958164499543, test acc = 0.9599999785423279, time = 0.0019309520721435547\n","Testing at step=12, batch=60, test loss = 0.16733294135420906, test acc = 0.949999988079071, time = 0.0019648075103759766\n","Testing at step=12, batch=80, test loss = 0.13545372017803278, test acc = 0.949999988079071, time = 0.001970052719116211\n","Step 12 finished in 22.58403253555298, Train loss = 0.2745830815228811, Test loss = 0.269955880246911; Train Acc = 0.9232166687647502, Test Acc = 0.9240000009536743\n","Training at step=13, batch=0, train loss = 0.38036958795087733, train acc = 0.8799999952316284, time = 0.009049415588378906\n","Training at step=13, batch=120, train loss = 0.33312473973056406, train acc = 0.8899999856948853, time = 0.009154558181762695\n","Training at step=13, batch=240, train loss = 0.2497561095971207, train acc = 0.9300000071525574, time = 0.008996963500976562\n","Training at step=13, batch=360, train loss = 0.17624009969882173, train acc = 0.9599999785423279, time = 0.008919000625610352\n","Training at step=13, batch=480, train loss = 0.12077643214608486, train acc = 0.9700000286102295, time = 0.008975505828857422\n","Testing at step=13, batch=0, test loss = 0.24981250553106565, test acc = 0.9599999785423279, time = 0.0020418167114257812\n","Testing at step=13, batch=20, test loss = 0.4616393916110255, test acc = 0.8999999761581421, time = 0.0020914077758789062\n","Testing at step=13, batch=40, test loss = 0.20975596078095582, test acc = 0.9200000166893005, time = 0.0019338130950927734\n","Testing at step=13, batch=60, test loss = 0.21409376058692595, test acc = 0.9200000166893005, time = 0.002000570297241211\n","Testing at step=13, batch=80, test loss = 0.20320869674556985, test acc = 0.9399999976158142, time = 0.0019237995147705078\n","Step 13 finished in 22.62598180770874, Train loss = 0.2732834116154994, Test loss = 0.27142460138086855; Train Acc = 0.9236166675885519, Test Acc = 0.9238000017404556\n","Training at step=14, batch=0, train loss = 0.330961068190117, train acc = 0.8999999761581421, time = 0.009109735488891602\n","Training at step=14, batch=120, train loss = 0.23154471729910864, train acc = 0.9300000071525574, time = 0.009011507034301758\n","Training at step=14, batch=240, train loss = 0.3715284682788652, train acc = 0.8999999761581421, time = 0.009029388427734375\n","Training at step=14, batch=360, train loss = 0.28633802382962814, train acc = 0.9200000166893005, time = 0.009100675582885742\n","Training at step=14, batch=480, train loss = 0.32697464308895136, train acc = 0.9100000262260437, time = 0.009076595306396484\n","Testing at step=14, batch=0, test loss = 0.29696368583924304, test acc = 0.9100000262260437, time = 0.0023813247680664062\n","Testing at step=14, batch=20, test loss = 0.4044957483590141, test acc = 0.9100000262260437, time = 0.0019061565399169922\n","Testing at step=14, batch=40, test loss = 0.23399705882328273, test acc = 0.9300000071525574, time = 0.0019407272338867188\n","Testing at step=14, batch=60, test loss = 0.30816936713158016, test acc = 0.9300000071525574, time = 0.0019783973693847656\n","Testing at step=14, batch=80, test loss = 0.20817849095809657, test acc = 0.9200000166893005, time = 0.001956939697265625\n","Step 14 finished in 22.759334564208984, Train loss = 0.27097589492463886, Test loss = 0.27217538360924; Train Acc = 0.9237500016887983, Test Acc = 0.9244000017642975\n","Training at step=15, batch=0, train loss = 0.28621584130404665, train acc = 0.9300000071525574, time = 0.00907135009765625\n","Training at step=15, batch=120, train loss = 0.3049724362776043, train acc = 0.9300000071525574, time = 0.009055614471435547\n","Training at step=15, batch=240, train loss = 0.37532177829230373, train acc = 0.8899999856948853, time = 0.009138345718383789\n","Training at step=15, batch=360, train loss = 0.2549219611640455, train acc = 0.8899999856948853, time = 0.008980512619018555\n","Training at step=15, batch=480, train loss = 0.3302192481325861, train acc = 0.9300000071525574, time = 0.009155750274658203\n","Testing at step=15, batch=0, test loss = 0.15039119489051203, test acc = 0.9700000286102295, time = 0.0019617080688476562\n","Testing at step=15, batch=20, test loss = 0.34960186753416095, test acc = 0.8600000143051147, time = 0.0019290447235107422\n","Testing at step=15, batch=40, test loss = 0.1606370463596286, test acc = 0.9399999976158142, time = 0.002017498016357422\n","Testing at step=15, batch=60, test loss = 0.18254912463910705, test acc = 0.9200000166893005, time = 0.0019249916076660156\n","Testing at step=15, batch=80, test loss = 0.27813327816617384, test acc = 0.9399999976158142, time = 0.0019974708557128906\n","Step 15 finished in 22.74914789199829, Train loss = 0.2695319614683529, Test loss = 0.27006892571930946; Train Acc = 0.925650001168251, Test Acc = 0.9241000032424926\n","Training at step=16, batch=0, train loss = 0.28701713897121856, train acc = 0.9100000262260437, time = 0.009200572967529297\n","Training at step=16, batch=120, train loss = 0.19997779915535155, train acc = 0.9399999976158142, time = 0.00900125503540039\n","Training at step=16, batch=240, train loss = 0.34721860756991324, train acc = 0.8899999856948853, time = 0.009059429168701172\n","Training at step=16, batch=360, train loss = 0.26878001621054765, train acc = 0.949999988079071, time = 0.008981466293334961\n","Training at step=16, batch=480, train loss = 0.21682253906020685, train acc = 0.9300000071525574, time = 0.009137868881225586\n","Testing at step=16, batch=0, test loss = 0.17242295974737115, test acc = 0.9300000071525574, time = 0.001995086669921875\n","Testing at step=16, batch=20, test loss = 0.12431139002208313, test acc = 0.9599999785423279, time = 0.0019447803497314453\n","Testing at step=16, batch=40, test loss = 0.21185840975512704, test acc = 0.9200000166893005, time = 0.0019223690032958984\n","Testing at step=16, batch=60, test loss = 0.2040648621879149, test acc = 0.949999988079071, time = 0.001913309097290039\n","Testing at step=16, batch=80, test loss = 0.2582021548052415, test acc = 0.9100000262260437, time = 0.0019762516021728516\n","Step 16 finished in 22.82147526741028, Train loss = 0.268901085172789, Test loss = 0.26870935558881826; Train Acc = 0.924950001736482, Test Acc = 0.9239000004529953\n","Training at step=17, batch=0, train loss = 0.26312170635660675, train acc = 0.9200000166893005, time = 0.009155988693237305\n","Training at step=17, batch=120, train loss = 0.15046858421589576, train acc = 0.949999988079071, time = 0.009195566177368164\n","Training at step=17, batch=240, train loss = 0.15440987051132393, train acc = 0.9300000071525574, time = 0.009021520614624023\n","Training at step=17, batch=360, train loss = 0.23626616489480223, train acc = 0.9300000071525574, time = 0.009032249450683594\n","Training at step=17, batch=480, train loss = 0.3174495117452441, train acc = 0.9300000071525574, time = 0.009033679962158203\n","Testing at step=17, batch=0, test loss = 0.22380887204698305, test acc = 0.9300000071525574, time = 0.001995563507080078\n","Testing at step=17, batch=20, test loss = 0.4992179287715704, test acc = 0.8799999952316284, time = 0.0019276142120361328\n","Testing at step=17, batch=40, test loss = 0.2276358359976092, test acc = 0.9100000262260437, time = 0.001949310302734375\n","Testing at step=17, batch=60, test loss = 0.3702953313467109, test acc = 0.9100000262260437, time = 0.0019533634185791016\n","Testing at step=17, batch=80, test loss = 0.37524362696187213, test acc = 0.9200000166893005, time = 0.0019719600677490234\n","Step 17 finished in 22.776052474975586, Train loss = 0.26711232680259206, Test loss = 0.2700720519378238; Train Acc = 0.9259833351771036, Test Acc = 0.9227000010013581\n","Training at step=18, batch=0, train loss = 0.295600758272214, train acc = 0.9100000262260437, time = 0.009112834930419922\n","Training at step=18, batch=120, train loss = 0.37647642581509944, train acc = 0.8799999952316284, time = 0.009114742279052734\n","Training at step=18, batch=240, train loss = 0.1911352413509045, train acc = 0.9599999785423279, time = 0.009044408798217773\n","Training at step=18, batch=360, train loss = 0.21937703308002146, train acc = 0.949999988079071, time = 0.009086370468139648\n","Training at step=18, batch=480, train loss = 0.1856282656495805, train acc = 0.9399999976158142, time = 0.009222745895385742\n","Testing at step=18, batch=0, test loss = 0.2398312328452394, test acc = 0.9399999976158142, time = 0.0020291805267333984\n","Testing at step=18, batch=20, test loss = 0.23891490547266625, test acc = 0.949999988079071, time = 0.0020711421966552734\n","Testing at step=18, batch=40, test loss = 0.14541060582852833, test acc = 0.949999988079071, time = 0.0020384788513183594\n","Testing at step=18, batch=60, test loss = 0.1851173683587396, test acc = 0.9200000166893005, time = 0.0020580291748046875\n","Testing at step=18, batch=80, test loss = 0.279568212662261, test acc = 0.8999999761581421, time = 0.0020246505737304688\n","Step 18 finished in 22.859533071517944, Train loss = 0.2663131263038273, Test loss = 0.26642963351844295; Train Acc = 0.9262000040213267, Test Acc = 0.9243000012636184\n","Training at step=19, batch=0, train loss = 0.3517556918037068, train acc = 0.8999999761581421, time = 0.009410619735717773\n","Training at step=19, batch=120, train loss = 0.1639283970884771, train acc = 0.949999988079071, time = 0.009094953536987305\n","Training at step=19, batch=240, train loss = 0.3054267195251697, train acc = 0.8999999761581421, time = 0.009129047393798828\n","Training at step=19, batch=360, train loss = 0.2834215747032948, train acc = 0.9399999976158142, time = 0.009053945541381836\n","Training at step=19, batch=480, train loss = 0.28134089550388913, train acc = 0.9200000166893005, time = 0.009150028228759766\n","Testing at step=19, batch=0, test loss = 0.28988587752156975, test acc = 0.9599999785423279, time = 0.0020356178283691406\n","Testing at step=19, batch=20, test loss = 0.30878870093354976, test acc = 0.9200000166893005, time = 0.0019719600677490234\n","Testing at step=19, batch=40, test loss = 0.24083458949292388, test acc = 0.8899999856948853, time = 0.0019116401672363281\n","Testing at step=19, batch=60, test loss = 0.2785099016238519, test acc = 0.9300000071525574, time = 0.0019407272338867188\n","Testing at step=19, batch=80, test loss = 0.25765664253030635, test acc = 0.9399999976158142, time = 0.001964569091796875\n","Step 19 finished in 22.827157974243164, Train loss = 0.26585298654844963, Test loss = 0.26931856960788997; Train Acc = 0.9260333344340325, Test Acc = 0.923700003027916\n","Training at step=20, batch=0, train loss = 0.21585115798075707, train acc = 0.9300000071525574, time = 0.00910806655883789\n","Training at step=20, batch=120, train loss = 0.3054695963353756, train acc = 0.9200000166893005, time = 0.009020090103149414\n","Training at step=20, batch=240, train loss = 0.23744205801270712, train acc = 0.9300000071525574, time = 0.008966684341430664\n","Training at step=20, batch=360, train loss = 0.29098622823975456, train acc = 0.9200000166893005, time = 0.009099721908569336\n","Training at step=20, batch=480, train loss = 0.3177650870555915, train acc = 0.8600000143051147, time = 0.009067058563232422\n","Testing at step=20, batch=0, test loss = 0.24335790967270238, test acc = 0.9300000071525574, time = 0.001978158950805664\n","Testing at step=20, batch=20, test loss = 0.3709986847160172, test acc = 0.8899999856948853, time = 0.001997232437133789\n","Testing at step=20, batch=40, test loss = 0.2527142473958104, test acc = 0.9100000262260437, time = 0.0019652843475341797\n","Testing at step=20, batch=60, test loss = 0.18664258413603962, test acc = 0.9300000071525574, time = 0.0019502639770507812\n","Testing at step=20, batch=80, test loss = 0.1581437997979393, test acc = 0.9700000286102295, time = 0.0019478797912597656\n","Step 20 finished in 22.75105905532837, Train loss = 0.26482815129520637, Test loss = 0.26617122417121686; Train Acc = 0.9261166694760322, Test Acc = 0.9245000010728837\n","Training at step=21, batch=0, train loss = 0.18964463327237616, train acc = 0.9700000286102295, time = 0.009102344512939453\n","Training at step=21, batch=120, train loss = 0.20753669576762082, train acc = 0.9100000262260437, time = 0.009012937545776367\n","Training at step=21, batch=240, train loss = 0.4267592477438984, train acc = 0.8899999856948853, time = 0.009077072143554688\n","Training at step=21, batch=360, train loss = 0.3219105590572609, train acc = 0.9300000071525574, time = 0.00901031494140625\n","Training at step=21, batch=480, train loss = 0.22784745814593307, train acc = 0.9300000071525574, time = 0.008951187133789062\n","Testing at step=21, batch=0, test loss = 0.15320036254433053, test acc = 0.9599999785423279, time = 0.0020055770874023438\n","Testing at step=21, batch=20, test loss = 0.24207671179891718, test acc = 0.9399999976158142, time = 0.001968860626220703\n","Testing at step=21, batch=40, test loss = 0.23983390325326914, test acc = 0.949999988079071, time = 0.0019562244415283203\n","Testing at step=21, batch=60, test loss = 0.2525898190734301, test acc = 0.9300000071525574, time = 0.0020134449005126953\n","Testing at step=21, batch=80, test loss = 0.13579875989132695, test acc = 0.9599999785423279, time = 0.0019800662994384766\n","Step 21 finished in 22.732582092285156, Train loss = 0.2639529999977285, Test loss = 0.26676624393714127; Train Acc = 0.9271500017245611, Test Acc = 0.9249000012874603\n","Training at step=22, batch=0, train loss = 0.16947188506525243, train acc = 0.9800000190734863, time = 0.009942770004272461\n","Training at step=22, batch=120, train loss = 0.21845978277046313, train acc = 0.9100000262260437, time = 0.009181022644042969\n","Training at step=22, batch=240, train loss = 0.18409567454297046, train acc = 0.949999988079071, time = 0.009145498275756836\n","Training at step=22, batch=360, train loss = 0.2040887481405257, train acc = 0.949999988079071, time = 0.008947372436523438\n","Training at step=22, batch=480, train loss = 0.2023578073525843, train acc = 0.949999988079071, time = 0.00902104377746582\n","Testing at step=22, batch=0, test loss = 0.09806356236900404, test acc = 0.9900000095367432, time = 0.0019707679748535156\n","Testing at step=22, batch=20, test loss = 0.09129663131371629, test acc = 0.9700000286102295, time = 0.0019342899322509766\n","Testing at step=22, batch=40, test loss = 0.2967407917807911, test acc = 0.9399999976158142, time = 0.002110719680786133\n","Testing at step=22, batch=60, test loss = 0.2238247641201286, test acc = 0.8999999761581421, time = 0.0020170211791992188\n","Testing at step=22, batch=80, test loss = 0.3362008193591599, test acc = 0.8999999761581421, time = 0.0019998550415039062\n","Step 22 finished in 22.80265188217163, Train loss = 0.26285757391452297, Test loss = 0.26761747283318643; Train Acc = 0.9271666692694028, Test Acc = 0.9232000023126602\n","Training at step=23, batch=0, train loss = 0.2889176474280129, train acc = 0.9100000262260437, time = 0.009165525436401367\n","Training at step=23, batch=120, train loss = 0.24843843340563412, train acc = 0.9300000071525574, time = 0.00940084457397461\n","Training at step=23, batch=240, train loss = 0.15461079357474836, train acc = 0.9399999976158142, time = 0.009068965911865234\n","Training at step=23, batch=360, train loss = 0.146087322313561, train acc = 0.949999988079071, time = 0.009045124053955078\n","Training at step=23, batch=480, train loss = 0.2076668327027892, train acc = 0.949999988079071, time = 0.00905609130859375\n","Testing at step=23, batch=0, test loss = 0.2818777507974851, test acc = 0.9399999976158142, time = 0.0019502639770507812\n","Testing at step=23, batch=20, test loss = 0.42234239920759237, test acc = 0.9200000166893005, time = 0.002039194107055664\n","Testing at step=23, batch=40, test loss = 0.14511241522410928, test acc = 0.9599999785423279, time = 0.0019636154174804688\n","Testing at step=23, batch=60, test loss = 0.32512835347766783, test acc = 0.9100000262260437, time = 0.0019299983978271484\n","Testing at step=23, batch=80, test loss = 0.27118167432570595, test acc = 0.9200000166893005, time = 0.0020759105682373047\n","Step 23 finished in 22.80229687690735, Train loss = 0.26241549910230266, Test loss = 0.2679392685675514; Train Acc = 0.9274833358327548, Test Acc = 0.9246000039577484\n","Training at step=24, batch=0, train loss = 0.19965239264880652, train acc = 0.9399999976158142, time = 0.009221315383911133\n","Training at step=24, batch=120, train loss = 0.23577510871685375, train acc = 0.9200000166893005, time = 0.009120702743530273\n","Training at step=24, batch=240, train loss = 0.14942509731490417, train acc = 0.9599999785423279, time = 0.009055137634277344\n","Training at step=24, batch=360, train loss = 0.284191493515493, train acc = 0.9399999976158142, time = 0.009230613708496094\n","Training at step=24, batch=480, train loss = 0.2082625212595262, train acc = 0.9100000262260437, time = 0.00895833969116211\n","Testing at step=24, batch=0, test loss = 0.24600778584913158, test acc = 0.9300000071525574, time = 0.001959562301635742\n","Testing at step=24, batch=20, test loss = 0.19286403408274197, test acc = 0.9399999976158142, time = 0.0019147396087646484\n","Testing at step=24, batch=40, test loss = 0.18976311694410955, test acc = 0.9599999785423279, time = 0.0019490718841552734\n","Testing at step=24, batch=60, test loss = 0.23866190150449598, test acc = 0.9200000166893005, time = 0.0019350051879882812\n","Testing at step=24, batch=80, test loss = 0.35376674473271774, test acc = 0.9200000166893005, time = 0.001977682113647461\n","Step 24 finished in 22.764657974243164, Train loss = 0.2620794126828676, Test loss = 0.2688304655274656; Train Acc = 0.9272833348313967, Test Acc = 0.9237000000476837\n","Training at step=25, batch=0, train loss = 0.197399895275555, train acc = 0.9599999785423279, time = 0.00910329818725586\n","Training at step=25, batch=120, train loss = 0.19683995202169652, train acc = 0.9200000166893005, time = 0.009026288986206055\n","Training at step=25, batch=240, train loss = 0.38800666678506945, train acc = 0.9300000071525574, time = 0.008962392807006836\n","Training at step=25, batch=360, train loss = 0.1927417996438381, train acc = 0.9399999976158142, time = 0.009036064147949219\n","Training at step=25, batch=480, train loss = 0.2120156292575765, train acc = 0.9700000286102295, time = 0.009031534194946289\n","Testing at step=25, batch=0, test loss = 0.2583184677452943, test acc = 0.8799999952316284, time = 0.0019800662994384766\n","Testing at step=25, batch=20, test loss = 0.19737523505908008, test acc = 0.9300000071525574, time = 0.0019636154174804688\n","Testing at step=25, batch=40, test loss = 0.3000087901064267, test acc = 0.9300000071525574, time = 0.001966238021850586\n","Testing at step=25, batch=60, test loss = 0.2673350134101888, test acc = 0.8999999761581421, time = 0.0019381046295166016\n","Testing at step=25, batch=80, test loss = 0.3306746979423629, test acc = 0.9100000262260437, time = 0.0019159317016601562\n","Step 25 finished in 22.65982222557068, Train loss = 0.26086332020631164, Test loss = 0.26677697755756646; Train Acc = 0.9279000014066696, Test Acc = 0.9233000010251999\n","Training at step=26, batch=0, train loss = 0.2815658188463759, train acc = 0.8999999761581421, time = 0.009078502655029297\n","Training at step=26, batch=120, train loss = 0.1661944762995779, train acc = 0.9399999976158142, time = 0.008965492248535156\n","Training at step=26, batch=240, train loss = 0.22938030957075642, train acc = 0.9200000166893005, time = 0.009034156799316406\n","Training at step=26, batch=360, train loss = 0.2187487744904601, train acc = 0.9300000071525574, time = 0.009043216705322266\n","Training at step=26, batch=480, train loss = 0.21939151238974908, train acc = 0.9200000166893005, time = 0.009069204330444336\n","Testing at step=26, batch=0, test loss = 0.4480981793717198, test acc = 0.8999999761581421, time = 0.0019888877868652344\n","Testing at step=26, batch=20, test loss = 0.1640468801337021, test acc = 0.9700000286102295, time = 0.0019805431365966797\n","Testing at step=26, batch=40, test loss = 0.2846073641328237, test acc = 0.9200000166893005, time = 0.0020160675048828125\n","Testing at step=26, batch=60, test loss = 0.23865626778199617, test acc = 0.9399999976158142, time = 0.0019373893737792969\n","Testing at step=26, batch=80, test loss = 0.12317619099209738, test acc = 0.949999988079071, time = 0.001967191696166992\n","Step 26 finished in 22.643939971923828, Train loss = 0.26086375092872405, Test loss = 0.26762444634924537; Train Acc = 0.9272833350300789, Test Acc = 0.9253000020980835\n","Training at step=27, batch=0, train loss = 0.1314324956104705, train acc = 0.9700000286102295, time = 0.009812355041503906\n","Training at step=27, batch=120, train loss = 0.20098668550810145, train acc = 0.9599999785423279, time = 0.009086370468139648\n","Training at step=27, batch=240, train loss = 0.19704054431312795, train acc = 0.9399999976158142, time = 0.009011268615722656\n","Training at step=27, batch=360, train loss = 0.2544229908095131, train acc = 0.9100000262260437, time = 0.008960247039794922\n","Training at step=27, batch=480, train loss = 0.36400478985023904, train acc = 0.9399999976158142, time = 0.009052038192749023\n","Testing at step=27, batch=0, test loss = 0.16396075260855425, test acc = 0.949999988079071, time = 0.002002239227294922\n","Testing at step=27, batch=20, test loss = 0.1909494113745004, test acc = 0.9599999785423279, time = 0.0019273757934570312\n","Testing at step=27, batch=40, test loss = 0.3032892074704236, test acc = 0.9100000262260437, time = 0.0019364356994628906\n","Testing at step=27, batch=60, test loss = 0.29488180874054226, test acc = 0.8899999856948853, time = 0.0019600391387939453\n","Testing at step=27, batch=80, test loss = 0.3461499039932535, test acc = 0.9599999785423279, time = 0.0020143985748291016\n","Step 27 finished in 22.723918676376343, Train loss = 0.26015907997065674, Test loss = 0.2654984728280189; Train Acc = 0.9280666692058245, Test Acc = 0.9245000004768371\n","Training at step=28, batch=0, train loss = 0.18089681780088657, train acc = 0.949999988079071, time = 0.009067296981811523\n","Training at step=28, batch=120, train loss = 0.28156567498184565, train acc = 0.9200000166893005, time = 0.009018659591674805\n","Training at step=28, batch=240, train loss = 0.37221548543132216, train acc = 0.8799999952316284, time = 0.00906062126159668\n","Training at step=28, batch=360, train loss = 0.18921977532071443, train acc = 0.9599999785423279, time = 0.009003877639770508\n","Training at step=28, batch=480, train loss = 0.3120166322749264, train acc = 0.8999999761581421, time = 0.009049654006958008\n","Testing at step=28, batch=0, test loss = 0.18138512209749183, test acc = 0.9399999976158142, time = 0.001992464065551758\n","Testing at step=28, batch=20, test loss = 0.13127492534216467, test acc = 0.9700000286102295, time = 0.001966238021850586\n","Testing at step=28, batch=40, test loss = 0.20374403546456896, test acc = 0.9399999976158142, time = 0.001987457275390625\n","Testing at step=28, batch=60, test loss = 0.1452890721987035, test acc = 0.9399999976158142, time = 0.0019652843475341797\n","Testing at step=28, batch=80, test loss = 0.29374883846131117, test acc = 0.9200000166893005, time = 0.002000570297241211\n","Step 28 finished in 22.784090042114258, Train loss = 0.25943266374537977, Test loss = 0.26494726582442746; Train Acc = 0.9276166683435441, Test Acc = 0.9252000027894973\n","Training at step=29, batch=0, train loss = 0.23224086291408225, train acc = 0.9300000071525574, time = 0.00909876823425293\n","Training at step=29, batch=120, train loss = 0.2459898831477824, train acc = 0.8999999761581421, time = 0.009153366088867188\n","Training at step=29, batch=240, train loss = 0.2603371342014514, train acc = 0.9399999976158142, time = 0.009171724319458008\n","Training at step=29, batch=360, train loss = 0.21765295412115548, train acc = 0.9300000071525574, time = 0.009091615676879883\n","Training at step=29, batch=480, train loss = 0.3047995015728007, train acc = 0.8799999952316284, time = 0.009015798568725586\n","Testing at step=29, batch=0, test loss = 0.2375019777496472, test acc = 0.9100000262260437, time = 0.0019872188568115234\n","Testing at step=29, batch=20, test loss = 0.20462206105554273, test acc = 0.949999988079071, time = 0.0019719600677490234\n","Testing at step=29, batch=40, test loss = 0.2515920450691522, test acc = 0.9399999976158142, time = 0.0019214153289794922\n","Testing at step=29, batch=60, test loss = 0.22187277682005876, test acc = 0.9300000071525574, time = 0.0019674301147460938\n","Testing at step=29, batch=80, test loss = 0.27360470180669316, test acc = 0.9200000166893005, time = 0.001935720443725586\n","Step 29 finished in 22.82061767578125, Train loss = 0.2585935668907364, Test loss = 0.27070597536523067; Train Acc = 0.9283500010768573, Test Acc = 0.9224000018835068\n","Training at step=30, batch=0, train loss = 0.2986535457929231, train acc = 0.8999999761581421, time = 0.009066343307495117\n","Training at step=30, batch=120, train loss = 0.25773410424950827, train acc = 0.8999999761581421, time = 0.009075164794921875\n","Training at step=30, batch=240, train loss = 0.25616934064515323, train acc = 0.9399999976158142, time = 0.008989334106445312\n","Training at step=30, batch=360, train loss = 0.27166745684807136, train acc = 0.9300000071525574, time = 0.009010076522827148\n","Training at step=30, batch=480, train loss = 0.2835568649831215, train acc = 0.9100000262260437, time = 0.009012699127197266\n","Testing at step=30, batch=0, test loss = 0.38063285338022623, test acc = 0.8899999856948853, time = 0.001992940902709961\n","Testing at step=30, batch=20, test loss = 0.2523106801475882, test acc = 0.9200000166893005, time = 0.002012491226196289\n","Testing at step=30, batch=40, test loss = 0.22979993317959913, test acc = 0.9200000166893005, time = 0.0019707679748535156\n","Testing at step=30, batch=60, test loss = 0.26414289466135804, test acc = 0.9399999976158142, time = 0.0019567012786865234\n","Testing at step=30, batch=80, test loss = 0.20866221254416564, test acc = 0.949999988079071, time = 0.001958608627319336\n","Step 30 finished in 22.721858739852905, Train loss = 0.25859451324322774, Test loss = 0.2669738926259516; Train Acc = 0.9285166694720586, Test Acc = 0.9238000023365021\n","Training at step=31, batch=0, train loss = 0.14957114974894908, train acc = 0.949999988079071, time = 0.009139299392700195\n","Training at step=31, batch=120, train loss = 0.2378407530671231, train acc = 0.9300000071525574, time = 0.00914907455444336\n","Training at step=31, batch=240, train loss = 0.31378296668964595, train acc = 0.9200000166893005, time = 0.009046792984008789\n","Training at step=31, batch=360, train loss = 0.2032833221795223, train acc = 0.9399999976158142, time = 0.008962392807006836\n","Training at step=31, batch=480, train loss = 0.2412095173793894, train acc = 0.9200000166893005, time = 0.009077787399291992\n","Testing at step=31, batch=0, test loss = 0.3412947883594081, test acc = 0.9200000166893005, time = 0.001990795135498047\n","Testing at step=31, batch=20, test loss = 0.2751074837845907, test acc = 0.9100000262260437, time = 0.001916646957397461\n","Testing at step=31, batch=40, test loss = 0.350286637438467, test acc = 0.9200000166893005, time = 0.0019550323486328125\n","Testing at step=31, batch=60, test loss = 0.2204730022912789, test acc = 0.949999988079071, time = 0.0019116401672363281\n","Testing at step=31, batch=80, test loss = 0.20988907416049518, test acc = 0.9399999976158142, time = 0.00191497802734375\n","Step 31 finished in 22.714037656784058, Train loss = 0.2575565375318612, Test loss = 0.26647351723395507; Train Acc = 0.9287500015894572, Test Acc = 0.9249000000953674\n","Training at step=32, batch=0, train loss = 0.19086160547446857, train acc = 0.9800000190734863, time = 0.009233236312866211\n","Training at step=32, batch=120, train loss = 0.2435197959387562, train acc = 0.8999999761581421, time = 0.009038925170898438\n","Training at step=32, batch=240, train loss = 0.26544937202873925, train acc = 0.9399999976158142, time = 0.009071111679077148\n","Training at step=32, batch=360, train loss = 0.2510083598923024, train acc = 0.9399999976158142, time = 0.008930444717407227\n","Training at step=32, batch=480, train loss = 0.23282386771573235, train acc = 0.9399999976158142, time = 0.009048700332641602\n","Testing at step=32, batch=0, test loss = 0.2975150980667729, test acc = 0.949999988079071, time = 0.0019338130950927734\n","Testing at step=32, batch=20, test loss = 0.3173629662090922, test acc = 0.9100000262260437, time = 0.001955747604370117\n","Testing at step=32, batch=40, test loss = 0.3138641742774946, test acc = 0.9100000262260437, time = 0.0019409656524658203\n","Testing at step=32, batch=60, test loss = 0.26599329261153914, test acc = 0.9100000262260437, time = 0.0020055770874023438\n","Testing at step=32, batch=80, test loss = 0.15989508914409212, test acc = 0.949999988079071, time = 0.0019354820251464844\n","Step 32 finished in 22.801348209381104, Train loss = 0.2576090404816504, Test loss = 0.26695659126814186; Train Acc = 0.9286166696747145, Test Acc = 0.9242000031471252\n","Training at step=33, batch=0, train loss = 0.17127829834634326, train acc = 0.9399999976158142, time = 0.009008407592773438\n","Training at step=33, batch=120, train loss = 0.20569206860702793, train acc = 0.9399999976158142, time = 0.009013891220092773\n","Training at step=33, batch=240, train loss = 0.1327476325766842, train acc = 0.9599999785423279, time = 0.009079933166503906\n","Training at step=33, batch=360, train loss = 0.31319553491116126, train acc = 0.9300000071525574, time = 0.008944272994995117\n","Training at step=33, batch=480, train loss = 0.19589217465336642, train acc = 0.9300000071525574, time = 0.009072542190551758\n","Testing at step=33, batch=0, test loss = 0.43538298168641254, test acc = 0.8799999952316284, time = 0.0019140243530273438\n","Testing at step=33, batch=20, test loss = 0.36222157155453794, test acc = 0.9200000166893005, time = 0.0019245147705078125\n","Testing at step=33, batch=40, test loss = 0.1575084857591188, test acc = 0.949999988079071, time = 0.001941680908203125\n","Testing at step=33, batch=60, test loss = 0.45604064410309997, test acc = 0.9200000166893005, time = 0.001997232437133789\n","Testing at step=33, batch=80, test loss = 0.28026027750313764, test acc = 0.9399999976158142, time = 0.0019872188568115234\n","Step 33 finished in 22.698649406433105, Train loss = 0.2567336418622036, Test loss = 0.2644061306412168; Train Acc = 0.9289166677991549, Test Acc = 0.9255999994277954\n","Training at step=34, batch=0, train loss = 0.29056376510875953, train acc = 0.9100000262260437, time = 0.009174823760986328\n","Training at step=34, batch=120, train loss = 0.34545268320835526, train acc = 0.9100000262260437, time = 0.009057283401489258\n","Training at step=34, batch=240, train loss = 0.053587324110360324, train acc = 1.0, time = 0.008963346481323242\n","Training at step=34, batch=360, train loss = 0.23437856776698113, train acc = 0.9300000071525574, time = 0.009177207946777344\n","Training at step=34, batch=480, train loss = 0.3436477451093045, train acc = 0.8700000047683716, time = 0.009046077728271484\n","Testing at step=34, batch=0, test loss = 0.1940309130846462, test acc = 0.9200000166893005, time = 0.001949310302734375\n","Testing at step=34, batch=20, test loss = 0.14364027653998854, test acc = 0.9300000071525574, time = 0.0019805431365966797\n","Testing at step=34, batch=40, test loss = 0.4306604223550082, test acc = 0.9200000166893005, time = 0.0019235610961914062\n","Testing at step=34, batch=60, test loss = 0.2640502315925868, test acc = 0.9100000262260437, time = 0.0020422935485839844\n","Testing at step=34, batch=80, test loss = 0.12939725466803711, test acc = 0.9700000286102295, time = 0.0020935535430908203\n","Step 34 finished in 22.748112201690674, Train loss = 0.2566224083451453, Test loss = 0.2684023360742621; Train Acc = 0.9290833355983098, Test Acc = 0.9253999996185303\n","Training at step=35, batch=0, train loss = 0.36394263681571615, train acc = 0.9399999976158142, time = 0.009122371673583984\n","Training at step=35, batch=120, train loss = 0.25710059178113354, train acc = 0.9200000166893005, time = 0.008913516998291016\n","Training at step=35, batch=240, train loss = 0.17918370245292084, train acc = 0.9599999785423279, time = 0.009071588516235352\n","Training at step=35, batch=360, train loss = 0.48069635172008385, train acc = 0.8999999761581421, time = 0.009096860885620117\n","Training at step=35, batch=480, train loss = 0.21896667059342395, train acc = 0.949999988079071, time = 0.00904703140258789\n","Testing at step=35, batch=0, test loss = 0.21307777320272447, test acc = 0.9300000071525574, time = 0.0019867420196533203\n","Testing at step=35, batch=20, test loss = 0.13081049160273683, test acc = 0.9599999785423279, time = 0.002078533172607422\n","Testing at step=35, batch=40, test loss = 0.2949252787759742, test acc = 0.9200000166893005, time = 0.0019495487213134766\n","Testing at step=35, batch=60, test loss = 0.2660442492458723, test acc = 0.9399999976158142, time = 0.0019290447235107422\n","Testing at step=35, batch=80, test loss = 0.31157461028076733, test acc = 0.9300000071525574, time = 0.0019626617431640625\n","Step 35 finished in 22.716022491455078, Train loss = 0.2558132658432632, Test loss = 0.26495627845975006; Train Acc = 0.9296666691700618, Test Acc = 0.9259000021219254\n","Training at step=36, batch=0, train loss = 0.2080498054207317, train acc = 0.9300000071525574, time = 0.00906991958618164\n","Training at step=36, batch=120, train loss = 0.31681014689582837, train acc = 0.8899999856948853, time = 0.008968114852905273\n","Training at step=36, batch=240, train loss = 0.2061516357236361, train acc = 0.9300000071525574, time = 0.008938312530517578\n","Training at step=36, batch=360, train loss = 0.4638122465211578, train acc = 0.8999999761581421, time = 0.009052038192749023\n","Training at step=36, batch=480, train loss = 0.24097434391518952, train acc = 0.8999999761581421, time = 0.00905919075012207\n","Testing at step=36, batch=0, test loss = 0.1805739356167899, test acc = 0.9200000166893005, time = 0.001966238021850586\n","Testing at step=36, batch=20, test loss = 0.37652066970007303, test acc = 0.9200000166893005, time = 0.0019731521606445312\n","Testing at step=36, batch=40, test loss = 0.16471175700908916, test acc = 0.949999988079071, time = 0.0019516944885253906\n","Testing at step=36, batch=60, test loss = 0.1493751034439762, test acc = 0.949999988079071, time = 0.0019183158874511719\n","Testing at step=36, batch=80, test loss = 0.47516847776999016, test acc = 0.9300000071525574, time = 0.0019567012786865234\n","Step 36 finished in 22.82675313949585, Train loss = 0.25553085035458784, Test loss = 0.2672432173181219; Train Acc = 0.9298666684826216, Test Acc = 0.9253000015020371\n","Training at step=37, batch=0, train loss = 0.24207339066945147, train acc = 0.9200000166893005, time = 0.00918126106262207\n","Training at step=37, batch=120, train loss = 0.31199534080847846, train acc = 0.9300000071525574, time = 0.009231090545654297\n","Training at step=37, batch=240, train loss = 0.27028062749504944, train acc = 0.8999999761581421, time = 0.009106874465942383\n","Training at step=37, batch=360, train loss = 0.5508762136695898, train acc = 0.8500000238418579, time = 0.00919342041015625\n","Training at step=37, batch=480, train loss = 0.2643958863393441, train acc = 0.949999988079071, time = 0.00901651382446289\n","Testing at step=37, batch=0, test loss = 0.3160773439867599, test acc = 0.8999999761581421, time = 0.00196075439453125\n","Testing at step=37, batch=20, test loss = 0.16947620939999436, test acc = 0.949999988079071, time = 0.0020761489868164062\n","Testing at step=37, batch=40, test loss = 0.2424568407135282, test acc = 0.9200000166893005, time = 0.0019135475158691406\n","Testing at step=37, batch=60, test loss = 0.2543657431582175, test acc = 0.9599999785423279, time = 0.0019714832305908203\n","Testing at step=37, batch=80, test loss = 0.24161455762429437, test acc = 0.949999988079071, time = 0.0019211769104003906\n","Step 37 finished in 22.781437635421753, Train loss = 0.2552990797577261, Test loss = 0.26518603591260415; Train Acc = 0.9293666687607766, Test Acc = 0.9264000004529953\n","Training at step=38, batch=0, train loss = 0.20741304005391928, train acc = 0.9399999976158142, time = 0.009122133255004883\n","Training at step=38, batch=120, train loss = 0.16242187761773288, train acc = 0.949999988079071, time = 0.009074687957763672\n","Training at step=38, batch=240, train loss = 0.44574360752425335, train acc = 0.8799999952316284, time = 0.009091377258300781\n","Training at step=38, batch=360, train loss = 0.14440498383083902, train acc = 0.9599999785423279, time = 0.009290695190429688\n","Training at step=38, batch=480, train loss = 0.2046645171114379, train acc = 0.9200000166893005, time = 0.009048223495483398\n","Testing at step=38, batch=0, test loss = 0.3884414792334154, test acc = 0.9300000071525574, time = 0.001967191696166992\n","Testing at step=38, batch=20, test loss = 0.25586928043047436, test acc = 0.9399999976158142, time = 0.0019578933715820312\n","Testing at step=38, batch=40, test loss = 0.15056694105226978, test acc = 0.9399999976158142, time = 0.0019707679748535156\n","Testing at step=38, batch=60, test loss = 0.20800281479476923, test acc = 0.949999988079071, time = 0.0019526481628417969\n","Testing at step=38, batch=80, test loss = 0.1260396227711163, test acc = 0.9700000286102295, time = 0.002000570297241211\n","Step 38 finished in 22.9560866355896, Train loss = 0.25466209716640503, Test loss = 0.2680523077859647; Train Acc = 0.9294500013192495, Test Acc = 0.9255000054836273\n","Training at step=39, batch=0, train loss = 0.29223711531696267, train acc = 0.9100000262260437, time = 0.009116888046264648\n","Training at step=39, batch=120, train loss = 0.30280937658724605, train acc = 0.8700000047683716, time = 0.008877754211425781\n","Training at step=39, batch=240, train loss = 0.2599691869507792, train acc = 0.9300000071525574, time = 0.00912165641784668\n","Training at step=39, batch=360, train loss = 0.29561209771209157, train acc = 0.9100000262260437, time = 0.0090484619140625\n","Training at step=39, batch=480, train loss = 0.5311876067494585, train acc = 0.8999999761581421, time = 0.008997440338134766\n","Testing at step=39, batch=0, test loss = 0.38040087455553473, test acc = 0.8799999952316284, time = 0.0019466876983642578\n","Testing at step=39, batch=20, test loss = 0.2918881965696124, test acc = 0.8999999761581421, time = 0.001960277557373047\n","Testing at step=39, batch=40, test loss = 0.3046804944539571, test acc = 0.9100000262260437, time = 0.0019440650939941406\n","Testing at step=39, batch=60, test loss = 0.26713585166598725, test acc = 0.9300000071525574, time = 0.0019464492797851562\n","Testing at step=39, batch=80, test loss = 0.1827936449755338, test acc = 0.9599999785423279, time = 0.002010822296142578\n","Step 39 finished in 22.84306263923645, Train loss = 0.254331965566098, Test loss = 0.2651971514041863; Train Acc = 0.9295000018676122, Test Acc = 0.9252000021934509\n","Training at step=40, batch=0, train loss = 0.3545612605594377, train acc = 0.9399999976158142, time = 0.009186029434204102\n","Training at step=40, batch=120, train loss = 0.31688993258822906, train acc = 0.9300000071525574, time = 0.009013652801513672\n","Training at step=40, batch=240, train loss = 0.31304417879126234, train acc = 0.8899999856948853, time = 0.009095191955566406\n","Training at step=40, batch=360, train loss = 0.20522246843068234, train acc = 0.9599999785423279, time = 0.009018898010253906\n","Training at step=40, batch=480, train loss = 0.46564341549171706, train acc = 0.8899999856948853, time = 0.009127378463745117\n","Testing at step=40, batch=0, test loss = 0.18561460904785998, test acc = 0.9700000286102295, time = 0.001955270767211914\n","Testing at step=40, batch=20, test loss = 0.48128220183079246, test acc = 0.8600000143051147, time = 0.0019769668579101562\n","Testing at step=40, batch=40, test loss = 0.3841325318958681, test acc = 0.8899999856948853, time = 0.0019638538360595703\n","Testing at step=40, batch=60, test loss = 0.2093670476102064, test acc = 0.949999988079071, time = 0.0019867420196533203\n","Testing at step=40, batch=80, test loss = 0.32874871951606566, test acc = 0.8999999761581421, time = 0.0019769668579101562\n","Step 40 finished in 22.83118224143982, Train loss = 0.25398052231132584, Test loss = 0.2642031386535082; Train Acc = 0.9299666676918665, Test Acc = 0.925100001692772\n","Training at step=41, batch=0, train loss = 0.29227986876696166, train acc = 0.9300000071525574, time = 0.009189367294311523\n","Training at step=41, batch=120, train loss = 0.1112033383531709, train acc = 0.9700000286102295, time = 0.009028911590576172\n","Training at step=41, batch=240, train loss = 0.13674826065596382, train acc = 0.949999988079071, time = 0.009039163589477539\n","Training at step=41, batch=360, train loss = 0.26169924310335657, train acc = 0.9300000071525574, time = 0.00898289680480957\n","Training at step=41, batch=480, train loss = 0.28094173143473694, train acc = 0.9200000166893005, time = 0.008972406387329102\n","Testing at step=41, batch=0, test loss = 0.2347287178137362, test acc = 0.9100000262260437, time = 0.0019180774688720703\n","Testing at step=41, batch=20, test loss = 0.33136178747843437, test acc = 0.9100000262260437, time = 0.001922607421875\n","Testing at step=41, batch=40, test loss = 0.18216207178304042, test acc = 0.9300000071525574, time = 0.0019724369049072266\n","Testing at step=41, batch=60, test loss = 0.24825099760264724, test acc = 0.8799999952316284, time = 0.0020270347595214844\n","Testing at step=41, batch=80, test loss = 0.2477263500442228, test acc = 0.949999988079071, time = 0.0019903182983398438\n","Step 41 finished in 22.779433488845825, Train loss = 0.25390219883335396, Test loss = 0.2622941826749452; Train Acc = 0.9298000005880992, Test Acc = 0.9261000025272369\n","Training at step=42, batch=0, train loss = 0.2357803730669603, train acc = 0.9300000071525574, time = 0.00915074348449707\n","Training at step=42, batch=120, train loss = 0.4488813613470889, train acc = 0.8999999761581421, time = 0.009025335311889648\n","Training at step=42, batch=240, train loss = 0.31882878505401235, train acc = 0.9300000071525574, time = 0.009092330932617188\n","Training at step=42, batch=360, train loss = 0.5730228804517279, train acc = 0.8999999761581421, time = 0.009124517440795898\n","Training at step=42, batch=480, train loss = 0.2295421458158971, train acc = 0.9300000071525574, time = 0.009125709533691406\n","Testing at step=42, batch=0, test loss = 0.24255808272268325, test acc = 0.9399999976158142, time = 0.0020096302032470703\n","Testing at step=42, batch=20, test loss = 0.4560621397835466, test acc = 0.8700000047683716, time = 0.001928567886352539\n","Testing at step=42, batch=40, test loss = 0.316057423394395, test acc = 0.9100000262260437, time = 0.0019080638885498047\n","Testing at step=42, batch=60, test loss = 0.3354970224927453, test acc = 0.8899999856948853, time = 0.0019452571868896484\n","Testing at step=42, batch=80, test loss = 0.3700151257988394, test acc = 0.8799999952316284, time = 0.0019981861114501953\n","Step 42 finished in 22.834752798080444, Train loss = 0.2532640744529608, Test loss = 0.2637189147303362; Train Acc = 0.930166667898496, Test Acc = 0.9261000031232833\n","Training at step=43, batch=0, train loss = 0.22150275703825176, train acc = 0.9700000286102295, time = 0.009110450744628906\n","Training at step=43, batch=120, train loss = 0.2337530085455741, train acc = 0.9399999976158142, time = 0.009159564971923828\n","Training at step=43, batch=240, train loss = 0.30487661666114757, train acc = 0.9200000166893005, time = 0.009176254272460938\n","Training at step=43, batch=360, train loss = 0.2996845813013224, train acc = 0.9300000071525574, time = 0.008995532989501953\n","Training at step=43, batch=480, train loss = 0.33795760728815283, train acc = 0.8999999761581421, time = 0.008972644805908203\n","Testing at step=43, batch=0, test loss = 0.32115324944302925, test acc = 0.8799999952316284, time = 0.0019459724426269531\n","Testing at step=43, batch=20, test loss = 0.2305758977193484, test acc = 0.9399999976158142, time = 0.0020036697387695312\n","Testing at step=43, batch=40, test loss = 0.22779775689286144, test acc = 0.9100000262260437, time = 0.0020220279693603516\n","Testing at step=43, batch=60, test loss = 0.5251513619400116, test acc = 0.9399999976158142, time = 0.002198934555053711\n","Testing at step=43, batch=80, test loss = 0.21570030565647516, test acc = 0.949999988079071, time = 0.002050638198852539\n","Step 43 finished in 22.869818925857544, Train loss = 0.2527323210937841, Test loss = 0.26908779567679125; Train Acc = 0.9303500019510587, Test Acc = 0.9258000022172928\n","Training at step=44, batch=0, train loss = 0.2722978184862073, train acc = 0.949999988079071, time = 0.009158134460449219\n","Training at step=44, batch=120, train loss = 0.31790653828664744, train acc = 0.9399999976158142, time = 0.008947372436523438\n","Training at step=44, batch=240, train loss = 0.29269074453405486, train acc = 0.8999999761581421, time = 0.009039163589477539\n","Training at step=44, batch=360, train loss = 0.2796881165492399, train acc = 0.9300000071525574, time = 0.009069204330444336\n","Training at step=44, batch=480, train loss = 0.39350067769810354, train acc = 0.9300000071525574, time = 0.009066581726074219\n","Testing at step=44, batch=0, test loss = 0.16997879700034393, test acc = 0.9100000262260437, time = 0.002019166946411133\n","Testing at step=44, batch=20, test loss = 0.23268320508846968, test acc = 0.9200000166893005, time = 0.002146482467651367\n","Testing at step=44, batch=40, test loss = 0.2851513421113874, test acc = 0.9200000166893005, time = 0.001972198486328125\n","Testing at step=44, batch=60, test loss = 0.26289488170110603, test acc = 0.9399999976158142, time = 0.0020236968994140625\n","Testing at step=44, batch=80, test loss = 0.182331124328248, test acc = 0.9399999976158142, time = 0.0019741058349609375\n","Step 44 finished in 22.815999031066895, Train loss = 0.25279938752193687, Test loss = 0.2676220620144905; Train Acc = 0.9299666688839594, Test Acc = 0.9241000026464462\n","Training at step=45, batch=0, train loss = 0.19062321994238707, train acc = 0.9599999785423279, time = 0.009110689163208008\n","Training at step=45, batch=120, train loss = 0.23055008336427224, train acc = 0.9100000262260437, time = 0.00895380973815918\n","Training at step=45, batch=240, train loss = 0.2945514981584394, train acc = 0.9399999976158142, time = 0.009156942367553711\n","Training at step=45, batch=360, train loss = 0.21972152726550148, train acc = 0.949999988079071, time = 0.009275197982788086\n","Training at step=45, batch=480, train loss = 0.2974661656746639, train acc = 0.9100000262260437, time = 0.009155511856079102\n","Testing at step=45, batch=0, test loss = 0.25605715030772613, test acc = 0.9300000071525574, time = 0.0020148754119873047\n","Testing at step=45, batch=20, test loss = 0.1596424319256508, test acc = 0.9700000286102295, time = 0.002003192901611328\n","Testing at step=45, batch=40, test loss = 0.3131652909759457, test acc = 0.8899999856948853, time = 0.0019800662994384766\n","Testing at step=45, batch=60, test loss = 0.2431021678846842, test acc = 0.9200000166893005, time = 0.0019845962524414062\n","Testing at step=45, batch=80, test loss = 0.32266010190119426, test acc = 0.8899999856948853, time = 0.0019178390502929688\n","Step 45 finished in 22.78242325782776, Train loss = 0.2521709514690444, Test loss = 0.2677445189961178; Train Acc = 0.9305833365519841, Test Acc = 0.9232000005245209\n","Training at step=46, batch=0, train loss = 0.37734122327925007, train acc = 0.9300000071525574, time = 0.009017229080200195\n","Training at step=46, batch=120, train loss = 0.16631447570756336, train acc = 0.9800000190734863, time = 0.008969306945800781\n","Training at step=46, batch=240, train loss = 0.23638161130284555, train acc = 0.949999988079071, time = 0.009022951126098633\n","Training at step=46, batch=360, train loss = 0.25114488097857957, train acc = 0.9300000071525574, time = 0.008977174758911133\n","Training at step=46, batch=480, train loss = 0.22618298959959074, train acc = 0.9700000286102295, time = 0.009106874465942383\n","Testing at step=46, batch=0, test loss = 0.16731537599776886, test acc = 0.9599999785423279, time = 0.002012014389038086\n","Testing at step=46, batch=20, test loss = 0.29584615350530535, test acc = 0.9300000071525574, time = 0.0019559860229492188\n","Testing at step=46, batch=40, test loss = 0.2193780758278092, test acc = 0.9200000166893005, time = 0.00200653076171875\n","Testing at step=46, batch=60, test loss = 0.34735262466852745, test acc = 0.8799999952316284, time = 0.001916646957397461\n","Testing at step=46, batch=80, test loss = 0.3011089605400485, test acc = 0.8899999856948853, time = 0.001954793930053711\n","Step 46 finished in 22.771570920944214, Train loss = 0.2518863069598945, Test loss = 0.26697595038008953; Train Acc = 0.9307666688164076, Test Acc = 0.9251999998092652\n","Training at step=47, batch=0, train loss = 0.1469071237650747, train acc = 0.9599999785423279, time = 0.009054899215698242\n","Training at step=47, batch=120, train loss = 0.23744584318161657, train acc = 0.8999999761581421, time = 0.009062051773071289\n","Training at step=47, batch=240, train loss = 0.2869123310931269, train acc = 0.9200000166893005, time = 0.009165525436401367\n","Training at step=47, batch=360, train loss = 0.3055932127900432, train acc = 0.8999999761581421, time = 0.009042024612426758\n","Training at step=47, batch=480, train loss = 0.16871635794355622, train acc = 0.9399999976158142, time = 0.009113311767578125\n","Testing at step=47, batch=0, test loss = 0.35752342994714176, test acc = 0.9200000166893005, time = 0.002017974853515625\n","Testing at step=47, batch=20, test loss = 0.20926143885547016, test acc = 0.9399999976158142, time = 0.001967191696166992\n","Testing at step=47, batch=40, test loss = 0.1714379997251057, test acc = 0.9399999976158142, time = 0.0020074844360351562\n","Testing at step=47, batch=60, test loss = 0.13715410364731329, test acc = 0.949999988079071, time = 0.0020036697387695312\n","Testing at step=47, batch=80, test loss = 0.0911640255433219, test acc = 0.9700000286102295, time = 0.0020558834075927734\n","Step 47 finished in 22.833399295806885, Train loss = 0.2515530367722015, Test loss = 0.2671886043431673; Train Acc = 0.9307500017682712, Test Acc = 0.9267000031471252\n","Training at step=48, batch=0, train loss = 0.21993300989082787, train acc = 0.9399999976158142, time = 0.009155988693237305\n","Training at step=48, batch=120, train loss = 0.2459008946813189, train acc = 0.9399999976158142, time = 0.008986949920654297\n","Training at step=48, batch=240, train loss = 0.17768508532940192, train acc = 0.949999988079071, time = 0.008970499038696289\n","Training at step=48, batch=360, train loss = 0.19933697451717408, train acc = 0.949999988079071, time = 0.008975028991699219\n","Training at step=48, batch=480, train loss = 0.31077101393328177, train acc = 0.9200000166893005, time = 0.009052515029907227\n","Testing at step=48, batch=0, test loss = 0.21451926303817415, test acc = 0.9700000286102295, time = 0.0020422935485839844\n","Testing at step=48, batch=20, test loss = 0.32612140016808744, test acc = 0.9300000071525574, time = 0.0021157264709472656\n","Testing at step=48, batch=40, test loss = 0.27778665468907143, test acc = 0.8899999856948853, time = 0.001953601837158203\n","Testing at step=48, batch=60, test loss = 0.21574623488259673, test acc = 0.9399999976158142, time = 0.001977205276489258\n","Testing at step=48, batch=80, test loss = 0.2006026154931356, test acc = 0.9100000262260437, time = 0.0019533634185791016\n","Step 48 finished in 22.767440795898438, Train loss = 0.25129099939094357, Test loss = 0.26850379945929975; Train Acc = 0.9301000012954076, Test Acc = 0.9234999978542328\n","Training at step=49, batch=0, train loss = 0.17649320425967732, train acc = 0.9399999976158142, time = 0.009033203125\n","Training at step=49, batch=120, train loss = 0.2552827137330568, train acc = 0.9100000262260437, time = 0.008989334106445312\n","Training at step=49, batch=240, train loss = 0.2544525212180102, train acc = 0.9300000071525574, time = 0.009058475494384766\n","Training at step=49, batch=360, train loss = 0.28158304596534334, train acc = 0.9200000166893005, time = 0.008957862854003906\n","Training at step=49, batch=480, train loss = 0.37242849206765316, train acc = 0.8700000047683716, time = 0.009011268615722656\n","Testing at step=49, batch=0, test loss = 0.21826816855525646, test acc = 0.9300000071525574, time = 0.0020380020141601562\n","Testing at step=49, batch=20, test loss = 0.115079349168966, test acc = 0.9599999785423279, time = 0.002100706100463867\n","Testing at step=49, batch=40, test loss = 0.15933061741835228, test acc = 0.9599999785423279, time = 0.0019652843475341797\n","Testing at step=49, batch=60, test loss = 0.23494673791736484, test acc = 0.9300000071525574, time = 0.0019638538360595703\n","Testing at step=49, batch=80, test loss = 0.4137566666571256, test acc = 0.8600000143051147, time = 0.001959562301635742\n","Step 49 finished in 22.79232907295227, Train loss = 0.2510714680100715, Test loss = 0.2666673384818318; Train Acc = 0.9310166673858961, Test Acc = 0.9259000021219254\n","Training at step=50, batch=0, train loss = 0.14993192532807387, train acc = 0.949999988079071, time = 0.009148359298706055\n","Training at step=50, batch=120, train loss = 0.21941906133071576, train acc = 0.9300000071525574, time = 0.009097099304199219\n","Training at step=50, batch=240, train loss = 0.23320183007760525, train acc = 0.949999988079071, time = 0.009110689163208008\n","Training at step=50, batch=360, train loss = 0.1464078128876632, train acc = 0.9599999785423279, time = 0.009172916412353516\n","Training at step=50, batch=480, train loss = 0.35938690678189356, train acc = 0.8600000143051147, time = 0.008990049362182617\n","Testing at step=50, batch=0, test loss = 0.20731441073809997, test acc = 0.9399999976158142, time = 0.0020215511322021484\n","Testing at step=50, batch=20, test loss = 0.3106657970506464, test acc = 0.8999999761581421, time = 0.0019364356994628906\n","Testing at step=50, batch=40, test loss = 0.24919662962516065, test acc = 0.9100000262260437, time = 0.0019447803497314453\n","Testing at step=50, batch=60, test loss = 0.13764519459331206, test acc = 0.9700000286102295, time = 0.0019254684448242188\n","Testing at step=50, batch=80, test loss = 0.33436876657856673, test acc = 0.8899999856948853, time = 0.0019545555114746094\n","Step 50 finished in 22.767587184906006, Train loss = 0.2504141116173554, Test loss = 0.26611094670918667; Train Acc = 0.9308666701118151, Test Acc = 0.9266000014543533\n","Training at step=51, batch=0, train loss = 0.16706723997646614, train acc = 0.9599999785423279, time = 0.009121894836425781\n","Training at step=51, batch=120, train loss = 0.3627657182966446, train acc = 0.9200000166893005, time = 0.009055376052856445\n","Training at step=51, batch=240, train loss = 0.3895262257182537, train acc = 0.8700000047683716, time = 0.00916743278503418\n","Training at step=51, batch=360, train loss = 0.3454588602460532, train acc = 0.9399999976158142, time = 0.009094953536987305\n","Training at step=51, batch=480, train loss = 0.1845364086598891, train acc = 0.9599999785423279, time = 0.009058237075805664\n","Testing at step=51, batch=0, test loss = 0.2873881144291115, test acc = 0.9200000166893005, time = 0.002062082290649414\n","Testing at step=51, batch=20, test loss = 0.1676318118200373, test acc = 0.9599999785423279, time = 0.0019540786743164062\n","Testing at step=51, batch=40, test loss = 0.16297968915745845, test acc = 0.9200000166893005, time = 0.0019271373748779297\n","Testing at step=51, batch=60, test loss = 0.20109212406816038, test acc = 0.9700000286102295, time = 0.001903533935546875\n","Testing at step=51, batch=80, test loss = 0.37020292846613345, test acc = 0.8799999952316284, time = 0.0019850730895996094\n","Step 51 finished in 22.78169536590576, Train loss = 0.2502169530789506, Test loss = 0.26535356612137295; Train Acc = 0.9306500009695688, Test Acc = 0.9262000012397766\n","Training at step=52, batch=0, train loss = 0.1398486742551576, train acc = 0.9599999785423279, time = 0.009161233901977539\n","Training at step=52, batch=120, train loss = 0.1493062134440132, train acc = 0.9399999976158142, time = 0.00902867317199707\n","Training at step=52, batch=240, train loss = 0.22489073856563618, train acc = 0.9300000071525574, time = 0.00963449478149414\n","Training at step=52, batch=360, train loss = 0.26721451640101657, train acc = 0.9300000071525574, time = 0.009098052978515625\n","Training at step=52, batch=480, train loss = 0.30120739446751377, train acc = 0.8999999761581421, time = 0.00908350944519043\n","Testing at step=52, batch=0, test loss = 0.25994897530474204, test acc = 0.9200000166893005, time = 0.0020279884338378906\n","Testing at step=52, batch=20, test loss = 0.2010814316432063, test acc = 0.949999988079071, time = 0.0019884109497070312\n","Testing at step=52, batch=40, test loss = 0.27592348555537155, test acc = 0.9399999976158142, time = 0.001972675323486328\n","Testing at step=52, batch=60, test loss = 0.3386945944956539, test acc = 0.8799999952316284, time = 0.0019788742065429688\n","Testing at step=52, batch=80, test loss = 0.1877331333126694, test acc = 0.949999988079071, time = 0.0019407272338867188\n","Step 52 finished in 22.776101112365723, Train loss = 0.24984758330173415, Test loss = 0.2689798886100563; Train Acc = 0.9308500018715858, Test Acc = 0.9236000043153763\n","Training at step=53, batch=0, train loss = 0.19219436383175434, train acc = 0.9100000262260437, time = 0.009042739868164062\n","Training at step=53, batch=120, train loss = 0.2217974100161806, train acc = 0.9399999976158142, time = 0.009013175964355469\n","Training at step=53, batch=240, train loss = 0.2511669608939243, train acc = 0.9100000262260437, time = 0.009192705154418945\n","Training at step=53, batch=360, train loss = 0.200865651448043, train acc = 0.9399999976158142, time = 0.009024620056152344\n","Training at step=53, batch=480, train loss = 0.27498663259517925, train acc = 0.8899999856948853, time = 0.008994340896606445\n","Testing at step=53, batch=0, test loss = 0.12836803830245713, test acc = 0.9599999785423279, time = 0.002012968063354492\n","Testing at step=53, batch=20, test loss = 0.17799307841139622, test acc = 0.9599999785423279, time = 0.0019636154174804688\n","Testing at step=53, batch=40, test loss = 0.17892774842788217, test acc = 0.9399999976158142, time = 0.0019130706787109375\n","Testing at step=53, batch=60, test loss = 0.14281005201554708, test acc = 0.9399999976158142, time = 0.002123117446899414\n","Testing at step=53, batch=80, test loss = 0.3181796612367107, test acc = 0.9100000262260437, time = 0.001996278762817383\n","Step 53 finished in 22.76914405822754, Train loss = 0.2498513259243373, Test loss = 0.26557706421757843; Train Acc = 0.9314166679978371, Test Acc = 0.9248000025749207\n","Training at step=54, batch=0, train loss = 0.20285544899248617, train acc = 0.949999988079071, time = 0.00919032096862793\n","Training at step=54, batch=120, train loss = 0.2539436423121542, train acc = 0.9300000071525574, time = 0.00907588005065918\n","Training at step=54, batch=240, train loss = 0.3478513926188648, train acc = 0.9100000262260437, time = 0.009166717529296875\n","Training at step=54, batch=360, train loss = 0.3380876734239846, train acc = 0.8799999952316284, time = 0.008972644805908203\n","Training at step=54, batch=480, train loss = 0.22103080723442747, train acc = 0.9399999976158142, time = 0.009108781814575195\n","Testing at step=54, batch=0, test loss = 0.20024457687897115, test acc = 0.9200000166893005, time = 0.0019452571868896484\n","Testing at step=54, batch=20, test loss = 0.23268881742801262, test acc = 0.9300000071525574, time = 0.0019676685333251953\n","Testing at step=54, batch=40, test loss = 0.27607815488882614, test acc = 0.9100000262260437, time = 0.0019898414611816406\n","Testing at step=54, batch=60, test loss = 0.25701284085796255, test acc = 0.9399999976158142, time = 0.0019524097442626953\n","Testing at step=54, batch=80, test loss = 0.38902658791900097, test acc = 0.8899999856948853, time = 0.0020291805267333984\n","Step 54 finished in 22.738082885742188, Train loss = 0.24934672528035187, Test loss = 0.26850389730761215; Train Acc = 0.9313833355903626, Test Acc = 0.9232000017166138\n","Training at step=55, batch=0, train loss = 0.1803795146359935, train acc = 0.949999988079071, time = 0.009037971496582031\n","Training at step=55, batch=120, train loss = 0.2942551211365575, train acc = 0.9200000166893005, time = 0.009015798568725586\n","Training at step=55, batch=240, train loss = 0.3761191671604416, train acc = 0.9200000166893005, time = 0.009171485900878906\n","Training at step=55, batch=360, train loss = 0.30116063955875894, train acc = 0.9100000262260437, time = 0.009130239486694336\n","Training at step=55, batch=480, train loss = 0.175628939103011, train acc = 0.949999988079071, time = 0.008994340896606445\n","Testing at step=55, batch=0, test loss = 0.29636665672120704, test acc = 0.9200000166893005, time = 0.0019683837890625\n","Testing at step=55, batch=20, test loss = 0.17797655411297608, test acc = 0.9399999976158142, time = 0.001909017562866211\n","Testing at step=55, batch=40, test loss = 0.3172969593969736, test acc = 0.9100000262260437, time = 0.002047300338745117\n","Testing at step=55, batch=60, test loss = 0.23470008986058719, test acc = 0.9200000166893005, time = 0.002007007598876953\n","Testing at step=55, batch=80, test loss = 0.1821849850491661, test acc = 0.9399999976158142, time = 0.002043485641479492\n","Step 55 finished in 22.804251670837402, Train loss = 0.24890342077024827, Test loss = 0.2666879898442945; Train Acc = 0.9307333354155223, Test Acc = 0.9247000044584275\n","Training at step=56, batch=0, train loss = 0.23378290614754577, train acc = 0.9200000166893005, time = 0.009060382843017578\n","Training at step=56, batch=120, train loss = 0.21395082680876853, train acc = 0.949999988079071, time = 0.009175300598144531\n","Training at step=56, batch=240, train loss = 0.3096361981062491, train acc = 0.9200000166893005, time = 0.00904703140258789\n","Training at step=56, batch=360, train loss = 0.2466200384079231, train acc = 0.9300000071525574, time = 0.009049177169799805\n","Training at step=56, batch=480, train loss = 0.1867480484467766, train acc = 0.9599999785423279, time = 0.009088754653930664\n","Testing at step=56, batch=0, test loss = 0.20972915289460914, test acc = 0.9599999785423279, time = 0.002056598663330078\n","Testing at step=56, batch=20, test loss = 0.29713990551692965, test acc = 0.8999999761581421, time = 0.0020563602447509766\n","Testing at step=56, batch=40, test loss = 0.12780966827893994, test acc = 0.949999988079071, time = 0.00188446044921875\n","Testing at step=56, batch=60, test loss = 0.34065196246933993, test acc = 0.9200000166893005, time = 0.0019659996032714844\n","Testing at step=56, batch=80, test loss = 0.3693855891828553, test acc = 0.8299999833106995, time = 0.001920938491821289\n","Step 56 finished in 22.961867332458496, Train loss = 0.248513146696488, Test loss = 0.266051767406908; Train Acc = 0.931783335407575, Test Acc = 0.9248999989032746\n","Training at step=57, batch=0, train loss = 0.16446633582008754, train acc = 0.9300000071525574, time = 0.00905466079711914\n","Training at step=57, batch=120, train loss = 0.4015472039308821, train acc = 0.8999999761581421, time = 0.009087562561035156\n","Training at step=57, batch=240, train loss = 0.24410501078896285, train acc = 0.9599999785423279, time = 0.009008407592773438\n","Training at step=57, batch=360, train loss = 0.230107688659014, train acc = 0.9300000071525574, time = 0.00899958610534668\n","Training at step=57, batch=480, train loss = 0.1886737799301779, train acc = 0.949999988079071, time = 0.009021759033203125\n","Testing at step=57, batch=0, test loss = 0.1760826338831667, test acc = 0.9100000262260437, time = 0.001996755599975586\n","Testing at step=57, batch=20, test loss = 0.21849575473713456, test acc = 0.9399999976158142, time = 0.0019731521606445312\n","Testing at step=57, batch=40, test loss = 0.18517320272334717, test acc = 0.9300000071525574, time = 0.0019390583038330078\n","Testing at step=57, batch=60, test loss = 0.4733537770489973, test acc = 0.8500000238418579, time = 0.0019161701202392578\n","Testing at step=57, batch=80, test loss = 0.2897836765018406, test acc = 0.9100000262260437, time = 0.001928567886352539\n","Step 57 finished in 22.854256868362427, Train loss = 0.24847662636368303, Test loss = 0.26563675905567413; Train Acc = 0.9308666683236758, Test Acc = 0.9259000051021576\n","Training at step=58, batch=0, train loss = 0.13856076768691708, train acc = 0.9700000286102295, time = 0.009078264236450195\n","Training at step=58, batch=120, train loss = 0.22355487663505508, train acc = 0.9300000071525574, time = 0.009026765823364258\n","Training at step=58, batch=240, train loss = 0.22578989444609285, train acc = 0.8899999856948853, time = 0.009117364883422852\n","Training at step=58, batch=360, train loss = 0.1355966620893484, train acc = 0.9599999785423279, time = 0.009003877639770508\n","Training at step=58, batch=480, train loss = 0.195426847005438, train acc = 0.9700000286102295, time = 0.008902311325073242\n","Testing at step=58, batch=0, test loss = 0.18733684073480752, test acc = 0.9300000071525574, time = 0.0020017623901367188\n","Testing at step=58, batch=20, test loss = 0.25010637623524856, test acc = 0.9200000166893005, time = 0.0020608901977539062\n","Testing at step=58, batch=40, test loss = 0.32639188179965856, test acc = 0.8899999856948853, time = 0.001967906951904297\n","Testing at step=58, batch=60, test loss = 0.17101709140216825, test acc = 0.9300000071525574, time = 0.0019140243530273438\n","Testing at step=58, batch=80, test loss = 0.4778355434587355, test acc = 0.8700000047683716, time = 0.0019452571868896484\n","Step 58 finished in 22.8583984375, Train loss = 0.24824322738874197, Test loss = 0.268628546191982; Train Acc = 0.9318333355585734, Test Acc = 0.9252000015974045\n","Training at step=59, batch=0, train loss = 0.2524371106168443, train acc = 0.9599999785423279, time = 0.009101629257202148\n","Training at step=59, batch=120, train loss = 0.27514357575836856, train acc = 0.8899999856948853, time = 0.009043455123901367\n","Training at step=59, batch=240, train loss = 0.3010235517871646, train acc = 0.9300000071525574, time = 0.009055614471435547\n","Training at step=59, batch=360, train loss = 0.2608764550808232, train acc = 0.9300000071525574, time = 0.009142398834228516\n","Training at step=59, batch=480, train loss = 0.25586910313447836, train acc = 0.9300000071525574, time = 0.009072303771972656\n","Testing at step=59, batch=0, test loss = 0.29704518423329657, test acc = 0.9399999976158142, time = 0.0019876956939697266\n","Testing at step=59, batch=20, test loss = 0.4208045022459087, test acc = 0.8899999856948853, time = 0.001983642578125\n","Testing at step=59, batch=40, test loss = 0.27762578648665587, test acc = 0.9300000071525574, time = 0.0019435882568359375\n","Testing at step=59, batch=60, test loss = 0.2187340660798002, test acc = 0.9300000071525574, time = 0.0020546913146972656\n","Testing at step=59, batch=80, test loss = 0.17610574883058455, test acc = 0.9599999785423279, time = 0.0019478797912597656\n","Step 59 finished in 22.82817029953003, Train loss = 0.24794089691601082, Test loss = 0.269423894674921; Train Acc = 0.9317000012596448, Test Acc = 0.9247000026702881\n","Training at step=60, batch=0, train loss = 0.24294074843981633, train acc = 0.9300000071525574, time = 0.009443283081054688\n","Training at step=60, batch=120, train loss = 0.1189795512762212, train acc = 0.949999988079071, time = 0.00890970230102539\n","Training at step=60, batch=240, train loss = 0.4087568877912687, train acc = 0.9100000262260437, time = 0.009123563766479492\n","Training at step=60, batch=360, train loss = 0.3387156480997824, train acc = 0.9300000071525574, time = 0.008963346481323242\n","Training at step=60, batch=480, train loss = 0.31015423523896724, train acc = 0.9200000166893005, time = 0.008944034576416016\n","Testing at step=60, batch=0, test loss = 0.3242116995053743, test acc = 0.8799999952316284, time = 0.0019614696502685547\n","Testing at step=60, batch=20, test loss = 0.29698688823720387, test acc = 0.9300000071525574, time = 0.0019707679748535156\n","Testing at step=60, batch=40, test loss = 0.2596897455346578, test acc = 0.9200000166893005, time = 0.0019333362579345703\n","Testing at step=60, batch=60, test loss = 0.2272450694916564, test acc = 0.9100000262260437, time = 0.0019519329071044922\n","Testing at step=60, batch=80, test loss = 0.2128549290339026, test acc = 0.949999988079071, time = 0.0019376277923583984\n","Step 60 finished in 22.811607599258423, Train loss = 0.24749490143303443, Test loss = 0.264942297197082; Train Acc = 0.9321166679263115, Test Acc = 0.9271000003814698\n","Training at step=61, batch=0, train loss = 0.308279180252809, train acc = 0.8999999761581421, time = 0.009110689163208008\n","Training at step=61, batch=120, train loss = 0.12818899405010087, train acc = 0.9599999785423279, time = 0.009307146072387695\n","Training at step=61, batch=240, train loss = 0.3562803815602677, train acc = 0.9300000071525574, time = 0.008970499038696289\n","Training at step=61, batch=360, train loss = 0.33080149375214885, train acc = 0.9200000166893005, time = 0.009004354476928711\n","Training at step=61, batch=480, train loss = 0.1511611252787036, train acc = 0.949999988079071, time = 0.009360551834106445\n","Testing at step=61, batch=0, test loss = 0.19922623527567368, test acc = 0.949999988079071, time = 0.0019237995147705078\n","Testing at step=61, batch=20, test loss = 0.4965669841155217, test acc = 0.8999999761581421, time = 0.001941680908203125\n","Testing at step=61, batch=40, test loss = 0.17354912320721685, test acc = 0.9599999785423279, time = 0.0019397735595703125\n","Testing at step=61, batch=60, test loss = 0.20431810762407188, test acc = 0.9399999976158142, time = 0.0019652843475341797\n","Testing at step=61, batch=80, test loss = 0.29350421737717225, test acc = 0.9200000166893005, time = 0.0019333362579345703\n","Step 61 finished in 22.82531213760376, Train loss = 0.24745972588216147, Test loss = 0.26753574724148305; Train Acc = 0.9317333349585533, Test Acc = 0.9256000047922135\n","Training at step=62, batch=0, train loss = 0.29792898278289953, train acc = 0.8799999952316284, time = 0.009055376052856445\n","Training at step=62, batch=120, train loss = 0.20694753747382394, train acc = 0.9200000166893005, time = 0.009056806564331055\n","Training at step=62, batch=240, train loss = 0.1937794459535396, train acc = 0.9399999976158142, time = 0.00903010368347168\n","Training at step=62, batch=360, train loss = 0.2894049600921324, train acc = 0.9100000262260437, time = 0.009076833724975586\n","Training at step=62, batch=480, train loss = 0.2355062543376945, train acc = 0.9200000166893005, time = 0.009461402893066406\n","Testing at step=62, batch=0, test loss = 0.23270903062683895, test acc = 0.9399999976158142, time = 0.0020003318786621094\n","Testing at step=62, batch=20, test loss = 0.38769223970850325, test acc = 0.9200000166893005, time = 0.0019414424896240234\n","Testing at step=62, batch=40, test loss = 0.23289385316873645, test acc = 0.8999999761581421, time = 0.0020155906677246094\n","Testing at step=62, batch=60, test loss = 0.20627833267839749, test acc = 0.9100000262260437, time = 0.0019409656524658203\n","Testing at step=62, batch=80, test loss = 0.2470423300589665, test acc = 0.9200000166893005, time = 0.0019872188568115234\n","Step 62 finished in 22.828420162200928, Train loss = 0.2467991408223452, Test loss = 0.26473178357889293; Train Acc = 0.9325666693846385, Test Acc = 0.926000000834465\n","Training at step=63, batch=0, train loss = 0.2510815785617545, train acc = 0.9200000166893005, time = 0.009015083312988281\n","Training at step=63, batch=120, train loss = 0.2522566520832056, train acc = 0.9100000262260437, time = 0.009072065353393555\n","Training at step=63, batch=240, train loss = 0.14973677477755953, train acc = 0.9599999785423279, time = 0.009057283401489258\n","Training at step=63, batch=360, train loss = 0.22951413657387376, train acc = 0.9399999976158142, time = 0.009030818939208984\n","Training at step=63, batch=480, train loss = 0.21364283573256976, train acc = 0.9100000262260437, time = 0.009132862091064453\n","Testing at step=63, batch=0, test loss = 0.26377453426753555, test acc = 0.8999999761581421, time = 0.0019600391387939453\n","Testing at step=63, batch=20, test loss = 0.3026490646663363, test acc = 0.8600000143051147, time = 0.0019423961639404297\n","Testing at step=63, batch=40, test loss = 0.2967161465651206, test acc = 0.9200000166893005, time = 0.0019593238830566406\n","Testing at step=63, batch=60, test loss = 0.3141316773029603, test acc = 0.8999999761581421, time = 0.0019271373748779297\n","Testing at step=63, batch=80, test loss = 0.15809940349142906, test acc = 0.9599999785423279, time = 0.0019626617431640625\n","Step 63 finished in 22.752387523651123, Train loss = 0.2467117224506446, Test loss = 0.26586269030127907; Train Acc = 0.9318333354592323, Test Acc = 0.9241999995708465\n","Training at step=64, batch=0, train loss = 0.2778164308053554, train acc = 0.949999988079071, time = 0.009171485900878906\n","Training at step=64, batch=120, train loss = 0.20569178925194403, train acc = 0.9200000166893005, time = 0.008951663970947266\n","Training at step=64, batch=240, train loss = 0.15707234964686617, train acc = 0.949999988079071, time = 0.00905919075012207\n","Training at step=64, batch=360, train loss = 0.24528759147002188, train acc = 0.9300000071525574, time = 0.008970260620117188\n","Training at step=64, batch=480, train loss = 0.15929300605303612, train acc = 0.9599999785423279, time = 0.009091615676879883\n","Testing at step=64, batch=0, test loss = 0.42651308491576406, test acc = 0.8799999952316284, time = 0.002053499221801758\n","Testing at step=64, batch=20, test loss = 0.36856608867373025, test acc = 0.9100000262260437, time = 0.0019834041595458984\n","Testing at step=64, batch=40, test loss = 0.19701877495653947, test acc = 0.9200000166893005, time = 0.002009153366088867\n","Testing at step=64, batch=60, test loss = 0.25533196735680286, test acc = 0.8999999761581421, time = 0.001949310302734375\n","Testing at step=64, batch=80, test loss = 0.1567484664374291, test acc = 0.949999988079071, time = 0.0019767284393310547\n","Step 64 finished in 22.920021295547485, Train loss = 0.2468013802470854, Test loss = 0.2655506696689978; Train Acc = 0.9322333353757858, Test Acc = 0.9264000016450882\n","Training at step=65, batch=0, train loss = 0.16982868647854896, train acc = 0.9599999785423279, time = 0.009092569351196289\n","Training at step=65, batch=120, train loss = 0.3309787832946332, train acc = 0.9300000071525574, time = 0.009146451950073242\n","Training at step=65, batch=240, train loss = 0.18924692587414704, train acc = 0.949999988079071, time = 0.00897979736328125\n","Training at step=65, batch=360, train loss = 0.24606334909046407, train acc = 0.9599999785423279, time = 0.00911855697631836\n","Training at step=65, batch=480, train loss = 0.2974880078065683, train acc = 0.9300000071525574, time = 0.009150266647338867\n","Testing at step=65, batch=0, test loss = 0.17290497602317922, test acc = 0.9100000262260437, time = 0.0019850730895996094\n","Testing at step=65, batch=20, test loss = 0.27657197404865536, test acc = 0.9300000071525574, time = 0.001990795135498047\n","Testing at step=65, batch=40, test loss = 0.2671904538923096, test acc = 0.9300000071525574, time = 0.001967906951904297\n","Testing at step=65, batch=60, test loss = 0.17996577736217373, test acc = 0.9399999976158142, time = 0.001931905746459961\n","Testing at step=65, batch=80, test loss = 0.4064461531578709, test acc = 0.8700000047683716, time = 0.0019829273223876953\n","Step 65 finished in 23.004690408706665, Train loss = 0.24625144151734144, Test loss = 0.2658257968655366; Train Acc = 0.9326000020901362, Test Acc = 0.9262000024318695\n","Training at step=66, batch=0, train loss = 0.18632301822367356, train acc = 0.9399999976158142, time = 0.009111166000366211\n","Training at step=66, batch=120, train loss = 0.23414437807661667, train acc = 0.949999988079071, time = 0.009345769882202148\n","Training at step=66, batch=240, train loss = 0.22000547791486458, train acc = 0.9399999976158142, time = 0.008955955505371094\n","Training at step=66, batch=360, train loss = 0.2041783651706064, train acc = 0.9300000071525574, time = 0.009085416793823242\n","Training at step=66, batch=480, train loss = 0.21945737276378657, train acc = 0.949999988079071, time = 0.009623050689697266\n","Testing at step=66, batch=0, test loss = 0.1455420896330153, test acc = 0.949999988079071, time = 0.002008676528930664\n","Testing at step=66, batch=20, test loss = 0.4225135832226609, test acc = 0.9100000262260437, time = 0.001995563507080078\n","Testing at step=66, batch=40, test loss = 0.2182101484249861, test acc = 0.9300000071525574, time = 0.0018894672393798828\n","Testing at step=66, batch=60, test loss = 0.16290170459331663, test acc = 0.949999988079071, time = 0.0019915103912353516\n","Testing at step=66, batch=80, test loss = 0.2981407962368587, test acc = 0.8899999856948853, time = 0.001972198486328125\n","Step 66 finished in 22.932650089263916, Train loss = 0.24619923741755695, Test loss = 0.26831353547557746; Train Acc = 0.9319000028570493, Test Acc = 0.9241000038385391\n","Training at step=67, batch=0, train loss = 0.21658593153066932, train acc = 0.9300000071525574, time = 0.009169816970825195\n","Training at step=67, batch=120, train loss = 0.23421525560700351, train acc = 0.9300000071525574, time = 0.009113311767578125\n","Training at step=67, batch=240, train loss = 0.19704253545633127, train acc = 0.949999988079071, time = 0.00904703140258789\n","Training at step=67, batch=360, train loss = 0.2524444147932437, train acc = 0.9200000166893005, time = 0.008972406387329102\n","Training at step=67, batch=480, train loss = 0.2731624738377847, train acc = 0.9300000071525574, time = 0.008974790573120117\n","Testing at step=67, batch=0, test loss = 0.2996614586720538, test acc = 0.9200000166893005, time = 0.001992940902709961\n","Testing at step=67, batch=20, test loss = 0.26682037084944304, test acc = 0.9200000166893005, time = 0.0019845962524414062\n","Testing at step=67, batch=40, test loss = 0.13704335982797178, test acc = 0.949999988079071, time = 0.0019350051879882812\n","Testing at step=67, batch=60, test loss = 0.4735109943298211, test acc = 0.8999999761581421, time = 0.0019447803497314453\n","Testing at step=67, batch=80, test loss = 0.24202285986060157, test acc = 0.9200000166893005, time = 0.001922607421875\n","Step 67 finished in 22.876532793045044, Train loss = 0.24558866219179548, Test loss = 0.2665476757336162; Train Acc = 0.9328000014026959, Test Acc = 0.9257000011205673\n","Training at step=68, batch=0, train loss = 0.2534717238918814, train acc = 0.9100000262260437, time = 0.009127378463745117\n","Training at step=68, batch=120, train loss = 0.3605234965770863, train acc = 0.9200000166893005, time = 0.009262800216674805\n","Training at step=68, batch=240, train loss = 0.180307285871465, train acc = 0.9300000071525574, time = 0.008982181549072266\n","Training at step=68, batch=360, train loss = 0.07577766855785299, train acc = 0.9800000190734863, time = 0.008994817733764648\n","Training at step=68, batch=480, train loss = 0.17569183155999454, train acc = 0.9300000071525574, time = 0.009029150009155273\n","Testing at step=68, batch=0, test loss = 0.22556040457099702, test acc = 0.9300000071525574, time = 0.0019617080688476562\n","Testing at step=68, batch=20, test loss = 0.37423159710804627, test acc = 0.8899999856948853, time = 0.0019490718841552734\n","Testing at step=68, batch=40, test loss = 0.21690540725919744, test acc = 0.9100000262260437, time = 0.001963376998901367\n","Testing at step=68, batch=60, test loss = 0.41755344923816856, test acc = 0.8899999856948853, time = 0.0019664764404296875\n","Testing at step=68, batch=80, test loss = 0.16397280419822416, test acc = 0.9399999976158142, time = 0.001928091049194336\n","Step 68 finished in 22.95561695098877, Train loss = 0.24532188735014696, Test loss = 0.268386979342001; Train Acc = 0.9324666677912077, Test Acc = 0.9258000004291534\n","Training at step=69, batch=0, train loss = 0.19476240182220103, train acc = 0.9599999785423279, time = 0.00902700424194336\n","Training at step=69, batch=120, train loss = 0.11372458677476793, train acc = 0.9800000190734863, time = 0.009031057357788086\n","Training at step=69, batch=240, train loss = 0.2217838449934946, train acc = 0.9300000071525574, time = 0.008886337280273438\n","Training at step=69, batch=360, train loss = 0.41258726866877204, train acc = 0.9200000166893005, time = 0.008974790573120117\n","Training at step=69, batch=480, train loss = 0.3593619371942164, train acc = 0.9100000262260437, time = 0.00886845588684082\n","Testing at step=69, batch=0, test loss = 0.12481778091886778, test acc = 0.9599999785423279, time = 0.0018990039825439453\n","Testing at step=69, batch=20, test loss = 0.29822883913863996, test acc = 0.8999999761581421, time = 0.001928567886352539\n","Testing at step=69, batch=40, test loss = 0.2411800563366665, test acc = 0.8999999761581421, time = 0.0020117759704589844\n","Testing at step=69, batch=60, test loss = 0.3417678876091499, test acc = 0.9100000262260437, time = 0.0019216537475585938\n","Testing at step=69, batch=80, test loss = 0.23404507358781515, test acc = 0.8999999761581421, time = 0.001898050308227539\n","Step 69 finished in 22.839219331741333, Train loss = 0.24528675549934764, Test loss = 0.26786975017967907; Train Acc = 0.9325166675448417, Test Acc = 0.9253000009059906\n","Training at step=70, batch=0, train loss = 0.20572864842041585, train acc = 0.9399999976158142, time = 0.009050369262695312\n","Training at step=70, batch=120, train loss = 0.10796011776670023, train acc = 0.9599999785423279, time = 0.009055137634277344\n","Training at step=70, batch=240, train loss = 0.2665348971504321, train acc = 0.9200000166893005, time = 0.009071588516235352\n","Training at step=70, batch=360, train loss = 0.276199668850902, train acc = 0.9399999976158142, time = 0.009016036987304688\n","Training at step=70, batch=480, train loss = 0.24855392513790034, train acc = 0.9599999785423279, time = 0.009062051773071289\n","Testing at step=70, batch=0, test loss = 0.42037472273094, test acc = 0.9200000166893005, time = 0.001958131790161133\n","Testing at step=70, batch=20, test loss = 0.1827877481365163, test acc = 0.9599999785423279, time = 0.0019550323486328125\n","Testing at step=70, batch=40, test loss = 0.18991912785199325, test acc = 0.949999988079071, time = 0.0019071102142333984\n","Testing at step=70, batch=60, test loss = 0.23760755490265556, test acc = 0.9100000262260437, time = 0.0028722286224365234\n","Testing at step=70, batch=80, test loss = 0.27238245289765717, test acc = 0.8999999761581421, time = 0.0019745826721191406\n","Step 70 finished in 22.757779598236084, Train loss = 0.24487161633529608, Test loss = 0.26727747900285764; Train Acc = 0.9329666675130526, Test Acc = 0.9237000000476837\n","Training at step=71, batch=0, train loss = 0.20672035132417352, train acc = 0.949999988079071, time = 0.009136438369750977\n","Training at step=71, batch=120, train loss = 0.25576200012478617, train acc = 0.949999988079071, time = 0.009038448333740234\n","Training at step=71, batch=240, train loss = 0.16738342449922883, train acc = 0.9399999976158142, time = 0.009046316146850586\n","Training at step=71, batch=360, train loss = 0.3973466280223861, train acc = 0.9399999976158142, time = 0.009044885635375977\n","Training at step=71, batch=480, train loss = 0.373009283556292, train acc = 0.8999999761581421, time = 0.009013175964355469\n","Testing at step=71, batch=0, test loss = 0.3229472383068559, test acc = 0.9100000262260437, time = 0.0019774436950683594\n","Testing at step=71, batch=20, test loss = 0.3214401337528748, test acc = 0.8999999761581421, time = 0.0019001960754394531\n","Testing at step=71, batch=40, test loss = 0.2745262529168387, test acc = 0.9300000071525574, time = 0.001972198486328125\n","Testing at step=71, batch=60, test loss = 0.0985781510386453, test acc = 0.9700000286102295, time = 0.0019500255584716797\n","Testing at step=71, batch=80, test loss = 0.2879660918759495, test acc = 0.9300000071525574, time = 0.001928091049194336\n","Step 71 finished in 22.871103525161743, Train loss = 0.24488353529439053, Test loss = 0.2667069070078114; Train Acc = 0.9322833358248075, Test Acc = 0.926500004529953\n","Training at step=72, batch=0, train loss = 0.27595804846956584, train acc = 0.9200000166893005, time = 0.00914144515991211\n","Training at step=72, batch=120, train loss = 0.33837183495285894, train acc = 0.9100000262260437, time = 0.008907556533813477\n","Training at step=72, batch=240, train loss = 0.2723451616188019, train acc = 0.9200000166893005, time = 0.008961915969848633\n","Training at step=72, batch=360, train loss = 0.223526450688545, train acc = 0.9300000071525574, time = 0.009021520614624023\n","Training at step=72, batch=480, train loss = 0.12806332707806628, train acc = 0.9599999785423279, time = 0.008915185928344727\n","Testing at step=72, batch=0, test loss = 0.21743260079438623, test acc = 0.949999988079071, time = 0.001998424530029297\n","Testing at step=72, batch=20, test loss = 0.19006125987231873, test acc = 0.9399999976158142, time = 0.0019042491912841797\n","Testing at step=72, batch=40, test loss = 0.22033664127292055, test acc = 0.9300000071525574, time = 0.0019927024841308594\n","Testing at step=72, batch=60, test loss = 0.12912751253221283, test acc = 0.949999988079071, time = 0.0019123554229736328\n","Testing at step=72, batch=80, test loss = 0.36385984327053333, test acc = 0.9200000166893005, time = 0.0019409656524658203\n","Step 72 finished in 22.82447600364685, Train loss = 0.24429310729750037, Test loss = 0.2668932198568619; Train Acc = 0.932533334394296, Test Acc = 0.9255999976396561\n","Training at step=73, batch=0, train loss = 0.18595203018762899, train acc = 0.9399999976158142, time = 0.009056806564331055\n","Training at step=73, batch=120, train loss = 0.12402782768115238, train acc = 0.9800000190734863, time = 0.008980274200439453\n","Training at step=73, batch=240, train loss = 0.46409143832425037, train acc = 0.9200000166893005, time = 0.009002208709716797\n","Training at step=73, batch=360, train loss = 0.21378490458252147, train acc = 0.9200000166893005, time = 0.009115934371948242\n","Training at step=73, batch=480, train loss = 0.3272979806511889, train acc = 0.9300000071525574, time = 0.008913993835449219\n","Testing at step=73, batch=0, test loss = 0.1878454343598239, test acc = 0.9300000071525574, time = 0.0019526481628417969\n","Testing at step=73, batch=20, test loss = 0.3340881207701112, test acc = 0.9300000071525574, time = 0.001953125\n","Testing at step=73, batch=40, test loss = 0.3357066679243502, test acc = 0.8799999952316284, time = 0.0019369125366210938\n","Testing at step=73, batch=60, test loss = 0.16590331868912048, test acc = 0.9399999976158142, time = 0.0019214153289794922\n","Testing at step=73, batch=80, test loss = 0.19326129050980304, test acc = 0.9300000071525574, time = 0.001951456069946289\n","Step 73 finished in 22.794222831726074, Train loss = 0.24425587463825457, Test loss = 0.2673416188363438; Train Acc = 0.9324333354830742, Test Acc = 0.9255000001192093\n","Training at step=74, batch=0, train loss = 0.41334114405461037, train acc = 0.8700000047683716, time = 0.009079456329345703\n","Training at step=74, batch=120, train loss = 0.31986731791282036, train acc = 0.9399999976158142, time = 0.008929252624511719\n","Training at step=74, batch=240, train loss = 0.19842696167998286, train acc = 0.949999988079071, time = 0.009036064147949219\n","Training at step=74, batch=360, train loss = 0.2304645672902954, train acc = 0.9399999976158142, time = 0.009092569351196289\n","Training at step=74, batch=480, train loss = 0.2562709754180664, train acc = 0.9200000166893005, time = 0.009040117263793945\n","Testing at step=74, batch=0, test loss = 0.11343524105185558, test acc = 0.9599999785423279, time = 0.001965045928955078\n","Testing at step=74, batch=20, test loss = 0.2553535485453993, test acc = 0.9599999785423279, time = 0.0019505023956298828\n","Testing at step=74, batch=40, test loss = 0.211413445510238, test acc = 0.9399999976158142, time = 0.001928567886352539\n","Testing at step=74, batch=60, test loss = 0.34607171778197887, test acc = 0.9100000262260437, time = 0.0019555091857910156\n","Testing at step=74, batch=80, test loss = 0.2965481929472406, test acc = 0.9100000262260437, time = 0.0019865036010742188\n","Step 74 finished in 22.870392322540283, Train loss = 0.24411959512506523, Test loss = 0.2674036415313032; Train Acc = 0.9326166685422261, Test Acc = 0.9269000035524368\n","Training at step=75, batch=0, train loss = 0.23644331980730482, train acc = 0.9599999785423279, time = 0.009316444396972656\n","Training at step=75, batch=120, train loss = 0.2216563314881947, train acc = 0.9399999976158142, time = 0.00901937484741211\n","Training at step=75, batch=240, train loss = 0.22318960256313905, train acc = 0.9599999785423279, time = 0.009063005447387695\n","Training at step=75, batch=360, train loss = 0.23887731289093345, train acc = 0.9300000071525574, time = 0.00902557373046875\n","Training at step=75, batch=480, train loss = 0.44143162027068605, train acc = 0.9200000166893005, time = 0.009069681167602539\n","Testing at step=75, batch=0, test loss = 0.2642506311942152, test acc = 0.9200000166893005, time = 0.0019867420196533203\n","Testing at step=75, batch=20, test loss = 0.24137029305224134, test acc = 0.9100000262260437, time = 0.0020117759704589844\n","Testing at step=75, batch=40, test loss = 0.2831591210437884, test acc = 0.9200000166893005, time = 0.001951456069946289\n","Testing at step=75, batch=60, test loss = 0.13371933233750513, test acc = 0.949999988079071, time = 0.0020012855529785156\n","Testing at step=75, batch=80, test loss = 0.24282505797253473, test acc = 0.8999999761581421, time = 0.0019521713256835938\n","Step 75 finished in 22.86281108856201, Train loss = 0.24367483142532856, Test loss = 0.2662059117784931; Train Acc = 0.9329333348075549, Test Acc = 0.9265000015497208\n","Training at step=76, batch=0, train loss = 0.13847647596253668, train acc = 0.949999988079071, time = 0.009274959564208984\n","Training at step=76, batch=120, train loss = 0.20974120730607368, train acc = 0.9399999976158142, time = 0.009055376052856445\n","Training at step=76, batch=240, train loss = 0.20736210343477257, train acc = 0.9300000071525574, time = 0.009106874465942383\n","Training at step=76, batch=360, train loss = 0.15400269714738374, train acc = 0.9700000286102295, time = 0.009126901626586914\n","Training at step=76, batch=480, train loss = 0.27987412601642453, train acc = 0.949999988079071, time = 0.009130716323852539\n","Testing at step=76, batch=0, test loss = 0.21253213085295944, test acc = 0.9700000286102295, time = 0.0020492076873779297\n","Testing at step=76, batch=20, test loss = 0.19112881728370745, test acc = 0.9200000166893005, time = 0.001965761184692383\n","Testing at step=76, batch=40, test loss = 0.32448616200365704, test acc = 0.8700000047683716, time = 0.0019791126251220703\n","Testing at step=76, batch=60, test loss = 0.11586505681276464, test acc = 0.949999988079071, time = 0.002028226852416992\n","Testing at step=76, batch=80, test loss = 0.2006568603732074, test acc = 0.949999988079071, time = 0.0019757747650146484\n","Step 76 finished in 22.853198766708374, Train loss = 0.24378584415402296, Test loss = 0.2700760250179707; Train Acc = 0.9328000008066495, Test Acc = 0.9243000024557113\n","Training at step=77, batch=0, train loss = 0.14752037731085138, train acc = 0.949999988079071, time = 0.00921487808227539\n","Training at step=77, batch=120, train loss = 0.1031743967758751, train acc = 0.9700000286102295, time = 0.009253740310668945\n","Training at step=77, batch=240, train loss = 0.25826172690545085, train acc = 0.9300000071525574, time = 0.009011983871459961\n","Training at step=77, batch=360, train loss = 0.22256612014774987, train acc = 0.9399999976158142, time = 0.009062528610229492\n","Training at step=77, batch=480, train loss = 0.2836831540749256, train acc = 0.9100000262260437, time = 0.009045124053955078\n","Testing at step=77, batch=0, test loss = 0.25704215236077976, test acc = 0.9100000262260437, time = 0.001994609832763672\n","Testing at step=77, batch=20, test loss = 0.28487363028141893, test acc = 0.9200000166893005, time = 0.001936197280883789\n","Testing at step=77, batch=40, test loss = 0.25752850813857975, test acc = 0.9399999976158142, time = 0.002043008804321289\n","Testing at step=77, batch=60, test loss = 0.27186637009671255, test acc = 0.8999999761581421, time = 0.0019528865814208984\n","Testing at step=77, batch=80, test loss = 0.43906141528745357, test acc = 0.8999999761581421, time = 0.0019445419311523438\n","Step 77 finished in 22.82784938812256, Train loss = 0.2429024667665693, Test loss = 0.2666391375601721; Train Acc = 0.9327333343029022, Test Acc = 0.9256000018119812\n","Training at step=78, batch=0, train loss = 0.41687411419374365, train acc = 0.8899999856948853, time = 0.009248018264770508\n","Training at step=78, batch=120, train loss = 0.3824029508465766, train acc = 0.9100000262260437, time = 0.00900721549987793\n","Training at step=78, batch=240, train loss = 0.22118675932169896, train acc = 0.9200000166893005, time = 0.009001493453979492\n","Training at step=78, batch=360, train loss = 0.28760600963954036, train acc = 0.9100000262260437, time = 0.009093046188354492\n","Training at step=78, batch=480, train loss = 0.2654599155270032, train acc = 0.9599999785423279, time = 0.00913095474243164\n","Testing at step=78, batch=0, test loss = 0.409533240831184, test acc = 0.8700000047683716, time = 0.002010345458984375\n","Testing at step=78, batch=20, test loss = 0.21629831733814356, test acc = 0.9300000071525574, time = 0.0019979476928710938\n","Testing at step=78, batch=40, test loss = 0.21158191748986777, test acc = 0.9399999976158142, time = 0.001972675323486328\n","Testing at step=78, batch=60, test loss = 0.15569468945203202, test acc = 0.949999988079071, time = 0.002012968063354492\n","Testing at step=78, batch=80, test loss = 0.35232035463152633, test acc = 0.8999999761581421, time = 0.0020275115966796875\n","Step 78 finished in 22.89315629005432, Train loss = 0.24313755609703, Test loss = 0.2674179495400057; Train Acc = 0.9333166681726773, Test Acc = 0.9253000003099442\n","Training at step=79, batch=0, train loss = 0.17357943326015962, train acc = 0.9399999976158142, time = 0.009186267852783203\n","Training at step=79, batch=120, train loss = 0.2898834057319265, train acc = 0.9300000071525574, time = 0.009037971496582031\n","Training at step=79, batch=240, train loss = 0.3385705778436556, train acc = 0.8999999761581421, time = 0.009023189544677734\n","Training at step=79, batch=360, train loss = 0.49812188297570315, train acc = 0.8600000143051147, time = 0.009155511856079102\n","Training at step=79, batch=480, train loss = 0.14897165058906023, train acc = 0.9599999785423279, time = 0.009228229522705078\n","Testing at step=79, batch=0, test loss = 0.21401695924958994, test acc = 0.9599999785423279, time = 0.001981019973754883\n","Testing at step=79, batch=20, test loss = 0.42133196515434435, test acc = 0.8799999952316284, time = 0.001953125\n","Testing at step=79, batch=40, test loss = 0.22797828469848333, test acc = 0.949999988079071, time = 0.002009153366088867\n","Testing at step=79, batch=60, test loss = 0.2842834211294007, test acc = 0.9100000262260437, time = 0.0019314289093017578\n","Testing at step=79, batch=80, test loss = 0.16507573545381005, test acc = 0.9599999785423279, time = 0.002017498016357422\n","Step 79 finished in 22.780663013458252, Train loss = 0.2429431581951489, Test loss = 0.2693604883938077; Train Acc = 0.9325833350419999, Test Acc = 0.9260999995470047\n","Training at step=80, batch=0, train loss = 0.2659381146697635, train acc = 0.9300000071525574, time = 0.009235620498657227\n","Training at step=80, batch=120, train loss = 0.2862882393904443, train acc = 0.8999999761581421, time = 0.009060859680175781\n","Training at step=80, batch=240, train loss = 0.32034342542298666, train acc = 0.9100000262260437, time = 0.009016036987304688\n","Training at step=80, batch=360, train loss = 0.29810433811195636, train acc = 0.949999988079071, time = 0.009067296981811523\n","Training at step=80, batch=480, train loss = 0.0738157126322231, train acc = 0.9900000095367432, time = 0.009076356887817383\n","Testing at step=80, batch=0, test loss = 0.19973449942924235, test acc = 0.9399999976158142, time = 0.0020134449005126953\n","Testing at step=80, batch=20, test loss = 0.16623752428992014, test acc = 0.949999988079071, time = 0.0019328594207763672\n","Testing at step=80, batch=40, test loss = 0.3284002196431555, test acc = 0.949999988079071, time = 0.0019192695617675781\n","Testing at step=80, batch=60, test loss = 0.1114644117298714, test acc = 0.9599999785423279, time = 0.001960277557373047\n","Testing at step=80, batch=80, test loss = 0.2350791275039517, test acc = 0.9100000262260437, time = 0.0019364356994628906\n","Step 80 finished in 22.816885709762573, Train loss = 0.24257473467921623, Test loss = 0.26980428829775616; Train Acc = 0.933333335618178, Test Acc = 0.9234000027179718\n","Training at step=81, batch=0, train loss = 0.1879638483811175, train acc = 0.949999988079071, time = 0.009204864501953125\n","Training at step=81, batch=120, train loss = 0.19942418212767682, train acc = 0.9300000071525574, time = 0.008971691131591797\n","Training at step=81, batch=240, train loss = 0.2434129352858174, train acc = 0.9599999785423279, time = 0.009000539779663086\n","Training at step=81, batch=360, train loss = 0.26351194899399333, train acc = 0.9399999976158142, time = 0.009041070938110352\n","Training at step=81, batch=480, train loss = 0.26002311989684296, train acc = 0.8999999761581421, time = 0.009017229080200195\n","Testing at step=81, batch=0, test loss = 0.25270225770301635, test acc = 0.9399999976158142, time = 0.0021042823791503906\n","Testing at step=81, batch=20, test loss = 0.4327808206506294, test acc = 0.8899999856948853, time = 0.001960277557373047\n","Testing at step=81, batch=40, test loss = 0.2909967265527322, test acc = 0.9200000166893005, time = 0.002003908157348633\n","Testing at step=81, batch=60, test loss = 0.2610581339328114, test acc = 0.9399999976158142, time = 0.001981496810913086\n","Testing at step=81, batch=80, test loss = 0.201568610110923, test acc = 0.9300000071525574, time = 0.002046823501586914\n","Step 81 finished in 22.72164487838745, Train loss = 0.2426431480955575, Test loss = 0.2679765483052305; Train Acc = 0.9335000017285346, Test Acc = 0.9265000021457672\n","Training at step=82, batch=0, train loss = 0.39670110113136337, train acc = 0.8899999856948853, time = 0.009325981140136719\n","Training at step=82, batch=120, train loss = 0.20840937782238014, train acc = 0.9300000071525574, time = 0.009058475494384766\n","Training at step=82, batch=240, train loss = 0.37819094846981083, train acc = 0.8899999856948853, time = 0.00902104377746582\n","Training at step=82, batch=360, train loss = 0.24531076806722216, train acc = 0.9399999976158142, time = 0.00896906852722168\n","Training at step=82, batch=480, train loss = 0.295411180971494, train acc = 0.8799999952316284, time = 0.009083032608032227\n","Testing at step=82, batch=0, test loss = 0.39288633836577946, test acc = 0.8899999856948853, time = 0.002274751663208008\n","Testing at step=82, batch=20, test loss = 0.3773532378965786, test acc = 0.949999988079071, time = 0.001971006393432617\n","Testing at step=82, batch=40, test loss = 0.29501790631452157, test acc = 0.9599999785423279, time = 0.0019109249114990234\n","Testing at step=82, batch=60, test loss = 0.28953066272982264, test acc = 0.8899999856948853, time = 0.0019812583923339844\n","Testing at step=82, batch=80, test loss = 0.19891247921175148, test acc = 0.9200000166893005, time = 0.0019192695617675781\n","Step 82 finished in 22.845510959625244, Train loss = 0.2416863814866899, Test loss = 0.2711472629225235; Train Acc = 0.9341500011086464, Test Acc = 0.9225000035762787\n","Training at step=83, batch=0, train loss = 0.2905564127742914, train acc = 0.9399999976158142, time = 0.009713888168334961\n","Training at step=83, batch=120, train loss = 0.2768913646856291, train acc = 0.9399999976158142, time = 0.009029388427734375\n","Training at step=83, batch=240, train loss = 0.2363196718363767, train acc = 0.9599999785423279, time = 0.008920907974243164\n","Training at step=83, batch=360, train loss = 0.3336066944000112, train acc = 0.949999988079071, time = 0.009038448333740234\n","Training at step=83, batch=480, train loss = 0.2820862865074845, train acc = 0.9100000262260437, time = 0.009055614471435547\n","Testing at step=83, batch=0, test loss = 0.19662687308623947, test acc = 0.949999988079071, time = 0.0020558834075927734\n","Testing at step=83, batch=20, test loss = 0.4779960136780096, test acc = 0.9200000166893005, time = 0.0020110607147216797\n","Testing at step=83, batch=40, test loss = 0.2387472734560185, test acc = 0.9200000166893005, time = 0.0019958019256591797\n","Testing at step=83, batch=60, test loss = 0.2844811139050054, test acc = 0.9100000262260437, time = 0.0019216537475585938\n","Testing at step=83, batch=80, test loss = 0.18790712910758287, test acc = 0.9399999976158142, time = 0.0019690990447998047\n","Step 83 finished in 22.730120420455933, Train loss = 0.24223602157117094, Test loss = 0.2716617870597145; Train Acc = 0.9332166682680448, Test Acc = 0.9249000018835067\n","Training at step=84, batch=0, train loss = 0.20199264143014545, train acc = 0.949999988079071, time = 0.009055376052856445\n","Training at step=84, batch=120, train loss = 0.1949178222081396, train acc = 0.9399999976158142, time = 0.009310722351074219\n","Training at step=84, batch=240, train loss = 0.231222325016677, train acc = 0.9300000071525574, time = 0.009083986282348633\n","Training at step=84, batch=360, train loss = 0.2060371792828527, train acc = 0.949999988079071, time = 0.008939266204833984\n","Training at step=84, batch=480, train loss = 0.15870962081479742, train acc = 0.9399999976158142, time = 0.009104251861572266\n","Testing at step=84, batch=0, test loss = 0.378859047576701, test acc = 0.9300000071525574, time = 0.001990795135498047\n","Testing at step=84, batch=20, test loss = 0.2381513518619763, test acc = 0.9300000071525574, time = 0.0019307136535644531\n","Testing at step=84, batch=40, test loss = 0.26292201424025935, test acc = 0.9200000166893005, time = 0.002019166946411133\n","Testing at step=84, batch=60, test loss = 0.34829178141250255, test acc = 0.8700000047683716, time = 0.0020546913146972656\n","Testing at step=84, batch=80, test loss = 0.1726957711056235, test acc = 0.9399999976158142, time = 0.002009868621826172\n","Step 84 finished in 22.836262464523315, Train loss = 0.24178700773251122, Test loss = 0.2696125914997907; Train Acc = 0.9334500013788541, Test Acc = 0.9239000010490418\n","Training at step=85, batch=0, train loss = 0.41674439126440643, train acc = 0.9100000262260437, time = 0.009087800979614258\n","Training at step=85, batch=120, train loss = 0.2223317973173198, train acc = 0.9200000166893005, time = 0.00901651382446289\n","Training at step=85, batch=240, train loss = 0.15835158197924454, train acc = 0.949999988079071, time = 0.009168624877929688\n","Training at step=85, batch=360, train loss = 0.21455183146774406, train acc = 0.9200000166893005, time = 0.009017705917358398\n","Training at step=85, batch=480, train loss = 0.19060378992075294, train acc = 0.949999988079071, time = 0.009015798568725586\n","Testing at step=85, batch=0, test loss = 0.23577047235736007, test acc = 0.9100000262260437, time = 0.001980304718017578\n","Testing at step=85, batch=20, test loss = 0.14127144030639863, test acc = 0.9599999785423279, time = 0.0019681453704833984\n","Testing at step=85, batch=40, test loss = 0.3724342046983289, test acc = 0.8999999761581421, time = 0.0019333362579345703\n","Testing at step=85, batch=60, test loss = 0.2221789342091686, test acc = 0.9599999785423279, time = 0.0020334720611572266\n","Testing at step=85, batch=80, test loss = 0.4235008686426784, test acc = 0.8799999952316284, time = 0.0020003318786621094\n","Step 85 finished in 22.82367491722107, Train loss = 0.24198409800450718, Test loss = 0.26951648220834123; Train Acc = 0.9335833357771238, Test Acc = 0.924899999499321\n","Training at step=86, batch=0, train loss = 0.12927925727780307, train acc = 0.949999988079071, time = 0.009022712707519531\n","Training at step=86, batch=120, train loss = 0.149600176562099, train acc = 0.9599999785423279, time = 0.009089469909667969\n","Training at step=86, batch=240, train loss = 0.3337107887728795, train acc = 0.8700000047683716, time = 0.009038925170898438\n","Training at step=86, batch=360, train loss = 0.19467743113909008, train acc = 0.9300000071525574, time = 0.00912022590637207\n","Training at step=86, batch=480, train loss = 0.17344904157805816, train acc = 0.949999988079071, time = 0.008995294570922852\n","Testing at step=86, batch=0, test loss = 0.2600009435749406, test acc = 0.9100000262260437, time = 0.0019598007202148438\n","Testing at step=86, batch=20, test loss = 0.30603323372356106, test acc = 0.949999988079071, time = 0.0019767284393310547\n","Testing at step=86, batch=40, test loss = 0.3968389940311676, test acc = 0.8899999856948853, time = 0.001920461654663086\n","Testing at step=86, batch=60, test loss = 0.22544685480037102, test acc = 0.9300000071525574, time = 0.0020041465759277344\n","Testing at step=86, batch=80, test loss = 0.17142983809743031, test acc = 0.9399999976158142, time = 0.001895904541015625\n","Step 86 finished in 22.690433263778687, Train loss = 0.24158966306437302, Test loss = 0.2676329623753659; Train Acc = 0.933433336019516, Test Acc = 0.9254000002145767\n","Training at step=87, batch=0, train loss = 0.1679480549266141, train acc = 0.949999988079071, time = 0.009052515029907227\n","Training at step=87, batch=120, train loss = 0.2608294811366768, train acc = 0.9300000071525574, time = 0.009046792984008789\n","Training at step=87, batch=240, train loss = 0.18806689236342028, train acc = 0.9300000071525574, time = 0.008987188339233398\n","Training at step=87, batch=360, train loss = 0.3245851693382724, train acc = 0.9399999976158142, time = 0.00899195671081543\n","Training at step=87, batch=480, train loss = 0.16128894662149876, train acc = 0.949999988079071, time = 0.009004354476928711\n","Testing at step=87, batch=0, test loss = 0.29543532909495523, test acc = 0.949999988079071, time = 0.0019638538360595703\n","Testing at step=87, batch=20, test loss = 0.17124725321169187, test acc = 0.949999988079071, time = 0.002013683319091797\n","Testing at step=87, batch=40, test loss = 0.22650123152197854, test acc = 0.9300000071525574, time = 0.0019752979278564453\n","Testing at step=87, batch=60, test loss = 0.326687101692401, test acc = 0.9399999976158142, time = 0.0019445419311523438\n","Testing at step=87, batch=80, test loss = 0.2684765732060259, test acc = 0.9100000262260437, time = 0.002072572708129883\n","Step 87 finished in 22.855857610702515, Train loss = 0.24105641100123867, Test loss = 0.26759277146186755; Train Acc = 0.9335833337903022, Test Acc = 0.9257000064849854\n","Training at step=88, batch=0, train loss = 0.16978637976934283, train acc = 0.949999988079071, time = 0.009096145629882812\n","Training at step=88, batch=120, train loss = 0.2521981585018057, train acc = 0.8999999761581421, time = 0.009009599685668945\n","Training at step=88, batch=240, train loss = 0.27175374771032224, train acc = 0.9300000071525574, time = 0.009035825729370117\n","Training at step=88, batch=360, train loss = 0.1863199619317029, train acc = 0.9399999976158142, time = 0.009215593338012695\n","Training at step=88, batch=480, train loss = 0.23579028341929884, train acc = 0.9200000166893005, time = 0.009037017822265625\n","Testing at step=88, batch=0, test loss = 0.29432386510617375, test acc = 0.8999999761581421, time = 0.002070188522338867\n","Testing at step=88, batch=20, test loss = 0.1387515908665239, test acc = 0.9599999785423279, time = 0.0019116401672363281\n","Testing at step=88, batch=40, test loss = 0.21594513402258364, test acc = 0.9399999976158142, time = 0.002019166946411133\n","Testing at step=88, batch=60, test loss = 0.2662909981466258, test acc = 0.9300000071525574, time = 0.0019314289093017578\n","Testing at step=88, batch=80, test loss = 0.2785705005159001, test acc = 0.9300000071525574, time = 0.001953125\n","Step 88 finished in 22.769646644592285, Train loss = 0.2409461614225296, Test loss = 0.2697016252602438; Train Acc = 0.9333666678269704, Test Acc = 0.9250999993085861\n","Training at step=89, batch=0, train loss = 0.19178440694547444, train acc = 0.9399999976158142, time = 0.009139299392700195\n","Training at step=89, batch=120, train loss = 0.26126393568249, train acc = 0.8999999761581421, time = 0.009068489074707031\n","Training at step=89, batch=240, train loss = 0.29192659929976417, train acc = 0.9399999976158142, time = 0.009057998657226562\n","Training at step=89, batch=360, train loss = 0.1484605020015741, train acc = 0.9399999976158142, time = 0.008925437927246094\n","Training at step=89, batch=480, train loss = 0.4866508900544002, train acc = 0.8899999856948853, time = 0.008972644805908203\n","Testing at step=89, batch=0, test loss = 0.4093451706173257, test acc = 0.8999999761581421, time = 0.0019943714141845703\n","Testing at step=89, batch=20, test loss = 0.2925704867725979, test acc = 0.9100000262260437, time = 0.001988649368286133\n","Testing at step=89, batch=40, test loss = 0.24968909166389636, test acc = 0.9200000166893005, time = 0.0020110607147216797\n","Testing at step=89, batch=60, test loss = 0.41132326346752796, test acc = 0.8700000047683716, time = 0.0019419193267822266\n","Testing at step=89, batch=80, test loss = 0.38059195559349973, test acc = 0.8999999761581421, time = 0.0020220279693603516\n","Step 89 finished in 22.747785806655884, Train loss = 0.24126212237820974, Test loss = 0.2669004956240771; Train Acc = 0.9329000007112821, Test Acc = 0.9259000027179718\n","Training at step=90, batch=0, train loss = 0.19646664333430497, train acc = 0.9399999976158142, time = 0.009053707122802734\n","Training at step=90, batch=120, train loss = 0.18204436786186723, train acc = 0.949999988079071, time = 0.009025812149047852\n","Training at step=90, batch=240, train loss = 0.3820010773842641, train acc = 0.9200000166893005, time = 0.009159326553344727\n","Training at step=90, batch=360, train loss = 0.4409085369984649, train acc = 0.8500000238418579, time = 0.008877277374267578\n","Training at step=90, batch=480, train loss = 0.23304903810092573, train acc = 0.9399999976158142, time = 0.008983612060546875\n","Testing at step=90, batch=0, test loss = 0.13477568289615827, test acc = 0.949999988079071, time = 0.001970052719116211\n","Testing at step=90, batch=20, test loss = 0.35359506461573764, test acc = 0.8899999856948853, time = 0.00203704833984375\n","Testing at step=90, batch=40, test loss = 0.24391526044734207, test acc = 0.9399999976158142, time = 0.0019268989562988281\n","Testing at step=90, batch=60, test loss = 0.30490017457857443, test acc = 0.9300000071525574, time = 0.001958608627319336\n","Testing at step=90, batch=80, test loss = 0.17204791039420378, test acc = 0.9399999976158142, time = 0.0019674301147460938\n","Step 90 finished in 22.97505521774292, Train loss = 0.24078260890384068, Test loss = 0.2685516740800815; Train Acc = 0.9335000017285346, Test Acc = 0.9251000022888184\n","Training at step=91, batch=0, train loss = 0.20113758464967924, train acc = 0.949999988079071, time = 0.009155750274658203\n","Training at step=91, batch=120, train loss = 0.1435223991346805, train acc = 0.9700000286102295, time = 0.009011983871459961\n","Training at step=91, batch=240, train loss = 0.3425954324798772, train acc = 0.9300000071525574, time = 0.009183406829833984\n","Training at step=91, batch=360, train loss = 0.311724967936734, train acc = 0.949999988079071, time = 0.009093523025512695\n","Training at step=91, batch=480, train loss = 0.13734413684938043, train acc = 0.949999988079071, time = 0.009061098098754883\n","Testing at step=91, batch=0, test loss = 0.18217981021151686, test acc = 0.9399999976158142, time = 0.0020258426666259766\n","Testing at step=91, batch=20, test loss = 0.4083128782053203, test acc = 0.8600000143051147, time = 0.0019838809967041016\n","Testing at step=91, batch=40, test loss = 0.39171736722437667, test acc = 0.8999999761581421, time = 0.001981496810913086\n","Testing at step=91, batch=60, test loss = 0.21326110972143641, test acc = 0.9399999976158142, time = 0.001989126205444336\n","Testing at step=91, batch=80, test loss = 0.3951719476541599, test acc = 0.8899999856948853, time = 0.0019681453704833984\n","Step 91 finished in 22.973912000656128, Train loss = 0.24005840478883764, Test loss = 0.26887264381552184; Train Acc = 0.9334333341320356, Test Acc = 0.9250999993085861\n","Training at step=92, batch=0, train loss = 0.08127808076883934, train acc = 0.9800000190734863, time = 0.00906991958618164\n","Training at step=92, batch=120, train loss = 0.1392573617794134, train acc = 0.9700000286102295, time = 0.009052038192749023\n","Training at step=92, batch=240, train loss = 0.18828282122911094, train acc = 0.9700000286102295, time = 0.009094476699829102\n","Training at step=92, batch=360, train loss = 0.29783947298953245, train acc = 0.9300000071525574, time = 0.009100198745727539\n","Training at step=92, batch=480, train loss = 0.2110795834309102, train acc = 0.9599999785423279, time = 0.008947134017944336\n","Testing at step=92, batch=0, test loss = 0.4556782533303921, test acc = 0.9100000262260437, time = 0.002021312713623047\n","Testing at step=92, batch=20, test loss = 0.17394482728478586, test acc = 0.949999988079071, time = 0.0019576549530029297\n","Testing at step=92, batch=40, test loss = 0.399453166170278, test acc = 0.9200000166893005, time = 0.0019328594207763672\n","Testing at step=92, batch=60, test loss = 0.22422529080419656, test acc = 0.9300000071525574, time = 0.0020117759704589844\n","Testing at step=92, batch=80, test loss = 0.3360898799223007, test acc = 0.949999988079071, time = 0.0020279884338378906\n","Step 92 finished in 22.91912317276001, Train loss = 0.24001599898861622, Test loss = 0.2706449591400246; Train Acc = 0.933533335228761, Test Acc = 0.924500002861023\n","Training at step=93, batch=0, train loss = 0.32712425263998507, train acc = 0.9200000166893005, time = 0.009153604507446289\n","Training at step=93, batch=120, train loss = 0.23311032424095196, train acc = 0.9399999976158142, time = 0.009121179580688477\n","Training at step=93, batch=240, train loss = 0.2794975040773433, train acc = 0.9200000166893005, time = 0.009144067764282227\n","Training at step=93, batch=360, train loss = 0.26049398944623836, train acc = 0.9399999976158142, time = 0.009042024612426758\n","Training at step=93, batch=480, train loss = 0.21964284101574502, train acc = 0.9300000071525574, time = 0.008932113647460938\n","Testing at step=93, batch=0, test loss = 0.25806022247971677, test acc = 0.949999988079071, time = 0.0020177364349365234\n","Testing at step=93, batch=20, test loss = 0.3421764775631571, test acc = 0.8799999952316284, time = 0.001960277557373047\n","Testing at step=93, batch=40, test loss = 0.22077265274826757, test acc = 0.9599999785423279, time = 0.0019850730895996094\n","Testing at step=93, batch=60, test loss = 0.28420967098988903, test acc = 0.9200000166893005, time = 0.002032756805419922\n","Testing at step=93, batch=80, test loss = 0.2605182025469844, test acc = 0.9300000071525574, time = 0.0019526481628417969\n","Step 93 finished in 22.902944564819336, Train loss = 0.24023468092458755, Test loss = 0.26815368603530404; Train Acc = 0.9341166673103968, Test Acc = 0.925500002503395\n","Training at step=94, batch=0, train loss = 0.1525093408829723, train acc = 0.9599999785423279, time = 0.009129524230957031\n","Training at step=94, batch=120, train loss = 0.2825840296975592, train acc = 0.8999999761581421, time = 0.00898599624633789\n","Training at step=94, batch=240, train loss = 0.18030571162310835, train acc = 0.949999988079071, time = 0.009069442749023438\n","Training at step=94, batch=360, train loss = 0.2576966468767328, train acc = 0.9300000071525574, time = 0.009127140045166016\n","Training at step=94, batch=480, train loss = 0.41659939291575016, train acc = 0.8899999856948853, time = 0.009100914001464844\n","Testing at step=94, batch=0, test loss = 0.28657960170563035, test acc = 0.9300000071525574, time = 0.0022220611572265625\n","Testing at step=94, batch=20, test loss = 0.43012732298983075, test acc = 0.8899999856948853, time = 0.00199127197265625\n","Testing at step=94, batch=40, test loss = 0.2902085768090046, test acc = 0.9200000166893005, time = 0.0019297599792480469\n","Testing at step=94, batch=60, test loss = 0.2730768352355328, test acc = 0.9300000071525574, time = 0.001984834671020508\n","Testing at step=94, batch=80, test loss = 0.19866755313569953, test acc = 0.9300000071525574, time = 0.0019004344940185547\n","Step 94 finished in 22.906750440597534, Train loss = 0.23983113784558066, Test loss = 0.2700439779588991; Train Acc = 0.9339000016450882, Test Acc = 0.9252000015974045\n","Training at step=95, batch=0, train loss = 0.19193949656796622, train acc = 0.9399999976158142, time = 0.009486675262451172\n","Training at step=95, batch=120, train loss = 0.2174168402268994, train acc = 0.9300000071525574, time = 0.009042978286743164\n","Training at step=95, batch=240, train loss = 0.170067508459543, train acc = 0.949999988079071, time = 0.009052038192749023\n","Training at step=95, batch=360, train loss = 0.33035486355839, train acc = 0.9100000262260437, time = 0.00914144515991211\n","Training at step=95, batch=480, train loss = 0.3037994213405948, train acc = 0.9200000166893005, time = 0.008894920349121094\n","Testing at step=95, batch=0, test loss = 0.4236143500696478, test acc = 0.8700000047683716, time = 0.0019800662994384766\n","Testing at step=95, batch=20, test loss = 0.30051087215679995, test acc = 0.949999988079071, time = 0.0019724369049072266\n","Testing at step=95, batch=40, test loss = 0.2020319287616778, test acc = 0.9700000286102295, time = 0.0019450187683105469\n","Testing at step=95, batch=60, test loss = 0.3839579149817116, test acc = 0.8899999856948853, time = 0.0019185543060302734\n","Testing at step=95, batch=80, test loss = 0.20324468821476674, test acc = 0.9599999785423279, time = 0.001954317092895508\n","Step 95 finished in 22.845585584640503, Train loss = 0.23983153897387866, Test loss = 0.26870224528858083; Train Acc = 0.9340500019987424, Test Acc = 0.9274000024795532\n","Training at step=96, batch=0, train loss = 0.26053076016290083, train acc = 0.9300000071525574, time = 0.00911712646484375\n","Training at step=96, batch=120, train loss = 0.2891738266674953, train acc = 0.9300000071525574, time = 0.008960485458374023\n","Training at step=96, batch=240, train loss = 0.20782902566886413, train acc = 0.9399999976158142, time = 0.008980035781860352\n","Training at step=96, batch=360, train loss = 0.27451188269468063, train acc = 0.9200000166893005, time = 0.009181976318359375\n","Training at step=96, batch=480, train loss = 0.19035275259050646, train acc = 0.9200000166893005, time = 0.009229660034179688\n","Testing at step=96, batch=0, test loss = 0.28554004828293617, test acc = 0.9200000166893005, time = 0.002068758010864258\n","Testing at step=96, batch=20, test loss = 0.39901124118165404, test acc = 0.8799999952316284, time = 0.002068758010864258\n","Testing at step=96, batch=40, test loss = 0.28138610379549944, test acc = 0.9100000262260437, time = 0.0020062923431396484\n","Testing at step=96, batch=60, test loss = 0.34433936400393145, test acc = 0.8899999856948853, time = 0.0019617080688476562\n","Testing at step=96, batch=80, test loss = 0.32939361826181845, test acc = 0.9100000262260437, time = 0.001920938491821289\n","Step 96 finished in 22.88205075263977, Train loss = 0.23971625217606782, Test loss = 0.2693482568672031; Train Acc = 0.9340500017007192, Test Acc = 0.925699999332428\n","Training at step=97, batch=0, train loss = 0.20920303160555295, train acc = 0.9399999976158142, time = 0.009203195571899414\n","Training at step=97, batch=120, train loss = 0.2784675624889429, train acc = 0.9300000071525574, time = 0.009065866470336914\n","Training at step=97, batch=240, train loss = 0.14900819140136248, train acc = 0.949999988079071, time = 0.009070634841918945\n","Training at step=97, batch=360, train loss = 0.12566572506871876, train acc = 0.9700000286102295, time = 0.010124921798706055\n","Training at step=97, batch=480, train loss = 0.20060397403429509, train acc = 0.949999988079071, time = 0.00911092758178711\n","Testing at step=97, batch=0, test loss = 0.368211195312356, test acc = 0.8999999761581421, time = 0.002022266387939453\n","Testing at step=97, batch=20, test loss = 0.16841281180692647, test acc = 0.949999988079071, time = 0.001998424530029297\n","Testing at step=97, batch=40, test loss = 0.28818418995682626, test acc = 0.8899999856948853, time = 0.0019598007202148438\n","Testing at step=97, batch=60, test loss = 0.3161197502017129, test acc = 0.8899999856948853, time = 0.001956462860107422\n","Testing at step=97, batch=80, test loss = 0.2199335909148509, test acc = 0.9399999976158142, time = 0.001978158950805664\n","Step 97 finished in 22.835930109024048, Train loss = 0.23959155901535278, Test loss = 0.2725238941164847; Train Acc = 0.9337666684389114, Test Acc = 0.9243000012636184\n","Training at step=98, batch=0, train loss = 0.27422281944193944, train acc = 0.9100000262260437, time = 0.009152650833129883\n","Training at step=98, batch=120, train loss = 0.44388340996440356, train acc = 0.9200000166893005, time = 0.009074687957763672\n","Training at step=98, batch=240, train loss = 0.322896030591375, train acc = 0.9399999976158142, time = 0.009035348892211914\n","Training at step=98, batch=360, train loss = 0.25354858181949047, train acc = 0.9300000071525574, time = 0.008955001831054688\n","Training at step=98, batch=480, train loss = 0.3699533614470774, train acc = 0.949999988079071, time = 0.008976221084594727\n","Testing at step=98, batch=0, test loss = 0.35831887316922606, test acc = 0.9100000262260437, time = 0.0020461082458496094\n","Testing at step=98, batch=20, test loss = 0.23845156816500612, test acc = 0.9200000166893005, time = 0.0020020008087158203\n","Testing at step=98, batch=40, test loss = 0.3364834839980437, test acc = 0.9200000166893005, time = 0.001981496810913086\n","Testing at step=98, batch=60, test loss = 0.32035916157420297, test acc = 0.8999999761581421, time = 0.0019168853759765625\n","Testing at step=98, batch=80, test loss = 0.06682117763907276, test acc = 0.9900000095367432, time = 0.0019388198852539062\n","Step 98 finished in 22.92630887031555, Train loss = 0.23929388328958448, Test loss = 0.2708436777915149; Train Acc = 0.9337333350380261, Test Acc = 0.9247000008821488\n","Training at step=99, batch=0, train loss = 0.25535880664283483, train acc = 0.949999988079071, time = 0.009126663208007812\n","Training at step=99, batch=120, train loss = 0.28081966935011543, train acc = 0.8999999761581421, time = 0.008986234664916992\n","Training at step=99, batch=240, train loss = 0.4263687879899019, train acc = 0.8899999856948853, time = 0.00903630256652832\n","Training at step=99, batch=360, train loss = 0.190311678366231, train acc = 0.9300000071525574, time = 0.009001970291137695\n","Training at step=99, batch=480, train loss = 0.15279259037409187, train acc = 0.949999988079071, time = 0.009044885635375977\n","Testing at step=99, batch=0, test loss = 0.19731569752473146, test acc = 0.9200000166893005, time = 0.0020008087158203125\n","Testing at step=99, batch=20, test loss = 0.38423670061012216, test acc = 0.8600000143051147, time = 0.0019390583038330078\n","Testing at step=99, batch=40, test loss = 0.1923853719143875, test acc = 0.949999988079071, time = 0.0019168853759765625\n","Testing at step=99, batch=60, test loss = 0.25883412537134026, test acc = 0.9300000071525574, time = 0.0019712448120117188\n","Testing at step=99, batch=80, test loss = 0.25121894208952467, test acc = 0.949999988079071, time = 0.0019817352294921875\n","Step 99 finished in 22.86955499649048, Train loss = 0.23901725146388833, Test loss = 0.2715778456396697; Train Acc = 0.9344666687647502, Test Acc = 0.9254000025987625\n"]}]},{"cell_type":"code","source":["# plot\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n","\n","# Plot the losses\n","ax1.plot(train_losses, label='Train Loss')\n","ax1.plot(test_losses, label='Test Loss')\n","ax1.set_xlabel('Epoch')\n","ax1.set_ylabel('Loss')\n","ax1.set_title('Loss vs. Epoch')\n","ax1.legend()\n","\n","# Plot the accuracies\n","ax2.plot(train_accs, label='Train Accuracy')\n","ax2.plot(test_accs, label='Test Accuracy')\n","ax2.set_xlabel('Epoch')\n","ax2.set_ylabel('Accuracy')\n","ax2.set_title('Accuracy vs. Epoch')\n","ax2.legend()\n","\n","# Adjust the spacing between subplots\n","plt.tight_layout()\n","\n","# Display the plots\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":309},"id":"V7YYHs33saIx","executionInfo":{"status":"ok","timestamp":1712093854908,"user_tz":-660,"elapsed":1263,"user":{"displayName":"P. W.","userId":"06457912707533471190"}},"outputId":"d1f320d0-a339-46a4-fbb6-b9f4fc265cfa"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x400 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]}]}