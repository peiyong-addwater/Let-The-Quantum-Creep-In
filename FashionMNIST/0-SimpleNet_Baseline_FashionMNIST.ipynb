{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 618301,
     "status": "ok",
     "timestamp": 1712091541763,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "acHJk38WrBOi",
    "outputId": "ba1ec555-f0d0-45fd-df87-2f537a902cab",
    "ExecuteTime": {
     "end_time": "2024-04-06T20:15:36.030435Z",
     "start_time": "2024-04-06T20:13:41.133148Z"
    }
   },
   "source": [
    "!pip install torch torchvision torchaudio pennylane cotengra quimb torchmetrics --upgrade"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.1+cu116)\r\n",
      "Collecting torch\r\n",
      "  Downloading torch-2.2.2-cp39-cp39-manylinux1_x86_64.whl (755.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m755.5/755.5 MB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\r\n",
      "Collecting torchvision\r\n",
      "  Downloading torchvision-0.17.2-cp39-cp39-manylinux1_x86_64.whl (6.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.9/6.9 MB\u001B[0m \u001B[31m41.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.12.1+cu116)\r\n",
      "Collecting torchaudio\r\n",
      "  Downloading torchaudio-2.2.2-cp39-cp39-manylinux1_x86_64.whl (3.3 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.3/3.3 MB\u001B[0m \u001B[31m106.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting pennylane\r\n",
      "  Downloading PennyLane-0.35.1-py3-none-any.whl (1.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m106.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting cotengra\r\n",
      "  Downloading cotengra-0.5.6-py3-none-any.whl (148 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m148.0/148.0 kB\u001B[0m \u001B[31m42.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting quimb\r\n",
      "  Downloading quimb-1.7.3-py3-none-any.whl (500 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m500.7/500.7 kB\u001B[0m \u001B[31m79.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting torchmetrics\r\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m841.5/841.5 kB\u001B[0m \u001B[31m86.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\r\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.0/196.0 MB\u001B[0m \u001B[31m10.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m88.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\r\n",
      "Collecting sympy\r\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m5.7/5.7 MB\u001B[0m \u001B[31m98.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cufft-cu12==11.0.2.54\r\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.6/121.6 MB\u001B[0m \u001B[31m16.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m823.6/823.6 kB\u001B[0m \u001B[31m82.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-curand-cu12==10.3.2.106\r\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m32.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\r\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 MB\u001B[0m \u001B[31m15.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-nccl-cu12==2.19.3\r\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.0/166.0 MB\u001B[0m \u001B[31m12.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m58.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-nvtx-cu12==12.1.105\r\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/99.1 kB\u001B[0m \u001B[31m29.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting triton==2.2.0\r\n",
      "  Downloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m167.9/167.9 MB\u001B[0m \u001B[31m12.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\r\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\r\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.6/410.6 MB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting typing-extensions>=4.8.0\r\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26\r\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m731.7/731.7 MB\u001B[0m \u001B[31m3.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting nvidia-nvjitlink-cu12\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m77.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\r\n",
      "Collecting semantic-version>=2.7\r\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\r\n",
      "Collecting rustworkx\r\n",
      "  Downloading rustworkx-0.14.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m106.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\r\n",
      "Collecting appdirs\r\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Collecting autograd\r\n",
      "  Downloading autograd-1.6.2-py3-none-any.whl (49 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.3/49.3 kB\u001B[0m \u001B[31m16.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting autoray>=0.6.1\r\n",
      "  Downloading autoray-0.6.9-py3-none-any.whl (49 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.8/49.8 kB\u001B[0m \u001B[31m17.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\r\n",
      "Collecting toml\r\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\r\n",
      "Collecting pennylane-lightning>=0.35\r\n",
      "  Downloading PennyLane_Lightning-0.35.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.5 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.5/18.5 MB\u001B[0m \u001B[31m79.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\r\n",
      "Collecting numba>=0.39\r\n",
      "  Downloading numba-0.59.1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.7/3.7 MB\u001B[0m \u001B[31m111.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\r\n",
      "Collecting cytoolz>=0.8.0\r\n",
      "  Downloading cytoolz-0.12.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m108.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting lightning-utilities>=0.8.0\r\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\r\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\r\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0\r\n",
      "  Downloading llvmlite-0.42.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m43.8/43.8 MB\u001B[0m \u001B[31m36.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\r\n",
      "Collecting mpmath>=0.19\r\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m536.2/536.2 kB\u001B[0m \u001B[31m79.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: mpmath, appdirs, typing-extensions, triton, toml, sympy, semantic-version, rustworkx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, llvmlite, cytoolz, autoray, autograd, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, lightning-utilities, cotengra, quimb, nvidia-cusolver-cu12, torch, torchvision, torchmetrics, torchaudio, pennylane-lightning, pennylane\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.4.0\r\n",
      "    Uninstalling typing_extensions-4.4.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.4.0\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.12.1+cu116\r\n",
      "    Uninstalling torch-1.12.1+cu116:\r\n",
      "      Successfully uninstalled torch-1.12.1+cu116\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.13.1+cu116\r\n",
      "    Uninstalling torchvision-0.13.1+cu116:\r\n",
      "      Successfully uninstalled torchvision-0.13.1+cu116\r\n",
      "  Attempting uninstall: torchaudio\r\n",
      "    Found existing installation: torchaudio 0.12.1+cu116\r\n",
      "    Uninstalling torchaudio-0.12.1+cu116:\r\n",
      "      Successfully uninstalled torchaudio-0.12.1+cu116\r\n",
      "Successfully installed appdirs-1.4.4 autograd-1.6.2 autoray-0.6.9 cotengra-0.5.6 cytoolz-0.12.3 lightning-utilities-0.11.2 llvmlite-0.42.0 mpmath-1.3.0 numba-0.59.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pennylane-0.35.1 pennylane-lightning-0.35.1 quimb-1.7.3 rustworkx-0.14.2 semantic-version-2.10.0 sympy-1.12 toml-0.10.2 torch-2.2.2 torchaudio-2.2.2 torchmetrics-1.3.2 torchvision-0.17.2 triton-2.2.0 typing-extensions-4.11.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8020,
     "status": "ok",
     "timestamp": 1712091549780,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "FB_BuVzCrMNk",
    "outputId": "235fd337-f4c4-4ba7-9547-7b96570eac2e",
    "ExecuteTime": {
     "end_time": "2024-04-06T20:15:40.761798Z",
     "start_time": "2024-04-06T20:15:36.031982Z"
    }
   },
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "\n",
    "\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "#REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5513,
     "status": "ok",
     "timestamp": 1712091555290,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "K7wJnOiEralR",
    "outputId": "a21dab31-4609-4119-cbdd-0b84f74ac35f",
    "ExecuteTime": {
     "end_time": "2024-04-06T20:15:48.999706Z",
     "start_time": "2024-04-06T20:15:40.763128Z"
    }
   },
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:02<00:00, 13013609.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 347217.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:03<00:00, 1383208.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 18129535.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n",
      "tensor([4, 1, 7, 3, 5, 8, 2, 6, 4, 6, 6, 3, 6, 9, 9, 7, 8, 7, 7, 7, 2, 7, 3, 8,\n",
      "        8, 8, 2, 2, 8, 2, 1, 5, 8, 7, 8, 2, 1, 4, 8, 7, 0, 0, 8, 9, 7, 5, 3, 6,\n",
      "        9, 3, 9, 8, 7, 1, 8, 8, 0, 7, 3, 0, 6, 4, 5, 8])\n",
      "tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.7176,\n",
      "         0.5686,  0.8588,  0.2235,  0.0902,  0.9059,  0.5529,  0.6000,  0.6078,\n",
      "         0.6392,  0.6314,  0.6157,  0.8039,  0.4039, -0.0039,  0.9922,  0.6314,\n",
      "         0.7490, -0.6235, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1712091555780,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "t6wDzOaJrmZ4",
    "outputId": "e216a519-beac-46f4-de29-cd0e9e280630",
    "ExecuteTime": {
     "end_time": "2024-04-06T20:15:52.939979Z",
     "start_time": "2024-04-06T20:15:49.001868Z"
    }
   },
   "source": [
    "class SimpleNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SimpleNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=3),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Conv2d(32, 16, kernel_size=3),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(16*28*28, 10),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "net = SimpleNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "SimpleNet(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=12544, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDNHjKy4sSWZ",
    "outputId": "a0ef5fa2-455a-4017-d32f-bc0e38c7cdab",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712093844984,
     "user_tz": -660,
     "elapsed": 2289206,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-04-06T20:15:52.942021Z"
    }
   },
   "source": [
    "import torchmetrics\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.2\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 600, Number of test batches = 100\n",
      "Print every train batch = 120, Print every test batch = 20\n",
      "Training at step=0, batch=0, train loss = 2.306792765239672, train acc = 0.10999999940395355, time = 0.18320798873901367\n",
      "Training at step=0, batch=120, train loss = 0.7192115776557848, train acc = 0.75, time = 0.008365869522094727\n",
      "Training at step=0, batch=240, train loss = 0.7989091330869033, train acc = 0.75, time = 0.008182048797607422\n",
      "Training at step=0, batch=360, train loss = 0.4937506498953356, train acc = 0.7799999713897705, time = 0.008211135864257812\n",
      "Training at step=0, batch=480, train loss = 0.5181211140449858, train acc = 0.8399999737739563, time = 0.008206605911254883\n",
      "Testing at step=0, batch=0, test loss = 0.6360847186959648, test acc = 0.75, time = 0.0016481876373291016\n",
      "Testing at step=0, batch=20, test loss = 0.5886414608914923, test acc = 0.7599999904632568, time = 0.001678466796875\n",
      "Testing at step=0, batch=40, test loss = 0.4538919952490481, test acc = 0.8199999928474426, time = 0.0015566349029541016\n",
      "Testing at step=0, batch=60, test loss = 0.6404311024974219, test acc = 0.7699999809265137, time = 0.001562356948852539\n",
      "Testing at step=0, batch=80, test loss = 0.5357739669468177, test acc = 0.800000011920929, time = 0.0015544891357421875\n",
      "Step 0 finished in 16.621954202651978, Train loss = 0.7241927799136987, Test loss = 0.552099417133723; Train Acc = 0.7505166640629372, Test Acc = 0.7998999989032746\n",
      "Training at step=1, batch=0, train loss = 0.46399548580780836, train acc = 0.8299999833106995, time = 0.008294105529785156\n",
      "Training at step=1, batch=120, train loss = 0.40264857414024574, train acc = 0.8899999856948853, time = 0.008233308792114258\n",
      "Training at step=1, batch=240, train loss = 0.4986857496317565, train acc = 0.8299999833106995, time = 0.008230209350585938\n",
      "Training at step=1, batch=360, train loss = 0.6168165248964933, train acc = 0.7900000214576721, time = 0.008227825164794922\n",
      "Training at step=1, batch=480, train loss = 0.469224702977591, train acc = 0.8100000023841858, time = 0.00818490982055664\n",
      "Testing at step=1, batch=0, test loss = 0.3711550374113526, test acc = 0.8600000143051147, time = 0.0017352104187011719\n",
      "Testing at step=1, batch=20, test loss = 0.5452622015875417, test acc = 0.7900000214576721, time = 0.0019927024841308594\n",
      "Testing at step=1, batch=40, test loss = 0.38660541707551666, test acc = 0.8399999737739563, time = 0.0015904903411865234\n",
      "Testing at step=1, batch=60, test loss = 0.4591920123697774, test acc = 0.8299999833106995, time = 0.0015597343444824219\n",
      "Testing at step=1, batch=80, test loss = 0.36857745237536593, test acc = 0.8799999952316284, time = 0.0015590190887451172\n",
      "Step 1 finished in 16.391932010650635, Train loss = 0.49692725900398776, Test loss = 0.5064217838536715; Train Acc = 0.8246166641513507, Test Acc = 0.8216999971866608\n",
      "Training at step=2, batch=0, train loss = 0.3371824946261799, train acc = 0.8799999952316284, time = 0.008341789245605469\n",
      "Training at step=2, batch=120, train loss = 0.4848131618693195, train acc = 0.8500000238418579, time = 0.00818490982055664\n",
      "Training at step=2, batch=240, train loss = 0.31114203338905677, train acc = 0.8999999761581421, time = 0.00825190544128418\n",
      "Training at step=2, batch=360, train loss = 0.48018303521439276, train acc = 0.8299999833106995, time = 0.008258819580078125\n",
      "Training at step=2, batch=480, train loss = 0.4668734893780957, train acc = 0.8399999737739563, time = 0.00821828842163086\n",
      "Testing at step=2, batch=0, test loss = 0.4212198128844494, test acc = 0.8399999737739563, time = 0.0015990734100341797\n",
      "Testing at step=2, batch=20, test loss = 0.35084438522175426, test acc = 0.8399999737739563, time = 0.0015833377838134766\n",
      "Testing at step=2, batch=40, test loss = 0.443658368693325, test acc = 0.8199999928474426, time = 0.0015799999237060547\n",
      "Testing at step=2, batch=60, test loss = 0.39005118180216813, test acc = 0.8799999952316284, time = 0.0015728473663330078\n",
      "Testing at step=2, batch=80, test loss = 0.64570032695551, test acc = 0.7599999904632568, time = 0.0015799999237060547\n",
      "Step 2 finished in 16.37726402282715, Train loss = 0.46781972211485234, Test loss = 0.48556077941750453; Train Acc = 0.8363000000516574, Test Acc = 0.8260999971628189\n",
      "Training at step=3, batch=0, train loss = 0.4243443713220897, train acc = 0.8600000143051147, time = 0.008396148681640625\n",
      "Training at step=3, batch=120, train loss = 0.3715240127980108, train acc = 0.8899999856948853, time = 0.008167505264282227\n",
      "Training at step=3, batch=240, train loss = 0.5189557425643915, train acc = 0.8500000238418579, time = 0.00819540023803711\n",
      "Training at step=3, batch=360, train loss = 0.4298189555258507, train acc = 0.8299999833106995, time = 0.008165597915649414\n",
      "Training at step=3, batch=480, train loss = 0.4381475860195585, train acc = 0.8299999833106995, time = 0.008190631866455078\n",
      "Testing at step=3, batch=0, test loss = 0.5603308217612496, test acc = 0.8100000023841858, time = 0.0016579627990722656\n",
      "Testing at step=3, batch=20, test loss = 0.4667419089405675, test acc = 0.8700000047683716, time = 0.0015654563903808594\n",
      "Testing at step=3, batch=40, test loss = 0.44732270621781217, test acc = 0.8500000238418579, time = 0.0015532970428466797\n",
      "Testing at step=3, batch=60, test loss = 0.5504358674905664, test acc = 0.7900000214576721, time = 0.0015568733215332031\n",
      "Testing at step=3, batch=80, test loss = 0.3684473062859875, test acc = 0.8899999856948853, time = 0.0015611648559570312\n",
      "Step 3 finished in 16.288172721862793, Train loss = 0.4504732604781617, Test loss = 0.4715339074227788; Train Acc = 0.8435833312074343, Test Acc = 0.8336999982595443\n",
      "Training at step=4, batch=0, train loss = 0.4363385129708158, train acc = 0.8299999833106995, time = 0.00825810432434082\n",
      "Training at step=4, batch=120, train loss = 0.5314069536940424, train acc = 0.8399999737739563, time = 0.008304119110107422\n",
      "Training at step=4, batch=240, train loss = 0.4863975285878008, train acc = 0.8600000143051147, time = 0.008259296417236328\n",
      "Training at step=4, batch=360, train loss = 0.38242816838291227, train acc = 0.8299999833106995, time = 0.008382320404052734\n",
      "Training at step=4, batch=480, train loss = 0.3381365888493791, train acc = 0.8600000143051147, time = 0.008234739303588867\n",
      "Testing at step=4, batch=0, test loss = 0.3083508542660185, test acc = 0.8999999761581421, time = 0.0016639232635498047\n",
      "Testing at step=4, batch=20, test loss = 0.559700677947077, test acc = 0.8100000023841858, time = 0.0015718936920166016\n",
      "Testing at step=4, batch=40, test loss = 0.4951218078975044, test acc = 0.8100000023841858, time = 0.0016014575958251953\n",
      "Testing at step=4, batch=60, test loss = 0.4563012213848767, test acc = 0.8500000238418579, time = 0.0015704631805419922\n",
      "Testing at step=4, batch=80, test loss = 0.4160543933651401, test acc = 0.8600000143051147, time = 0.0015521049499511719\n",
      "Step 4 finished in 16.330678462982178, Train loss = 0.438984516942279, Test loss = 0.4662736931406332; Train Acc = 0.8485499987999598, Test Acc = 0.836800001859665\n",
      "Training at step=5, batch=0, train loss = 0.41948765694591794, train acc = 0.8700000047683716, time = 0.008250713348388672\n",
      "Training at step=5, batch=120, train loss = 0.6085427798412801, train acc = 0.7799999713897705, time = 0.00827336311340332\n",
      "Training at step=5, batch=240, train loss = 0.3744132394386542, train acc = 0.8700000047683716, time = 0.008208990097045898\n",
      "Training at step=5, batch=360, train loss = 0.4300533373282562, train acc = 0.8299999833106995, time = 0.008177518844604492\n",
      "Training at step=5, batch=480, train loss = 0.4087986574662619, train acc = 0.8399999737739563, time = 0.008290529251098633\n",
      "Testing at step=5, batch=0, test loss = 0.4695604929186263, test acc = 0.8199999928474426, time = 0.001619577407836914\n",
      "Testing at step=5, batch=20, test loss = 0.4860410174948722, test acc = 0.8199999928474426, time = 0.0015723705291748047\n",
      "Testing at step=5, batch=40, test loss = 0.35736931296642516, test acc = 0.8600000143051147, time = 0.001573324203491211\n",
      "Testing at step=5, batch=60, test loss = 0.44521422310437186, test acc = 0.8399999737739563, time = 0.0015969276428222656\n",
      "Testing at step=5, batch=80, test loss = 0.3091224456738059, test acc = 0.8799999952316284, time = 0.0015666484832763672\n",
      "Step 5 finished in 16.337376356124878, Train loss = 0.4327017824841274, Test loss = 0.46753177408702895; Train Acc = 0.8490333315730095, Test Acc = 0.8341999977827073\n",
      "Training at step=6, batch=0, train loss = 0.45890517201631253, train acc = 0.8500000238418579, time = 0.008316516876220703\n",
      "Training at step=6, batch=120, train loss = 0.41506585721568084, train acc = 0.7900000214576721, time = 0.008203506469726562\n",
      "Training at step=6, batch=240, train loss = 0.2884103736707081, train acc = 0.9100000262260437, time = 0.008191585540771484\n",
      "Training at step=6, batch=360, train loss = 0.453539878902758, train acc = 0.8399999737739563, time = 0.008274078369140625\n",
      "Training at step=6, batch=480, train loss = 0.39554968045110195, train acc = 0.8199999928474426, time = 0.008143901824951172\n",
      "Testing at step=6, batch=0, test loss = 0.6872105906942013, test acc = 0.7400000095367432, time = 0.0016100406646728516\n",
      "Testing at step=6, batch=20, test loss = 0.3344630602418485, test acc = 0.9300000071525574, time = 0.0016031265258789062\n",
      "Testing at step=6, batch=40, test loss = 0.5501747292698542, test acc = 0.8299999833106995, time = 0.0016088485717773438\n",
      "Testing at step=6, batch=60, test loss = 0.45642097206697924, test acc = 0.8600000143051147, time = 0.0016334056854248047\n",
      "Testing at step=6, batch=80, test loss = 0.48577676394970254, test acc = 0.8199999928474426, time = 0.0015959739685058594\n",
      "Step 6 finished in 16.335495233535767, Train loss = 0.42687215068410805, Test loss = 0.45957655937722336; Train Acc = 0.8525833331545194, Test Acc = 0.8395999950170517\n",
      "Training at step=7, batch=0, train loss = 0.48803011375398747, train acc = 0.8899999856948853, time = 0.008328676223754883\n",
      "Training at step=7, batch=120, train loss = 0.39368673787946884, train acc = 0.8299999833106995, time = 0.008137702941894531\n",
      "Training at step=7, batch=240, train loss = 0.40454729056086064, train acc = 0.8500000238418579, time = 0.00830388069152832\n",
      "Training at step=7, batch=360, train loss = 0.32373223175444793, train acc = 0.8899999856948853, time = 0.008215188980102539\n",
      "Training at step=7, batch=480, train loss = 0.5212297319156919, train acc = 0.8100000023841858, time = 0.008296489715576172\n",
      "Testing at step=7, batch=0, test loss = 0.42035555270719044, test acc = 0.8399999737739563, time = 0.0015969276428222656\n",
      "Testing at step=7, batch=20, test loss = 0.5692979740058497, test acc = 0.7799999713897705, time = 0.0015766620635986328\n",
      "Testing at step=7, batch=40, test loss = 0.41017400816411703, test acc = 0.8999999761581421, time = 0.001575469970703125\n",
      "Testing at step=7, batch=60, test loss = 0.42915623678437803, test acc = 0.8299999833106995, time = 0.0015799999237060547\n",
      "Testing at step=7, batch=80, test loss = 0.5106044828717562, test acc = 0.8399999737739563, time = 0.0015795230865478516\n",
      "Step 7 finished in 16.37747073173523, Train loss = 0.4228418993531209, Test loss = 0.47277055483167; Train Acc = 0.8538999994595845, Test Acc = 0.8327999979257583\n",
      "Training at step=8, batch=0, train loss = 0.3655015256848981, train acc = 0.8500000238418579, time = 0.008415699005126953\n",
      "Training at step=8, batch=120, train loss = 0.5165872559478756, train acc = 0.8500000238418579, time = 0.008231401443481445\n",
      "Training at step=8, batch=240, train loss = 0.33491016210869423, train acc = 0.8899999856948853, time = 0.008188486099243164\n",
      "Training at step=8, batch=360, train loss = 0.4273836706980078, train acc = 0.8700000047683716, time = 0.008322477340698242\n",
      "Training at step=8, batch=480, train loss = 0.46053264686629886, train acc = 0.8399999737739563, time = 0.008348703384399414\n",
      "Testing at step=8, batch=0, test loss = 0.34516584883616636, test acc = 0.8500000238418579, time = 0.0016057491302490234\n",
      "Testing at step=8, batch=20, test loss = 0.5111371089419473, test acc = 0.8700000047683716, time = 0.0015788078308105469\n",
      "Testing at step=8, batch=40, test loss = 0.6002053788307389, test acc = 0.7799999713897705, time = 0.0016219615936279297\n",
      "Testing at step=8, batch=60, test loss = 0.5211527804278253, test acc = 0.8199999928474426, time = 0.001580953598022461\n",
      "Testing at step=8, batch=80, test loss = 0.5368436106821856, test acc = 0.8199999928474426, time = 0.0015997886657714844\n",
      "Step 8 finished in 16.390426874160767, Train loss = 0.4180452636112983, Test loss = 0.44983546665118446; Train Acc = 0.8549333331982295, Test Acc = 0.8423000007867814\n",
      "Training at step=9, batch=0, train loss = 0.284836047433183, train acc = 0.8999999761581421, time = 0.008371829986572266\n",
      "Training at step=9, batch=120, train loss = 0.32037470754335323, train acc = 0.8799999952316284, time = 0.008157014846801758\n",
      "Training at step=9, batch=240, train loss = 0.28958465714803217, train acc = 0.8799999952316284, time = 0.008152484893798828\n",
      "Training at step=9, batch=360, train loss = 0.4391540291618158, train acc = 0.8500000238418579, time = 0.008129596710205078\n",
      "Training at step=9, batch=480, train loss = 0.5192846415216763, train acc = 0.8299999833106995, time = 0.008167505264282227\n",
      "Testing at step=9, batch=0, test loss = 0.47225576888292714, test acc = 0.8500000238418579, time = 0.001750946044921875\n",
      "Testing at step=9, batch=20, test loss = 0.4780916940466631, test acc = 0.8500000238418579, time = 0.0015783309936523438\n",
      "Testing at step=9, batch=40, test loss = 0.31681198792598436, test acc = 0.8600000143051147, time = 0.0016031265258789062\n",
      "Testing at step=9, batch=60, test loss = 0.4597937025537878, test acc = 0.8399999737739563, time = 0.0015707015991210938\n",
      "Testing at step=9, batch=80, test loss = 0.3803351186632028, test acc = 0.8399999737739563, time = 0.0015978813171386719\n",
      "Step 9 finished in 16.323655605316162, Train loss = 0.41488398308193497, Test loss = 0.45123776578624586; Train Acc = 0.8559500005841255, Test Acc = 0.8421999996900559\n",
      "Training at step=10, batch=0, train loss = 0.4448896218884789, train acc = 0.8500000238418579, time = 0.00830984115600586\n",
      "Training at step=10, batch=120, train loss = 0.6128450514883765, train acc = 0.7799999713897705, time = 0.00824284553527832\n",
      "Training at step=10, batch=240, train loss = 0.3893809247290784, train acc = 0.8600000143051147, time = 0.008170366287231445\n",
      "Training at step=10, batch=360, train loss = 0.5860890376629866, train acc = 0.7599999904632568, time = 0.00833582878112793\n",
      "Training at step=10, batch=480, train loss = 0.510062452288967, train acc = 0.8199999928474426, time = 0.008243560791015625\n",
      "Testing at step=10, batch=0, test loss = 0.4761172259900186, test acc = 0.8399999737739563, time = 0.0015780925750732422\n",
      "Testing at step=10, batch=20, test loss = 0.47637549676419433, test acc = 0.8100000023841858, time = 0.0015676021575927734\n",
      "Testing at step=10, batch=40, test loss = 0.39981457780274354, test acc = 0.8299999833106995, time = 0.0016129016876220703\n",
      "Testing at step=10, batch=60, test loss = 0.31555159889898127, test acc = 0.8700000047683716, time = 0.001600027084350586\n",
      "Testing at step=10, batch=80, test loss = 0.34972161958485, test acc = 0.8999999761581421, time = 0.0016269683837890625\n",
      "Step 10 finished in 16.38085389137268, Train loss = 0.41353249836053935, Test loss = 0.44585788796957615; Train Acc = 0.8566833313306172, Test Acc = 0.8432999956607818\n",
      "Training at step=11, batch=0, train loss = 0.4821007034546612, train acc = 0.8600000143051147, time = 0.008311748504638672\n",
      "Training at step=11, batch=120, train loss = 0.3468526387872101, train acc = 0.8899999856948853, time = 0.008321285247802734\n",
      "Training at step=11, batch=240, train loss = 0.41412007472087553, train acc = 0.8299999833106995, time = 0.008181095123291016\n",
      "Training at step=11, batch=360, train loss = 0.5051528069638244, train acc = 0.8500000238418579, time = 0.008161783218383789\n",
      "Training at step=11, batch=480, train loss = 0.42167626833945543, train acc = 0.8600000143051147, time = 0.00831747055053711\n",
      "Testing at step=11, batch=0, test loss = 0.3421301291738657, test acc = 0.8700000047683716, time = 0.0017180442810058594\n",
      "Testing at step=11, batch=20, test loss = 0.3737172128474163, test acc = 0.8700000047683716, time = 0.0015692710876464844\n",
      "Testing at step=11, batch=40, test loss = 0.31619276197422796, test acc = 0.8999999761581421, time = 0.0016033649444580078\n",
      "Testing at step=11, batch=60, test loss = 0.40520569964733, test acc = 0.8500000238418579, time = 0.001558542251586914\n",
      "Testing at step=11, batch=80, test loss = 0.3749530826793573, test acc = 0.8500000238418579, time = 0.0015649795532226562\n",
      "Step 11 finished in 16.33412003517151, Train loss = 0.4088233507591385, Test loss = 0.44709694612122874; Train Acc = 0.8583833326896032, Test Acc = 0.8416000020503998\n",
      "Training at step=12, batch=0, train loss = 0.3308330454881368, train acc = 0.9200000166893005, time = 0.008368968963623047\n",
      "Training at step=12, batch=120, train loss = 0.5652058466304525, train acc = 0.7799999713897705, time = 0.00822138786315918\n",
      "Training at step=12, batch=240, train loss = 0.4974552925307535, train acc = 0.800000011920929, time = 0.008175373077392578\n",
      "Training at step=12, batch=360, train loss = 0.33683023947903123, train acc = 0.8700000047683716, time = 0.008253812789916992\n",
      "Training at step=12, batch=480, train loss = 0.48475954139686983, train acc = 0.8399999737739563, time = 0.008309364318847656\n",
      "Testing at step=12, batch=0, test loss = 0.40783377830960726, test acc = 0.8100000023841858, time = 0.0015778541564941406\n",
      "Testing at step=12, batch=20, test loss = 0.47404626923072807, test acc = 0.8299999833106995, time = 0.0016603469848632812\n",
      "Testing at step=12, batch=40, test loss = 0.411755536143348, test acc = 0.8500000238418579, time = 0.0015649795532226562\n",
      "Testing at step=12, batch=60, test loss = 0.520994651462511, test acc = 0.8299999833106995, time = 0.0015933513641357422\n",
      "Testing at step=12, batch=80, test loss = 0.3863402984029285, test acc = 0.8600000143051147, time = 0.0016057491302490234\n",
      "Step 12 finished in 16.265769481658936, Train loss = 0.40584988674752465, Test loss = 0.45087096249492165; Train Acc = 0.8594999985893568, Test Acc = 0.839699998497963\n",
      "Training at step=13, batch=0, train loss = 0.5956236837774095, train acc = 0.8100000023841858, time = 0.008299827575683594\n",
      "Training at step=13, batch=120, train loss = 0.3583460536355371, train acc = 0.8799999952316284, time = 0.008309125900268555\n",
      "Training at step=13, batch=240, train loss = 0.38666976070093356, train acc = 0.8500000238418579, time = 0.00817108154296875\n",
      "Training at step=13, batch=360, train loss = 0.4536201599269787, train acc = 0.8600000143051147, time = 0.008180856704711914\n",
      "Training at step=13, batch=480, train loss = 0.2936629612917794, train acc = 0.9100000262260437, time = 0.008203268051147461\n",
      "Testing at step=13, batch=0, test loss = 0.4036594382073331, test acc = 0.8299999833106995, time = 0.0016772747039794922\n",
      "Testing at step=13, batch=20, test loss = 0.5522390850724406, test acc = 0.8399999737739563, time = 0.0015745162963867188\n",
      "Testing at step=13, batch=40, test loss = 0.47570836330445915, test acc = 0.8500000238418579, time = 0.0015559196472167969\n",
      "Testing at step=13, batch=60, test loss = 0.4197503674378714, test acc = 0.8600000143051147, time = 0.0015578269958496094\n",
      "Testing at step=13, batch=80, test loss = 0.503192069214697, test acc = 0.8299999833106995, time = 0.001565694808959961\n",
      "Step 13 finished in 16.30500054359436, Train loss = 0.4033505810142347, Test loss = 0.4463709409901185; Train Acc = 0.8614666668574015, Test Acc = 0.8415999990701676\n",
      "Training at step=14, batch=0, train loss = 0.36056805565617805, train acc = 0.8700000047683716, time = 0.008346796035766602\n",
      "Training at step=14, batch=120, train loss = 0.28240523978749055, train acc = 0.8999999761581421, time = 0.008167266845703125\n",
      "Training at step=14, batch=240, train loss = 0.45738037003284854, train acc = 0.8399999737739563, time = 0.008286476135253906\n",
      "Training at step=14, batch=360, train loss = 0.5241879203362414, train acc = 0.8399999737739563, time = 0.008231878280639648\n",
      "Training at step=14, batch=480, train loss = 0.3447960377470436, train acc = 0.8999999761581421, time = 0.008215665817260742\n",
      "Testing at step=14, batch=0, test loss = 0.3929341815207006, test acc = 0.8799999952316284, time = 0.0015871524810791016\n",
      "Testing at step=14, batch=20, test loss = 0.3471005221551554, test acc = 0.8899999856948853, time = 0.0015692710876464844\n",
      "Testing at step=14, batch=40, test loss = 0.38933346383259093, test acc = 0.8799999952316284, time = 0.0015742778778076172\n",
      "Testing at step=14, batch=60, test loss = 0.5327985521853444, test acc = 0.8500000238418579, time = 0.0016067028045654297\n",
      "Testing at step=14, batch=80, test loss = 0.41633485805965614, test acc = 0.8299999833106995, time = 0.0016093254089355469\n",
      "Step 14 finished in 16.351501941680908, Train loss = 0.4020389145885073, Test loss = 0.44616095016774054; Train Acc = 0.8613166657090187, Test Acc = 0.841199997663498\n",
      "Training at step=15, batch=0, train loss = 0.4855686250394936, train acc = 0.8299999833106995, time = 0.008470535278320312\n",
      "Training at step=15, batch=120, train loss = 0.49935739694552056, train acc = 0.8100000023841858, time = 0.008179903030395508\n",
      "Training at step=15, batch=240, train loss = 0.2500289043931851, train acc = 0.9100000262260437, time = 0.008156299591064453\n",
      "Training at step=15, batch=360, train loss = 0.6214149320785645, train acc = 0.8100000023841858, time = 0.011501073837280273\n",
      "Training at step=15, batch=480, train loss = 0.29903756442929513, train acc = 0.8999999761581421, time = 0.008102178573608398\n",
      "Testing at step=15, batch=0, test loss = 0.3681293932527923, test acc = 0.8600000143051147, time = 0.0017247200012207031\n",
      "Testing at step=15, batch=20, test loss = 0.6758527647618859, test acc = 0.8100000023841858, time = 0.001577138900756836\n",
      "Testing at step=15, batch=40, test loss = 0.48277268429833997, test acc = 0.7900000214576721, time = 0.0016078948974609375\n",
      "Testing at step=15, batch=60, test loss = 0.42389930585532354, test acc = 0.8799999952316284, time = 0.0015707015991210938\n",
      "Testing at step=15, batch=80, test loss = 0.5841290857043293, test acc = 0.7799999713897705, time = 0.00159454345703125\n",
      "Step 15 finished in 16.32817792892456, Train loss = 0.39930673672730693, Test loss = 0.4427615080082741; Train Acc = 0.8619499995311102, Test Acc = 0.8433000004291534\n",
      "Training at step=16, batch=0, train loss = 0.3630646164069074, train acc = 0.8899999856948853, time = 0.008265495300292969\n",
      "Training at step=16, batch=120, train loss = 0.3536447150864215, train acc = 0.8899999856948853, time = 0.008148670196533203\n",
      "Training at step=16, batch=240, train loss = 0.3449266499953378, train acc = 0.8799999952316284, time = 0.008273601531982422\n",
      "Training at step=16, batch=360, train loss = 0.3342819975368064, train acc = 0.9100000262260437, time = 0.00816202163696289\n",
      "Training at step=16, batch=480, train loss = 0.3539657967316693, train acc = 0.8500000238418579, time = 0.008328437805175781\n",
      "Testing at step=16, batch=0, test loss = 0.5558324523431785, test acc = 0.8199999928474426, time = 0.0015671253204345703\n",
      "Testing at step=16, batch=20, test loss = 0.5041871688051114, test acc = 0.7900000214576721, time = 0.001573324203491211\n",
      "Testing at step=16, batch=40, test loss = 0.4214547968506942, test acc = 0.8600000143051147, time = 0.0015590190887451172\n",
      "Testing at step=16, batch=60, test loss = 0.3927164252353928, test acc = 0.8700000047683716, time = 0.0015549659729003906\n",
      "Testing at step=16, batch=80, test loss = 0.41366925811288874, test acc = 0.8500000238418579, time = 0.0015628337860107422\n",
      "Step 16 finished in 16.339972019195557, Train loss = 0.3981798736447315, Test loss = 0.4408723513357787; Train Acc = 0.8622166670362155, Test Acc = 0.8437999999523162\n",
      "Training at step=17, batch=0, train loss = 0.513972807084246, train acc = 0.8299999833106995, time = 0.008277416229248047\n",
      "Training at step=17, batch=120, train loss = 0.3194454647976628, train acc = 0.8999999761581421, time = 0.008269309997558594\n",
      "Training at step=17, batch=240, train loss = 0.38174202940903035, train acc = 0.9200000166893005, time = 0.008187294006347656\n",
      "Training at step=17, batch=360, train loss = 0.5931338243154357, train acc = 0.800000011920929, time = 0.008164405822753906\n",
      "Training at step=17, batch=480, train loss = 0.6380872652805768, train acc = 0.8500000238418579, time = 0.008205175399780273\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V7YYHs33saIx",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712093845925,
     "user_tz": -660,
     "elapsed": 953,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "928859bd-acd3-46ea-be13-39efadae9e50",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "plt.savefig(\"fashionmnist_simple_cnn.pdf\")\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"fashionmnist_simple_cnn.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
