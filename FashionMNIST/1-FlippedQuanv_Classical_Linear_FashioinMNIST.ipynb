{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100",
   "authorship_tag": "ABX9TyO9ycf2GLNwAh4wkdecvhq8"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ebnFsErY0EM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462301946,
     "user_tz": -660,
     "elapsed": 73092,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "710b1d81-a854-4328-959c-884f3b89fa61"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pennylane\n",
      "  Downloading PennyLane-0.35.1-py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m22.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting pennylane-lightning\n",
      "  Downloading PennyLane_Lightning-0.35.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.5/18.5 MB\u001B[0m \u001B[31m52.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting cotengra\n",
      "  Downloading cotengra-0.5.6-py3-none-any.whl (148 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m148.0/148.0 kB\u001B[0m \u001B[31m22.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting quimb\n",
      "  Downloading quimb-1.7.3-py3-none-any.whl (500 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m500.7/500.7 kB\u001B[0m \u001B[31m52.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting torchmetrics\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m841.5/841.5 kB\u001B[0m \u001B[31m71.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.25.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.11.4)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.2.1)\n",
      "Collecting rustworkx (from pennylane)\n",
      "  Downloading rustworkx-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m83.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
      "Collecting semantic-version>=2.7 (from pennylane)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting autoray>=0.6.1 (from pennylane)\n",
      "  Downloading autoray-0.6.9-py3-none-any.whl (49 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.8/49.8 kB\u001B[0m \u001B[31m8.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.10.0)\n",
      "Collecting pennylane-lightning-gpu (from pennylane-lightning)\n",
      "  Downloading PennyLane_Lightning_GPU-0.35.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.9/6.9 MB\u001B[0m \u001B[31m95.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting cytoolz>=0.8.0 (from quimb)\n",
      "  Downloading cytoolz-0.12.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m94.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numba>=0.39 in /usr/local/lib/python3.10/dist-packages (from quimb) (0.58.1)\n",
      "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from quimb) (5.9.5)\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.10/dist-packages (from quimb) (4.66.2)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.39->quimb) (0.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m70.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m823.6/823.6 kB\u001B[0m \u001B[31m73.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m107.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m731.7/731.7 MB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.6/410.6 MB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.6/121.6 MB\u001B[0m \u001B[31m14.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m30.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 MB\u001B[0m \u001B[31m12.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.0/196.0 MB\u001B[0m \u001B[31m8.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.0/166.0 MB\u001B[0m \u001B[31m6.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/99.1 kB\u001B[0m \u001B[31m16.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m87.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Installing collected packages: semantic-version, rustworkx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, cytoolz, autoray, nvidia-cusparse-cu12, nvidia-cudnn-cu12, cotengra, quimb, nvidia-cusolver-cu12, torchmetrics, pennylane-lightning, pennylane, pennylane-lightning-gpu\n",
      "Successfully installed autoray-0.6.9 cotengra-0.5.6 cytoolz-0.12.3 lightning-utilities-0.11.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pennylane-0.35.1 pennylane-lightning-0.35.1 pennylane-lightning-gpu-0.35.1 quimb-1.7.3 rustworkx-0.14.2 semantic-version-2.10.0 torchmetrics-1.3.2\n"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!pip install torch torchvision torchaudio pennylane pennylane-lightning pennylane-lightning[gpu] cotengra quimb torchmetrics --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VN2wD-U7bGse",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462309702,
     "user_tz": -660,
     "elapsed": 7760,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "5cf6575c-ec02-4d3d-b222-1464aa8c1914"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data"
   ],
   "metadata": {
    "id": "9WmQPQuLbSFw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUhQUP2dbTja",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462317565,
     "user_tz": -660,
     "elapsed": 7868,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "5be0db17-8633-45dd-cde0-19eaec483b24"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 26421880/26421880 [00:02<00:00, 9842984.70it/s] \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 166852.73it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4422102/4422102 [00:01<00:00, 3106938.54it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 8464240.29it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n",
      "tensor([4, 1, 7, 3, 5, 8, 2, 6, 4, 6, 6, 3, 6, 9, 9, 7, 8, 7, 7, 7, 2, 7, 3, 8,\n",
      "        8, 8, 2, 2, 8, 2, 1, 5, 8, 7, 8, 2, 1, 4, 8, 7, 0, 0, 8, 9, 7, 5, 3, 6,\n",
      "        9, 3, 9, 8, 7, 1, 8, 8, 0, 7, 3, 0, 6, 4, 5, 8])\n",
      "tensor([-1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j,  0.7176+0.j,  0.5686+0.j,  0.8588+0.j,  0.2235+0.j,  0.0902+0.j,\n",
      "         0.9059+0.j,  0.5529+0.j,  0.6000+0.j,  0.6078+0.j,  0.6392+0.j,  0.6314+0.j,\n",
      "         0.6157+0.j,  0.8039+0.j,  0.4039+0.j, -0.0039+0.j,  0.9922+0.j,  0.6314+0.j,\n",
      "         0.7490+0.j, -0.6235+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j], dtype=torch.complex64)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Some Utilities"
   ],
   "metadata": {
    "id": "kQI8WWY6byQq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "ket = {\n",
    "    '0':torch.tensor([1.,0.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '1':torch.tensor([0.,1.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '+':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) + torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device)),\n",
    "    '-':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) - torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device))\n",
    "}\n",
    "\n",
    "pauli = {\n",
    "    'I':torch.tensor([[1.,0.],[0.,1.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'X':torch.tensor([[0.,1.],[1.,0.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Y':torch.tensor([[0., -1.j],[1.j, 0]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Z':torch.tensor([[1.,0.],[0.,-1.]], dtype = COMPLEX_DTYPE, device=device)\n",
    "}\n",
    "\n",
    "def tensor_product(*args):\n",
    "  input_list = [a for a in args]\n",
    "  return functools.reduce(torch.kron, input_list)\n",
    "\n",
    "def multi_qubit_identity(n_qubits:int):\n",
    "  assert n_qubits>0\n",
    "  if n_qubits == 1:\n",
    "    return pauli['I']\n",
    "  else:\n",
    "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
    "\n",
    "pauli_words_su4 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
    "      #print(key1, key2)\n",
    "      #print(pauli[key1])\n",
    "      #print(pauli[key2])\n",
    "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
    "      #print(pauli_words_su4[key1+key2])\n",
    "\n",
    "pauli_words_su8 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      if not key1+key2+key3 == 'III':\n",
    "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
    "\n",
    "pauli_words_su16 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        if not key1+key2+key3+key4 == 'IIII':\n",
    "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
    "              pauli[key1],\n",
    "              pauli[key2],\n",
    "              pauli[key3],\n",
    "              pauli[key4]\n",
    "          )\n",
    "\n",
    "pauli_words_su32 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        for key5 in pauli.keys():\n",
    "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
    "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
    "                pauli[key1],\n",
    "                pauli[key2],\n",
    "                pauli[key3],\n",
    "                pauli[key4],\n",
    "                pauli[key5]\n",
    "            )\n",
    "\n",
    "def su32_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = list(pauli_words_su32.values())\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, torch.stack(dict_values))\n",
    "  #print(generator.shape)\n",
    "  return torch.matrix_exp(1j*generator)\n",
    "\n",
    "def su4_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = torch.stack(list(pauli_words_su4.values()))\n",
    "  #print(dict_values.shape)\n",
    "  generator = torch.einsum(\"i, ijk -> jk\", params, dict_values)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "test_params = torch.randn(4**2-1,  device=device).type(COMPLEX_DTYPE)\n",
    "print(test_params.shape)\n",
    "test_op = su4_op(test_params)\n",
    "print(test_op)\n",
    "print(qml.is_unitary(qml.QubitUnitary(test_op.cpu().numpy(), wires=[0,1])))\n",
    "\n",
    "rx = torch.linalg.matrix_exp(1j*torch.pi*pauli[\"X\"]/2)\n",
    "print(qml.is_unitary(qml.QubitUnitary(rx.cpu().numpy(), wires=[0])))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7B4DTncWbwQl",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320391,
     "user_tz": -660,
     "elapsed": 2829,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "72ab9fd9-1a03-484b-e92a-147788cf05aa"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([15])\n",
      "tensor([[ 0.5814+0.5563j, -0.2624+0.0584j, -0.2256-0.0349j, -0.3977-0.2645j],\n",
      "        [-0.1473+0.0662j,  0.5740+0.3966j, -0.6925-0.0374j,  0.0056-0.0791j],\n",
      "        [ 0.2395-0.4834j,  0.1500-0.2045j,  0.0092-0.1729j,  0.0740-0.7805j],\n",
      "        [ 0.0519-0.1811j, -0.4827-0.3790j, -0.6611+0.0019j,  0.3540+0.1599j]],\n",
      "       device='cuda:0', dtype=torch.complex64)\n",
      "False\n",
      "True\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def measure_sv(\n",
    "    state,\n",
    "    observable\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Measure a statevector with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not\n",
    "  \"\"\"\n",
    "  expectation_value = torch.conj(state)@observable@state\n",
    "  return torch.real(expectation_value)\n",
    "\n",
    "def measure_dm(\n",
    "    rho,\n",
    "    observable\n",
    "):\n",
    "  \"\"\"\n",
    "  Measure a density matrix with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not.\n",
    "  \"\"\"\n",
    "  product = torch.matmul(rho, observable)\n",
    "\n",
    "  # Calculate the trace, which is the sum of diagonal elements\n",
    "  trace = torch.trace(product)\n",
    "\n",
    "  # The expectation value should be real for physical observables\n",
    "  return torch.real(trace)\n",
    "\n",
    "vmap_measure_sv = torch.vmap(measure_sv, in_dims=(None, 0), out_dims=0)\n",
    "vmap_measure_dm = torch.vmap(measure_dm, in_dims=(None, 0), out_dims=0)\n",
    "\n",
    "# assuming the input patch observables (hermitianized) has shape (batchsize,n_patches, c, h, w)\n",
    "# assuming the input set statevectors has shape (c, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_sv_batched_ob = torch.vmap(measure_sv, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "# assuming the input set desnity matrices has shape (batchsize, n_patches, c, 2**n, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_dm_batched_ob = torch.vmap(measure_dm, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "def bitstring_to_state(bitstring:str):\n",
    "  \"\"\"\n",
    "  Convert a bit string, like '0101001' or '+-+-101'\n",
    "  to a statevector. Each character in the bitstring must be among\n",
    "  0, 1, + and -\n",
    "  \"\"\"\n",
    "  assert len(bitstring)>0\n",
    "  for c in bitstring:\n",
    "    assert c in ['0', '1', '+', '-']\n",
    "  single_qubit_states = [ket[c] for c in bitstring]\n",
    "  return tensor_product(*single_qubit_states)\n",
    "\n",
    "\n",
    "#test_patch = torch.randn(size=[5, 2, 3, 4,4], dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_herm_patch = (torch.einsum(\"...jk->...kj\", test_patch)+test_patch)/2\n",
    "#print(test_herm_patch)\n",
    "#print(\"all patches shape\", test_herm_patch.shape)\n",
    "#test_sv = torch.stack([bitstring_to_state('++')]*3, axis = 0)\n",
    "#print(test_sv)\n",
    "#print(\"all sv shape\", test_sv.shape)\n",
    "#res = vmap_measure_channel_sv_batched_ob(test_sv, test_herm_patch)\n",
    "#print(res)\n",
    "#print(\"res shape\",res.shape)\n",
    "\n",
    "#for batchid in range(test_patch.shape[0]):\n",
    "#  batchitem = test_patch[batchid]\n",
    "#  for patch_idx in range(batchitem.shape[0]):\n",
    "#    patch = batchitem[patch_idx]\n",
    "#    for ch_idx in range(patch.shape[0]):\n",
    "#      print(f\"batchid={batchid}; patchid = {patch_idx}; chid = {ch_idx}\")\n",
    "#      ch = patch[ch_idx]\n",
    "#      #print(ch.shape)\n",
    "#      sv_ch = test_sv[ch_idx]\n",
    "#      #print(sv_ch.shape)\n",
    "#      print(measure_sv(sv_ch, ch))\n"
   ],
   "metadata": {
    "id": "Xs0c2F1eBnGc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320391,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flipped Quanvolution Kernel\n",
    "\n",
    "First, let's define the function to extract patches for a single kernel."
   ],
   "metadata": {
    "id": "fFZqT6bpElaL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_patches(image, patch_size, stride, padding=None):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image with multiple input channels and optional custom padding.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (in_channels, height, width).\n",
    "        patch_size (int): Size of the square patches to extract.\n",
    "        stride (int): Stride between patches.\n",
    "        padding (tuple): Padding value(s) for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n",
    "    pad_l, pad_r, pad_t, pad_b = padding if padding is not None else (0,0,0,0)\n",
    "\n",
    "    if padding is not None:\n",
    "        image = torch.nn.functional.pad(image, (pad_l, pad_r, pad_t, pad_b), mode='constant')\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    height, width = image.shape[-2],  image.shape[-1]\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = image[..., i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    patches = torch.stack(patches, dim=-4)\n",
    "    return patches # has shape (batchsize, n_patches, channel, h, w)\n",
    "\n",
    "\n",
    "#single_img = torch.stack([torch.arange(32*32*1,dtype = torch.double, device=device).reshape((32,32)).type(torch.cdouble)]*3)\n",
    "\n",
    "#test_img = torch.stack([single_img]*2)\n",
    "#print(test_img[0][...,:4,:4])\n",
    "#patches = extract_patches(test_img, patch_size=4, stride=2,padding=(0,0,0,0))\n",
    "#print(patches.shape)\n",
    "#print(patches[0].shape)\n",
    "#print(patches[0][0])\n"
   ],
   "metadata": {
    "id": "He4HdMRHC7T6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320391,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    }
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Three-by-Three Quanv Kernel Function\n",
    "\n",
    "For 3 by 3 patches, we only need a 2-qubit parameterised state, generated by the SU4 gate applied to the $|00\\rangle$ state. The first thing to do is to pad the 3 by 3 patches to 4 by 4. However, for input with size\n",
    "\n",
    "$$\n",
    "D_1\\times H_1 \\times W_1\n",
    "$$\n",
    "\n",
    "where $D_1$ is the number of input channels, we will need $D_1$ different two-qubit states, generated by $D_1$ different parameterised circuits. These circuits will produce $D_1$ expectation values. Suppose each circuit is parameterised by $\\theta_i, i\\in \\{1,2,\\cdots, D_1\\}$, the parameterised (pure) state can be written as\n",
    "\n",
    "$$\n",
    "|\\psi (\\theta_i)⟩ = SU4(\\theta_i)|00⟩\n",
    "$$\n",
    "\n",
    "For a channel $M_i$ of patch $M$ with shape $4\\times 4$, the expectation value is calculated by\n",
    "\n",
    "$$\n",
    "⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$\n",
    "\n",
    "Then, a single pixel value of the output channel corresponding this single kernel is\n",
    "\n",
    "$$\n",
    "\\frac{1}{D_1}\\sum_{i=1}^{D_1}⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$"
   ],
   "metadata": {
    "id": "9hRqflooEqKN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_2q_param_state(theta):\n",
    "  state = bitstring_to_state('00')\n",
    "  state = torch.matmul(su4_op(theta), state)\n",
    "  return state\n",
    "\n",
    "vmap_generate_2q_param_state = torch.vmap(generate_2q_param_state, in_dims=0, out_dims = 0)\n",
    "\n",
    "def single_kernel_op_over_batched_patches(thetas, patch):\n",
    "  # patch has shape (c_in, h, w)\n",
    "  # thetas has shape (c_in, 4^2-1) for SU4 gates\n",
    "  n_theta = thetas.shape[-2]\n",
    "  n_channel = patch.shape[-3]\n",
    "  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n",
    "  states = vmap_generate_2q_param_state(thetas)\n",
    "  #print(\"States shape\", states.shape)\n",
    "  patch = torch.nn.functional.pad(patch, (0, 1, 0, 1), mode='constant')\n",
    "  patch_t = torch.einsum(\"...jk->...kj\", patch)\n",
    "  herm_patch = (patch_t+patch)/2 # has dim (batchsize, num_patches, c, h, w)\n",
    "  #print(\"Herm patch shape\", herm_patch.shape)\n",
    "  channel_out = vmap_measure_channel_sv_batched_ob(states, herm_patch) # has dim (batchsize,n_patches, c)\n",
    "  return torch.sum(channel_out, axis = -1)/n_theta # has dim (batchszie, n_patches)\n",
    "\n",
    "\n",
    "#test_params = torch.randn((1,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_img = dummy_x.to(device)#.type(torch.cdouble)\n",
    "#print(test_img.shape)\n",
    "#test_patches = extract_patches(test_img, patch_size=3, stride=1,padding=(1,1,1,1))\n",
    "#print(test_patches.shape)\n",
    "#print(test_params.shape)\n",
    "#print(vmap_generate_2q_param_state(test_params))\n",
    "#test_out = single_kernel_op_over_batched_patches(test_params, test_patches)\n",
    "#print(test_out.shape)\n",
    "#h = (32-3+1*2)//1 +1\n",
    "#h**2\n"
   ],
   "metadata": {
    "id": "Yzn4KEt5ErG7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320391,
     "user_tz": -660,
     "elapsed": 5,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multiple Output Channels"
   ],
   "metadata": {
    "id": "A4BXEKd8I67_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# For multiple channel output\n",
    "# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n",
    "vmap_vmap_single_kernel_op_through_extracted_patches = torch.vmap(single_kernel_op_over_batched_patches, in_dims=(0, None), out_dims=-2) # output has dim (batchsize, c_out, n_patches)\n",
    "\n",
    "#c_in = 1\n",
    "#c_out = 2\n",
    "\n",
    "#test_params2 = torch.randn((c_out, c_in,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_out2 = vmap_vmap_single_kernel_op_through_extracted_patches(test_params2, test_patches)\n",
    "#print(test_out2.shape)\n",
    "#test_out_2_features = test_out2.reshape((-1,c_out, 32, 32))\n",
    "#print(test_out_2_features.shape)\n"
   ],
   "metadata": {
    "id": "72vkHV_BI80l",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320392,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pytorch Module for Three-by-Three Flipped Quanv"
   ],
   "metadata": {
    "id": "NuZ6G5-sKK_J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FlippedQuanv3x3(torch.nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, padding):\n",
    "    super(FlippedQuanv3x3, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.pad_l, self.pad_r, self.pad_t, self.pad_b = padding if padding is not None else (0,0,0,0)\n",
    "    self.pad = (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n",
    "    self.weight= torch.nn.Parameter(torch.randn((out_channels, in_channels, 4**2-1)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn((out_channels, 1)).type(COMPLEX_DTYPE))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batchsize ,c_in, h, w)\n",
    "    # weight has shape (c_out, c_in, 15)\n",
    "    # bias has shape (c_out, 1)\n",
    "    x = x.type(COMPLEX_DTYPE)\n",
    "    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n",
    "    h_out = (h_in-3+(self.pad_t+self.pad_b))//self.stride +1\n",
    "    w_out = (w_in-3+(self.pad_l+self.pad_r))//self.stride +1\n",
    "\n",
    "    #print(self.weight.shape)\n",
    "    #print(patches.shape)\n",
    "\n",
    "    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n",
    "\n",
    "\n",
    "    out = out + self.bias\n",
    "    return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n",
    "\n",
    "#test_module = FlippedQuanv3x3(in_channels=1, out_channels=2, stride=1, padding=(1,1,1,1)).to(device)\n",
    "#print(dummy_x.shape)\n",
    "#test_out = test_module(dummy_x.to(device))\n",
    "#print(test_out.shape)"
   ],
   "metadata": {
    "id": "Gww_XdJ5KPJt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320392,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Flipped Quanvolution Replacing Conv2d\n",
    "\n",
    "First, let's just replace `Conv2d` with `FlippedQuanv3x3`."
   ],
   "metadata": {
    "id": "1yumZTjDVu63"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class HybridNet(torch.nn.Module):\n",
    "  class HybridNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HybridNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(16*28*28, 10).type(REAL_DTYPE)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "net = HybridNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4RC5ou-VzbE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711462320392,
     "user_tz": -660,
     "elapsed": 6,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "f2e92dd2-63f4-4ed1-d191-6b016304911e"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "HybridNet(\n",
      "  (layers): Sequential(\n",
      "    (0): FlippedQuanv3x3()\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): FlippedQuanv3x3()\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=28800, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-4-79ba4bf7b279>:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n",
      "<ipython-input-9-30705f998bf7>:30: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:301.)\n",
      "  return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torchmetrics\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.2\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4ZY59q1WS4x",
    "outputId": "0e7cbfcd-1078-4c56-c675-1afbfa385c85",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711488461440,
     "user_tz": -660,
     "elapsed": 18075888,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 600, Number of test batches = 100\n",
      "Print every train batch = 120, Print every test batch = 20\n"
     ]
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-79ba4bf7b279>:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training at step=0, batch=0, train loss = 2.3099093437194824, train acc = 0.1899999976158142, time = 0.5304265022277832\n",
      "Training at step=0, batch=120, train loss = 0.9131782054901123, train acc = 0.6000000238418579, time = 0.3844726085662842\n",
      "Training at step=0, batch=240, train loss = 0.7171719074249268, train acc = 0.7699999809265137, time = 0.38014936447143555\n",
      "Training at step=0, batch=360, train loss = 0.7265015244483948, train acc = 0.699999988079071, time = 0.3768734931945801\n",
      "Training at step=0, batch=480, train loss = 0.6448343396186829, train acc = 0.7400000095367432, time = 0.37972187995910645\n",
      "Testing at step=0, batch=0, test loss = 0.5709424018859863, test acc = 0.7799999713897705, time = 0.13343358039855957\n",
      "Testing at step=0, batch=20, test loss = 0.7080667018890381, test acc = 0.699999988079071, time = 0.13454556465148926\n",
      "Testing at step=0, batch=40, test loss = 0.7220212817192078, test acc = 0.6800000071525574, time = 0.13560795783996582\n",
      "Testing at step=0, batch=60, test loss = 0.8250802755355835, test acc = 0.6399999856948853, time = 0.1338646411895752\n",
      "Testing at step=0, batch=80, test loss = 0.6380612254142761, test acc = 0.7599999904632568, time = 0.13400721549987793\n",
      "Step 0 finished in 258.97042059898376, Train loss = 0.8647514878710111, Test loss = 0.8022860366106034; Train Acc = 0.6915333330010375, Test Acc = 0.6765000003576279\n",
      "Training at step=1, batch=0, train loss = 0.7150142788887024, train acc = 0.7200000286102295, time = 0.3821752071380615\n",
      "Training at step=1, batch=120, train loss = 0.5841301083564758, train acc = 0.8100000023841858, time = 0.37785935401916504\n",
      "Training at step=1, batch=240, train loss = 0.7324406504631042, train acc = 0.6899999976158142, time = 0.3783862590789795\n",
      "Training at step=1, batch=360, train loss = 0.5687245726585388, train acc = 0.7599999904632568, time = 0.3797037601470947\n",
      "Training at step=1, batch=480, train loss = 0.5003746747970581, train acc = 0.800000011920929, time = 0.38269686698913574\n",
      "Testing at step=1, batch=0, test loss = 0.6186163425445557, test acc = 0.7799999713897705, time = 0.13443517684936523\n",
      "Testing at step=1, batch=20, test loss = 0.837675929069519, test acc = 0.7799999713897705, time = 0.13487529754638672\n",
      "Testing at step=1, batch=40, test loss = 0.7030200958251953, test acc = 0.800000011920929, time = 0.13584494590759277\n",
      "Testing at step=1, batch=60, test loss = 0.6866734027862549, test acc = 0.7699999809265137, time = 0.13533473014831543\n",
      "Testing at step=1, batch=80, test loss = 0.6646737456321716, test acc = 0.7599999904632568, time = 0.13573360443115234\n",
      "Step 1 finished in 259.5324628353119, Train loss = 0.6377490668992202, Test loss = 0.7467227858304978; Train Acc = 0.7733833327889442, Test Acc = 0.7563999980688095\n",
      "Training at step=2, batch=0, train loss = 0.7477589249610901, train acc = 0.7799999713897705, time = 0.3807055950164795\n",
      "Training at step=2, batch=120, train loss = 0.5512592196464539, train acc = 0.7799999713897705, time = 0.3782045841217041\n",
      "Training at step=2, batch=240, train loss = 0.6108764410018921, train acc = 0.8100000023841858, time = 0.379589319229126\n",
      "Training at step=2, batch=360, train loss = 0.468187153339386, train acc = 0.8299999833106995, time = 0.3790879249572754\n",
      "Training at step=2, batch=480, train loss = 0.6274173259735107, train acc = 0.7400000095367432, time = 0.382434606552124\n",
      "Testing at step=2, batch=0, test loss = 0.7053954601287842, test acc = 0.7400000095367432, time = 0.13521599769592285\n",
      "Testing at step=2, batch=20, test loss = 0.6664872765541077, test acc = 0.7900000214576721, time = 0.1362595558166504\n",
      "Testing at step=2, batch=40, test loss = 0.7168490886688232, test acc = 0.7400000095367432, time = 0.1460871696472168\n",
      "Testing at step=2, batch=60, test loss = 0.6091920137405396, test acc = 0.7699999809265137, time = 0.13581180572509766\n",
      "Testing at step=2, batch=80, test loss = 0.6216840147972107, test acc = 0.7799999713897705, time = 0.13499236106872559\n",
      "Step 2 finished in 260.0060524940491, Train loss = 0.5867162327468395, Test loss = 0.5836331140995026; Train Acc = 0.7909833308060964, Test Acc = 0.7943999969959259\n",
      "Training at step=3, batch=0, train loss = 0.5490286350250244, train acc = 0.7799999713897705, time = 0.38490748405456543\n",
      "Training at step=3, batch=120, train loss = 0.5555674433708191, train acc = 0.8299999833106995, time = 0.3791799545288086\n",
      "Training at step=3, batch=240, train loss = 0.6110271215438843, train acc = 0.7699999809265137, time = 0.3772294521331787\n",
      "Training at step=3, batch=360, train loss = 0.5207403302192688, train acc = 0.8299999833106995, time = 0.3825051784515381\n",
      "Training at step=3, batch=480, train loss = 0.6374667882919312, train acc = 0.7900000214576721, time = 0.37932515144348145\n",
      "Testing at step=3, batch=0, test loss = 0.6235318183898926, test acc = 0.8100000023841858, time = 0.13515520095825195\n",
      "Testing at step=3, batch=20, test loss = 0.7601084113121033, test acc = 0.7200000286102295, time = 0.13467192649841309\n",
      "Testing at step=3, batch=40, test loss = 0.6225614547729492, test acc = 0.7699999809265137, time = 0.13500142097473145\n",
      "Testing at step=3, batch=60, test loss = 0.6093103289604187, test acc = 0.7599999904632568, time = 0.1336836814880371\n",
      "Testing at step=3, batch=80, test loss = 0.6743859052658081, test acc = 0.7200000286102295, time = 0.13501644134521484\n",
      "Step 3 finished in 260.70165061950684, Train loss = 0.5555858819186688, Test loss = 0.6950980460643769; Train Acc = 0.80338333239158, Test Acc = 0.7364000010490418\n",
      "Training at step=4, batch=0, train loss = 0.6316978335380554, train acc = 0.8199999928474426, time = 0.37949180603027344\n",
      "Training at step=4, batch=120, train loss = 0.5863181948661804, train acc = 0.8199999928474426, time = 0.3811628818511963\n",
      "Training at step=4, batch=240, train loss = 0.43827423453330994, train acc = 0.8299999833106995, time = 0.37954258918762207\n",
      "Training at step=4, batch=360, train loss = 0.4623251259326935, train acc = 0.8100000023841858, time = 0.3770484924316406\n",
      "Training at step=4, batch=480, train loss = 0.552360475063324, train acc = 0.8199999928474426, time = 0.37656402587890625\n",
      "Testing at step=4, batch=0, test loss = 0.5748783349990845, test acc = 0.7599999904632568, time = 0.1336650848388672\n",
      "Testing at step=4, batch=20, test loss = 0.6584646701812744, test acc = 0.7699999809265137, time = 0.13534259796142578\n",
      "Testing at step=4, batch=40, test loss = 0.5314327478408813, test acc = 0.8399999737739563, time = 0.13340258598327637\n",
      "Testing at step=4, batch=60, test loss = 0.43917152285575867, test acc = 0.8500000238418579, time = 0.13353323936462402\n",
      "Testing at step=4, batch=80, test loss = 0.6841673851013184, test acc = 0.75, time = 0.13479304313659668\n",
      "Step 4 finished in 259.66228461265564, Train loss = 0.5294058462480704, Test loss = 0.5722171422839165; Train Acc = 0.8124833307663599, Test Acc = 0.8011999976634979\n",
      "Training at step=5, batch=0, train loss = 0.5800684094429016, train acc = 0.8199999928474426, time = 0.37940311431884766\n",
      "Training at step=5, batch=120, train loss = 0.6130601763725281, train acc = 0.7400000095367432, time = 0.3767235279083252\n",
      "Training at step=5, batch=240, train loss = 0.5415660738945007, train acc = 0.8399999737739563, time = 0.37694334983825684\n",
      "Training at step=5, batch=360, train loss = 0.4115709960460663, train acc = 0.8399999737739563, time = 0.3759801387786865\n",
      "Training at step=5, batch=480, train loss = 0.4300839602947235, train acc = 0.8299999833106995, time = 0.3775346279144287\n",
      "Testing at step=5, batch=0, test loss = 0.6710283160209656, test acc = 0.7699999809265137, time = 0.13521504402160645\n",
      "Testing at step=5, batch=20, test loss = 0.7617027759552002, test acc = 0.7599999904632568, time = 0.13459348678588867\n",
      "Testing at step=5, batch=40, test loss = 0.6426591277122498, test acc = 0.8100000023841858, time = 0.13466119766235352\n",
      "Testing at step=5, batch=60, test loss = 0.5710330605506897, test acc = 0.8100000023841858, time = 0.13341665267944336\n",
      "Testing at step=5, batch=80, test loss = 0.4238244295120239, test acc = 0.8500000238418579, time = 0.13536310195922852\n",
      "Step 5 finished in 258.8854138851166, Train loss = 0.5172822632888953, Test loss = 0.5953640493750573; Train Acc = 0.8159333294630051, Test Acc = 0.7957999986410141\n",
      "Training at step=6, batch=0, train loss = 0.5072881579399109, train acc = 0.8100000023841858, time = 0.37932896614074707\n",
      "Training at step=6, batch=120, train loss = 0.47097477316856384, train acc = 0.8500000238418579, time = 0.37984561920166016\n",
      "Training at step=6, batch=240, train loss = 0.7200169563293457, train acc = 0.75, time = 0.3787987232208252\n",
      "Training at step=6, batch=360, train loss = 0.44157928228378296, train acc = 0.8600000143051147, time = 0.3777782917022705\n",
      "Training at step=6, batch=480, train loss = 0.6426893472671509, train acc = 0.8100000023841858, time = 0.37761378288269043\n",
      "Testing at step=6, batch=0, test loss = 0.5614600777626038, test acc = 0.7699999809265137, time = 0.1371619701385498\n",
      "Testing at step=6, batch=20, test loss = 0.5478984713554382, test acc = 0.800000011920929, time = 0.1357734203338623\n",
      "Testing at step=6, batch=40, test loss = 0.6282706260681152, test acc = 0.7599999904632568, time = 0.1352987289428711\n",
      "Testing at step=6, batch=60, test loss = 0.5946729183197021, test acc = 0.8399999737739563, time = 0.1416792869567871\n",
      "Testing at step=6, batch=80, test loss = 0.5976877808570862, test acc = 0.8100000023841858, time = 0.13481688499450684\n",
      "Step 6 finished in 259.4722533226013, Train loss = 0.4979228669901689, Test loss = 0.5124561586976051; Train Acc = 0.8220166660348575, Test Acc = 0.8217999976873398\n",
      "Training at step=7, batch=0, train loss = 0.37860846519470215, train acc = 0.8399999737739563, time = 0.3794679641723633\n",
      "Training at step=7, batch=120, train loss = 0.49339616298675537, train acc = 0.7799999713897705, time = 0.3768489360809326\n",
      "Training at step=7, batch=240, train loss = 0.5027524828910828, train acc = 0.8100000023841858, time = 0.37607717514038086\n",
      "Training at step=7, batch=360, train loss = 0.5930866003036499, train acc = 0.7900000214576721, time = 0.37699437141418457\n",
      "Training at step=7, batch=480, train loss = 0.49601227045059204, train acc = 0.8299999833106995, time = 0.3778727054595947\n",
      "Testing at step=7, batch=0, test loss = 0.44454723596572876, test acc = 0.8399999737739563, time = 0.13634204864501953\n",
      "Testing at step=7, batch=20, test loss = 0.43129467964172363, test acc = 0.8199999928474426, time = 0.13412046432495117\n",
      "Testing at step=7, batch=40, test loss = 0.5909662842750549, test acc = 0.75, time = 0.13458251953125\n",
      "Testing at step=7, batch=60, test loss = 0.5013935565948486, test acc = 0.800000011920929, time = 0.13556194305419922\n",
      "Testing at step=7, batch=80, test loss = 0.5482416749000549, test acc = 0.8299999833106995, time = 0.13553738594055176\n",
      "Step 7 finished in 259.1272974014282, Train loss = 0.4894125632445018, Test loss = 0.5081170713901519; Train Acc = 0.8274999969204266, Test Acc = 0.8160999983549118\n",
      "Training at step=8, batch=0, train loss = 0.4271857738494873, train acc = 0.8600000143051147, time = 0.3782496452331543\n",
      "Training at step=8, batch=120, train loss = 0.6835058331489563, train acc = 0.75, time = 0.3756582736968994\n",
      "Training at step=8, batch=240, train loss = 0.5843337774276733, train acc = 0.8600000143051147, time = 0.38901400566101074\n",
      "Training at step=8, batch=360, train loss = 0.48730412125587463, train acc = 0.7900000214576721, time = 0.37653517723083496\n",
      "Training at step=8, batch=480, train loss = 0.6632142663002014, train acc = 0.7799999713897705, time = 0.3805809020996094\n",
      "Testing at step=8, batch=0, test loss = 0.38589614629745483, test acc = 0.8600000143051147, time = 0.13483095169067383\n",
      "Testing at step=8, batch=20, test loss = 0.5917578339576721, test acc = 0.7699999809265137, time = 0.13671398162841797\n",
      "Testing at step=8, batch=40, test loss = 0.5391764044761658, test acc = 0.8199999928474426, time = 0.13448023796081543\n",
      "Testing at step=8, batch=60, test loss = 0.5151537656784058, test acc = 0.7699999809265137, time = 0.13565373420715332\n",
      "Testing at step=8, batch=80, test loss = 0.46614035964012146, test acc = 0.8199999928474426, time = 0.13492584228515625\n",
      "Step 8 finished in 259.5841760635376, Train loss = 0.48160201345880826, Test loss = 0.5344431373476982; Train Acc = 0.8294333322842916, Test Acc = 0.808400000333786\n",
      "Training at step=9, batch=0, train loss = 0.45079419016838074, train acc = 0.8199999928474426, time = 0.3783082962036133\n",
      "Training at step=9, batch=120, train loss = 0.7166883945465088, train acc = 0.75, time = 0.38263368606567383\n",
      "Training at step=9, batch=240, train loss = 0.4454595148563385, train acc = 0.8100000023841858, time = 0.3778724670410156\n",
      "Training at step=9, batch=360, train loss = 0.3380490839481354, train acc = 0.8999999761581421, time = 0.37946271896362305\n",
      "Training at step=9, batch=480, train loss = 0.4022110402584076, train acc = 0.8299999833106995, time = 0.38227200508117676\n",
      "Testing at step=9, batch=0, test loss = 0.3855353891849518, test acc = 0.8299999833106995, time = 0.1355118751525879\n",
      "Testing at step=9, batch=20, test loss = 0.45201531052589417, test acc = 0.8399999737739563, time = 0.1356503963470459\n",
      "Testing at step=9, batch=40, test loss = 0.5462039709091187, test acc = 0.8600000143051147, time = 0.13785696029663086\n",
      "Testing at step=9, batch=60, test loss = 0.4646918773651123, test acc = 0.8600000143051147, time = 0.1359541416168213\n",
      "Testing at step=9, batch=80, test loss = 0.4366120398044586, test acc = 0.8700000047683716, time = 0.13518738746643066\n",
      "Step 9 finished in 259.7424635887146, Train loss = 0.48229666002094745, Test loss = 0.4795279896259308; Train Acc = 0.8293666642904282, Test Acc = 0.8335999995470047\n",
      "Training at step=10, batch=0, train loss = 0.2909407913684845, train acc = 0.9300000071525574, time = 0.3775961399078369\n",
      "Training at step=10, batch=120, train loss = 0.5605250000953674, train acc = 0.7799999713897705, time = 0.3799588680267334\n",
      "Training at step=10, batch=240, train loss = 0.4787982702255249, train acc = 0.800000011920929, time = 0.37791919708251953\n",
      "Training at step=10, batch=360, train loss = 0.3074929714202881, train acc = 0.8899999856948853, time = 0.38750529289245605\n",
      "Training at step=10, batch=480, train loss = 0.2654365301132202, train acc = 0.9300000071525574, time = 0.38268518447875977\n",
      "Testing at step=10, batch=0, test loss = 0.34216058254241943, test acc = 0.8999999761581421, time = 0.13483166694641113\n",
      "Testing at step=10, batch=20, test loss = 0.5349552631378174, test acc = 0.800000011920929, time = 0.1356804370880127\n",
      "Testing at step=10, batch=40, test loss = 0.7020632028579712, test acc = 0.8399999737739563, time = 0.13544964790344238\n",
      "Testing at step=10, batch=60, test loss = 0.5087511539459229, test acc = 0.8399999737739563, time = 0.135422945022583\n",
      "Testing at step=10, batch=80, test loss = 0.4689352810382843, test acc = 0.8500000238418579, time = 0.13547205924987793\n",
      "Step 10 finished in 259.78471994400024, Train loss = 0.4713065932194392, Test loss = 0.48946734607219694; Train Acc = 0.8347666642069816, Test Acc = 0.8284999966621399\n",
      "Training at step=11, batch=0, train loss = 0.33429840207099915, train acc = 0.9100000262260437, time = 0.383908748626709\n",
      "Training at step=11, batch=120, train loss = 0.3833058476448059, train acc = 0.8500000238418579, time = 0.3819761276245117\n",
      "Training at step=11, batch=240, train loss = 0.43273964524269104, train acc = 0.8600000143051147, time = 0.37799501419067383\n",
      "Training at step=11, batch=360, train loss = 0.4028995931148529, train acc = 0.8199999928474426, time = 0.37984800338745117\n",
      "Training at step=11, batch=480, train loss = 0.59173983335495, train acc = 0.7400000095367432, time = 0.37981700897216797\n",
      "Testing at step=11, batch=0, test loss = 0.64872145652771, test acc = 0.7099999785423279, time = 0.13570570945739746\n",
      "Testing at step=11, batch=20, test loss = 0.314136803150177, test acc = 0.9200000166893005, time = 0.13494157791137695\n",
      "Testing at step=11, batch=40, test loss = 0.47070202231407166, test acc = 0.8600000143051147, time = 0.1354830265045166\n",
      "Testing at step=11, batch=60, test loss = 0.4670569598674774, test acc = 0.8399999737739563, time = 0.13531208038330078\n",
      "Testing at step=11, batch=80, test loss = 0.42647784948349, test acc = 0.8299999833106995, time = 0.136094331741333\n",
      "Step 11 finished in 260.77054023742676, Train loss = 0.4586696199079355, Test loss = 0.4648131810128689; Train Acc = 0.8386833318074545, Test Acc = 0.8379999953508377\n",
      "Training at step=12, batch=0, train loss = 0.4951760768890381, train acc = 0.8100000023841858, time = 0.3867349624633789\n",
      "Training at step=12, batch=120, train loss = 0.4792225658893585, train acc = 0.8399999737739563, time = 0.37923097610473633\n",
      "Training at step=12, batch=240, train loss = 0.5728765726089478, train acc = 0.8399999737739563, time = 0.3796262741088867\n",
      "Training at step=12, batch=360, train loss = 0.41035595536231995, train acc = 0.8399999737739563, time = 0.38128018379211426\n",
      "Training at step=12, batch=480, train loss = 0.4296886920928955, train acc = 0.8299999833106995, time = 0.38123607635498047\n",
      "Testing at step=12, batch=0, test loss = 0.49553894996643066, test acc = 0.8199999928474426, time = 0.13584208488464355\n",
      "Testing at step=12, batch=20, test loss = 0.3281649351119995, test acc = 0.8899999856948853, time = 0.13538408279418945\n",
      "Testing at step=12, batch=40, test loss = 0.35903969407081604, test acc = 0.8700000047683716, time = 0.13506793975830078\n",
      "Testing at step=12, batch=60, test loss = 0.38622406125068665, test acc = 0.8700000047683716, time = 0.13775157928466797\n",
      "Testing at step=12, batch=80, test loss = 0.4605112373828888, test acc = 0.8299999833106995, time = 0.13581418991088867\n",
      "Step 12 finished in 260.53449726104736, Train loss = 0.45336222092310585, Test loss = 0.4735830169916153; Train Acc = 0.840533331533273, Test Acc = 0.8353000003099441\n",
      "Training at step=13, batch=0, train loss = 0.6780464053153992, train acc = 0.7599999904632568, time = 0.37955236434936523\n",
      "Training at step=13, batch=120, train loss = 0.3879048228263855, train acc = 0.8899999856948853, time = 0.3783905506134033\n",
      "Training at step=13, batch=240, train loss = 0.5360437035560608, train acc = 0.7900000214576721, time = 0.3859548568725586\n",
      "Training at step=13, batch=360, train loss = 0.4061960279941559, train acc = 0.8399999737739563, time = 0.37781453132629395\n",
      "Training at step=13, batch=480, train loss = 0.5997781157493591, train acc = 0.8199999928474426, time = 0.3790171146392822\n",
      "Testing at step=13, batch=0, test loss = 0.4023768901824951, test acc = 0.8700000047683716, time = 0.13393568992614746\n",
      "Testing at step=13, batch=20, test loss = 0.3364564776420593, test acc = 0.8700000047683716, time = 0.13564109802246094\n",
      "Testing at step=13, batch=40, test loss = 0.4426390826702118, test acc = 0.8799999952316284, time = 0.13452386856079102\n",
      "Testing at step=13, batch=60, test loss = 0.5617955923080444, test acc = 0.7699999809265137, time = 0.13381338119506836\n",
      "Testing at step=13, batch=80, test loss = 0.5740595459938049, test acc = 0.8100000023841858, time = 0.13410282135009766\n",
      "Step 13 finished in 259.0851311683655, Train loss = 0.4501879225174586, Test loss = 0.47463049784302713; Train Acc = 0.8424999992052714, Test Acc = 0.8327999943494797\n",
      "Training at step=14, batch=0, train loss = 0.5543322563171387, train acc = 0.8199999928474426, time = 0.3787038326263428\n",
      "Training at step=14, batch=120, train loss = 0.3519977927207947, train acc = 0.8500000238418579, time = 0.3763577938079834\n",
      "Training at step=14, batch=240, train loss = 0.4590955376625061, train acc = 0.8500000238418579, time = 0.3778834342956543\n",
      "Training at step=14, batch=360, train loss = 0.3949764668941498, train acc = 0.8600000143051147, time = 0.3841876983642578\n",
      "Training at step=14, batch=480, train loss = 0.556097149848938, train acc = 0.7900000214576721, time = 0.3766136169433594\n",
      "Testing at step=14, batch=0, test loss = 0.459224671125412, test acc = 0.8299999833106995, time = 0.13491559028625488\n",
      "Testing at step=14, batch=20, test loss = 0.37706875801086426, test acc = 0.8799999952316284, time = 0.13388872146606445\n",
      "Testing at step=14, batch=40, test loss = 0.33870139718055725, test acc = 0.8700000047683716, time = 0.13498306274414062\n",
      "Testing at step=14, batch=60, test loss = 0.5898422002792358, test acc = 0.7699999809265137, time = 0.13455510139465332\n",
      "Testing at step=14, batch=80, test loss = 0.4248470067977905, test acc = 0.7900000214576721, time = 0.14150428771972656\n",
      "Step 14 finished in 259.39928340911865, Train loss = 0.4463068679968516, Test loss = 0.445670545399189; Train Acc = 0.8426833315690359, Test Acc = 0.8445999997854233\n",
      "Training at step=15, batch=0, train loss = 0.474150687456131, train acc = 0.8600000143051147, time = 0.3784337043762207\n",
      "Training at step=15, batch=120, train loss = 0.39007556438446045, train acc = 0.8299999833106995, time = 0.37870311737060547\n",
      "Training at step=15, batch=240, train loss = 0.429399311542511, train acc = 0.8299999833106995, time = 0.37778401374816895\n",
      "Training at step=15, batch=360, train loss = 0.3830248713493347, train acc = 0.8799999952316284, time = 0.3780057430267334\n",
      "Training at step=15, batch=480, train loss = 0.5899600386619568, train acc = 0.8299999833106995, time = 0.3753964900970459\n",
      "Testing at step=15, batch=0, test loss = 0.5125386118888855, test acc = 0.7599999904632568, time = 0.13578367233276367\n",
      "Testing at step=15, batch=20, test loss = 0.45977628231048584, test acc = 0.8899999856948853, time = 0.13512611389160156\n",
      "Testing at step=15, batch=40, test loss = 0.4193955957889557, test acc = 0.8700000047683716, time = 0.136688232421875\n",
      "Testing at step=15, batch=60, test loss = 0.5687820911407471, test acc = 0.7799999713897705, time = 0.13568806648254395\n",
      "Testing at step=15, batch=80, test loss = 0.5324152112007141, test acc = 0.7599999904632568, time = 0.13541889190673828\n",
      "Step 15 finished in 259.5443603992462, Train loss = 0.43904622380932173, Test loss = 0.4712468421459198; Train Acc = 0.8455833317836126, Test Acc = 0.8328999942541122\n",
      "Training at step=16, batch=0, train loss = 0.44624924659729004, train acc = 0.8600000143051147, time = 0.3788008689880371\n",
      "Training at step=16, batch=120, train loss = 0.5263512134552002, train acc = 0.8199999928474426, time = 0.37818408012390137\n",
      "Training at step=16, batch=240, train loss = 0.3321104049682617, train acc = 0.8899999856948853, time = 0.3809483051300049\n",
      "Training at step=16, batch=360, train loss = 0.33578139543533325, train acc = 0.8999999761581421, time = 0.3791487216949463\n",
      "Training at step=16, batch=480, train loss = 0.4897617697715759, train acc = 0.8100000023841858, time = 0.3773469924926758\n",
      "Testing at step=16, batch=0, test loss = 0.36366507411003113, test acc = 0.8799999952316284, time = 0.13495302200317383\n",
      "Testing at step=16, batch=20, test loss = 0.555280327796936, test acc = 0.7900000214576721, time = 0.13532114028930664\n",
      "Testing at step=16, batch=40, test loss = 0.43088290095329285, test acc = 0.8600000143051147, time = 0.13495564460754395\n",
      "Testing at step=16, batch=60, test loss = 0.47789591550827026, test acc = 0.8199999928474426, time = 0.1454911231994629\n",
      "Testing at step=16, batch=80, test loss = 0.4635511636734009, test acc = 0.8500000238418579, time = 0.1344287395477295\n",
      "Step 16 finished in 259.99296402931213, Train loss = 0.43283038906753063, Test loss = 0.4657055994868278; Train Acc = 0.8468166673183442, Test Acc = 0.8378999978303909\n",
      "Training at step=17, batch=0, train loss = 0.5012831687927246, train acc = 0.8199999928474426, time = 0.3904993534088135\n",
      "Training at step=17, batch=120, train loss = 0.5645466446876526, train acc = 0.8199999928474426, time = 0.3788444995880127\n",
      "Training at step=17, batch=240, train loss = 0.480586975812912, train acc = 0.8500000238418579, time = 0.38122057914733887\n",
      "Training at step=17, batch=360, train loss = 0.4083501398563385, train acc = 0.8600000143051147, time = 0.3791067600250244\n",
      "Training at step=17, batch=480, train loss = 0.4263829290866852, train acc = 0.8500000238418579, time = 0.37956786155700684\n",
      "Testing at step=17, batch=0, test loss = 0.5118359923362732, test acc = 0.8199999928474426, time = 0.13497114181518555\n",
      "Testing at step=17, batch=20, test loss = 0.4345303475856781, test acc = 0.8600000143051147, time = 0.1346883773803711\n",
      "Testing at step=17, batch=40, test loss = 0.49671638011932373, test acc = 0.8100000023841858, time = 0.13583660125732422\n",
      "Testing at step=17, batch=60, test loss = 0.5374794006347656, test acc = 0.8399999737739563, time = 0.1354351043701172\n",
      "Testing at step=17, batch=80, test loss = 0.39210250973701477, test acc = 0.8600000143051147, time = 0.13466835021972656\n",
      "Step 17 finished in 260.1416687965393, Train loss = 0.4222144723931948, Test loss = 0.4558478957414627; Train Acc = 0.8519666662812233, Test Acc = 0.8412999993562699\n",
      "Training at step=18, batch=0, train loss = 0.4192633032798767, train acc = 0.8299999833106995, time = 0.3802027702331543\n",
      "Training at step=18, batch=120, train loss = 0.3285883963108063, train acc = 0.8799999952316284, time = 0.378812313079834\n",
      "Training at step=18, batch=240, train loss = 0.49760329723358154, train acc = 0.8100000023841858, time = 0.37758588790893555\n",
      "Training at step=18, batch=360, train loss = 0.3822387754917145, train acc = 0.8899999856948853, time = 0.3791937828063965\n",
      "Training at step=18, batch=480, train loss = 0.5186500549316406, train acc = 0.8199999928474426, time = 0.3821556568145752\n",
      "Testing at step=18, batch=0, test loss = 0.4709737300872803, test acc = 0.8299999833106995, time = 0.13360810279846191\n",
      "Testing at step=18, batch=20, test loss = 0.534518837928772, test acc = 0.8100000023841858, time = 0.1340184211730957\n",
      "Testing at step=18, batch=40, test loss = 0.4358287453651428, test acc = 0.8399999737739563, time = 0.13592243194580078\n",
      "Testing at step=18, batch=60, test loss = 0.4374149441719055, test acc = 0.8700000047683716, time = 0.13335633277893066\n",
      "Testing at step=18, batch=80, test loss = 0.5517134070396423, test acc = 0.8100000023841858, time = 0.13437104225158691\n",
      "Step 18 finished in 259.9562015533447, Train loss = 0.424800414070487, Test loss = 0.4597003373503685; Train Acc = 0.851599998474121, Test Acc = 0.8382999992370606\n",
      "Training at step=19, batch=0, train loss = 0.439373254776001, train acc = 0.8299999833106995, time = 0.38135218620300293\n",
      "Training at step=19, batch=120, train loss = 0.46014127135276794, train acc = 0.8299999833106995, time = 0.3774266242980957\n",
      "Training at step=19, batch=240, train loss = 0.2798857092857361, train acc = 0.8899999856948853, time = 0.37496185302734375\n",
      "Training at step=19, batch=360, train loss = 0.4902353286743164, train acc = 0.75, time = 0.3966031074523926\n",
      "Training at step=19, batch=480, train loss = 0.48797789216041565, train acc = 0.8199999928474426, time = 0.3778054714202881\n",
      "Testing at step=19, batch=0, test loss = 0.4745144248008728, test acc = 0.8500000238418579, time = 0.13453412055969238\n",
      "Testing at step=19, batch=20, test loss = 0.4492601156234741, test acc = 0.8399999737739563, time = 0.13589191436767578\n",
      "Testing at step=19, batch=40, test loss = 0.47339820861816406, test acc = 0.8199999928474426, time = 0.13759541511535645\n",
      "Testing at step=19, batch=60, test loss = 0.397068589925766, test acc = 0.8600000143051147, time = 0.13486099243164062\n",
      "Testing at step=19, batch=80, test loss = 0.352781742811203, test acc = 0.8399999737739563, time = 0.1345200538635254\n",
      "Step 19 finished in 259.1411418914795, Train loss = 0.4185967243462801, Test loss = 0.4267291177809238; Train Acc = 0.8533833321928977, Test Acc = 0.8504999983310699\n",
      "Training at step=20, batch=0, train loss = 0.4858812093734741, train acc = 0.8500000238418579, time = 0.3790409564971924\n",
      "Training at step=20, batch=120, train loss = 0.4338948428630829, train acc = 0.8299999833106995, time = 0.37886905670166016\n",
      "Training at step=20, batch=240, train loss = 0.43435120582580566, train acc = 0.8700000047683716, time = 0.3810231685638428\n",
      "Training at step=20, batch=360, train loss = 0.3505955934524536, train acc = 0.8500000238418579, time = 0.3898789882659912\n",
      "Training at step=20, batch=480, train loss = 0.4441487491130829, train acc = 0.8299999833106995, time = 0.3759951591491699\n",
      "Testing at step=20, batch=0, test loss = 0.42804285883903503, test acc = 0.8600000143051147, time = 0.13518524169921875\n",
      "Testing at step=20, batch=20, test loss = 0.6278377771377563, test acc = 0.7900000214576721, time = 0.1349959373474121\n",
      "Testing at step=20, batch=40, test loss = 0.7026203274726868, test acc = 0.7699999809265137, time = 0.13520407676696777\n",
      "Testing at step=20, batch=60, test loss = 0.39726948738098145, test acc = 0.8399999737739563, time = 0.13656902313232422\n",
      "Testing at step=20, batch=80, test loss = 0.34987834095954895, test acc = 0.8299999833106995, time = 0.1339101791381836\n",
      "Step 20 finished in 259.3617730140686, Train loss = 0.4064790006975333, Test loss = 0.4211385589838028; Train Acc = 0.8574833324551583, Test Acc = 0.8529000008106231\n",
      "Training at step=21, batch=0, train loss = 0.37185460329055786, train acc = 0.8500000238418579, time = 0.37961626052856445\n",
      "Training at step=21, batch=120, train loss = 0.33939188718795776, train acc = 0.8899999856948853, time = 0.37766075134277344\n",
      "Training at step=21, batch=240, train loss = 0.4302109181880951, train acc = 0.8799999952316284, time = 0.3770461082458496\n",
      "Training at step=21, batch=360, train loss = 0.4805751442909241, train acc = 0.8399999737739563, time = 0.378511905670166\n",
      "Training at step=21, batch=480, train loss = 0.4800766110420227, train acc = 0.8399999737739563, time = 0.3771049976348877\n",
      "Testing at step=21, batch=0, test loss = 0.44838860630989075, test acc = 0.8199999928474426, time = 0.1347064971923828\n",
      "Testing at step=21, batch=20, test loss = 0.4042438864707947, test acc = 0.8799999952316284, time = 0.13427519798278809\n",
      "Testing at step=21, batch=40, test loss = 0.39927658438682556, test acc = 0.8700000047683716, time = 0.14151978492736816\n",
      "Testing at step=21, batch=60, test loss = 0.4292772710323334, test acc = 0.8799999952316284, time = 0.13536953926086426\n",
      "Testing at step=21, batch=80, test loss = 0.5864701271057129, test acc = 0.8100000023841858, time = 0.13514423370361328\n",
      "Step 21 finished in 259.2244212627411, Train loss = 0.40176769805451235, Test loss = 0.4287607136368752; Train Acc = 0.8587833336989085, Test Acc = 0.8522000008821488\n",
      "Training at step=22, batch=0, train loss = 0.41419517993927, train acc = 0.8299999833106995, time = 0.38217830657958984\n",
      "Training at step=22, batch=120, train loss = 0.5308970212936401, train acc = 0.8500000238418579, time = 0.3794269561767578\n",
      "Training at step=22, batch=240, train loss = 0.3241756856441498, train acc = 0.9100000262260437, time = 0.3818833827972412\n",
      "Training at step=22, batch=360, train loss = 0.6193210482597351, train acc = 0.8199999928474426, time = 0.3958930969238281\n",
      "Training at step=22, batch=480, train loss = 0.34728968143463135, train acc = 0.8799999952316284, time = 0.37824034690856934\n",
      "Testing at step=22, batch=0, test loss = 0.2911255657672882, test acc = 0.9100000262260437, time = 0.1353590488433838\n",
      "Testing at step=22, batch=20, test loss = 0.4229722321033478, test acc = 0.8799999952316284, time = 0.13552618026733398\n",
      "Testing at step=22, batch=40, test loss = 0.3726186454296112, test acc = 0.8700000047683716, time = 0.13576936721801758\n",
      "Testing at step=22, batch=60, test loss = 0.43751224875450134, test acc = 0.8799999952316284, time = 0.13422369956970215\n",
      "Testing at step=22, batch=80, test loss = 0.29978325963020325, test acc = 0.8899999856948853, time = 0.13428020477294922\n",
      "Step 22 finished in 260.3115756511688, Train loss = 0.40297635066012544, Test loss = 0.4215789729356766; Train Acc = 0.8583499989906946, Test Acc = 0.8565999972820282\n",
      "Training at step=23, batch=0, train loss = 0.371443510055542, train acc = 0.8799999952316284, time = 0.37883806228637695\n",
      "Training at step=23, batch=120, train loss = 0.4339536726474762, train acc = 0.8500000238418579, time = 0.3787119388580322\n",
      "Training at step=23, batch=240, train loss = 0.2598712146282196, train acc = 0.9100000262260437, time = 0.37851929664611816\n",
      "Training at step=23, batch=360, train loss = 0.2931010127067566, train acc = 0.8600000143051147, time = 0.37868475914001465\n",
      "Training at step=23, batch=480, train loss = 0.2396807223558426, train acc = 0.9200000166893005, time = 0.37880587577819824\n",
      "Testing at step=23, batch=0, test loss = 0.3930480480194092, test acc = 0.8799999952316284, time = 0.13556599617004395\n",
      "Testing at step=23, batch=20, test loss = 0.42600956559181213, test acc = 0.8299999833106995, time = 0.13499855995178223\n",
      "Testing at step=23, batch=40, test loss = 0.3915000259876251, test acc = 0.8299999833106995, time = 0.13455700874328613\n",
      "Testing at step=23, batch=60, test loss = 0.2786686420440674, test acc = 0.8899999856948853, time = 0.1354055404663086\n",
      "Testing at step=23, batch=80, test loss = 0.4551179111003876, test acc = 0.8500000238418579, time = 0.1400299072265625\n",
      "Step 23 finished in 260.1056275367737, Train loss = 0.395465431958437, Test loss = 0.41997229486703874; Train Acc = 0.8608666661381722, Test Acc = 0.8557999974489212\n",
      "Training at step=24, batch=0, train loss = 0.4659728705883026, train acc = 0.7900000214576721, time = 0.37821030616760254\n",
      "Training at step=24, batch=120, train loss = 0.32269200682640076, train acc = 0.8899999856948853, time = 0.380312442779541\n",
      "Training at step=24, batch=240, train loss = 0.5097367167472839, train acc = 0.8399999737739563, time = 0.376147985458374\n",
      "Training at step=24, batch=360, train loss = 0.3630516827106476, train acc = 0.8799999952316284, time = 0.37815022468566895\n",
      "Training at step=24, batch=480, train loss = 0.40318217873573303, train acc = 0.8999999761581421, time = 0.3771176338195801\n",
      "Testing at step=24, batch=0, test loss = 0.3561691343784332, test acc = 0.8600000143051147, time = 0.13460040092468262\n",
      "Testing at step=24, batch=20, test loss = 0.3221254050731659, test acc = 0.9300000071525574, time = 0.13390469551086426\n",
      "Testing at step=24, batch=40, test loss = 0.3310258984565735, test acc = 0.9100000262260437, time = 0.1343550682067871\n",
      "Testing at step=24, batch=60, test loss = 0.35019972920417786, test acc = 0.8799999952316284, time = 0.1332383155822754\n",
      "Testing at step=24, batch=80, test loss = 0.46820229291915894, test acc = 0.8399999737739563, time = 0.13445639610290527\n",
      "Step 24 finished in 259.5055441856384, Train loss = 0.39314843515555065, Test loss = 0.43285061374306677; Train Acc = 0.8625499999523163, Test Acc = 0.8514999997615814\n",
      "Training at step=25, batch=0, train loss = 0.40051841735839844, train acc = 0.8799999952316284, time = 0.38955140113830566\n",
      "Training at step=25, batch=120, train loss = 0.33488112688064575, train acc = 0.8799999952316284, time = 0.3773362636566162\n",
      "Training at step=25, batch=240, train loss = 0.31675463914871216, train acc = 0.9200000166893005, time = 0.381237268447876\n",
      "Training at step=25, batch=360, train loss = 0.3433540463447571, train acc = 0.8700000047683716, time = 0.3773508071899414\n",
      "Training at step=25, batch=480, train loss = 0.372099369764328, train acc = 0.8299999833106995, time = 0.3814733028411865\n",
      "Testing at step=25, batch=0, test loss = 0.46545425057411194, test acc = 0.8899999856948853, time = 0.1361072063446045\n",
      "Testing at step=25, batch=20, test loss = 0.38422104716300964, test acc = 0.8199999928474426, time = 0.13640713691711426\n",
      "Testing at step=25, batch=40, test loss = 0.5144835114479065, test acc = 0.8199999928474426, time = 0.1347670555114746\n",
      "Testing at step=25, batch=60, test loss = 0.3821033537387848, test acc = 0.8600000143051147, time = 0.13476991653442383\n",
      "Testing at step=25, batch=80, test loss = 0.43892231583595276, test acc = 0.8799999952316284, time = 0.13424015045166016\n",
      "Step 25 finished in 259.686794757843, Train loss = 0.38749450974166394, Test loss = 0.4205031280219555; Train Acc = 0.8621166658401489, Test Acc = 0.8491999977827072\n",
      "Training at step=26, batch=0, train loss = 0.6059116125106812, train acc = 0.7799999713897705, time = 0.380129337310791\n",
      "Training at step=26, batch=120, train loss = 0.4539090394973755, train acc = 0.8399999737739563, time = 0.3801558017730713\n",
      "Training at step=26, batch=240, train loss = 0.4432983696460724, train acc = 0.8600000143051147, time = 0.37703824043273926\n",
      "Training at step=26, batch=360, train loss = 0.442209929227829, train acc = 0.8600000143051147, time = 0.3768460750579834\n",
      "Training at step=26, batch=480, train loss = 0.41349491477012634, train acc = 0.8399999737739563, time = 0.3768172264099121\n",
      "Testing at step=26, batch=0, test loss = 0.4355138838291168, test acc = 0.8199999928474426, time = 0.13599705696105957\n",
      "Testing at step=26, batch=20, test loss = 0.5352392196655273, test acc = 0.8399999737739563, time = 0.13497018814086914\n",
      "Testing at step=26, batch=40, test loss = 0.43130794167518616, test acc = 0.8299999833106995, time = 0.13437366485595703\n",
      "Testing at step=26, batch=60, test loss = 0.35669228434562683, test acc = 0.8299999833106995, time = 0.13495540618896484\n",
      "Testing at step=26, batch=80, test loss = 0.3664640784263611, test acc = 0.8700000047683716, time = 0.13410401344299316\n",
      "Step 26 finished in 259.0142488479614, Train loss = 0.3826939463367065, Test loss = 0.3956183472275734; Train Acc = 0.8663999999562899, Test Acc = 0.8618000000715256\n",
      "Training at step=27, batch=0, train loss = 0.5173717737197876, train acc = 0.8199999928474426, time = 0.37900614738464355\n",
      "Training at step=27, batch=120, train loss = 0.3014902174472809, train acc = 0.8999999761581421, time = 0.37742137908935547\n",
      "Training at step=27, batch=240, train loss = 0.37720540165901184, train acc = 0.8999999761581421, time = 0.37619543075561523\n",
      "Training at step=27, batch=360, train loss = 0.4395238161087036, train acc = 0.8100000023841858, time = 0.3788282871246338\n",
      "Training at step=27, batch=480, train loss = 0.3760349750518799, train acc = 0.8399999737739563, time = 0.3791086673736572\n",
      "Testing at step=27, batch=0, test loss = 0.3278391659259796, test acc = 0.8999999761581421, time = 0.13405847549438477\n",
      "Testing at step=27, batch=20, test loss = 0.2481163740158081, test acc = 0.9200000166893005, time = 0.1342782974243164\n",
      "Testing at step=27, batch=40, test loss = 0.5056278109550476, test acc = 0.8299999833106995, time = 0.13621306419372559\n",
      "Testing at step=27, batch=60, test loss = 0.46278902888298035, test acc = 0.8500000238418579, time = 0.13479089736938477\n",
      "Testing at step=27, batch=80, test loss = 0.3645220100879669, test acc = 0.8999999761581421, time = 0.13401269912719727\n",
      "Step 27 finished in 258.9713354110718, Train loss = 0.37311103704075016, Test loss = 0.39815741285681727; Train Acc = 0.8703166660666466, Test Acc = 0.8606999963521957\n",
      "Training at step=28, batch=0, train loss = 0.3788224458694458, train acc = 0.8899999856948853, time = 0.38193178176879883\n",
      "Training at step=28, batch=120, train loss = 0.3701545000076294, train acc = 0.8799999952316284, time = 0.3853151798248291\n",
      "Training at step=28, batch=240, train loss = 0.4066201448440552, train acc = 0.8600000143051147, time = 0.3761587142944336\n",
      "Training at step=28, batch=360, train loss = 0.321105033159256, train acc = 0.8899999856948853, time = 0.37586045265197754\n",
      "Training at step=28, batch=480, train loss = 0.29807835817337036, train acc = 0.9100000262260437, time = 0.39020609855651855\n",
      "Testing at step=28, batch=0, test loss = 0.34873807430267334, test acc = 0.9200000166893005, time = 0.13586974143981934\n",
      "Testing at step=28, batch=20, test loss = 0.36540716886520386, test acc = 0.8500000238418579, time = 0.13364100456237793\n",
      "Testing at step=28, batch=40, test loss = 0.40470659732818604, test acc = 0.8500000238418579, time = 0.13454699516296387\n",
      "Testing at step=28, batch=60, test loss = 0.30253270268440247, test acc = 0.9100000262260437, time = 0.13387298583984375\n",
      "Testing at step=28, batch=80, test loss = 0.36041060090065, test acc = 0.9200000166893005, time = 0.13525676727294922\n",
      "Step 28 finished in 259.13751554489136, Train loss = 0.36910304812093575, Test loss = 0.39562213912606237; Train Acc = 0.8712999992569288, Test Acc = 0.8616000020503998\n",
      "Training at step=29, batch=0, train loss = 0.35839012265205383, train acc = 0.8799999952316284, time = 0.37981700897216797\n",
      "Training at step=29, batch=120, train loss = 0.34891846776008606, train acc = 0.8600000143051147, time = 0.37755823135375977\n",
      "Training at step=29, batch=240, train loss = 0.48536932468414307, train acc = 0.8600000143051147, time = 0.375960111618042\n",
      "Training at step=29, batch=360, train loss = 0.4652400314807892, train acc = 0.8799999952316284, time = 0.37646007537841797\n",
      "Training at step=29, batch=480, train loss = 0.36300021409988403, train acc = 0.8700000047683716, time = 0.3768129348754883\n",
      "Testing at step=29, batch=0, test loss = 0.3536869287490845, test acc = 0.8799999952316284, time = 0.13541793823242188\n",
      "Testing at step=29, batch=20, test loss = 0.3898342549800873, test acc = 0.8700000047683716, time = 0.1401515007019043\n",
      "Testing at step=29, batch=40, test loss = 0.2554669678211212, test acc = 0.9300000071525574, time = 0.13547945022583008\n",
      "Testing at step=29, batch=60, test loss = 0.26538923382759094, test acc = 0.8899999856948853, time = 0.13335537910461426\n",
      "Testing at step=29, batch=80, test loss = 0.4618337154388428, test acc = 0.8500000238418579, time = 0.13401246070861816\n",
      "Step 29 finished in 258.54960012435913, Train loss = 0.36472676366567613, Test loss = 0.3861203584074974; Train Acc = 0.8705000000198683, Test Acc = 0.8669000017642975\n",
      "Training at step=30, batch=0, train loss = 0.2533334195613861, train acc = 0.9100000262260437, time = 0.3804621696472168\n",
      "Training at step=30, batch=120, train loss = 0.3973270654678345, train acc = 0.8700000047683716, time = 0.3770308494567871\n",
      "Training at step=30, batch=240, train loss = 0.2791951894760132, train acc = 0.9300000071525574, time = 0.37632179260253906\n",
      "Training at step=30, batch=360, train loss = 0.35332682728767395, train acc = 0.8899999856948853, time = 0.37874341011047363\n",
      "Training at step=30, batch=480, train loss = 0.37111273407936096, train acc = 0.8999999761581421, time = 0.38929295539855957\n",
      "Testing at step=30, batch=0, test loss = 0.36705002188682556, test acc = 0.8600000143051147, time = 0.13440155982971191\n",
      "Testing at step=30, batch=20, test loss = 0.4333793520927429, test acc = 0.8500000238418579, time = 0.1366887092590332\n",
      "Testing at step=30, batch=40, test loss = 0.19554010033607483, test acc = 0.9399999976158142, time = 0.13412809371948242\n",
      "Testing at step=30, batch=60, test loss = 0.46377110481262207, test acc = 0.8500000238418579, time = 0.13633489608764648\n",
      "Testing at step=30, batch=80, test loss = 0.22162221372127533, test acc = 0.9100000262260437, time = 0.1347217559814453\n",
      "Step 30 finished in 258.7915620803833, Train loss = 0.3583957617978255, Test loss = 0.38967552125453947; Train Acc = 0.8747666656970978, Test Acc = 0.8646000015735626\n",
      "Training at step=31, batch=0, train loss = 0.30968376994132996, train acc = 0.8399999737739563, time = 0.3785107135772705\n",
      "Training at step=31, batch=120, train loss = 0.27825072407722473, train acc = 0.8799999952316284, time = 0.37804746627807617\n",
      "Training at step=31, batch=240, train loss = 0.355864018201828, train acc = 0.8899999856948853, time = 0.38103532791137695\n",
      "Training at step=31, batch=360, train loss = 0.2898363173007965, train acc = 0.8799999952316284, time = 0.37903380393981934\n",
      "Training at step=31, batch=480, train loss = 0.24500365555286407, train acc = 0.8999999761581421, time = 0.37726855278015137\n",
      "Testing at step=31, batch=0, test loss = 0.5327228307723999, test acc = 0.7799999713897705, time = 0.14822983741760254\n",
      "Testing at step=31, batch=20, test loss = 0.35130682587623596, test acc = 0.8700000047683716, time = 0.13576579093933105\n",
      "Testing at step=31, batch=40, test loss = 0.45935627818107605, test acc = 0.8399999737739563, time = 0.1333620548248291\n",
      "Testing at step=31, batch=60, test loss = 0.4777710735797882, test acc = 0.8399999737739563, time = 0.134385347366333\n",
      "Testing at step=31, batch=80, test loss = 0.291997492313385, test acc = 0.8600000143051147, time = 0.13378643989562988\n",
      "Step 31 finished in 259.3541326522827, Train loss = 0.3593619877845049, Test loss = 0.4329265658557415; Train Acc = 0.8742500009139379, Test Acc = 0.8488999968767166\n",
      "Training at step=32, batch=0, train loss = 0.2908521592617035, train acc = 0.8799999952316284, time = 0.37752318382263184\n",
      "Training at step=32, batch=120, train loss = 0.422560453414917, train acc = 0.8500000238418579, time = 0.37729978561401367\n",
      "Training at step=32, batch=240, train loss = 0.28581753373146057, train acc = 0.9200000166893005, time = 0.38150715827941895\n",
      "Training at step=32, batch=360, train loss = 0.3236488103866577, train acc = 0.8899999856948853, time = 0.37834906578063965\n",
      "Training at step=32, batch=480, train loss = 0.3064449727535248, train acc = 0.8899999856948853, time = 0.38362908363342285\n",
      "Testing at step=32, batch=0, test loss = 0.635043740272522, test acc = 0.75, time = 0.13438987731933594\n",
      "Testing at step=32, batch=20, test loss = 0.32090115547180176, test acc = 0.8999999761581421, time = 0.1353919506072998\n",
      "Testing at step=32, batch=40, test loss = 0.2675286829471588, test acc = 0.8999999761581421, time = 0.13385725021362305\n",
      "Testing at step=32, batch=60, test loss = 0.5080978870391846, test acc = 0.800000011920929, time = 0.13452577590942383\n",
      "Testing at step=32, batch=80, test loss = 0.33607715368270874, test acc = 0.8700000047683716, time = 0.13512754440307617\n",
      "Step 32 finished in 259.371906042099, Train loss = 0.3625171992679437, Test loss = 0.434891482591629; Train Acc = 0.8728500008583069, Test Acc = 0.8412999993562699\n",
      "Training at step=33, batch=0, train loss = 0.276083379983902, train acc = 0.9100000262260437, time = 0.3791792392730713\n",
      "Training at step=33, batch=120, train loss = 0.3549717366695404, train acc = 0.8700000047683716, time = 0.37682414054870605\n",
      "Training at step=33, batch=240, train loss = 0.25511035323143005, train acc = 0.8999999761581421, time = 0.3796658515930176\n",
      "Training at step=33, batch=360, train loss = 0.416916161775589, train acc = 0.8500000238418579, time = 0.37868690490722656\n",
      "Training at step=33, batch=480, train loss = 0.4129808843135834, train acc = 0.8600000143051147, time = 0.3771650791168213\n",
      "Testing at step=33, batch=0, test loss = 0.566489577293396, test acc = 0.8299999833106995, time = 0.13508248329162598\n",
      "Testing at step=33, batch=20, test loss = 0.4812045395374298, test acc = 0.8199999928474426, time = 0.13429689407348633\n",
      "Testing at step=33, batch=40, test loss = 0.31358209252357483, test acc = 0.9100000262260437, time = 0.13584423065185547\n",
      "Testing at step=33, batch=60, test loss = 0.3376113176345825, test acc = 0.8899999856948853, time = 0.13422727584838867\n",
      "Testing at step=33, batch=80, test loss = 0.47126874327659607, test acc = 0.8299999833106995, time = 0.13477110862731934\n",
      "Step 33 finished in 259.48306608200073, Train loss = 0.35340881372491517, Test loss = 0.3969969998300076; Train Acc = 0.8755000019073487, Test Acc = 0.8619999974966049\n",
      "Training at step=34, batch=0, train loss = 0.41140642762184143, train acc = 0.8700000047683716, time = 0.3802220821380615\n",
      "Training at step=34, batch=120, train loss = 0.2511126399040222, train acc = 0.9399999976158142, time = 0.38819074630737305\n",
      "Training at step=34, batch=240, train loss = 0.5052704811096191, train acc = 0.800000011920929, time = 0.3765430450439453\n",
      "Training at step=34, batch=360, train loss = 0.36195385456085205, train acc = 0.8700000047683716, time = 0.3770761489868164\n",
      "Training at step=34, batch=480, train loss = 0.3199003040790558, train acc = 0.8799999952316284, time = 0.37938737869262695\n",
      "Testing at step=34, batch=0, test loss = 0.3036949038505554, test acc = 0.9100000262260437, time = 0.13803362846374512\n",
      "Testing at step=34, batch=20, test loss = 0.3296951651573181, test acc = 0.8799999952316284, time = 0.13506627082824707\n",
      "Testing at step=34, batch=40, test loss = 0.3305349349975586, test acc = 0.8999999761581421, time = 0.13453364372253418\n",
      "Testing at step=34, batch=60, test loss = 0.40025728940963745, test acc = 0.8100000023841858, time = 0.13349032402038574\n",
      "Testing at step=34, batch=80, test loss = 0.44524380564689636, test acc = 0.8199999928474426, time = 0.13372087478637695\n",
      "Step 34 finished in 259.1959080696106, Train loss = 0.34644616427520913, Test loss = 0.38878943890333173; Train Acc = 0.8777833327651023, Test Acc = 0.861199997663498\n",
      "Training at step=35, batch=0, train loss = 0.45147332549095154, train acc = 0.8600000143051147, time = 0.37970423698425293\n",
      "Training at step=35, batch=120, train loss = 0.348743200302124, train acc = 0.8899999856948853, time = 0.3805081844329834\n",
      "Training at step=35, batch=240, train loss = 0.20703642070293427, train acc = 0.9300000071525574, time = 0.37711262702941895\n",
      "Training at step=35, batch=360, train loss = 0.27266737818717957, train acc = 0.8799999952316284, time = 0.37724804878234863\n",
      "Training at step=35, batch=480, train loss = 0.3554205000400543, train acc = 0.9100000262260437, time = 0.3774418830871582\n",
      "Testing at step=35, batch=0, test loss = 0.3198995888233185, test acc = 0.8500000238418579, time = 0.13515305519104004\n",
      "Testing at step=35, batch=20, test loss = 0.41598010063171387, test acc = 0.8500000238418579, time = 0.13431715965270996\n",
      "Testing at step=35, batch=40, test loss = 0.3983410894870758, test acc = 0.8700000047683716, time = 0.13439226150512695\n",
      "Testing at step=35, batch=60, test loss = 0.40151160955429077, test acc = 0.8399999737739563, time = 0.13379383087158203\n",
      "Testing at step=35, batch=80, test loss = 0.6443590521812439, test acc = 0.800000011920929, time = 0.13804292678833008\n",
      "Step 35 finished in 258.7710976600647, Train loss = 0.34110614396631717, Test loss = 0.41341491147875786; Train Acc = 0.8806833319862684, Test Acc = 0.8532999986410141\n",
      "Training at step=36, batch=0, train loss = 0.360067218542099, train acc = 0.8700000047683716, time = 0.37953662872314453\n",
      "Training at step=36, batch=120, train loss = 0.27893656492233276, train acc = 0.8600000143051147, time = 0.37725114822387695\n",
      "Training at step=36, batch=240, train loss = 0.31533578038215637, train acc = 0.8700000047683716, time = 0.37624239921569824\n",
      "Training at step=36, batch=360, train loss = 0.2584571838378906, train acc = 0.9300000071525574, time = 0.37549924850463867\n",
      "Training at step=36, batch=480, train loss = 0.32699817419052124, train acc = 0.8799999952316284, time = 0.38207125663757324\n",
      "Testing at step=36, batch=0, test loss = 0.34107086062431335, test acc = 0.9100000262260437, time = 0.13696575164794922\n",
      "Testing at step=36, batch=20, test loss = 0.6269909143447876, test acc = 0.7900000214576721, time = 0.13670015335083008\n",
      "Testing at step=36, batch=40, test loss = 0.30482637882232666, test acc = 0.8700000047683716, time = 0.1357564926147461\n",
      "Testing at step=36, batch=60, test loss = 0.3370486795902252, test acc = 0.8999999761581421, time = 0.13495588302612305\n",
      "Testing at step=36, batch=80, test loss = 0.2775173783302307, test acc = 0.8899999856948853, time = 0.13605833053588867\n",
      "Step 36 finished in 259.12666153907776, Train loss = 0.33601404070854185, Test loss = 0.3839678855240345; Train Acc = 0.8825000003973643, Test Acc = 0.8639999991655349\n",
      "Training at step=37, batch=0, train loss = 0.26545125246047974, train acc = 0.9100000262260437, time = 0.3813302516937256\n",
      "Training at step=37, batch=120, train loss = 0.3062555491924286, train acc = 0.9100000262260437, time = 0.37975645065307617\n",
      "Training at step=37, batch=240, train loss = 0.3053266406059265, train acc = 0.8600000143051147, time = 0.379575252532959\n",
      "Training at step=37, batch=360, train loss = 0.4007481634616852, train acc = 0.8399999737739563, time = 0.3815443515777588\n",
      "Training at step=37, batch=480, train loss = 0.2022407203912735, train acc = 0.9300000071525574, time = 0.37969160079956055\n",
      "Testing at step=37, batch=0, test loss = 0.29033738374710083, test acc = 0.8999999761581421, time = 0.13608574867248535\n",
      "Testing at step=37, batch=20, test loss = 0.38573604822158813, test acc = 0.8799999952316284, time = 0.13466620445251465\n",
      "Testing at step=37, batch=40, test loss = 0.4309133291244507, test acc = 0.8199999928474426, time = 0.13593006134033203\n",
      "Testing at step=37, batch=60, test loss = 0.48395487666130066, test acc = 0.8399999737739563, time = 0.13508224487304688\n",
      "Testing at step=37, batch=80, test loss = 0.3190813660621643, test acc = 0.8700000047683716, time = 0.13608622550964355\n",
      "Step 37 finished in 260.6441493034363, Train loss = 0.33789170246571304, Test loss = 0.3613881678879261; Train Acc = 0.8808166657884916, Test Acc = 0.8718999993801116\n",
      "Training at step=38, batch=0, train loss = 0.339974969625473, train acc = 0.8500000238418579, time = 0.38527560234069824\n",
      "Training at step=38, batch=120, train loss = 0.25094178318977356, train acc = 0.9200000166893005, time = 0.37757396697998047\n",
      "Training at step=38, batch=240, train loss = 0.37709152698516846, train acc = 0.8399999737739563, time = 0.3836510181427002\n",
      "Training at step=38, batch=360, train loss = 0.2562410235404968, train acc = 0.9300000071525574, time = 0.38137078285217285\n",
      "Training at step=38, batch=480, train loss = 0.2737826704978943, train acc = 0.9399999976158142, time = 0.3805351257324219\n",
      "Testing at step=38, batch=0, test loss = 0.3533059358596802, test acc = 0.8899999856948853, time = 0.13609910011291504\n",
      "Testing at step=38, batch=20, test loss = 0.562079668045044, test acc = 0.8199999928474426, time = 0.13617849349975586\n",
      "Testing at step=38, batch=40, test loss = 0.3769993185997009, test acc = 0.8600000143051147, time = 0.1360034942626953\n",
      "Testing at step=38, batch=60, test loss = 0.4461214542388916, test acc = 0.8500000238418579, time = 0.13594365119934082\n",
      "Testing at step=38, batch=80, test loss = 0.3987158238887787, test acc = 0.8899999856948853, time = 0.13585686683654785\n",
      "Step 38 finished in 260.2785506248474, Train loss = 0.33469679777820904, Test loss = 0.3519982296228409; Train Acc = 0.882350001235803, Test Acc = 0.875900000333786\n",
      "Training at step=39, batch=0, train loss = 0.18252502381801605, train acc = 0.949999988079071, time = 0.3815147876739502\n",
      "Training at step=39, batch=120, train loss = 0.3033970892429352, train acc = 0.8700000047683716, time = 0.3828885555267334\n",
      "Training at step=39, batch=240, train loss = 0.3326604962348938, train acc = 0.8700000047683716, time = 0.37986159324645996\n",
      "Training at step=39, batch=360, train loss = 0.2758912742137909, train acc = 0.9100000262260437, time = 0.38071727752685547\n",
      "Training at step=39, batch=480, train loss = 0.2744268476963043, train acc = 0.9100000262260437, time = 0.3776082992553711\n",
      "Testing at step=39, batch=0, test loss = 0.5458568334579468, test acc = 0.8199999928474426, time = 0.13360309600830078\n",
      "Testing at step=39, batch=20, test loss = 0.3017141819000244, test acc = 0.8799999952316284, time = 0.13423395156860352\n",
      "Testing at step=39, batch=40, test loss = 0.4099612534046173, test acc = 0.8799999952316284, time = 0.1352224349975586\n",
      "Testing at step=39, batch=60, test loss = 0.3449493646621704, test acc = 0.8700000047683716, time = 0.13456988334655762\n",
      "Testing at step=39, batch=80, test loss = 0.3734188973903656, test acc = 0.8299999833106995, time = 0.13436222076416016\n",
      "Step 39 finished in 260.43752908706665, Train loss = 0.3319395802170038, Test loss = 0.37517782241106035; Train Acc = 0.8838166668017705, Test Acc = 0.8648000007867813\n",
      "Training at step=40, batch=0, train loss = 0.42489883303642273, train acc = 0.8500000238418579, time = 0.37737464904785156\n",
      "Training at step=40, batch=120, train loss = 0.24765066802501678, train acc = 0.9399999976158142, time = 0.3782076835632324\n",
      "Training at step=40, batch=240, train loss = 0.33534592390060425, train acc = 0.8600000143051147, time = 0.3801391124725342\n",
      "Training at step=40, batch=360, train loss = 0.4231070280075073, train acc = 0.8199999928474426, time = 0.37750244140625\n",
      "Training at step=40, batch=480, train loss = 0.5769774913787842, train acc = 0.800000011920929, time = 0.38041067123413086\n",
      "Testing at step=40, batch=0, test loss = 0.4557405114173889, test acc = 0.8600000143051147, time = 0.13653182983398438\n",
      "Testing at step=40, batch=20, test loss = 0.36640769243240356, test acc = 0.8999999761581421, time = 0.134690523147583\n",
      "Testing at step=40, batch=40, test loss = 0.4181838929653168, test acc = 0.8600000143051147, time = 0.13544082641601562\n",
      "Testing at step=40, batch=60, test loss = 0.47376787662506104, test acc = 0.8600000143051147, time = 0.13484549522399902\n",
      "Testing at step=40, batch=80, test loss = 0.3758902847766876, test acc = 0.8700000047683716, time = 0.1367475986480713\n",
      "Step 40 finished in 258.8514428138733, Train loss = 0.33272008708367745, Test loss = 0.354111201018095; Train Acc = 0.8835166675845781, Test Acc = 0.8762999981641769\n",
      "Training at step=41, batch=0, train loss = 0.3786139190196991, train acc = 0.8600000143051147, time = 0.38361239433288574\n",
      "Training at step=41, batch=120, train loss = 0.21314649283885956, train acc = 0.9200000166893005, time = 0.37765002250671387\n",
      "Training at step=41, batch=240, train loss = 0.31356117129325867, train acc = 0.9100000262260437, time = 0.37665557861328125\n",
      "Training at step=41, batch=360, train loss = 0.23095928132534027, train acc = 0.9300000071525574, time = 0.3777761459350586\n",
      "Training at step=41, batch=480, train loss = 0.39141708612442017, train acc = 0.8399999737739563, time = 0.37815427780151367\n",
      "Testing at step=41, batch=0, test loss = 0.40516647696495056, test acc = 0.8399999737739563, time = 0.1386394500732422\n",
      "Testing at step=41, batch=20, test loss = 0.2745864987373352, test acc = 0.8799999952316284, time = 0.13989472389221191\n",
      "Testing at step=41, batch=40, test loss = 0.43983927369117737, test acc = 0.8399999737739563, time = 0.13408637046813965\n",
      "Testing at step=41, batch=60, test loss = 0.4009711444377899, test acc = 0.8700000047683716, time = 0.13397884368896484\n",
      "Testing at step=41, batch=80, test loss = 0.3091781735420227, test acc = 0.8899999856948853, time = 0.13303041458129883\n",
      "Step 41 finished in 258.6803958415985, Train loss = 0.32223749935626983, Test loss = 0.36429395273327825; Train Acc = 0.886200000445048, Test Acc = 0.8712999999523163\n",
      "Training at step=42, batch=0, train loss = 0.33890509605407715, train acc = 0.9100000262260437, time = 0.37836289405822754\n",
      "Training at step=42, batch=120, train loss = 0.3459557592868805, train acc = 0.8799999952316284, time = 0.37755823135375977\n",
      "Training at step=42, batch=240, train loss = 0.4365096390247345, train acc = 0.8600000143051147, time = 0.377316951751709\n",
      "Training at step=42, batch=360, train loss = 0.3644987940788269, train acc = 0.8799999952316284, time = 0.377791166305542\n",
      "Training at step=42, batch=480, train loss = 0.4000767767429352, train acc = 0.8600000143051147, time = 0.37657833099365234\n",
      "Testing at step=42, batch=0, test loss = 0.2165578007698059, test acc = 0.9399999976158142, time = 0.137542724609375\n",
      "Testing at step=42, batch=20, test loss = 0.2964572608470917, test acc = 0.8700000047683716, time = 0.13483691215515137\n",
      "Testing at step=42, batch=40, test loss = 0.26785382628440857, test acc = 0.8899999856948853, time = 0.13476943969726562\n",
      "Testing at step=42, batch=60, test loss = 0.6698482632637024, test acc = 0.7699999809265137, time = 0.1346421241760254\n",
      "Testing at step=42, batch=80, test loss = 0.2893192172050476, test acc = 0.8700000047683716, time = 0.13394546508789062\n",
      "Step 42 finished in 260.34180665016174, Train loss = 0.323591440419356, Test loss = 0.35733810380101205; Train Acc = 0.8858833343784014, Test Acc = 0.8747000014781952\n",
      "Training at step=43, batch=0, train loss = 0.41342616081237793, train acc = 0.8399999737739563, time = 0.38153672218322754\n",
      "Training at step=43, batch=120, train loss = 0.28955790400505066, train acc = 0.8799999952316284, time = 0.37854576110839844\n",
      "Training at step=43, batch=240, train loss = 0.5843815207481384, train acc = 0.8299999833106995, time = 0.3774533271789551\n",
      "Training at step=43, batch=360, train loss = 0.16465935111045837, train acc = 0.9300000071525574, time = 0.37800049781799316\n",
      "Training at step=43, batch=480, train loss = 0.335544228553772, train acc = 0.8700000047683716, time = 0.37647414207458496\n",
      "Testing at step=43, batch=0, test loss = 0.356621652841568, test acc = 0.8600000143051147, time = 0.13493108749389648\n",
      "Testing at step=43, batch=20, test loss = 0.4167745113372803, test acc = 0.8399999737739563, time = 0.1347522735595703\n",
      "Testing at step=43, batch=40, test loss = 0.2787194550037384, test acc = 0.8999999761581421, time = 0.13411879539489746\n",
      "Testing at step=43, batch=60, test loss = 0.5273507237434387, test acc = 0.7900000214576721, time = 0.13331151008605957\n",
      "Testing at step=43, batch=80, test loss = 0.48606112599372864, test acc = 0.8399999737739563, time = 0.13440823554992676\n",
      "Step 43 finished in 259.74311447143555, Train loss = 0.3231934872517983, Test loss = 0.3729271845519543; Train Acc = 0.8852666675051053, Test Acc = 0.8636999994516372\n",
      "Training at step=44, batch=0, train loss = 0.41131454706192017, train acc = 0.8500000238418579, time = 0.3803527355194092\n",
      "Training at step=44, batch=120, train loss = 0.3601846694946289, train acc = 0.8700000047683716, time = 0.3769397735595703\n",
      "Training at step=44, batch=240, train loss = 0.26009616255760193, train acc = 0.9100000262260437, time = 0.3817408084869385\n",
      "Training at step=44, batch=360, train loss = 0.3977116644382477, train acc = 0.8999999761581421, time = 0.38411736488342285\n",
      "Training at step=44, batch=480, train loss = 0.16329587996006012, train acc = 0.9300000071525574, time = 0.3938863277435303\n",
      "Testing at step=44, batch=0, test loss = 0.3710154592990875, test acc = 0.8700000047683716, time = 0.13833308219909668\n",
      "Testing at step=44, batch=20, test loss = 0.38077884912490845, test acc = 0.8700000047683716, time = 0.1357097625732422\n",
      "Testing at step=44, batch=40, test loss = 0.2510964572429657, test acc = 0.9200000166893005, time = 0.1372239589691162\n",
      "Testing at step=44, batch=60, test loss = 0.45431289076805115, test acc = 0.8299999833106995, time = 0.13706493377685547\n",
      "Testing at step=44, batch=80, test loss = 0.3745633661746979, test acc = 0.8899999856948853, time = 0.13742351531982422\n",
      "Step 44 finished in 261.275274515152, Train loss = 0.32230557621767125, Test loss = 0.40155055552721025; Train Acc = 0.8878000005086263, Test Acc = 0.8607000017166138\n",
      "Training at step=45, batch=0, train loss = 0.3755679428577423, train acc = 0.8299999833106995, time = 0.383331298828125\n",
      "Training at step=45, batch=120, train loss = 0.3143104016780853, train acc = 0.8999999761581421, time = 0.37964439392089844\n",
      "Training at step=45, batch=240, train loss = 0.3166263699531555, train acc = 0.8500000238418579, time = 0.38671112060546875\n",
      "Training at step=45, batch=360, train loss = 0.3563278615474701, train acc = 0.8799999952316284, time = 0.38459110260009766\n",
      "Training at step=45, batch=480, train loss = 0.3356374502182007, train acc = 0.8999999761581421, time = 0.3884313106536865\n",
      "Testing at step=45, batch=0, test loss = 0.350726842880249, test acc = 0.8999999761581421, time = 0.13668274879455566\n",
      "Testing at step=45, batch=20, test loss = 0.5612131357192993, test acc = 0.8199999928474426, time = 0.13659906387329102\n",
      "Testing at step=45, batch=40, test loss = 0.30280598998069763, test acc = 0.8600000143051147, time = 0.13691067695617676\n",
      "Testing at step=45, batch=60, test loss = 0.3889821767807007, test acc = 0.8700000047683716, time = 0.1360940933227539\n",
      "Testing at step=45, batch=80, test loss = 0.32673367857933044, test acc = 0.8899999856948853, time = 0.13726449012756348\n",
      "Step 45 finished in 261.9418284893036, Train loss = 0.31612752032776675, Test loss = 0.3688539505004883; Train Acc = 0.8876666667064032, Test Acc = 0.8720000010728836\n",
      "Training at step=46, batch=0, train loss = 0.270669162273407, train acc = 0.8899999856948853, time = 0.3826463222503662\n",
      "Training at step=46, batch=120, train loss = 0.23589299619197845, train acc = 0.8899999856948853, time = 0.380626916885376\n",
      "Training at step=46, batch=240, train loss = 0.3987961709499359, train acc = 0.8399999737739563, time = 0.3760213851928711\n",
      "Training at step=46, batch=360, train loss = 0.3565481901168823, train acc = 0.8799999952316284, time = 0.37711620330810547\n",
      "Training at step=46, batch=480, train loss = 0.17643433809280396, train acc = 0.9599999785423279, time = 0.37801671028137207\n",
      "Testing at step=46, batch=0, test loss = 0.341942697763443, test acc = 0.8999999761581421, time = 0.1346909999847412\n",
      "Testing at step=46, batch=20, test loss = 0.28867802023887634, test acc = 0.8899999856948853, time = 0.13630127906799316\n",
      "Testing at step=46, batch=40, test loss = 0.532646894454956, test acc = 0.8600000143051147, time = 0.13633990287780762\n",
      "Testing at step=46, batch=60, test loss = 0.48995959758758545, test acc = 0.8299999833106995, time = 0.1344468593597412\n",
      "Testing at step=46, batch=80, test loss = 0.22851267457008362, test acc = 0.9300000071525574, time = 0.13789796829223633\n",
      "Step 46 finished in 260.04704451560974, Train loss = 0.3173542709896962, Test loss = 0.363304497897625; Train Acc = 0.8878999997178714, Test Acc = 0.8730000001192093\n",
      "Training at step=47, batch=0, train loss = 0.276857852935791, train acc = 0.9100000262260437, time = 0.3790397644042969\n",
      "Training at step=47, batch=120, train loss = 0.25402694940567017, train acc = 0.8899999856948853, time = 0.37743353843688965\n",
      "Training at step=47, batch=240, train loss = 0.36629486083984375, train acc = 0.8600000143051147, time = 0.37862110137939453\n",
      "Training at step=47, batch=360, train loss = 0.427541047334671, train acc = 0.8399999737739563, time = 0.3786747455596924\n",
      "Training at step=47, batch=480, train loss = 0.31167182326316833, train acc = 0.8999999761581421, time = 0.3775656223297119\n",
      "Testing at step=47, batch=0, test loss = 0.3827968239784241, test acc = 0.8299999833106995, time = 0.13537192344665527\n",
      "Testing at step=47, batch=20, test loss = 0.2874665856361389, test acc = 0.8600000143051147, time = 0.13438892364501953\n",
      "Testing at step=47, batch=40, test loss = 0.30552512407302856, test acc = 0.9100000262260437, time = 0.1346437931060791\n",
      "Testing at step=47, batch=60, test loss = 0.4843769371509552, test acc = 0.8100000023841858, time = 0.1370387077331543\n",
      "Testing at step=47, batch=80, test loss = 0.35457706451416016, test acc = 0.8600000143051147, time = 0.1357405185699463\n",
      "Step 47 finished in 259.12963008880615, Train loss = 0.3158587787548701, Test loss = 0.3438830970227718; Train Acc = 0.8887166662017504, Test Acc = 0.8791999989748001\n",
      "Training at step=48, batch=0, train loss = 0.16788433492183685, train acc = 0.9399999976158142, time = 0.3801877498626709\n",
      "Training at step=48, batch=120, train loss = 0.27031630277633667, train acc = 0.8799999952316284, time = 0.38075685501098633\n",
      "Training at step=48, batch=240, train loss = 0.2669028341770172, train acc = 0.8999999761581421, time = 0.38605666160583496\n",
      "Training at step=48, batch=360, train loss = 0.22104017436504364, train acc = 0.9200000166893005, time = 0.379260778427124\n",
      "Training at step=48, batch=480, train loss = 0.23301970958709717, train acc = 0.9100000262260437, time = 0.37810468673706055\n",
      "Testing at step=48, batch=0, test loss = 0.38747525215148926, test acc = 0.8899999856948853, time = 0.1362771987915039\n",
      "Testing at step=48, batch=20, test loss = 0.2512463331222534, test acc = 0.9300000071525574, time = 0.13497591018676758\n",
      "Testing at step=48, batch=40, test loss = 0.3395634889602661, test acc = 0.8899999856948853, time = 0.13467001914978027\n",
      "Testing at step=48, batch=60, test loss = 0.37108826637268066, test acc = 0.8500000238418579, time = 0.13434076309204102\n",
      "Testing at step=48, batch=80, test loss = 0.21129171550273895, test acc = 0.9200000166893005, time = 0.13587284088134766\n",
      "Step 48 finished in 259.7009987831116, Train loss = 0.31281716970105966, Test loss = 0.3438959833979607; Train Acc = 0.8896833337346712, Test Acc = 0.8787000000476837\n",
      "Training at step=49, batch=0, train loss = 0.31956687569618225, train acc = 0.9100000262260437, time = 0.3791520595550537\n",
      "Training at step=49, batch=120, train loss = 0.22344037890434265, train acc = 0.8999999761581421, time = 0.3789026737213135\n",
      "Training at step=49, batch=240, train loss = 0.4531986117362976, train acc = 0.8700000047683716, time = 0.37746191024780273\n",
      "Training at step=49, batch=360, train loss = 0.17891760170459747, train acc = 0.9399999976158142, time = 0.378251314163208\n",
      "Training at step=49, batch=480, train loss = 0.40427806973457336, train acc = 0.8799999952316284, time = 0.38516712188720703\n",
      "Testing at step=49, batch=0, test loss = 0.4677395522594452, test acc = 0.8199999928474426, time = 0.13484907150268555\n",
      "Testing at step=49, batch=20, test loss = 0.28435906767845154, test acc = 0.8899999856948853, time = 0.13473939895629883\n",
      "Testing at step=49, batch=40, test loss = 0.30709803104400635, test acc = 0.8799999952316284, time = 0.13434910774230957\n",
      "Testing at step=49, batch=60, test loss = 0.3064501881599426, test acc = 0.8799999952316284, time = 0.13483595848083496\n",
      "Testing at step=49, batch=80, test loss = 0.5339773297309875, test acc = 0.8299999833106995, time = 0.1342458724975586\n",
      "Step 49 finished in 260.0186905860901, Train loss = 0.3133961241071423, Test loss = 0.38193483114242555; Train Acc = 0.8881499998768171, Test Acc = 0.8700999993085862\n",
      "Training at step=50, batch=0, train loss = 0.30520978569984436, train acc = 0.8999999761581421, time = 0.38115501403808594\n",
      "Training at step=50, batch=120, train loss = 0.27793437242507935, train acc = 0.8999999761581421, time = 0.3780381679534912\n",
      "Training at step=50, batch=240, train loss = 0.19501391053199768, train acc = 0.9399999976158142, time = 0.3804957866668701\n",
      "Training at step=50, batch=360, train loss = 0.2660571336746216, train acc = 0.8799999952316284, time = 0.3834264278411865\n",
      "Training at step=50, batch=480, train loss = 0.2547639012336731, train acc = 0.9399999976158142, time = 0.39414525032043457\n",
      "Testing at step=50, batch=0, test loss = 0.28373202681541443, test acc = 0.9200000166893005, time = 0.13572025299072266\n",
      "Testing at step=50, batch=20, test loss = 0.37335577607154846, test acc = 0.8899999856948853, time = 0.13698363304138184\n",
      "Testing at step=50, batch=40, test loss = 0.3676374554634094, test acc = 0.8500000238418579, time = 0.1362917423248291\n",
      "Testing at step=50, batch=60, test loss = 0.4472622573375702, test acc = 0.8700000047683716, time = 0.13758087158203125\n",
      "Testing at step=50, batch=80, test loss = 0.4191037714481354, test acc = 0.8299999833106995, time = 0.13691282272338867\n",
      "Step 50 finished in 261.81575202941895, Train loss = 0.3080521088714401, Test loss = 0.3363121843338013; Train Acc = 0.8921166671315829, Test Acc = 0.8779000008106231\n",
      "Training at step=51, batch=0, train loss = 0.2454661726951599, train acc = 0.9200000166893005, time = 0.38431811332702637\n",
      "Training at step=51, batch=120, train loss = 0.3589901626110077, train acc = 0.8899999856948853, time = 0.3822941780090332\n",
      "Training at step=51, batch=240, train loss = 0.2871064245700836, train acc = 0.8999999761581421, time = 0.3813169002532959\n",
      "Training at step=51, batch=360, train loss = 0.231800839304924, train acc = 0.9300000071525574, time = 0.38184022903442383\n",
      "Training at step=51, batch=480, train loss = 0.35608822107315063, train acc = 0.8600000143051147, time = 0.3811988830566406\n",
      "Testing at step=51, batch=0, test loss = 0.4420814514160156, test acc = 0.8199999928474426, time = 0.1442403793334961\n",
      "Testing at step=51, batch=20, test loss = 0.31495916843414307, test acc = 0.9200000166893005, time = 0.1413261890411377\n",
      "Testing at step=51, batch=40, test loss = 0.3764610290527344, test acc = 0.8799999952316284, time = 0.14119577407836914\n",
      "Testing at step=51, batch=60, test loss = 0.4138771891593933, test acc = 0.8100000023841858, time = 0.13521456718444824\n",
      "Testing at step=51, batch=80, test loss = 0.34741097688674927, test acc = 0.8899999856948853, time = 0.1363067626953125\n",
      "Step 51 finished in 261.9639620780945, Train loss = 0.3078601775070032, Test loss = 0.3392919780313969; Train Acc = 0.8905833345651627, Test Acc = 0.8803000009059906\n",
      "Training at step=52, batch=0, train loss = 0.18580394983291626, train acc = 0.9300000071525574, time = 0.38150930404663086\n",
      "Training at step=52, batch=120, train loss = 0.42943131923675537, train acc = 0.8299999833106995, time = 0.3782055377960205\n",
      "Training at step=52, batch=240, train loss = 0.17984937131404877, train acc = 0.949999988079071, time = 0.376326322555542\n",
      "Training at step=52, batch=360, train loss = 0.2638138234615326, train acc = 0.8999999761581421, time = 0.38006114959716797\n",
      "Training at step=52, batch=480, train loss = 0.4233326315879822, train acc = 0.8500000238418579, time = 0.3760831356048584\n",
      "Testing at step=52, batch=0, test loss = 0.32476311922073364, test acc = 0.8999999761581421, time = 0.1345045566558838\n",
      "Testing at step=52, batch=20, test loss = 0.4310648739337921, test acc = 0.8199999928474426, time = 0.13502788543701172\n",
      "Testing at step=52, batch=40, test loss = 0.3497655987739563, test acc = 0.8899999856948853, time = 0.13442063331604004\n",
      "Testing at step=52, batch=60, test loss = 0.4169287383556366, test acc = 0.8700000047683716, time = 0.13747429847717285\n",
      "Testing at step=52, batch=80, test loss = 0.3091570734977722, test acc = 0.8799999952316284, time = 0.13424205780029297\n",
      "Step 52 finished in 259.51586079597473, Train loss = 0.3056199969847997, Test loss = 0.3340641455352306; Train Acc = 0.8918833334247271, Test Acc = 0.8799999994039536\n",
      "Training at step=53, batch=0, train loss = 0.21275608241558075, train acc = 0.9399999976158142, time = 0.3826429843902588\n",
      "Training at step=53, batch=120, train loss = 0.16885538399219513, train acc = 0.9300000071525574, time = 0.38266658782958984\n",
      "Training at step=53, batch=240, train loss = 0.47151416540145874, train acc = 0.8700000047683716, time = 0.3765408992767334\n",
      "Training at step=53, batch=360, train loss = 0.2532661557197571, train acc = 0.8999999761581421, time = 0.3766155242919922\n",
      "Training at step=53, batch=480, train loss = 0.4474870562553406, train acc = 0.7900000214576721, time = 0.3778226375579834\n",
      "Testing at step=53, batch=0, test loss = 0.2889390289783478, test acc = 0.9100000262260437, time = 0.1348893642425537\n",
      "Testing at step=53, batch=20, test loss = 0.3663145899772644, test acc = 0.8700000047683716, time = 0.13421845436096191\n",
      "Testing at step=53, batch=40, test loss = 0.2752656936645508, test acc = 0.8500000238418579, time = 0.13480424880981445\n",
      "Testing at step=53, batch=60, test loss = 0.39418652653694153, test acc = 0.8899999856948853, time = 0.13425779342651367\n",
      "Testing at step=53, batch=80, test loss = 0.3921254277229309, test acc = 0.8700000047683716, time = 0.13787317276000977\n",
      "Step 53 finished in 259.01775336265564, Train loss = 0.3071890241652727, Test loss = 0.3705333189666271; Train Acc = 0.8921666669845582, Test Acc = 0.8635000014305114\n",
      "Training at step=54, batch=0, train loss = 0.25611868500709534, train acc = 0.9100000262260437, time = 0.3780698776245117\n",
      "Training at step=54, batch=120, train loss = 0.2732354402542114, train acc = 0.8799999952316284, time = 0.378342866897583\n",
      "Training at step=54, batch=240, train loss = 0.43670740723609924, train acc = 0.8799999952316284, time = 0.38285279273986816\n",
      "Training at step=54, batch=360, train loss = 0.40253448486328125, train acc = 0.8700000047683716, time = 0.37789320945739746\n",
      "Training at step=54, batch=480, train loss = 0.19826649129390717, train acc = 0.9399999976158142, time = 0.3770871162414551\n",
      "Testing at step=54, batch=0, test loss = 0.22557662427425385, test acc = 0.9100000262260437, time = 0.13683676719665527\n",
      "Testing at step=54, batch=20, test loss = 0.17844423651695251, test acc = 0.9200000166893005, time = 0.13428378105163574\n",
      "Testing at step=54, batch=40, test loss = 0.46548327803611755, test acc = 0.8299999833106995, time = 0.13418340682983398\n",
      "Testing at step=54, batch=60, test loss = 0.5317632555961609, test acc = 0.8399999737739563, time = 0.1341261863708496\n",
      "Testing at step=54, batch=80, test loss = 0.36328139901161194, test acc = 0.8600000143051147, time = 0.13498735427856445\n",
      "Step 54 finished in 259.01995038986206, Train loss = 0.30359544331828753, Test loss = 0.3309300521016121; Train Acc = 0.8925000003973643, Test Acc = 0.882700001001358\n",
      "Training at step=55, batch=0, train loss = 0.48348626494407654, train acc = 0.8199999928474426, time = 0.37717509269714355\n",
      "Training at step=55, batch=120, train loss = 0.1993596851825714, train acc = 0.9200000166893005, time = 0.3774867057800293\n",
      "Training at step=55, batch=240, train loss = 0.21349115669727325, train acc = 0.9200000166893005, time = 0.37872862815856934\n",
      "Training at step=55, batch=360, train loss = 0.15445774793624878, train acc = 0.9599999785423279, time = 0.37663817405700684\n",
      "Training at step=55, batch=480, train loss = 0.3128035068511963, train acc = 0.8999999761581421, time = 0.378772497177124\n",
      "Testing at step=55, batch=0, test loss = 0.3835057020187378, test acc = 0.8600000143051147, time = 0.13424468040466309\n",
      "Testing at step=55, batch=20, test loss = 0.25388216972351074, test acc = 0.9100000262260437, time = 0.13468670845031738\n",
      "Testing at step=55, batch=40, test loss = 0.41656771302223206, test acc = 0.8700000047683716, time = 0.1344161033630371\n",
      "Testing at step=55, batch=60, test loss = 0.2452799677848816, test acc = 0.8999999761581421, time = 0.1349501609802246\n",
      "Testing at step=55, batch=80, test loss = 0.33533111214637756, test acc = 0.8600000143051147, time = 0.1343827247619629\n",
      "Step 55 finished in 259.64972591400146, Train loss = 0.3007616707434257, Test loss = 0.3438426305353641; Train Acc = 0.8930666674176851, Test Acc = 0.8752999973297119\n",
      "Training at step=56, batch=0, train loss = 0.35758763551712036, train acc = 0.8500000238418579, time = 0.3785855770111084\n",
      "Training at step=56, batch=120, train loss = 0.3032805025577545, train acc = 0.8700000047683716, time = 0.3757777214050293\n",
      "Training at step=56, batch=240, train loss = 0.3727401793003082, train acc = 0.8600000143051147, time = 0.3775789737701416\n",
      "Training at step=56, batch=360, train loss = 0.21712984144687653, train acc = 0.9200000166893005, time = 0.3763606548309326\n",
      "Training at step=56, batch=480, train loss = 0.41926753520965576, train acc = 0.8500000238418579, time = 0.3794844150543213\n",
      "Testing at step=56, batch=0, test loss = 0.45950955152511597, test acc = 0.8600000143051147, time = 0.13434672355651855\n",
      "Testing at step=56, batch=20, test loss = 0.36120322346687317, test acc = 0.8799999952316284, time = 0.13388943672180176\n",
      "Testing at step=56, batch=40, test loss = 0.3482103645801544, test acc = 0.8799999952316284, time = 0.1337294578552246\n",
      "Testing at step=56, batch=60, test loss = 0.29328206181526184, test acc = 0.8899999856948853, time = 0.13323044776916504\n",
      "Testing at step=56, batch=80, test loss = 0.20504017174243927, test acc = 0.9300000071525574, time = 0.13464808464050293\n",
      "Step 56 finished in 259.346391916275, Train loss = 0.30110398488740125, Test loss = 0.3320924437046051; Train Acc = 0.8932333344221115, Test Acc = 0.8784000009298325\n",
      "Training at step=57, batch=0, train loss = 0.3779723048210144, train acc = 0.8600000143051147, time = 0.37843799591064453\n",
      "Training at step=57, batch=120, train loss = 0.3119827210903168, train acc = 0.8700000047683716, time = 0.37779664993286133\n",
      "Training at step=57, batch=240, train loss = 0.3469657897949219, train acc = 0.9100000262260437, time = 0.3820455074310303\n",
      "Training at step=57, batch=360, train loss = 0.21964046359062195, train acc = 0.9399999976158142, time = 0.3760848045349121\n",
      "Training at step=57, batch=480, train loss = 0.2415245771408081, train acc = 0.9399999976158142, time = 0.37668347358703613\n",
      "Testing at step=57, batch=0, test loss = 0.3646181523799896, test acc = 0.8299999833106995, time = 0.13368916511535645\n",
      "Testing at step=57, batch=20, test loss = 0.3383849263191223, test acc = 0.8799999952316284, time = 0.13463234901428223\n",
      "Testing at step=57, batch=40, test loss = 0.2019910365343094, test acc = 0.9100000262260437, time = 0.13445520401000977\n",
      "Testing at step=57, batch=60, test loss = 0.5441049337387085, test acc = 0.8399999737739563, time = 0.13424158096313477\n",
      "Testing at step=57, batch=80, test loss = 0.2564542591571808, test acc = 0.8700000047683716, time = 0.13518285751342773\n",
      "Step 57 finished in 259.1582546234131, Train loss = 0.2988044704993566, Test loss = 0.341746861487627; Train Acc = 0.8944166668256124, Test Acc = 0.8774999994039535\n",
      "Training at step=58, batch=0, train loss = 0.26634684205055237, train acc = 0.9100000262260437, time = 0.387998104095459\n",
      "Training at step=58, batch=120, train loss = 0.4167299270629883, train acc = 0.8500000238418579, time = 0.3774116039276123\n",
      "Training at step=58, batch=240, train loss = 0.329230397939682, train acc = 0.9100000262260437, time = 0.3754556179046631\n",
      "Training at step=58, batch=360, train loss = 0.34468498826026917, train acc = 0.8500000238418579, time = 0.37582945823669434\n",
      "Training at step=58, batch=480, train loss = 0.2243700921535492, train acc = 0.8999999761581421, time = 0.37566661834716797\n",
      "Testing at step=58, batch=0, test loss = 0.29639074206352234, test acc = 0.8700000047683716, time = 0.13471555709838867\n",
      "Testing at step=58, batch=20, test loss = 0.3559977412223816, test acc = 0.8600000143051147, time = 0.13454747200012207\n",
      "Testing at step=58, batch=40, test loss = 0.3785318434238434, test acc = 0.7900000214576721, time = 0.1351017951965332\n",
      "Testing at step=58, batch=60, test loss = 0.5234494805335999, test acc = 0.8600000143051147, time = 0.13633513450622559\n",
      "Testing at step=58, batch=80, test loss = 0.48636767268180847, test acc = 0.8399999737739563, time = 0.13485383987426758\n",
      "Step 58 finished in 259.0849792957306, Train loss = 0.2981414229174455, Test loss = 0.3543348656594753; Train Acc = 0.8944333343704541, Test Acc = 0.8693000006675721\n",
      "Training at step=59, batch=0, train loss = 0.33890312910079956, train acc = 0.8500000238418579, time = 0.38811755180358887\n",
      "Training at step=59, batch=120, train loss = 0.14391089975833893, train acc = 0.9300000071525574, time = 0.3778507709503174\n",
      "Training at step=59, batch=240, train loss = 0.23513877391815186, train acc = 0.9200000166893005, time = 0.3828468322753906\n",
      "Training at step=59, batch=360, train loss = 0.2973993420600891, train acc = 0.8799999952316284, time = 0.37850069999694824\n",
      "Training at step=59, batch=480, train loss = 0.2953752875328064, train acc = 0.8999999761581421, time = 0.3759193420410156\n",
      "Testing at step=59, batch=0, test loss = 0.3483870327472687, test acc = 0.8700000047683716, time = 0.134749174118042\n",
      "Testing at step=59, batch=20, test loss = 0.3015052378177643, test acc = 0.8999999761581421, time = 0.13423609733581543\n",
      "Testing at step=59, batch=40, test loss = 0.500663161277771, test acc = 0.7599999904632568, time = 0.1344757080078125\n",
      "Testing at step=59, batch=60, test loss = 0.2804015576839447, test acc = 0.8999999761581421, time = 0.13409709930419922\n",
      "Testing at step=59, batch=80, test loss = 0.41998180747032166, test acc = 0.8500000238418579, time = 0.13562703132629395\n",
      "Step 59 finished in 259.0274794101715, Train loss = 0.29400835689157245, Test loss = 0.3276753757894039; Train Acc = 0.8959500017762184, Test Acc = 0.8821000009775162\n",
      "Training at step=60, batch=0, train loss = 0.3300718069076538, train acc = 0.8899999856948853, time = 0.38055968284606934\n",
      "Training at step=60, batch=120, train loss = 0.4232892692089081, train acc = 0.8500000238418579, time = 0.37685465812683105\n",
      "Training at step=60, batch=240, train loss = 0.28795015811920166, train acc = 0.9200000166893005, time = 0.37805771827697754\n",
      "Training at step=60, batch=360, train loss = 0.28804269433021545, train acc = 0.9200000166893005, time = 0.3769040107727051\n",
      "Training at step=60, batch=480, train loss = 0.2517082393169403, train acc = 0.8899999856948853, time = 0.3761742115020752\n",
      "Testing at step=60, batch=0, test loss = 0.39060086011886597, test acc = 0.8199999928474426, time = 0.13504719734191895\n",
      "Testing at step=60, batch=20, test loss = 0.29886361956596375, test acc = 0.8999999761581421, time = 0.13647246360778809\n",
      "Testing at step=60, batch=40, test loss = 0.3535500466823578, test acc = 0.8500000238418579, time = 0.13513660430908203\n",
      "Testing at step=60, batch=60, test loss = 0.3821815550327301, test acc = 0.8500000238418579, time = 0.13492035865783691\n",
      "Testing at step=60, batch=80, test loss = 0.4384007155895233, test acc = 0.8100000023841858, time = 0.1342935562133789\n",
      "Step 60 finished in 258.87046480178833, Train loss = 0.29682858935246864, Test loss = 0.33297938719391823; Train Acc = 0.8950500004490216, Test Acc = 0.8811999994516373\n",
      "Training at step=61, batch=0, train loss = 0.25700002908706665, train acc = 0.9200000166893005, time = 0.3797929286956787\n",
      "Training at step=61, batch=120, train loss = 0.3539857566356659, train acc = 0.8399999737739563, time = 0.37683939933776855\n",
      "Training at step=61, batch=240, train loss = 0.3495740592479706, train acc = 0.8600000143051147, time = 0.3780508041381836\n",
      "Training at step=61, batch=360, train loss = 0.21584059298038483, train acc = 0.9100000262260437, time = 0.37806010246276855\n",
      "Training at step=61, batch=480, train loss = 0.31953516602516174, train acc = 0.9100000262260437, time = 0.3780059814453125\n",
      "Testing at step=61, batch=0, test loss = 0.493719220161438, test acc = 0.8399999737739563, time = 0.13415813446044922\n",
      "Testing at step=61, batch=20, test loss = 0.47690850496292114, test acc = 0.8299999833106995, time = 0.13469338417053223\n",
      "Testing at step=61, batch=40, test loss = 0.4803605377674103, test acc = 0.8700000047683716, time = 0.13449692726135254\n",
      "Testing at step=61, batch=60, test loss = 0.3531399667263031, test acc = 0.8899999856948853, time = 0.13469314575195312\n",
      "Testing at step=61, batch=80, test loss = 0.2904914617538452, test acc = 0.9100000262260437, time = 0.13433432579040527\n",
      "Step 61 finished in 258.9744486808777, Train loss = 0.293205493837595, Test loss = 0.37218599930405616; Train Acc = 0.8956333342194557, Test Acc = 0.8625000011920929\n",
      "Training at step=62, batch=0, train loss = 0.3196118175983429, train acc = 0.8899999856948853, time = 0.3778212070465088\n",
      "Training at step=62, batch=120, train loss = 0.36791184544563293, train acc = 0.8600000143051147, time = 0.3807399272918701\n",
      "Training at step=62, batch=240, train loss = 0.36641600728034973, train acc = 0.8399999737739563, time = 0.3797729015350342\n",
      "Training at step=62, batch=360, train loss = 0.17164064943790436, train acc = 0.9599999785423279, time = 0.385225772857666\n",
      "Training at step=62, batch=480, train loss = 0.24774232506752014, train acc = 0.9200000166893005, time = 0.38906097412109375\n",
      "Testing at step=62, batch=0, test loss = 0.37828004360198975, test acc = 0.8600000143051147, time = 0.13930726051330566\n",
      "Testing at step=62, batch=20, test loss = 0.44216057658195496, test acc = 0.8600000143051147, time = 0.1387007236480713\n",
      "Testing at step=62, batch=40, test loss = 0.4543735980987549, test acc = 0.8700000047683716, time = 0.13938117027282715\n",
      "Testing at step=62, batch=60, test loss = 0.4555649161338806, test acc = 0.8199999928474426, time = 0.13868498802185059\n",
      "Testing at step=62, batch=80, test loss = 0.4883325695991516, test acc = 0.7900000214576721, time = 0.13848567008972168\n",
      "Step 62 finished in 262.33931279182434, Train loss = 0.2932905651877324, Test loss = 0.41130814924836157; Train Acc = 0.896616667509079, Test Acc = 0.8528000026941299\n",
      "Training at step=63, batch=0, train loss = 0.2847607433795929, train acc = 0.9100000262260437, time = 0.39006543159484863\n",
      "Training at step=63, batch=120, train loss = 0.23834308981895447, train acc = 0.8999999761581421, time = 0.38275909423828125\n",
      "Training at step=63, batch=240, train loss = 0.4982237219810486, train acc = 0.8600000143051147, time = 0.3902013301849365\n",
      "Training at step=63, batch=360, train loss = 0.21159255504608154, train acc = 0.9200000166893005, time = 0.4002876281738281\n",
      "Training at step=63, batch=480, train loss = 0.339651495218277, train acc = 0.8600000143051147, time = 0.38952040672302246\n",
      "Testing at step=63, batch=0, test loss = 0.35642892122268677, test acc = 0.8999999761581421, time = 0.13870859146118164\n",
      "Testing at step=63, batch=20, test loss = 0.32171162962913513, test acc = 0.8899999856948853, time = 0.1393451690673828\n",
      "Testing at step=63, batch=40, test loss = 0.3068605661392212, test acc = 0.9100000262260437, time = 0.1389625072479248\n",
      "Testing at step=63, batch=60, test loss = 0.4193994402885437, test acc = 0.8100000023841858, time = 0.138657808303833\n",
      "Testing at step=63, batch=80, test loss = 0.38313034176826477, test acc = 0.8700000047683716, time = 0.1386427879333496\n",
      "Step 63 finished in 266.38860869407654, Train loss = 0.2920247203608354, Test loss = 0.33734710127115247; Train Acc = 0.8964333338538806, Test Acc = 0.8811000007390976\n",
      "Training at step=64, batch=0, train loss = 0.3444359302520752, train acc = 0.8899999856948853, time = 0.392498254776001\n",
      "Training at step=64, batch=120, train loss = 0.27578723430633545, train acc = 0.8999999761581421, time = 0.3901822566986084\n",
      "Training at step=64, batch=240, train loss = 0.26217931509017944, train acc = 0.8799999952316284, time = 0.38884544372558594\n",
      "Training at step=64, batch=360, train loss = 0.25992268323898315, train acc = 0.9300000071525574, time = 0.3912179470062256\n",
      "Training at step=64, batch=480, train loss = 0.28055357933044434, train acc = 0.9200000166893005, time = 0.38811707496643066\n",
      "Testing at step=64, batch=0, test loss = 0.2613249719142914, test acc = 0.8899999856948853, time = 0.13808321952819824\n",
      "Testing at step=64, batch=20, test loss = 0.3838391900062561, test acc = 0.8399999737739563, time = 0.13879823684692383\n",
      "Testing at step=64, batch=40, test loss = 0.300455242395401, test acc = 0.9200000166893005, time = 0.13882780075073242\n",
      "Testing at step=64, batch=60, test loss = 0.3542522192001343, test acc = 0.8500000238418579, time = 0.1385660171508789\n",
      "Testing at step=64, batch=80, test loss = 0.3561074435710907, test acc = 0.8799999952316284, time = 0.13943767547607422\n",
      "Step 64 finished in 266.6647503376007, Train loss = 0.2889375255505244, Test loss = 0.3569814072549343; Train Acc = 0.8988833335042, Test Acc = 0.8725000005960465\n",
      "Training at step=65, batch=0, train loss = 0.28356337547302246, train acc = 0.9399999976158142, time = 0.3898749351501465\n",
      "Training at step=65, batch=120, train loss = 0.36080798506736755, train acc = 0.8799999952316284, time = 0.3894171714782715\n",
      "Training at step=65, batch=240, train loss = 0.13027715682983398, train acc = 0.949999988079071, time = 0.3905489444732666\n",
      "Training at step=65, batch=360, train loss = 0.3373851478099823, train acc = 0.9200000166893005, time = 0.39096641540527344\n",
      "Training at step=65, batch=480, train loss = 0.26106902956962585, train acc = 0.8999999761581421, time = 0.3908827304840088\n",
      "Testing at step=65, batch=0, test loss = 0.37824589014053345, test acc = 0.8600000143051147, time = 0.13884449005126953\n",
      "Testing at step=65, batch=20, test loss = 0.442612886428833, test acc = 0.8500000238418579, time = 0.14003705978393555\n",
      "Testing at step=65, batch=40, test loss = 0.3414047658443451, test acc = 0.8799999952316284, time = 0.14045405387878418\n",
      "Testing at step=65, batch=60, test loss = 0.26425570249557495, test acc = 0.8999999761581421, time = 0.13985109329223633\n",
      "Testing at step=65, batch=80, test loss = 0.3069058656692505, test acc = 0.8999999761581421, time = 0.1400766372680664\n",
      "Step 65 finished in 266.8300766944885, Train loss = 0.2892904644956191, Test loss = 0.3328226278722286; Train Acc = 0.8973500010371208, Test Acc = 0.8792000019550323\n",
      "Training at step=66, batch=0, train loss = 0.3064846992492676, train acc = 0.9100000262260437, time = 0.3939661979675293\n",
      "Training at step=66, batch=120, train loss = 0.43150249123573303, train acc = 0.8700000047683716, time = 0.39005470275878906\n",
      "Training at step=66, batch=240, train loss = 0.15822795033454895, train acc = 0.949999988079071, time = 0.38229966163635254\n",
      "Training at step=66, batch=360, train loss = 0.3444163203239441, train acc = 0.8600000143051147, time = 0.39061641693115234\n",
      "Training at step=66, batch=480, train loss = 0.18849687278270721, train acc = 0.9599999785423279, time = 0.3890800476074219\n",
      "Testing at step=66, batch=0, test loss = 0.3009481728076935, test acc = 0.9100000262260437, time = 0.13649678230285645\n",
      "Testing at step=66, batch=20, test loss = 0.34903252124786377, test acc = 0.8399999737739563, time = 0.13866043090820312\n",
      "Testing at step=66, batch=40, test loss = 0.360975980758667, test acc = 0.8600000143051147, time = 0.13859820365905762\n",
      "Testing at step=66, batch=60, test loss = 0.41715964674949646, test acc = 0.8199999928474426, time = 0.1393260955810547\n",
      "Testing at step=66, batch=80, test loss = 0.17091764509677887, test acc = 0.9300000071525574, time = 0.13872528076171875\n",
      "Step 66 finished in 265.95176005363464, Train loss = 0.28209909702340763, Test loss = 0.35212142780423167; Train Acc = 0.9001833355426788, Test Acc = 0.8740999984741211\n",
      "Training at step=67, batch=0, train loss = 0.23307132720947266, train acc = 0.9200000166893005, time = 0.3909780979156494\n",
      "Training at step=67, batch=120, train loss = 0.15024127066135406, train acc = 0.9700000286102295, time = 0.3890869617462158\n",
      "Training at step=67, batch=240, train loss = 0.37819766998291016, train acc = 0.8500000238418579, time = 0.38556575775146484\n",
      "Training at step=67, batch=360, train loss = 0.15363885462284088, train acc = 0.9200000166893005, time = 0.4011704921722412\n",
      "Training at step=67, batch=480, train loss = 0.3172186017036438, train acc = 0.8600000143051147, time = 0.38196730613708496\n",
      "Testing at step=67, batch=0, test loss = 0.39697226881980896, test acc = 0.8399999737739563, time = 0.1385343074798584\n",
      "Testing at step=67, batch=20, test loss = 0.23702333867549896, test acc = 0.8999999761581421, time = 0.13876008987426758\n",
      "Testing at step=67, batch=40, test loss = 0.40278851985931396, test acc = 0.8100000023841858, time = 0.13927626609802246\n",
      "Testing at step=67, batch=60, test loss = 0.2452755868434906, test acc = 0.9100000262260437, time = 0.13801217079162598\n",
      "Testing at step=67, batch=80, test loss = 0.4239751696586609, test acc = 0.8500000238418579, time = 0.1388530731201172\n",
      "Step 67 finished in 266.3943884372711, Train loss = 0.28434277161955834, Test loss = 0.32754191309213637; Train Acc = 0.8988833343982696, Test Acc = 0.8809999990463256\n",
      "Training at step=68, batch=0, train loss = 0.32887592911720276, train acc = 0.8999999761581421, time = 0.39139676094055176\n",
      "Training at step=68, batch=120, train loss = 0.2671404182910919, train acc = 0.8899999856948853, time = 0.39067912101745605\n",
      "Training at step=68, batch=240, train loss = 0.2599954605102539, train acc = 0.8999999761581421, time = 0.388887882232666\n",
      "Training at step=68, batch=360, train loss = 0.25564640760421753, train acc = 0.9200000166893005, time = 0.3902304172515869\n",
      "Training at step=68, batch=480, train loss = 0.26809418201446533, train acc = 0.8999999761581421, time = 0.3839750289916992\n",
      "Testing at step=68, batch=0, test loss = 0.26635316014289856, test acc = 0.8899999856948853, time = 0.13981270790100098\n",
      "Testing at step=68, batch=20, test loss = 0.3814343214035034, test acc = 0.8399999737739563, time = 0.13927936553955078\n",
      "Testing at step=68, batch=40, test loss = 0.2883034944534302, test acc = 0.8899999856948853, time = 0.1390228271484375\n",
      "Testing at step=68, batch=60, test loss = 0.39086055755615234, test acc = 0.8899999856948853, time = 0.13898873329162598\n",
      "Testing at step=68, batch=80, test loss = 0.3329123258590698, test acc = 0.8700000047683716, time = 0.1405329704284668\n",
      "Step 68 finished in 266.16246485710144, Train loss = 0.2814972254261374, Test loss = 0.32726690694689753; Train Acc = 0.9010666672388713, Test Acc = 0.8810999995470047\n",
      "Training at step=69, batch=0, train loss = 0.17036281526088715, train acc = 0.9300000071525574, time = 0.3898313045501709\n",
      "Training at step=69, batch=120, train loss = 0.3300301432609558, train acc = 0.8799999952316284, time = 0.3872218132019043\n",
      "Training at step=69, batch=240, train loss = 0.3948647975921631, train acc = 0.8799999952316284, time = 0.38703298568725586\n",
      "Training at step=69, batch=360, train loss = 0.22527135908603668, train acc = 0.9100000262260437, time = 0.3921189308166504\n",
      "Training at step=69, batch=480, train loss = 0.31746408343315125, train acc = 0.9200000166893005, time = 0.3895268440246582\n",
      "Testing at step=69, batch=0, test loss = 0.3273662328720093, test acc = 0.8799999952316284, time = 0.1390390396118164\n",
      "Testing at step=69, batch=20, test loss = 0.3421873450279236, test acc = 0.8799999952316284, time = 0.14153575897216797\n",
      "Testing at step=69, batch=40, test loss = 0.3609130382537842, test acc = 0.8799999952316284, time = 0.13943862915039062\n",
      "Testing at step=69, batch=60, test loss = 0.20867319405078888, test acc = 0.9200000166893005, time = 0.14019227027893066\n",
      "Testing at step=69, batch=80, test loss = 0.3208252787590027, test acc = 0.8999999761581421, time = 0.15030741691589355\n",
      "Step 69 finished in 265.9333083629608, Train loss = 0.2818023841828108, Test loss = 0.34617825359106064; Train Acc = 0.8999666675925255, Test Acc = 0.8746999996900559\n",
      "Training at step=70, batch=0, train loss = 0.2406749576330185, train acc = 0.8999999761581421, time = 0.38829469680786133\n",
      "Training at step=70, batch=120, train loss = 0.19341443479061127, train acc = 0.9200000166893005, time = 0.38971948623657227\n",
      "Training at step=70, batch=240, train loss = 0.17694124579429626, train acc = 0.9399999976158142, time = 0.38883161544799805\n",
      "Training at step=70, batch=360, train loss = 0.2817384600639343, train acc = 0.9200000166893005, time = 0.38126587867736816\n",
      "Training at step=70, batch=480, train loss = 0.175675168633461, train acc = 0.9399999976158142, time = 0.3902883529663086\n",
      "Testing at step=70, batch=0, test loss = 0.46194618940353394, test acc = 0.8399999737739563, time = 0.13996100425720215\n",
      "Testing at step=70, batch=20, test loss = 0.33915141224861145, test acc = 0.8899999856948853, time = 0.13911104202270508\n",
      "Testing at step=70, batch=40, test loss = 0.37517669796943665, test acc = 0.8700000047683716, time = 0.13863158226013184\n",
      "Testing at step=70, batch=60, test loss = 0.279265433549881, test acc = 0.8899999856948853, time = 0.1392989158630371\n",
      "Testing at step=70, batch=80, test loss = 0.5577123165130615, test acc = 0.800000011920929, time = 0.13845467567443848\n",
      "Step 70 finished in 266.1276559829712, Train loss = 0.28142512947320936, Test loss = 0.3284342668950558; Train Acc = 0.8995166676243146, Test Acc = 0.8821000027656555\n",
      "Training at step=71, batch=0, train loss = 0.3394598364830017, train acc = 0.8600000143051147, time = 0.3907785415649414\n",
      "Training at step=71, batch=120, train loss = 0.5327407121658325, train acc = 0.8399999737739563, time = 0.3822765350341797\n",
      "Training at step=71, batch=240, train loss = 0.26523587107658386, train acc = 0.8999999761581421, time = 0.39400434494018555\n",
      "Training at step=71, batch=360, train loss = 0.31350669264793396, train acc = 0.9100000262260437, time = 0.39258694648742676\n",
      "Training at step=71, batch=480, train loss = 0.2657999098300934, train acc = 0.9100000262260437, time = 0.39139676094055176\n",
      "Testing at step=71, batch=0, test loss = 0.32562434673309326, test acc = 0.8899999856948853, time = 0.1377100944519043\n",
      "Testing at step=71, batch=20, test loss = 0.4031873047351837, test acc = 0.8500000238418579, time = 0.13711762428283691\n",
      "Testing at step=71, batch=40, test loss = 0.3299817144870758, test acc = 0.8600000143051147, time = 0.1382298469543457\n",
      "Testing at step=71, batch=60, test loss = 0.38751211762428284, test acc = 0.8399999737739563, time = 0.13627004623413086\n",
      "Testing at step=71, batch=80, test loss = 0.3671233355998993, test acc = 0.8700000047683716, time = 0.1390399932861328\n",
      "Step 71 finished in 266.48088455200195, Train loss = 0.2796776240815719, Test loss = 0.3372225417196751; Train Acc = 0.9007000010212263, Test Acc = 0.8802999997138977\n",
      "Training at step=72, batch=0, train loss = 0.21370792388916016, train acc = 0.9399999976158142, time = 0.3916611671447754\n",
      "Training at step=72, batch=120, train loss = 0.27764007449150085, train acc = 0.9200000166893005, time = 0.3897590637207031\n",
      "Training at step=72, batch=240, train loss = 0.21061092615127563, train acc = 0.8999999761581421, time = 0.3909468650817871\n",
      "Training at step=72, batch=360, train loss = 0.3396201431751251, train acc = 0.8899999856948853, time = 0.39183926582336426\n",
      "Training at step=72, batch=480, train loss = 0.25983360409736633, train acc = 0.8799999952316284, time = 0.38870668411254883\n",
      "Testing at step=72, batch=0, test loss = 0.36964720487594604, test acc = 0.8700000047683716, time = 0.13877129554748535\n",
      "Testing at step=72, batch=20, test loss = 0.35696613788604736, test acc = 0.8700000047683716, time = 0.13910722732543945\n",
      "Testing at step=72, batch=40, test loss = 0.4744923710823059, test acc = 0.8199999928474426, time = 0.13972854614257812\n",
      "Testing at step=72, batch=60, test loss = 0.23848916590213776, test acc = 0.9599999785423279, time = 0.14014172554016113\n",
      "Testing at step=72, batch=80, test loss = 0.6469067335128784, test acc = 0.7900000214576721, time = 0.14032697677612305\n",
      "Step 72 finished in 266.6604480743408, Train loss = 0.2772824747115374, Test loss = 0.3516753040254116; Train Acc = 0.9015500020980834, Test Acc = 0.8769999986886978\n",
      "Training at step=73, batch=0, train loss = 0.3804546296596527, train acc = 0.8700000047683716, time = 0.390056848526001\n",
      "Training at step=73, batch=120, train loss = 0.31189820170402527, train acc = 0.8600000143051147, time = 0.39455294609069824\n",
      "Training at step=73, batch=240, train loss = 0.3466279208660126, train acc = 0.8899999856948853, time = 0.38763976097106934\n",
      "Training at step=73, batch=360, train loss = 0.23540134727954865, train acc = 0.9399999976158142, time = 0.38140320777893066\n",
      "Training at step=73, batch=480, train loss = 0.2753020226955414, train acc = 0.8700000047683716, time = 0.38847851753234863\n",
      "Testing at step=73, batch=0, test loss = 0.4771619141101837, test acc = 0.7900000214576721, time = 0.13561463356018066\n",
      "Testing at step=73, batch=20, test loss = 0.31004565954208374, test acc = 0.8899999856948853, time = 0.13800048828125\n",
      "Testing at step=73, batch=40, test loss = 0.2840001583099365, test acc = 0.8899999856948853, time = 0.13741064071655273\n",
      "Testing at step=73, batch=60, test loss = 0.23175518214702606, test acc = 0.9200000166893005, time = 0.13914060592651367\n",
      "Testing at step=73, batch=80, test loss = 0.33174392580986023, test acc = 0.8700000047683716, time = 0.13888812065124512\n",
      "Step 73 finished in 266.47179913520813, Train loss = 0.27665180798619987, Test loss = 0.32398839309811595; Train Acc = 0.901683333516121, Test Acc = 0.8862000000476837\n",
      "Training at step=74, batch=0, train loss = 0.2829413115978241, train acc = 0.8700000047683716, time = 0.393202543258667\n",
      "Training at step=74, batch=120, train loss = 0.1425980180501938, train acc = 0.9599999785423279, time = 0.389446496963501\n",
      "Training at step=74, batch=240, train loss = 0.20588308572769165, train acc = 0.949999988079071, time = 0.38904237747192383\n",
      "Training at step=74, batch=360, train loss = 0.25941434502601624, train acc = 0.8799999952316284, time = 0.3891596794128418\n",
      "Training at step=74, batch=480, train loss = 0.24214082956314087, train acc = 0.8999999761581421, time = 0.3882460594177246\n",
      "Testing at step=74, batch=0, test loss = 0.337301641702652, test acc = 0.8500000238418579, time = 0.1377091407775879\n",
      "Testing at step=74, batch=20, test loss = 0.3973279595375061, test acc = 0.8500000238418579, time = 0.1398918628692627\n",
      "Testing at step=74, batch=40, test loss = 0.2699713408946991, test acc = 0.8799999952316284, time = 0.13895130157470703\n",
      "Testing at step=74, batch=60, test loss = 0.3099185526371002, test acc = 0.8899999856948853, time = 0.13990545272827148\n",
      "Testing at step=74, batch=80, test loss = 0.2883036136627197, test acc = 0.9100000262260437, time = 0.13835787773132324\n",
      "Step 74 finished in 266.67307591438293, Train loss = 0.2786534326399366, Test loss = 0.31977008283138275; Train Acc = 0.9010666676362356, Test Acc = 0.8846000021696091\n",
      "Training at step=75, batch=0, train loss = 0.22217431664466858, train acc = 0.9100000262260437, time = 0.39066100120544434\n",
      "Training at step=75, batch=120, train loss = 0.3117690980434418, train acc = 0.8999999761581421, time = 0.3838205337524414\n",
      "Training at step=75, batch=240, train loss = 0.2888457775115967, train acc = 0.9300000071525574, time = 0.3897526264190674\n",
      "Training at step=75, batch=360, train loss = 0.2708929181098938, train acc = 0.9200000166893005, time = 0.3811612129211426\n",
      "Training at step=75, batch=480, train loss = 0.2261907011270523, train acc = 0.9100000262260437, time = 0.408005952835083\n",
      "Testing at step=75, batch=0, test loss = 0.3064340054988861, test acc = 0.8799999952316284, time = 0.13793563842773438\n",
      "Testing at step=75, batch=20, test loss = 0.37349867820739746, test acc = 0.8899999856948853, time = 0.14250493049621582\n",
      "Testing at step=75, batch=40, test loss = 0.3014673888683319, test acc = 0.8899999856948853, time = 0.13866662979125977\n",
      "Testing at step=75, batch=60, test loss = 0.39058682322502136, test acc = 0.8600000143051147, time = 0.13884377479553223\n",
      "Testing at step=75, batch=80, test loss = 0.4311816394329071, test acc = 0.8899999856948853, time = 0.1387467384338379\n",
      "Step 75 finished in 265.665292263031, Train loss = 0.2720767330874999, Test loss = 0.31849999755620956; Train Acc = 0.9027833333611488, Test Acc = 0.8851000010967255\n",
      "Training at step=76, batch=0, train loss = 0.2392522394657135, train acc = 0.9200000166893005, time = 0.38985538482666016\n",
      "Training at step=76, batch=120, train loss = 0.24173599481582642, train acc = 0.9100000262260437, time = 0.39356255531311035\n",
      "Training at step=76, batch=240, train loss = 0.18686142563819885, train acc = 0.949999988079071, time = 0.38745594024658203\n",
      "Training at step=76, batch=360, train loss = 0.31870248913764954, train acc = 0.9100000262260437, time = 0.3812553882598877\n",
      "Training at step=76, batch=480, train loss = 0.32514631748199463, train acc = 0.8999999761581421, time = 0.3932061195373535\n",
      "Testing at step=76, batch=0, test loss = 0.4265592098236084, test acc = 0.8399999737739563, time = 0.13690447807312012\n",
      "Testing at step=76, batch=20, test loss = 0.34843334555625916, test acc = 0.8600000143051147, time = 0.1354503631591797\n",
      "Testing at step=76, batch=40, test loss = 0.426956444978714, test acc = 0.8600000143051147, time = 0.1361255645751953\n",
      "Testing at step=76, batch=60, test loss = 0.3938557803630829, test acc = 0.8299999833106995, time = 0.13597679138183594\n",
      "Testing at step=76, batch=80, test loss = 0.39056137204170227, test acc = 0.8600000143051147, time = 0.13681578636169434\n",
      "Step 76 finished in 266.1846778392792, Train loss = 0.27333219716946283, Test loss = 0.32469679206609725; Train Acc = 0.9020000012715658, Test Acc = 0.8858000010251998\n",
      "Training at step=77, batch=0, train loss = 0.2065691500902176, train acc = 0.9200000166893005, time = 0.39040589332580566\n",
      "Training at step=77, batch=120, train loss = 0.2704235911369324, train acc = 0.8500000238418579, time = 0.38367700576782227\n",
      "Training at step=77, batch=240, train loss = 0.20340721309185028, train acc = 0.9399999976158142, time = 0.3913254737854004\n",
      "Training at step=77, batch=360, train loss = 0.26130959391593933, train acc = 0.9200000166893005, time = 0.3865382671356201\n",
      "Training at step=77, batch=480, train loss = 0.35528767108917236, train acc = 0.8500000238418579, time = 0.38883042335510254\n",
      "Testing at step=77, batch=0, test loss = 0.3547472357749939, test acc = 0.9100000262260437, time = 0.13750219345092773\n",
      "Testing at step=77, batch=20, test loss = 0.3821620047092438, test acc = 0.8600000143051147, time = 0.13843822479248047\n",
      "Testing at step=77, batch=40, test loss = 0.21667015552520752, test acc = 0.8999999761581421, time = 0.13649916648864746\n",
      "Testing at step=77, batch=60, test loss = 0.508058488368988, test acc = 0.8500000238418579, time = 0.13843798637390137\n",
      "Testing at step=77, batch=80, test loss = 0.3957674503326416, test acc = 0.8799999952316284, time = 0.1419236660003662\n",
      "Step 77 finished in 266.67106914520264, Train loss = 0.27688022890438635, Test loss = 0.32989621832966803; Train Acc = 0.9013166678945224, Test Acc = 0.8805000013113022\n",
      "Training at step=78, batch=0, train loss = 0.28412455320358276, train acc = 0.9100000262260437, time = 0.39605283737182617\n",
      "Training at step=78, batch=120, train loss = 0.26340052485466003, train acc = 0.8899999856948853, time = 0.38706064224243164\n",
      "Training at step=78, batch=240, train loss = 0.20359916985034943, train acc = 0.9200000166893005, time = 0.3950009346008301\n",
      "Training at step=78, batch=360, train loss = 0.20253883302211761, train acc = 0.9100000262260437, time = 0.3882308006286621\n",
      "Training at step=78, batch=480, train loss = 0.2305484265089035, train acc = 0.8899999856948853, time = 0.3897724151611328\n",
      "Testing at step=78, batch=0, test loss = 0.26079848408699036, test acc = 0.8999999761581421, time = 0.1387486457824707\n",
      "Testing at step=78, batch=20, test loss = 0.41231095790863037, test acc = 0.8399999737739563, time = 0.13883042335510254\n",
      "Testing at step=78, batch=40, test loss = 0.33833324909210205, test acc = 0.9100000262260437, time = 0.13848304748535156\n",
      "Testing at step=78, batch=60, test loss = 0.3109571933746338, test acc = 0.8799999952316284, time = 0.13903331756591797\n",
      "Testing at step=78, batch=80, test loss = 0.3311237096786499, test acc = 0.8500000238418579, time = 0.13831877708435059\n",
      "Step 78 finished in 266.65819549560547, Train loss = 0.27077924271424614, Test loss = 0.32363807886838913; Train Acc = 0.9031333351135253, Test Acc = 0.8862000012397766\n",
      "Training at step=79, batch=0, train loss = 0.2618864178657532, train acc = 0.9100000262260437, time = 0.3849313259124756\n",
      "Training at step=79, batch=120, train loss = 0.4534306228160858, train acc = 0.8500000238418579, time = 0.39652180671691895\n",
      "Training at step=79, batch=240, train loss = 0.2823081910610199, train acc = 0.9200000166893005, time = 0.38918018341064453\n",
      "Training at step=79, batch=360, train loss = 0.19455820322036743, train acc = 0.949999988079071, time = 0.39078831672668457\n",
      "Training at step=79, batch=480, train loss = 0.39346909523010254, train acc = 0.8600000143051147, time = 0.3895423412322998\n",
      "Testing at step=79, batch=0, test loss = 0.2464998960494995, test acc = 0.8999999761581421, time = 0.13711023330688477\n",
      "Testing at step=79, batch=20, test loss = 0.3843545913696289, test acc = 0.8600000143051147, time = 0.1396934986114502\n",
      "Testing at step=79, batch=40, test loss = 0.5060577392578125, test acc = 0.8399999737739563, time = 0.13884592056274414\n",
      "Testing at step=79, batch=60, test loss = 0.33314913511276245, test acc = 0.8799999952316284, time = 0.13941240310668945\n",
      "Testing at step=79, batch=80, test loss = 0.27851009368896484, test acc = 0.8700000047683716, time = 0.13674521446228027\n",
      "Step 79 finished in 266.43973898887634, Train loss = 0.26822783245394627, Test loss = 0.3285019348561764; Train Acc = 0.9048833354314169, Test Acc = 0.8843999999761581\n",
      "Training at step=80, batch=0, train loss = 0.19689561426639557, train acc = 0.9200000166893005, time = 0.392214298248291\n",
      "Training at step=80, batch=120, train loss = 0.2517284154891968, train acc = 0.8999999761581421, time = 0.3894486427307129\n",
      "Training at step=80, batch=240, train loss = 0.23737043142318726, train acc = 0.9200000166893005, time = 0.38850998878479004\n",
      "Training at step=80, batch=360, train loss = 0.25216150283813477, train acc = 0.9100000262260437, time = 0.38787126541137695\n",
      "Training at step=80, batch=480, train loss = 0.17621348798274994, train acc = 0.949999988079071, time = 0.3907184600830078\n",
      "Testing at step=80, batch=0, test loss = 0.2611564099788666, test acc = 0.8899999856948853, time = 0.13859105110168457\n",
      "Testing at step=80, batch=20, test loss = 0.3242235481739044, test acc = 0.9100000262260437, time = 0.13709020614624023\n",
      "Testing at step=80, batch=40, test loss = 0.2117438167333603, test acc = 0.9200000166893005, time = 0.13691139221191406\n",
      "Testing at step=80, batch=60, test loss = 0.28864380717277527, test acc = 0.9100000262260437, time = 0.13845467567443848\n",
      "Testing at step=80, batch=80, test loss = 0.40765243768692017, test acc = 0.8600000143051147, time = 0.1390843391418457\n",
      "Step 80 finished in 266.4510142803192, Train loss = 0.26606050655245783, Test loss = 0.32291638404130935; Train Acc = 0.9055000005165735, Test Acc = 0.8831000006198884\n",
      "Training at step=81, batch=0, train loss = 0.2037244290113449, train acc = 0.9399999976158142, time = 0.39072251319885254\n",
      "Training at step=81, batch=120, train loss = 0.2337522655725479, train acc = 0.9300000071525574, time = 0.38984084129333496\n",
      "Training at step=81, batch=240, train loss = 0.16165724396705627, train acc = 0.9399999976158142, time = 0.38845038414001465\n",
      "Training at step=81, batch=360, train loss = 0.21200215816497803, train acc = 0.9300000071525574, time = 0.38478684425354004\n",
      "Training at step=81, batch=480, train loss = 0.4187379479408264, train acc = 0.8600000143051147, time = 0.38912320137023926\n",
      "Testing at step=81, batch=0, test loss = 0.3287171423435211, test acc = 0.8500000238418579, time = 0.1531829833984375\n",
      "Testing at step=81, batch=20, test loss = 0.4491884708404541, test acc = 0.8399999737739563, time = 0.13756942749023438\n",
      "Testing at step=81, batch=40, test loss = 0.3227371275424957, test acc = 0.8999999761581421, time = 0.13860392570495605\n",
      "Testing at step=81, batch=60, test loss = 0.38291314244270325, test acc = 0.8100000023841858, time = 0.1385955810546875\n",
      "Testing at step=81, batch=80, test loss = 0.2743512988090515, test acc = 0.8899999856948853, time = 0.13995838165283203\n",
      "Step 81 finished in 266.3233711719513, Train loss = 0.26603800360113383, Test loss = 0.31979166984558105; Train Acc = 0.9062333342432976, Test Acc = 0.8867999982833862\n",
      "Training at step=82, batch=0, train loss = 0.3788347542285919, train acc = 0.8299999833106995, time = 0.3880887031555176\n",
      "Training at step=82, batch=120, train loss = 0.23987802863121033, train acc = 0.9200000166893005, time = 0.38927626609802246\n",
      "Training at step=82, batch=240, train loss = 0.4213508367538452, train acc = 0.8600000143051147, time = 0.3903789520263672\n",
      "Training at step=82, batch=360, train loss = 0.33733248710632324, train acc = 0.8799999952316284, time = 0.3830912113189697\n",
      "Training at step=82, batch=480, train loss = 0.27153730392456055, train acc = 0.8999999761581421, time = 0.389967679977417\n",
      "Testing at step=82, batch=0, test loss = 0.22688041627407074, test acc = 0.9300000071525574, time = 0.13912582397460938\n",
      "Testing at step=82, batch=20, test loss = 0.3501102924346924, test acc = 0.9200000166893005, time = 0.13994789123535156\n",
      "Testing at step=82, batch=40, test loss = 0.18202869594097137, test acc = 0.949999988079071, time = 0.13818049430847168\n",
      "Testing at step=82, batch=60, test loss = 0.3312605321407318, test acc = 0.8799999952316284, time = 0.13826870918273926\n",
      "Testing at step=82, batch=80, test loss = 0.2712923586368561, test acc = 0.9100000262260437, time = 0.1395251750946045\n",
      "Step 82 finished in 266.4230921268463, Train loss = 0.26968771195660035, Test loss = 0.3513959027826786; Train Acc = 0.9036500024795532, Test Acc = 0.8815999996662139\n",
      "Training at step=83, batch=0, train loss = 0.4180450141429901, train acc = 0.8399999737739563, time = 0.3897726535797119\n",
      "Training at step=83, batch=120, train loss = 0.14827708899974823, train acc = 0.949999988079071, time = 0.40061521530151367\n",
      "Training at step=83, batch=240, train loss = 0.2735017240047455, train acc = 0.8399999737739563, time = 0.38912534713745117\n",
      "Training at step=83, batch=360, train loss = 0.29108792543411255, train acc = 0.9100000262260437, time = 0.38189005851745605\n",
      "Training at step=83, batch=480, train loss = 0.16809967160224915, train acc = 0.9599999785423279, time = 0.39048051834106445\n",
      "Testing at step=83, batch=0, test loss = 0.2525486946105957, test acc = 0.9200000166893005, time = 0.13878321647644043\n",
      "Testing at step=83, batch=20, test loss = 0.3352128863334656, test acc = 0.9100000262260437, time = 0.13897442817687988\n",
      "Testing at step=83, batch=40, test loss = 0.33801960945129395, test acc = 0.8399999737739563, time = 0.1387922763824463\n",
      "Testing at step=83, batch=60, test loss = 0.23700900375843048, test acc = 0.9100000262260437, time = 0.13913536071777344\n",
      "Testing at step=83, batch=80, test loss = 0.2260315716266632, test acc = 0.9200000166893005, time = 0.13578104972839355\n",
      "Step 83 finished in 267.20448446273804, Train loss = 0.26554520760973294, Test loss = 0.3199927181005478; Train Acc = 0.9062333329518636, Test Acc = 0.8849000018835068\n",
      "Training at step=84, batch=0, train loss = 0.36522889137268066, train acc = 0.8899999856948853, time = 0.3863499164581299\n",
      "Training at step=84, batch=120, train loss = 0.21276241540908813, train acc = 0.9300000071525574, time = 0.391571044921875\n",
      "Training at step=84, batch=240, train loss = 0.21617698669433594, train acc = 0.8999999761581421, time = 0.39316701889038086\n",
      "Training at step=84, batch=360, train loss = 0.1697271317243576, train acc = 0.949999988079071, time = 0.388333797454834\n",
      "Training at step=84, batch=480, train loss = 0.22168870270252228, train acc = 0.9200000166893005, time = 0.3884470462799072\n",
      "Testing at step=84, batch=0, test loss = 0.352225661277771, test acc = 0.8700000047683716, time = 0.13913249969482422\n",
      "Testing at step=84, batch=20, test loss = 0.35576874017715454, test acc = 0.8799999952316284, time = 0.13593220710754395\n",
      "Testing at step=84, batch=40, test loss = 0.3399370610713959, test acc = 0.8899999856948853, time = 0.1377856731414795\n",
      "Testing at step=84, batch=60, test loss = 0.3294357657432556, test acc = 0.8999999761581421, time = 0.13750386238098145\n",
      "Testing at step=84, batch=80, test loss = 0.2249624878168106, test acc = 0.9300000071525574, time = 0.13703346252441406\n",
      "Step 84 finished in 266.35434341430664, Train loss = 0.2630654394999146, Test loss = 0.33523002564907073; Train Acc = 0.9069000026583671, Test Acc = 0.878600001335144\n",
      "Training at step=85, batch=0, train loss = 0.2587677240371704, train acc = 0.9100000262260437, time = 0.3831493854522705\n",
      "Training at step=85, batch=120, train loss = 0.3354037404060364, train acc = 0.9100000262260437, time = 0.3892240524291992\n",
      "Training at step=85, batch=240, train loss = 0.31401750445365906, train acc = 0.8899999856948853, time = 0.3814065456390381\n",
      "Training at step=85, batch=360, train loss = 0.25326842069625854, train acc = 0.9100000262260437, time = 0.38860034942626953\n",
      "Training at step=85, batch=480, train loss = 0.22901609539985657, train acc = 0.8899999856948853, time = 0.38297033309936523\n",
      "Testing at step=85, batch=0, test loss = 0.33346349000930786, test acc = 0.8500000238418579, time = 0.13841676712036133\n",
      "Testing at step=85, batch=20, test loss = 0.30541256070137024, test acc = 0.8999999761581421, time = 0.13928675651550293\n",
      "Testing at step=85, batch=40, test loss = 0.4249497652053833, test acc = 0.8799999952316284, time = 0.14253687858581543\n",
      "Testing at step=85, batch=60, test loss = 0.22581535577774048, test acc = 0.9200000166893005, time = 0.14149832725524902\n",
      "Testing at step=85, batch=80, test loss = 0.50993812084198, test acc = 0.8299999833106995, time = 0.1388077735900879\n",
      "Step 85 finished in 266.35313963890076, Train loss = 0.2617354279756546, Test loss = 0.33989145964384077; Train Acc = 0.9071666675806046, Test Acc = 0.8793000000715255\n",
      "Training at step=86, batch=0, train loss = 0.0883062332868576, train acc = 0.9700000286102295, time = 0.391326904296875\n",
      "Training at step=86, batch=120, train loss = 0.2444763034582138, train acc = 0.8999999761581421, time = 0.3885231018066406\n",
      "Training at step=86, batch=240, train loss = 0.29450228810310364, train acc = 0.9300000071525574, time = 0.38023972511291504\n",
      "Training at step=86, batch=360, train loss = 0.32363155484199524, train acc = 0.8700000047683716, time = 0.38742661476135254\n",
      "Training at step=86, batch=480, train loss = 0.270332932472229, train acc = 0.8999999761581421, time = 0.3804490566253662\n",
      "Testing at step=86, batch=0, test loss = 0.3574078381061554, test acc = 0.8600000143051147, time = 0.1398787498474121\n",
      "Testing at step=86, batch=20, test loss = 0.37909287214279175, test acc = 0.8199999928474426, time = 0.1376802921295166\n",
      "Testing at step=86, batch=40, test loss = 0.29379037022590637, test acc = 0.9100000262260437, time = 0.1384284496307373\n",
      "Testing at step=86, batch=60, test loss = 0.2830702066421509, test acc = 0.9100000262260437, time = 0.1404283046722412\n",
      "Testing at step=86, batch=80, test loss = 0.33156275749206543, test acc = 0.8600000143051147, time = 0.13871145248413086\n",
      "Step 86 finished in 266.3852038383484, Train loss = 0.25967912339915833, Test loss = 0.3358222749829292; Train Acc = 0.907016669511795, Test Acc = 0.8838000011444092\n",
      "Training at step=87, batch=0, train loss = 0.27297234535217285, train acc = 0.8999999761581421, time = 0.3902459144592285\n",
      "Training at step=87, batch=120, train loss = 0.4134536385536194, train acc = 0.8700000047683716, time = 0.38970255851745605\n",
      "Training at step=87, batch=240, train loss = 0.297503262758255, train acc = 0.8999999761581421, time = 0.38922595977783203\n",
      "Training at step=87, batch=360, train loss = 0.3857479393482208, train acc = 0.800000011920929, time = 0.3887510299682617\n",
      "Training at step=87, batch=480, train loss = 0.26456257700920105, train acc = 0.9100000262260437, time = 0.3890807628631592\n",
      "Testing at step=87, batch=0, test loss = 0.3362557888031006, test acc = 0.8899999856948853, time = 0.13695597648620605\n",
      "Testing at step=87, batch=20, test loss = 0.3461046516895294, test acc = 0.8299999833106995, time = 0.1365823745727539\n",
      "Testing at step=87, batch=40, test loss = 0.2882657051086426, test acc = 0.9200000166893005, time = 0.13884997367858887\n",
      "Testing at step=87, batch=60, test loss = 0.43270978331565857, test acc = 0.8500000238418579, time = 0.13727712631225586\n",
      "Testing at step=87, batch=80, test loss = 0.3167438507080078, test acc = 0.8799999952316284, time = 0.13750171661376953\n",
      "Step 87 finished in 266.264848947525, Train loss = 0.2615155130624771, Test loss = 0.32203685119748116; Train Acc = 0.9066833342115085, Test Acc = 0.8822000002861023\n",
      "Training at step=88, batch=0, train loss = 0.16943956911563873, train acc = 0.9399999976158142, time = 0.3933877944946289\n",
      "Training at step=88, batch=120, train loss = 0.297120064496994, train acc = 0.9100000262260437, time = 0.3956277370452881\n",
      "Training at step=88, batch=240, train loss = 0.29612138867378235, train acc = 0.8799999952316284, time = 0.38970398902893066\n",
      "Training at step=88, batch=360, train loss = 0.36535942554473877, train acc = 0.8700000047683716, time = 0.3880889415740967\n",
      "Training at step=88, batch=480, train loss = 0.24713584780693054, train acc = 0.8899999856948853, time = 0.3859138488769531\n",
      "Testing at step=88, batch=0, test loss = 0.25660887360572815, test acc = 0.9300000071525574, time = 0.13926076889038086\n",
      "Testing at step=88, batch=20, test loss = 0.39065778255462646, test acc = 0.8899999856948853, time = 0.13942503929138184\n",
      "Testing at step=88, batch=40, test loss = 0.25997135043144226, test acc = 0.9100000262260437, time = 0.13849854469299316\n",
      "Testing at step=88, batch=60, test loss = 0.241612508893013, test acc = 0.9200000166893005, time = 0.1400454044342041\n",
      "Testing at step=88, batch=80, test loss = 0.36711665987968445, test acc = 0.8799999952316284, time = 0.13932275772094727\n",
      "Step 88 finished in 266.6729099750519, Train loss = 0.26160787479331093, Test loss = 0.3397047330439091; Train Acc = 0.9072166686256726, Test Acc = 0.8797999984025955\n",
      "Training at step=89, batch=0, train loss = 0.3633173406124115, train acc = 0.8899999856948853, time = 0.3913094997406006\n",
      "Training at step=89, batch=120, train loss = 0.24860040843486786, train acc = 0.8999999761581421, time = 0.3775496482849121\n",
      "Training at step=89, batch=240, train loss = 0.2533774971961975, train acc = 0.9399999976158142, time = 0.3780655860900879\n",
      "Training at step=89, batch=360, train loss = 0.26594921946525574, train acc = 0.8999999761581421, time = 0.3818180561065674\n",
      "Training at step=89, batch=480, train loss = 0.29668670892715454, train acc = 0.8299999833106995, time = 0.3767356872558594\n",
      "Testing at step=89, batch=0, test loss = 0.5114341974258423, test acc = 0.7699999809265137, time = 0.13396906852722168\n",
      "Testing at step=89, batch=20, test loss = 0.2889432907104492, test acc = 0.8700000047683716, time = 0.13391327857971191\n",
      "Testing at step=89, batch=40, test loss = 0.3205544352531433, test acc = 0.8700000047683716, time = 0.1354198455810547\n",
      "Testing at step=89, batch=60, test loss = 0.1886473447084427, test acc = 0.9700000286102295, time = 0.1342158317565918\n",
      "Testing at step=89, batch=80, test loss = 0.28086766600608826, test acc = 0.9100000262260437, time = 0.13497495651245117\n",
      "Step 89 finished in 259.26723170280457, Train loss = 0.25586299803107976, Test loss = 0.3216835267841816; Train Acc = 0.9077666686971982, Test Acc = 0.888600001335144\n",
      "Training at step=90, batch=0, train loss = 0.19404901564121246, train acc = 0.9399999976158142, time = 0.37958741188049316\n",
      "Training at step=90, batch=120, train loss = 0.21107444167137146, train acc = 0.8999999761581421, time = 0.3940298557281494\n",
      "Training at step=90, batch=240, train loss = 0.3040490448474884, train acc = 0.8999999761581421, time = 0.37685656547546387\n",
      "Training at step=90, batch=360, train loss = 0.173477903008461, train acc = 0.9300000071525574, time = 0.3779122829437256\n",
      "Training at step=90, batch=480, train loss = 0.25368252396583557, train acc = 0.9300000071525574, time = 0.3787705898284912\n",
      "Testing at step=90, batch=0, test loss = 0.3256402313709259, test acc = 0.8899999856948853, time = 0.13376259803771973\n",
      "Testing at step=90, batch=20, test loss = 0.3112925589084625, test acc = 0.8700000047683716, time = 0.13561511039733887\n",
      "Testing at step=90, batch=40, test loss = 0.18686281144618988, test acc = 0.9300000071525574, time = 0.13451123237609863\n",
      "Testing at step=90, batch=60, test loss = 0.26553598046302795, test acc = 0.9300000071525574, time = 0.1335594654083252\n",
      "Testing at step=90, batch=80, test loss = 0.3155744671821594, test acc = 0.8399999737739563, time = 0.1449892520904541\n",
      "Step 90 finished in 258.9176323413849, Train loss = 0.25612545639276507, Test loss = 0.32923059180378916; Train Acc = 0.9082500011722247, Test Acc = 0.8849999994039536\n",
      "Training at step=91, batch=0, train loss = 0.21509267389774323, train acc = 0.8899999856948853, time = 0.37796545028686523\n",
      "Training at step=91, batch=120, train loss = 0.19204305112361908, train acc = 0.9100000262260437, time = 0.37795233726501465\n",
      "Training at step=91, batch=240, train loss = 0.15123599767684937, train acc = 0.9399999976158142, time = 0.37752819061279297\n",
      "Training at step=91, batch=360, train loss = 0.19949260354042053, train acc = 0.949999988079071, time = 0.3761563301086426\n",
      "Training at step=91, batch=480, train loss = 0.22317498922348022, train acc = 0.8799999952316284, time = 0.3802177906036377\n",
      "Testing at step=91, batch=0, test loss = 0.31825798749923706, test acc = 0.8799999952316284, time = 0.1349325180053711\n",
      "Testing at step=91, batch=20, test loss = 0.2737407982349396, test acc = 0.9100000262260437, time = 0.13754010200500488\n",
      "Testing at step=91, batch=40, test loss = 0.304623007774353, test acc = 0.9100000262260437, time = 0.13496136665344238\n",
      "Testing at step=91, batch=60, test loss = 0.29312604665756226, test acc = 0.8899999856948853, time = 0.13428759574890137\n",
      "Testing at step=91, batch=80, test loss = 0.4022164046764374, test acc = 0.8399999737739563, time = 0.13527774810791016\n",
      "Step 91 finished in 259.5137355327606, Train loss = 0.2538407520080606, Test loss = 0.32579774484038354; Train Acc = 0.9096500010291735, Test Acc = 0.8877000021934509\n",
      "Training at step=92, batch=0, train loss = 0.24159599840641022, train acc = 0.9100000262260437, time = 0.37843775749206543\n",
      "Training at step=92, batch=120, train loss = 0.13493189215660095, train acc = 0.949999988079071, time = 0.3765385150909424\n",
      "Training at step=92, batch=240, train loss = 0.27311596274375916, train acc = 0.9100000262260437, time = 0.38576221466064453\n",
      "Training at step=92, batch=360, train loss = 0.23494365811347961, train acc = 0.9200000166893005, time = 0.37663888931274414\n",
      "Training at step=92, batch=480, train loss = 0.21913377940654755, train acc = 0.9399999976158142, time = 0.37725257873535156\n",
      "Testing at step=92, batch=0, test loss = 0.49610617756843567, test acc = 0.8500000238418579, time = 0.1347370147705078\n",
      "Testing at step=92, batch=20, test loss = 0.20030373334884644, test acc = 0.9200000166893005, time = 0.14425992965698242\n",
      "Testing at step=92, batch=40, test loss = 0.12975597381591797, test acc = 0.949999988079071, time = 0.13401532173156738\n",
      "Testing at step=92, batch=60, test loss = 0.34472987055778503, test acc = 0.8500000238418579, time = 0.13608431816101074\n",
      "Testing at step=92, batch=80, test loss = 0.3022714853286743, test acc = 0.8999999761581421, time = 0.13439416885375977\n",
      "Step 92 finished in 259.36370515823364, Train loss = 0.25312532905489205, Test loss = 0.3196719565987587; Train Acc = 0.9101000008980433, Test Acc = 0.8862999999523162\n",
      "Training at step=93, batch=0, train loss = 0.30405598878860474, train acc = 0.8999999761581421, time = 0.3782162666320801\n",
      "Training at step=93, batch=120, train loss = 0.24015271663665771, train acc = 0.9300000071525574, time = 0.3772854804992676\n",
      "Training at step=93, batch=240, train loss = 0.2549247741699219, train acc = 0.9200000166893005, time = 0.3777744770050049\n",
      "Training at step=93, batch=360, train loss = 0.15149880945682526, train acc = 0.9399999976158142, time = 0.37682271003723145\n",
      "Training at step=93, batch=480, train loss = 0.1539769321680069, train acc = 0.9599999785423279, time = 0.37683629989624023\n",
      "Testing at step=93, batch=0, test loss = 0.18357108533382416, test acc = 0.9399999976158142, time = 0.13460326194763184\n",
      "Testing at step=93, batch=20, test loss = 0.2905542850494385, test acc = 0.8799999952316284, time = 0.13436269760131836\n",
      "Testing at step=93, batch=40, test loss = 0.29945147037506104, test acc = 0.8799999952316284, time = 0.13446760177612305\n",
      "Testing at step=93, batch=60, test loss = 0.42116573452949524, test acc = 0.8600000143051147, time = 0.13452649116516113\n",
      "Testing at step=93, batch=80, test loss = 0.35600170493125916, test acc = 0.8700000047683716, time = 0.1357870101928711\n",
      "Step 93 finished in 259.06035256385803, Train loss = 0.2515345566347241, Test loss = 0.33087722286581994; Train Acc = 0.9099500019351642, Test Acc = 0.8807000011205673\n",
      "Training at step=94, batch=0, train loss = 0.20612014830112457, train acc = 0.9200000166893005, time = 0.3847928047180176\n",
      "Training at step=94, batch=120, train loss = 0.20524711906909943, train acc = 0.9200000166893005, time = 0.37773609161376953\n",
      "Training at step=94, batch=240, train loss = 0.22282223403453827, train acc = 0.9100000262260437, time = 0.37697505950927734\n",
      "Training at step=94, batch=360, train loss = 0.25123292207717896, train acc = 0.9300000071525574, time = 0.381497859954834\n",
      "Training at step=94, batch=480, train loss = 0.3365207612514496, train acc = 0.9100000262260437, time = 0.37683773040771484\n",
      "Testing at step=94, batch=0, test loss = 0.3629787564277649, test acc = 0.8600000143051147, time = 0.13410210609436035\n",
      "Testing at step=94, batch=20, test loss = 0.36469948291778564, test acc = 0.8700000047683716, time = 0.13515663146972656\n",
      "Testing at step=94, batch=40, test loss = 0.3609539866447449, test acc = 0.8700000047683716, time = 0.13404107093811035\n",
      "Testing at step=94, batch=60, test loss = 0.4044751822948456, test acc = 0.8799999952316284, time = 0.13412761688232422\n",
      "Testing at step=94, batch=80, test loss = 0.32469260692596436, test acc = 0.8700000047683716, time = 0.134718656539917\n",
      "Step 94 finished in 259.36233043670654, Train loss = 0.24872901993493238, Test loss = 0.31659249812364576; Train Acc = 0.9121166677276293, Test Acc = 0.890799999833107\n",
      "Training at step=95, batch=0, train loss = 0.28446024656295776, train acc = 0.8999999761581421, time = 0.38054752349853516\n",
      "Training at step=95, batch=120, train loss = 0.16212953627109528, train acc = 0.949999988079071, time = 0.37850213050842285\n",
      "Training at step=95, batch=240, train loss = 0.18763624131679535, train acc = 0.9399999976158142, time = 0.37991857528686523\n",
      "Training at step=95, batch=360, train loss = 0.228939950466156, train acc = 0.9300000071525574, time = 0.3777885437011719\n",
      "Training at step=95, batch=480, train loss = 0.200729638338089, train acc = 0.9100000262260437, time = 0.3784677982330322\n",
      "Testing at step=95, batch=0, test loss = 0.3699552118778229, test acc = 0.8600000143051147, time = 0.13395285606384277\n",
      "Testing at step=95, batch=20, test loss = 0.3095667362213135, test acc = 0.8999999761581421, time = 0.13637423515319824\n",
      "Testing at step=95, batch=40, test loss = 0.35506394505500793, test acc = 0.8500000238418579, time = 0.1338191032409668\n",
      "Testing at step=95, batch=60, test loss = 0.33196189999580383, test acc = 0.8999999761581421, time = 0.13909196853637695\n",
      "Testing at step=95, batch=80, test loss = 0.4552413821220398, test acc = 0.8600000143051147, time = 0.13503599166870117\n",
      "Step 95 finished in 259.29441380500793, Train loss = 0.25059017387529214, Test loss = 0.30774660915136337; Train Acc = 0.9108166694641113, Test Acc = 0.8930000042915345\n",
      "Training at step=96, batch=0, train loss = 0.2134774923324585, train acc = 0.9300000071525574, time = 0.38265180587768555\n",
      "Training at step=96, batch=120, train loss = 0.3372906446456909, train acc = 0.8700000047683716, time = 0.3777008056640625\n",
      "Training at step=96, batch=240, train loss = 0.44433075189590454, train acc = 0.8299999833106995, time = 0.3770778179168701\n",
      "Training at step=96, batch=360, train loss = 0.25563472509384155, train acc = 0.9200000166893005, time = 0.37670397758483887\n",
      "Training at step=96, batch=480, train loss = 0.14610746502876282, train acc = 0.9599999785423279, time = 0.3808932304382324\n",
      "Testing at step=96, batch=0, test loss = 0.3255491554737091, test acc = 0.8799999952316284, time = 0.13510513305664062\n",
      "Testing at step=96, batch=20, test loss = 0.24098660051822662, test acc = 0.8899999856948853, time = 0.13454818725585938\n",
      "Testing at step=96, batch=40, test loss = 0.3074410557746887, test acc = 0.8999999761581421, time = 0.13648700714111328\n",
      "Testing at step=96, batch=60, test loss = 0.1602502316236496, test acc = 0.9700000286102295, time = 0.1346285343170166\n",
      "Testing at step=96, batch=80, test loss = 0.2745094895362854, test acc = 0.8700000047683716, time = 0.13887977600097656\n",
      "Step 96 finished in 258.77168321609497, Train loss = 0.24907149750739335, Test loss = 0.31097178295254707; Train Acc = 0.9119000013669332, Test Acc = 0.8923000007867813\n",
      "Training at step=97, batch=0, train loss = 0.23964743316173553, train acc = 0.9100000262260437, time = 0.3819262981414795\n",
      "Training at step=97, batch=120, train loss = 0.18081849813461304, train acc = 0.9300000071525574, time = 0.3773164749145508\n",
      "Training at step=97, batch=240, train loss = 0.30581969022750854, train acc = 0.9100000262260437, time = 0.3764951229095459\n",
      "Training at step=97, batch=360, train loss = 0.2349885106086731, train acc = 0.9300000071525574, time = 0.37918567657470703\n",
      "Training at step=97, batch=480, train loss = 0.26997125148773193, train acc = 0.8799999952316284, time = 0.3773682117462158\n",
      "Testing at step=97, batch=0, test loss = 0.34100043773651123, test acc = 0.8700000047683716, time = 0.1343977451324463\n",
      "Testing at step=97, batch=20, test loss = 0.3982478380203247, test acc = 0.8899999856948853, time = 0.1381242275238037\n",
      "Testing at step=97, batch=40, test loss = 0.28522586822509766, test acc = 0.8999999761581421, time = 0.1351938247680664\n",
      "Testing at step=97, batch=60, test loss = 0.1853223741054535, test acc = 0.9300000071525574, time = 0.13459396362304688\n",
      "Testing at step=97, batch=80, test loss = 0.3974092900753021, test acc = 0.8799999952316284, time = 0.13462567329406738\n",
      "Step 97 finished in 259.0749771595001, Train loss = 0.2457626991843184, Test loss = 0.30888364151120185; Train Acc = 0.9120333353678386, Test Acc = 0.8943000012636184\n",
      "Training at step=98, batch=0, train loss = 0.3408215045928955, train acc = 0.8600000143051147, time = 0.3934333324432373\n",
      "Training at step=98, batch=120, train loss = 0.2892489433288574, train acc = 0.8899999856948853, time = 0.376300573348999\n",
      "Training at step=98, batch=240, train loss = 0.24502037465572357, train acc = 0.9200000166893005, time = 0.3796229362487793\n",
      "Training at step=98, batch=360, train loss = 0.42233601212501526, train acc = 0.8600000143051147, time = 0.3790280818939209\n",
      "Training at step=98, batch=480, train loss = 0.24082699418067932, train acc = 0.9200000166893005, time = 0.38643622398376465\n",
      "Testing at step=98, batch=0, test loss = 0.4360715448856354, test acc = 0.8899999856948853, time = 0.13469290733337402\n",
      "Testing at step=98, batch=20, test loss = 0.31256115436553955, test acc = 0.8999999761581421, time = 0.1398909091949463\n",
      "Testing at step=98, batch=40, test loss = 0.26402899622917175, test acc = 0.9100000262260437, time = 0.13432526588439941\n",
      "Testing at step=98, batch=60, test loss = 0.40833863615989685, test acc = 0.8500000238418579, time = 0.13353538513183594\n",
      "Testing at step=98, batch=80, test loss = 0.30506518483161926, test acc = 0.8700000047683716, time = 0.13378310203552246\n",
      "Step 98 finished in 259.46684312820435, Train loss = 0.24705261489997307, Test loss = 0.3180288600921631; Train Acc = 0.9119166685144107, Test Acc = 0.8912999987602234\n",
      "Training at step=99, batch=0, train loss = 0.25648754835128784, train acc = 0.8899999856948853, time = 0.37921881675720215\n",
      "Training at step=99, batch=120, train loss = 0.11021117866039276, train acc = 0.9599999785423279, time = 0.3831784725189209\n",
      "Training at step=99, batch=240, train loss = 0.2044025957584381, train acc = 0.9100000262260437, time = 0.37708449363708496\n",
      "Training at step=99, batch=360, train loss = 0.3367290496826172, train acc = 0.8700000047683716, time = 0.38034629821777344\n",
      "Training at step=99, batch=480, train loss = 0.2586863338947296, train acc = 0.8899999856948853, time = 0.3786194324493408\n",
      "Testing at step=99, batch=0, test loss = 0.4818618893623352, test acc = 0.8199999928474426, time = 0.1347978115081787\n",
      "Testing at step=99, batch=20, test loss = 0.3083461821079254, test acc = 0.8999999761581421, time = 0.1353456974029541\n",
      "Testing at step=99, batch=40, test loss = 0.30398666858673096, test acc = 0.8799999952316284, time = 0.13483810424804688\n",
      "Testing at step=99, batch=60, test loss = 0.2929264307022095, test acc = 0.8899999856948853, time = 0.14171504974365234\n",
      "Testing at step=99, batch=80, test loss = 0.35276171565055847, test acc = 0.8700000047683716, time = 0.134307861328125\n",
      "Step 99 finished in 259.6227385997772, Train loss = 0.2451588665942351, Test loss = 0.309097770601511; Train Acc = 0.9123000030716261, Test Acc = 0.8937999999523163\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"1-fashionmnist_quanv_classical_linear.pdf\")\n",
    "# Display the plots\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "U0Q0vFm7B6cg",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1711488462684,
     "user_tz": -660,
     "elapsed": 1244,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "a805910a-6214-4218-d940-429f1bae5d2e"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "\n",
    "res_dict = {\n",
    "    \"train_losses\": train_losses,\n",
    "    \"test_losses\": test_losses,\n",
    "    \"train_accs\": train_accs,\n",
    "    \"test_accs\": test_accs\n",
    "}\n",
    "\n",
    "with open(\"1-fashionmnist_quanv_classical_linear.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ]
  }
 ]
}
