{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:22:52.948872Z",
     "iopub.status.busy": "2024-04-03T08:22:52.948409Z",
     "iopub.status.idle": "2024-04-03T08:24:31.832368Z",
     "shell.execute_reply": "2024-04-03T08:24:31.831751Z",
     "shell.execute_reply.started": "2024-04-03T08:22:52.948838Z"
    },
    "id": "5ebnFsErY0EM",
    "outputId": "e8932b66-715c-467f-912a-8b449c518214",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.12.1+cu116)\n",
      "Collecting torch\n",
      "  Downloading torch-2.2.2-cp39-cp39-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.13.1+cu116)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp39-cp39-manylinux1_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchaudio in /usr/local/lib/python3.9/dist-packages (0.12.1+cu116)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.2.2-cp39-cp39-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch) (2023.1.0)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=4.8.0\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-nccl-cu12==2.19.3\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.2.0\n",
      "  Downloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.23.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, typing-extensions, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1+cu116\n",
      "    Uninstalling torch-1.12.1+cu116:\n",
      "      Successfully uninstalled torch-1.12.1+cu116\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.13.1+cu116\n",
      "    Uninstalling torchvision-0.13.1+cu116:\n",
      "      Successfully uninstalled torchvision-0.13.1+cu116\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 0.12.1+cu116\n",
      "    Uninstalling torchaudio-0.12.1+cu116:\n",
      "      Successfully uninstalled torchaudio-0.12.1+cu116\n",
      "Successfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 sympy-1.12 torch-2.2.2 torchaudio-2.2.2 torchvision-0.17.2 triton-2.2.0 typing-extensions-4.10.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pennylane\n",
      "  Downloading PennyLane-0.35.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cotengra\n",
      "  Downloading cotengra-0.5.6-py3-none-any.whl (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.0/148.0 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting quimb\n",
      "  Downloading quimb-1.7.3-py3-none-any.whl (500 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.7/500.7 kB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchmetrics\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from pennylane) (2.28.2)\n",
      "Collecting pennylane-lightning>=0.35\n",
      "  Downloading PennyLane_Lightning-0.35.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.5/18.5 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting rustworkx\n",
      "  Downloading rustworkx-0.14.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pennylane) (4.10.0)\n",
      "Collecting autoray>=0.6.1\n",
      "  Downloading autoray-0.6.9-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from pennylane) (3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.23.4)\n",
      "Collecting autograd\n",
      "  Downloading autograd-1.6.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.9/dist-packages (from pennylane) (5.3.0)\n",
      "Collecting appdirs\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pennylane) (1.9.2)\n",
      "Collecting semantic-version>=2.7\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting numba>=0.39\n",
      "  Downloading numba-0.59.1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cytoolz>=0.8.0\n",
      "  Downloading cytoolz-0.12.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from quimb) (5.9.4)\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.9/dist-packages (from quimb) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (2.2.2)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics) (23.0)\n",
      "Collecting lightning-utilities>=0.8.0\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (66.1.1)\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0\n",
      "  Downloading llvmlite-0.42.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2023.1.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.10.0->torchmetrics) (3.9.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.9/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.99)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/lib/python3/dist-packages (from autograd->pennylane) (0.18.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->pennylane) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->pennylane) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->pennylane) (2019.11.28)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Installing collected packages: appdirs, toml, semantic-version, rustworkx, llvmlite, lightning-utilities, cytoolz, autoray, autograd, numba, cotengra, quimb, torchmetrics, pennylane-lightning, pennylane\n",
      "Successfully installed appdirs-1.4.4 autograd-1.6.2 autoray-0.6.9 cotengra-0.5.6 cytoolz-0.12.3 lightning-utilities-0.11.2 llvmlite-0.42.0 numba-0.59.1 pennylane-0.35.1 pennylane-lightning-0.35.1 quimb-1.7.3 rustworkx-0.14.2 semantic-version-2.10.0 toml-0.10.2 torchmetrics-1.3.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install packages\n",
    "!pip install torch torchvision torchaudio --upgrade\n",
    "!pip install pennylane cotengra quimb torchmetrics --upgrade\n",
    "#!pip install -U \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:25:45.093069Z",
     "iopub.status.busy": "2024-04-03T08:25:45.092790Z",
     "iopub.status.idle": "2024-04-03T08:26:54.096113Z",
     "shell.execute_reply": "2024-04-03T08:26:54.095258Z",
     "shell.execute_reply.started": "2024-04-03T08:25:45.093044Z"
    },
    "id": "VN2wD-U7bGse",
    "outputId": "efc73948-0832-402c-eafc-b0e6adce4a54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "\n",
    "\n",
    "#import pennylane as qml\n",
    "#import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "#prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "#torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WmQPQuLbSFw"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:54.097750Z",
     "iopub.status.busy": "2024-04-03T08:26:54.097431Z",
     "iopub.status.idle": "2024-04-03T08:26:54.405968Z",
     "shell.execute_reply": "2024-04-03T08:26:54.405273Z",
     "shell.execute_reply.started": "2024-04-03T08:26:54.097729Z"
    },
    "id": "rUhQUP2dbTja",
    "outputId": "0d3641db-a2ff-4689-8b7e-32e0e8b313d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 32, 32])\n",
      "torch.Size([32])\n",
      "tensor([4, 1, 7, 3, 5, 8, 2, 6, 4, 6, 6, 3, 6, 9, 9, 7, 8, 7, 7, 7, 2, 7, 3, 8,\n",
      "        8, 8, 2, 2, 8, 2, 1, 5])\n",
      "tensor([-1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j,  0.7176+0.j,  0.5686+0.j,  0.8588+0.j,  0.2235+0.j,  0.0902+0.j,\n",
      "         0.9059+0.j,  0.5529+0.j,  0.6000+0.j,  0.6078+0.j,  0.6392+0.j,  0.6314+0.j,\n",
      "         0.6157+0.j,  0.8039+0.j,  0.4039+0.j, -0.0039+0.j,  0.9922+0.j,  0.6314+0.j,\n",
      "         0.7490+0.j, -0.6235+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j, -1.0000+0.j,\n",
      "        -1.0000+0.j, -1.0000+0.j])\n"
     ]
    }
   ],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQI8WWY6byQq"
   },
   "source": [
    "# Some Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:54.407720Z",
     "iopub.status.busy": "2024-04-03T08:26:54.407518Z",
     "iopub.status.idle": "2024-04-03T08:26:56.071027Z",
     "shell.execute_reply": "2024-04-03T08:26:56.069162Z",
     "shell.execute_reply.started": "2024-04-03T08:26:54.407700Z"
    },
    "id": "7B4DTncWbwQl",
    "outputId": "38975b7c-66c4-49f5-e5fd-158e55028cdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-0.j, 0.+0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j,  0.+0.j, -1.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.+0.j, 1.+0.j],\n",
      "        [0.+0.j, 0.+0.j, 1.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 1.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-0.j, 0.+0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.+1.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  0.+0.j,  1.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j,  0.+0.j, -1.+0.j],\n",
      "        [ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, 0.+0.j, 0.-0.j, 0.-1.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-0.j, 0.-1.j],\n",
      "        [0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, 0.+1.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  0.-0.j,  0.-0.j, -1.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j,  1.-0.j,  0.-0.j],\n",
      "        [ 0.+0.j,  1.-0.j,  0.+0.j,  0.-0.j],\n",
      "        [-1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.+0.j, 0.-1.j, 0.-0.j],\n",
      "        [0.+0.j, -0.+0.j, 0.-0.j, 0.+1.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, -0.-1.j, 0.+0.j, -0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -0.+0.j, -1.+0.j]], device='cuda:0'), tensor([[ 0.+0.j,  1.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -0.+0.j, -1.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j]], device='cuda:0'), tensor([[0.+0.j, 0.-1.j, 0.+0.j, 0.-0.j],\n",
      "        [0.+1.j, 0.+0.j, 0.+0.j, 0.+0.j],\n",
      "        [0.+0.j, 0.-0.j, -0.+0.j, 0.+1.j],\n",
      "        [0.+0.j, 0.+0.j, -0.-1.j, -0.+0.j]], device='cuda:0'), tensor([[ 1.+0.j,  0.+0.j,  0.+0.j,  0.+0.j],\n",
      "        [ 0.+0.j, -1.+0.j,  0.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j,  0.+0.j, -1.+0.j, -0.+0.j],\n",
      "        [ 0.+0.j, -0.+0.j, -0.+0.j,  1.-0.j]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "ket = {\n",
    "    '0':torch.tensor([1.,0.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '1':torch.tensor([0.,1.], dtype = COMPLEX_DTYPE, device=device),\n",
    "    '+':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) + torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device)),\n",
    "    '-':(torch.tensor([1,0], dtype = COMPLEX_DTYPE, device=device) - torch.tensor([0,1], dtype = COMPLEX_DTYPE, device=device))/torch.sqrt(torch.tensor(2, dtype = COMPLEX_DTYPE, device=device))\n",
    "}\n",
    "\n",
    "pauli = {\n",
    "    'I':torch.tensor([[1.,0.],[0.,1.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'X':torch.tensor([[0.,1.],[1.,0.]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Y':torch.tensor([[0., -1.j],[1.j, 0]], dtype = COMPLEX_DTYPE, device=device),\n",
    "    'Z':torch.tensor([[1.,0.],[0.,-1.]], dtype = COMPLEX_DTYPE, device=device)\n",
    "}\n",
    "\n",
    "def tensor_product(*args):\n",
    "  input_list = [a for a in args]\n",
    "  return functools.reduce(torch.kron, input_list)\n",
    "\n",
    "def multi_qubit_identity(n_qubits:int):\n",
    "  assert n_qubits>0\n",
    "  if n_qubits == 1:\n",
    "    return pauli['I']\n",
    "  else:\n",
    "    return tensor_product(*[pauli['I'] for _ in range(n_qubits)])\n",
    "\n",
    "pauli_words_su4 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    if not (key1==key2 and key1=='I' and key2=='I'):\n",
    "      #print(key1, key2)\n",
    "      #print(pauli[key1])\n",
    "      #print(pauli[key2])\n",
    "      pauli_words_su4[key1+key2] = tensor_product(pauli[key1], pauli[key2])\n",
    "      #print(pauli_words_su4[key1+key2])\n",
    "\n",
    "pauli_words_su8 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      if not key1+key2+key3 == 'III':\n",
    "        pauli_words_su8[key1+key2+key3] = tensor_product(pauli[key1], pauli[key2], pauli[key3])\n",
    "\n",
    "pauli_words_su16 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        if not key1+key2+key3+key4 == 'IIII':\n",
    "          pauli_words_su16[key1+key2+key3+key4] = tensor_product(\n",
    "              pauli[key1],\n",
    "              pauli[key2],\n",
    "              pauli[key3],\n",
    "              pauli[key4]\n",
    "          )\n",
    "\n",
    "pauli_words_su32 = {}\n",
    "for key1 in pauli.keys():\n",
    "  for key2 in pauli.keys():\n",
    "    for key3 in pauli.keys():\n",
    "      for key4 in pauli.keys():\n",
    "        for key5 in pauli.keys():\n",
    "          if not key1+key2+key3+key4+key5 == 'IIIII':\n",
    "            pauli_words_su32[key1+key2+key3+key4+key5] = tensor_product(\n",
    "                pauli[key1],\n",
    "                pauli[key2],\n",
    "                pauli[key3],\n",
    "                pauli[key4],\n",
    "                pauli[key5]\n",
    "            )\n",
    "\n",
    "def su32_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = list(pauli_words_su32.values())\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, torch.stack(dict_values))\n",
    "  #print(generator.shape)\n",
    "  return torch.matrix_exp(1j*generator)\n",
    "\n",
    "def su4_op(\n",
    "    params\n",
    "):\n",
    "  dict_values = torch.stack(list(pauli_words_su4.values()))\n",
    "  #print(dict_values.shape)\n",
    "  generator = torch.einsum(\"i, ijk -> jk\", params, dict_values)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "def pauli_dict_func(key):\n",
    "    return pauli[key]\n",
    "\n",
    "def pauli_dict_func_multiple_keys(keys):\n",
    "    return list(map(pauli_dict_func, keys))\n",
    "\n",
    "def pauli_string_tensor_prod(pauli_string:str):\n",
    "    paulis_char = list(pauli_string)\n",
    "    paulis_mat = pauli_dict_func_multiple_keys(paulis_char)\n",
    "    return tensor_product(*paulis_mat)\n",
    "\n",
    "def generate_nqubit_pauli_strings(n_qubits:int):\n",
    "    assert n_qubits>0\n",
    "    pauli_labels = ['I', 'X', 'Y', 'Z']\n",
    "    pauli_strings = []\n",
    "    for labels in itertools.product(pauli_labels, repeat=n_qubits):\n",
    "        pauli_str = \"\".join(labels)\n",
    "        if pauli_str != 'I'*n_qubits:\n",
    "            pauli_strings.append(pauli_str)\n",
    "    return pauli_strings\n",
    "\n",
    "def generate_pauli_tensor_list(pauli_strings:list):\n",
    "    return list(map(pauli_string_tensor_prod, pauli_strings))\n",
    "\n",
    "\n",
    "print(\n",
    "    generate_pauli_tensor_list(generate_nqubit_pauli_strings(2))\n",
    ")\n",
    "\n",
    "\n",
    "#test_params = torch.randn(4**2-1,  device=device).type(COMPLEX_DTYPE)\n",
    "#print(test_params.shape)\n",
    "#test_op = su4_op(test_params)\n",
    "#print(test_op)\n",
    "#print(qml.is_unitary(qml.QubitUnitary(test_op.cpu().numpy(), wires=[0,1]))) # will be false if not torch.cdouble\n",
    "\n",
    "#rx = torch.linalg.matrix_exp(1j*torch.pi*pauli[\"X\"]/2)\n",
    "#print(qml.is_unitary(qml.QubitUnitary(rx.cpu().numpy(), wires=[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.073077Z",
     "iopub.status.busy": "2024-04-03T08:26:56.072615Z",
     "iopub.status.idle": "2024-04-03T08:26:56.079092Z",
     "shell.execute_reply": "2024-04-03T08:26:56.078390Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.073053Z"
    },
    "id": "Xs0c2F1eBnGc"
   },
   "outputs": [],
   "source": [
    "def measure_sv(\n",
    "    state,\n",
    "    observable\n",
    "    ):\n",
    "  \"\"\"\n",
    "  Measure a statevector with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not\n",
    "  \"\"\"\n",
    "  expectation_value = torch.conj(state)@observable@state\n",
    "  return torch.real(expectation_value)\n",
    "\n",
    "def measure_dm(\n",
    "    rho,\n",
    "    observable\n",
    "):\n",
    "  \"\"\"\n",
    "  Measure a density matrix with a Hermitian observable.\n",
    "  Note: No checking Hermitianicity of the observable or whether the observable\n",
    "  has all real eigenvalues or not.\n",
    "  \"\"\"\n",
    "  product = torch.matmul(rho, observable)\n",
    "\n",
    "  # Calculate the trace, which is the sum of diagonal elements\n",
    "  trace = torch.trace(product)\n",
    "\n",
    "  # The expectation value should be real for physical observables\n",
    "  return torch.real(trace)\n",
    "\n",
    "vmap_measure_sv = torch.vmap(measure_sv, in_dims=(None, 0), out_dims=0)\n",
    "vmap_measure_dm = torch.vmap(measure_dm, in_dims=(None, 0), out_dims=0)\n",
    "\n",
    "# assuming the input patch observables (hermitianized) has shape (batchsize,n_patches, c, h, w)\n",
    "# assuming the input set statevectors has shape (c, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_sv_batched_ob = torch.vmap(measure_sv, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "# assuming the input set desnity matrices has shape (batchsize, n_patches, c, 2**n, 2**n)\n",
    "# output should have the shape (batchsize,n_patches, channel)\n",
    "vmap_measure_channel_dm_batched_ob = torch.vmap(measure_dm, in_dims = (-2, -3),out_dims=-1)\n",
    "\n",
    "\n",
    "def bitstring_to_state(bitstring:str):\n",
    "  \"\"\"\n",
    "  Convert a bit string, like '0101001' or '+-+-101'\n",
    "  to a statevector. Each character in the bitstring must be among\n",
    "  0, 1, + and -\n",
    "  \"\"\"\n",
    "  assert len(bitstring)>0\n",
    "  for c in bitstring:\n",
    "    assert c in ['0', '1', '+', '-']\n",
    "  single_qubit_states = [ket[c] for c in bitstring]\n",
    "  return tensor_product(*single_qubit_states)\n",
    "\n",
    "\n",
    "#test_patch = torch.randn(size=[5, 2, 3, 4,4], dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_herm_patch = (torch.einsum(\"...jk->...kj\", test_patch)+test_patch)/2\n",
    "#print(test_herm_patch)\n",
    "#print(\"all patches shape\", test_herm_patch.shape)\n",
    "#test_sv = torch.stack([bitstring_to_state('++')]*3, axis = 0)\n",
    "#print(test_sv)\n",
    "#print(\"all sv shape\", test_sv.shape)\n",
    "#res = vmap_measure_channel_sv_batched_ob(test_sv, test_herm_patch)\n",
    "#print(res)\n",
    "#print(\"res shape\",res.shape)\n",
    "\n",
    "#for batchid in range(test_patch.shape[0]):\n",
    "#  batchitem = test_patch[batchid]\n",
    "#  for patch_idx in range(batchitem.shape[0]):\n",
    "#    patch = batchitem[patch_idx]\n",
    "#    for ch_idx in range(patch.shape[0]):\n",
    "#      print(f\"batchid={batchid}; patchid = {patch_idx}; chid = {ch_idx}\")\n",
    "#      ch = patch[ch_idx]\n",
    "#      #print(ch.shape)\n",
    "#      sv_ch = test_sv[ch_idx]\n",
    "#      #print(sv_ch.shape)\n",
    "#      print(measure_sv(sv_ch, ch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFZqT6bpElaL"
   },
   "source": [
    "# Flipped Quanvolution Kernel\n",
    "\n",
    "First, let's define the function to extract patches for a single kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.080443Z",
     "iopub.status.busy": "2024-04-03T08:26:56.079871Z",
     "iopub.status.idle": "2024-04-03T08:26:56.085801Z",
     "shell.execute_reply": "2024-04-03T08:26:56.085133Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.080420Z"
    },
    "id": "He4HdMRHC7T6"
   },
   "outputs": [],
   "source": [
    "def extract_patches(image, patch_size, stride, padding=None):\n",
    "    \"\"\"\n",
    "    Extracts patches from an image with multiple input channels and optional custom padding.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (in_channels, height, width).\n",
    "        patch_size (int): Size of the square patches to extract.\n",
    "        stride (int): Stride between patches.\n",
    "        padding (tuple): Padding value(s) for each dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of extracted patches of shape (num_patches, in_channels, patch_size, patch_size).\n",
    "    \"\"\"\n",
    "    in_channels, height, width = image.shape[-3], image.shape[-2], image.shape[-1]\n",
    "    pad_l, pad_r, pad_t, pad_b = padding if padding is not None else (0,0,0,0)\n",
    "\n",
    "    if padding is not None:\n",
    "        image = torch.nn.functional.pad(image, (pad_l, pad_r, pad_t, pad_b), mode='constant')\n",
    "    else:\n",
    "        image = image\n",
    "\n",
    "    height, width = image.shape[-2],  image.shape[-1]\n",
    "    num_patches_h = (height - patch_size) // stride + 1\n",
    "    num_patches_w = (width - patch_size) // stride + 1\n",
    "\n",
    "    patches = []\n",
    "    for i in range(num_patches_h):\n",
    "        for j in range(num_patches_w):\n",
    "            patch = image[..., i*stride:i*stride+patch_size, j*stride:j*stride+patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    patches = torch.stack(patches, dim=-4)\n",
    "    return patches # has shape (batchsize, n_patches, channel, h, w)\n",
    "\n",
    "\n",
    "#single_img = torch.stack([torch.arange(32*32*1,dtype = torch.double, device=device).reshape((32,32)).type(torch.cdouble)]*3)\n",
    "\n",
    "#test_img = torch.stack([single_img]*2)\n",
    "#print(test_img[0][...,:4,:4])\n",
    "#patches = extract_patches(test_img, patch_size=4, stride=2,padding=(0,0,0,0))\n",
    "#print(patches.shape)\n",
    "#print(patches[0].shape)\n",
    "#print(patches[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRqflooEqKN"
   },
   "source": [
    "## A Three-by-Three Quanv Kernel Function\n",
    "\n",
    "For 3 by 3 patches, we only need a 2-qubit parameterised state, generated by the SU4 gate applied to the $|00\\rangle$ state. The first thing to do is to pad the 3 by 3 patches to 4 by 4. However, for input with size\n",
    "\n",
    "$$\n",
    "D_1\\times H_1 \\times W_1\n",
    "$$\n",
    "\n",
    "where $D_1$ is the number of input channels, we will need $D_1$ different two-qubit states, generated by $D_1$ different parameterised circuits. These circuits will produce $D_1$ expectation values. Suppose each circuit is parameterised by $\\theta_i, i\\in \\{1,2,\\cdots, D_1\\}$, the parameterised (pure) state can be written as\n",
    "\n",
    "$$\n",
    "|\\psi (\\theta_i)⟩ = SU4(\\theta_i)|00⟩\n",
    "$$\n",
    "\n",
    "For a channel $M_i$ of patch $M$ with shape $4\\times 4$, the expectation value is calculated by\n",
    "\n",
    "$$\n",
    "⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$\n",
    "\n",
    "Then, a single pixel value of the output channel corresponding this single kernel is\n",
    "\n",
    "$$\n",
    "\\frac{1}{D_1}\\sum_{i=1}^{D_1}⟨\\psi(\\theta_i)|\\left(\\frac{M_i+M^T_i}{2}\\right)|\\psi(\\theta_i)⟩\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.086697Z",
     "iopub.status.busy": "2024-04-03T08:26:56.086517Z",
     "iopub.status.idle": "2024-04-03T08:26:56.091796Z",
     "shell.execute_reply": "2024-04-03T08:26:56.091178Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.086679Z"
    },
    "id": "Yzn4KEt5ErG7"
   },
   "outputs": [],
   "source": [
    "def generate_2q_param_state(theta):\n",
    "  state = bitstring_to_state('00')\n",
    "  state = torch.matmul(su4_op(theta), state)\n",
    "  return state\n",
    "\n",
    "vmap_generate_2q_param_state = torch.vmap(generate_2q_param_state, in_dims=0, out_dims = 0)\n",
    "\n",
    "def single_kernel_op_over_batched_patches(thetas, patch):\n",
    "  # patch has shape (c_in, h, w)\n",
    "  # thetas has shape (c_in, 4^2-1) for SU4 gates\n",
    "  n_theta = thetas.shape[-2]\n",
    "  n_channel = patch.shape[-3]\n",
    "  assert n_theta == n_channel, \"Thetas and patch must have the same number of channels.\"\n",
    "  states = vmap_generate_2q_param_state(thetas)\n",
    "  #print(\"States shape\", states.shape)\n",
    "  patch = torch.nn.functional.pad(patch, (0, 1, 0, 1), mode='constant')\n",
    "  patch_t = torch.einsum(\"...jk->...kj\", patch)\n",
    "  herm_patch = (patch_t+patch)/2 # has dim (batchsize, num_patches, c, h, w)\n",
    "  #print(\"Herm patch shape\", herm_patch.shape)\n",
    "  channel_out = vmap_measure_channel_sv_batched_ob(states, herm_patch) # has dim (batchsize,n_patches, c)\n",
    "  return torch.sum(channel_out, axis = -1)/n_theta # has dim (batchszie, n_patches)\n",
    "\n",
    "\n",
    "#test_params = torch.randn((1,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_img = dummy_x.to(device)#.type(torch.cdouble)\n",
    "#print(test_img.shape)\n",
    "#test_patches = extract_patches(test_img, patch_size=3, stride=1,padding=(1,1,1,1))\n",
    "#print(test_patches.shape)\n",
    "#print(test_params.shape)\n",
    "#print(vmap_generate_2q_param_state(test_params))\n",
    "#test_out = single_kernel_op_over_batched_patches(test_params, test_patches)\n",
    "#print(test_out.shape)\n",
    "#h = (32-3+1*2)//1 +1\n",
    "#h**2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4BXEKd8I67_"
   },
   "source": [
    "## Multiple Output Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.092668Z",
     "iopub.status.busy": "2024-04-03T08:26:56.092483Z",
     "iopub.status.idle": "2024-04-03T08:26:56.095952Z",
     "shell.execute_reply": "2024-04-03T08:26:56.095319Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.092650Z"
    },
    "id": "72vkHV_BI80l"
   },
   "outputs": [],
   "source": [
    "# For multiple channel output\n",
    "# parameter has shape (c_out, c_in, 4**2-1) for SU4 gates\n",
    "vmap_vmap_single_kernel_op_through_extracted_patches = torch.vmap(single_kernel_op_over_batched_patches, in_dims=(0, None), out_dims=-2) # output has dim (batchsize, c_out, n_patches)\n",
    "\n",
    "#c_in = 1\n",
    "#c_out = 2\n",
    "\n",
    "#test_params2 = torch.randn((c_out, c_in,4**2-1), dtype = torch.double, device=device).type(torch.cdouble)\n",
    "#test_out2 = vmap_vmap_single_kernel_op_through_extracted_patches(test_params2, test_patches)\n",
    "#print(test_out2.shape)\n",
    "#test_out_2_features = test_out2.reshape((-1,c_out, 32, 32))\n",
    "#print(test_out_2_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuZ6G5-sKK_J"
   },
   "source": [
    "## Pytorch Module for Three-by-Three Flipped Quanv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.096785Z",
     "iopub.status.busy": "2024-04-03T08:26:56.096604Z",
     "iopub.status.idle": "2024-04-03T08:26:56.103091Z",
     "shell.execute_reply": "2024-04-03T08:26:56.102542Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.096768Z"
    },
    "id": "Gww_XdJ5KPJt"
   },
   "outputs": [],
   "source": [
    "class FlippedQuanv3x3(torch.nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, padding):\n",
    "    super(FlippedQuanv3x3, self).__init__()\n",
    "    self.in_channels = in_channels\n",
    "    self.out_channels = out_channels\n",
    "    self.stride = stride\n",
    "    self.padding = padding\n",
    "    self.pad_l, self.pad_r, self.pad_t, self.pad_b = padding if padding is not None else (0,0,0,0)\n",
    "    self.pad = (self.pad_l, self.pad_r, self.pad_t, self.pad_b)\n",
    "    self.weight= torch.nn.Parameter(torch.randn((out_channels, in_channels, 4**2-1)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn((out_channels, 1)).type(COMPLEX_DTYPE))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has shape (batchsize ,c_in, h, w)\n",
    "    # weight has shape (c_out, c_in, 15)\n",
    "    # bias has shape (c_out, 1)\n",
    "    x = x.type(COMPLEX_DTYPE)\n",
    "    c_in, h_in, w_in = x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "    patches = extract_patches(x, patch_size=3, stride=self.stride, padding=self.pad)\n",
    "    h_out = (h_in-3+(self.pad_t+self.pad_b))//self.stride +1\n",
    "    w_out = (w_in-3+(self.pad_l+self.pad_r))//self.stride +1\n",
    "\n",
    "    #print(self.weight.shape)\n",
    "    #print(patches.shape)\n",
    "\n",
    "    out = vmap_vmap_single_kernel_op_through_extracted_patches(self.weight, patches)\n",
    "\n",
    "\n",
    "    out = out + self.bias\n",
    "    return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_channels={self.in_channels}, out_channels={self.out_channels}, stride={self.stride}, padding={self.padding}'\n",
    "\n",
    "#test_module = FlippedQuanv3x3(in_channels=1, out_channels=2, stride=1, padding=(1,1,1,1)).to(device)\n",
    "#print(dummy_x.shape)\n",
    "#test_out = test_module(dummy_x.to(device))\n",
    "#print(test_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jY9sQZ7Ynla"
   },
   "source": [
    "# Linear Layer\n",
    "\n",
    "For input dimension $D$, the quantum linear layer is a $n = \\lceil \\log_4(D+1)⌉$-qubit quantum circuit. Both the data encoding and parameterised circuit is achieved via the $SU(2^n)$ unitary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:56.103883Z",
     "iopub.status.busy": "2024-04-03T08:26:56.103708Z",
     "iopub.status.idle": "2024-04-03T08:26:57.562457Z",
     "shell.execute_reply": "2024-04-03T08:26:57.561682Z",
     "shell.execute_reply.started": "2024-04-03T08:26:56.103865Z"
    },
    "id": "AXxNIObFYnPW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0847, 0.6649, 0.0675, 0.1829], device='cuda:0')\n",
      "tensor(1.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def data_encode_unitary(padded_data, t):\n",
    "  original_dim = padded_data.shape[-1]\n",
    "  new_dim = torch.sqrt(torch.tensor(original_dim)).type(torch.int)\n",
    "  data = torch.reshape(padded_data, (new_dim, new_dim))\n",
    "  generator = (data + torch.einsum(\"...jk->...kj\", data))/2\n",
    "  return torch.linalg.matrix_exp(1.0j*generator*t)\n",
    "\n",
    "def su_n(params, pauli_string_tensor_list):\n",
    "  # params has dim 4**n-1\n",
    "  paulis = torch.stack(pauli_string_tensor_list)\n",
    "  generator = torch.einsum(\"i,ijk->jk\", params, paulis)\n",
    "  return torch.linalg.matrix_exp(1.0j*generator)\n",
    "\n",
    "def linear_layer_func(padded_data, params, pauli_string_tensor_list, observables, n_qubits):\n",
    "  n_rep = params.shape[0]\n",
    "  state = bitstring_to_state(\"+\"*n_qubits)\n",
    "  for i in range(n_rep):\n",
    "    data_unitary = data_encode_unitary(padded_data, 1/n_rep)\n",
    "    #print(data_unitary.shape)\n",
    "    #print(state.shape)\n",
    "    state = torch.matmul(\n",
    "      data_unitary,\n",
    "      state\n",
    "    )\n",
    "    sun_gate = su_n(params[i], pauli_string_tensor_list)\n",
    "    #print(sun_gate.shape)\n",
    "    state = torch.matmul(\n",
    "      sun_gate,\n",
    "      state\n",
    "    )\n",
    "  return vmap_measure_sv(state, observables)\n",
    "\n",
    "test_params = torch.randn((3, 4**2-1),device=device).type(COMPLEX_DTYPE)\n",
    "test_data = torch.randn(4**2, device=device).type(COMPLEX_DTYPE)\n",
    "test_obs = torch.stack([torch.outer(bitstring_to_state('00'), bitstring_to_state('00')),\n",
    "                        torch.outer(bitstring_to_state('01'), bitstring_to_state('01')),\n",
    "                        torch.outer(bitstring_to_state('10'), bitstring_to_state('10')),\n",
    "                         torch.outer(bitstring_to_state('11'), bitstring_to_state('11'))])\n",
    "test_pauli_string_tensor_list = generate_pauli_tensor_list(generate_nqubit_pauli_strings(2))\n",
    "\n",
    "\n",
    "test_out = linear_layer_func(test_data, test_params, test_pauli_string_tensor_list, test_obs, 2)\n",
    "print(test_out)\n",
    "print(torch.sum(test_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:57.564844Z",
     "iopub.status.busy": "2024-04-03T08:26:57.564417Z",
     "iopub.status.idle": "2024-04-03T08:26:57.576557Z",
     "shell.execute_reply": "2024-04-03T08:26:57.575906Z",
     "shell.execute_reply.started": "2024-04-03T08:26:57.564823Z"
    },
    "id": "2F4_SBgIYnMv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3414, 0.4328, 0.0295, 0.1964],\n",
      "        [0.4175, 0.1313, 0.2182, 0.2330],\n",
      "        [0.1889, 0.5187, 0.1613, 0.1311]], device='cuda:0')\n",
      "tensor(3.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "vmap_batch_linear_layer_func = torch.vmap(linear_layer_func, in_dims=(0, None, None, None, None), out_dims=0)\n",
    "\n",
    "test_data = torch.randn((3, 4**2), device=device).type(COMPLEX_DTYPE)\n",
    "test_out = vmap_batch_linear_layer_func(test_data, test_params, test_pauli_string_tensor_list, test_obs, 2)\n",
    "print(test_out)\n",
    "print(torch.sum(test_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryROGtnC_3po"
   },
   "source": [
    "## PyTorch Module for the Quantum Version of Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:57.577462Z",
     "iopub.status.busy": "2024-04-03T08:26:57.577280Z",
     "iopub.status.idle": "2024-04-03T08:26:57.605899Z",
     "shell.execute_reply": "2024-04-03T08:26:57.605187Z",
     "shell.execute_reply.started": "2024-04-03T08:26:57.577443Z"
    },
    "id": "RlTC952w_8VW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 45])\n",
      "torch.Size([16, 6])\n",
      "DataReUploadingLinear(in_dim=45, out_dim=6, n_qubits=3, n_reps=10)\n"
     ]
    }
   ],
   "source": [
    "class DataReUploadingLinear(torch.nn.Module):\n",
    "  def __init__(self, in_dim, out_dim, n_qubits, n_reps):\n",
    "    super(DataReUploadingLinear, self).__init__()\n",
    "    assert 2**n_qubits >= out_dim\n",
    "    assert 4**n_qubits >= in_dim\n",
    "    self.in_dim = in_dim\n",
    "    self.out_dim = out_dim\n",
    "    self.n_qubits = n_qubits\n",
    "    self.n_reps = n_reps\n",
    "    self.pauli_string_tensor_list = generate_pauli_tensor_list(generate_nqubit_pauli_strings(n_qubits))\n",
    "    self.param_dim = 4**n_qubits-1\n",
    "    self.params = torch.nn.Parameter(torch.randn((self.n_reps,self.param_dim)).type(COMPLEX_DTYPE))\n",
    "    self.bias = torch.nn.Parameter(torch.randn(out_dim).type(REAL_DTYPE))\n",
    "    self.observables = self.generate_observables()\n",
    "    self.pad_size = 4**n_qubits-self.in_dim\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x has size (batchsize, in_dim)\n",
    "    # pad x\n",
    "    x = torch.nn.functional.pad(x, (0, self.pad_size)).type(COMPLEX_DTYPE)\n",
    "    out = vmap_batch_linear_layer_func(x, self.params, self.pauli_string_tensor_list, self.observables, self.n_qubits)\n",
    "    out = out.type(REAL_DTYPE) + self.bias\n",
    "    return out\n",
    "\n",
    "  def generate_observables(self):\n",
    "    observables = []\n",
    "    for i in range(self.out_dim):\n",
    "      temp_bitstring = '{0:b}'.format(i).zfill(self.n_qubits)\n",
    "      ob = torch.outer(bitstring_to_state(temp_bitstring), bitstring_to_state(temp_bitstring))\n",
    "      observables.append(ob)\n",
    "    return torch.stack(observables)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "        return f'in_dim={self.in_dim}, out_dim={self.out_dim}, n_qubits={self.n_qubits}, n_reps={self.n_reps}'\n",
    "\n",
    "test_linear_module = DataReUploadingLinear(in_dim=45, out_dim=6, n_qubits=3, n_reps = 10).to(device)\n",
    "test_data = torch.randn((16, 45), device=device).type(COMPLEX_DTYPE)\n",
    "print(test_data.shape)\n",
    "test_out = test_linear_module(test_data.to(device))\n",
    "print(test_out.shape)\n",
    "print(test_linear_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yumZTjDVu63"
   },
   "source": [
    "# Replacing Classical Layers with Quantum Layers\n",
    "\n",
    "Let's replace `Conv2d` with `FlippedQuanv3x3`, and `Linear` with `QuantLinear`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:57.606924Z",
     "iopub.status.busy": "2024-04-03T08:26:57.606727Z",
     "iopub.status.idle": "2024-04-03T08:26:59.000203Z",
     "shell.execute_reply": "2024-04-03T08:26:58.999449Z",
     "shell.execute_reply.started": "2024-04-03T08:26:57.606905Z"
    },
    "id": "W4RC5ou-VzbE",
    "outputId": "9eb0b35d-ed18-431b-8a53-ba5216d3fd06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 32, 32])\n",
      "HybridNet(\n",
      "  (layers): Sequential(\n",
      "    (0): FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None)\n",
      "    (1): FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None)\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): DataReUploadingLinear(in_dim=12544, out_dim=10, n_qubits=7, n_reps=10)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91/4168075707.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n",
      "/tmp/ipykernel_91/1684779817.py:30: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:301.)\n",
      "  return out.reshape((-1,self.out_channels, h_out, w_out)).type(REAL_DTYPE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "class HybridNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(HybridNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        FlippedQuanv3x3(in_channels=1, out_channels=32, stride=1, padding=None), # 32->30\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        FlippedQuanv3x3(in_channels=32, out_channels=16, stride=1, padding=None), # 30->28\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        DataReUploadingLinear(16*28*28, 10, 7, 10)\n",
    "        #torch.nn.Linear(32*14*14, 10).type(REAL_DTYPE)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    logits = self.layers(x)\n",
    "    return logits\n",
    "\n",
    "\n",
    "net = HybridNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "execution": {
     "iopub.execute_input": "2024-04-03T08:26:59.001280Z",
     "iopub.status.busy": "2024-04-03T08:26:59.001080Z",
     "iopub.status.idle": "2024-04-03T17:06:02.379965Z",
     "shell.execute_reply": "2024-04-03T17:06:02.379200Z",
     "shell.execute_reply.started": "2024-04-03T08:26:59.001260Z"
    },
    "id": "x4ZY59q1WS4x",
    "outputId": "6532aea1-47a3-49f3-fe36-b774aee1905f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train batches = 300, Number of test batches = 50\n",
      "Print every train batch = 30, Print every test batch = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91/4168075707.py:85: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::matrix_exp. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at ../aten/src/ATen/functorch/BatchedFallback.cpp:82.)\n",
      "  return torch.linalg.matrix_exp(1.0j*generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training at step=0, batch=0, train loss = 2.6324901580810547, train acc = 0.11999999731779099, time = 0.9708313941955566\n",
      "Training at step=0, batch=30, train loss = 2.633659601211548, train acc = 0.09000000357627869, time = 0.9256386756896973\n",
      "Training at step=0, batch=60, train loss = 2.636003017425537, train acc = 0.10999999940395355, time = 0.919835090637207\n",
      "Training at step=0, batch=90, train loss = 2.7073450088500977, train acc = 0.07999999821186066, time = 0.9231009483337402\n",
      "Training at step=0, batch=120, train loss = 2.644988775253296, train acc = 0.07999999821186066, time = 0.9220941066741943\n",
      "Training at step=0, batch=150, train loss = 2.6260995864868164, train acc = 0.09000000357627869, time = 0.937889814376831\n",
      "Training at step=0, batch=180, train loss = 2.5308444499969482, train acc = 0.14499999582767487, time = 0.9341142177581787\n",
      "Training at step=0, batch=210, train loss = 2.5431880950927734, train acc = 0.08500000089406967, time = 0.936614990234375\n",
      "Training at step=0, batch=240, train loss = 2.4410648345947266, train acc = 0.0949999988079071, time = 0.9390780925750732\n",
      "Training at step=0, batch=270, train loss = 2.2734453678131104, train acc = 0.20000000298023224, time = 0.9584786891937256\n",
      "Testing at step=0, batch=0, test loss = 1.9773999452590942, test acc = 0.29499998688697815, time = 0.3387792110443115\n",
      "Testing at step=0, batch=5, test loss = 1.8858060836791992, test acc = 0.35499998927116394, time = 0.3401021957397461\n",
      "Testing at step=0, batch=10, test loss = 1.845513105392456, test acc = 0.4099999964237213, time = 0.34239721298217773\n",
      "Testing at step=0, batch=15, test loss = 1.8958139419555664, test acc = 0.33500000834465027, time = 0.34099578857421875\n",
      "Testing at step=0, batch=20, test loss = 1.992842197418213, test acc = 0.3199999928474426, time = 0.33533668518066406\n",
      "Testing at step=0, batch=25, test loss = 1.8771522045135498, test acc = 0.3700000047683716, time = 0.3457660675048828\n",
      "Testing at step=0, batch=30, test loss = 1.8399581909179688, test acc = 0.4050000011920929, time = 0.33496809005737305\n",
      "Testing at step=0, batch=35, test loss = 1.9402482509613037, test acc = 0.29499998688697815, time = 0.3359565734863281\n",
      "Testing at step=0, batch=40, test loss = 1.7956640720367432, test acc = 0.38999998569488525, time = 0.3390336036682129\n",
      "Testing at step=0, batch=45, test loss = 1.8283978700637817, test acc = 0.4000000059604645, time = 0.35030221939086914\n",
      "Step 0 finished in 306.2977685928345, Train loss = 2.521625447273254, Test loss = 1.8800033116340638; Train Acc = 0.11729999976853529, Test Acc = 0.36109999895095823\n",
      "Training at step=1, batch=0, train loss = 1.898888349533081, train acc = 0.36000001430511475, time = 0.9463081359863281\n",
      "Training at step=1, batch=30, train loss = 1.2082985639572144, train acc = 0.6000000238418579, time = 0.9523632526397705\n",
      "Training at step=1, batch=60, train loss = 0.9217548966407776, train acc = 0.675000011920929, time = 0.9486651420593262\n",
      "Training at step=1, batch=90, train loss = 0.796904981136322, train acc = 0.7099999785423279, time = 0.923468828201294\n",
      "Training at step=1, batch=120, train loss = 0.9339263439178467, train acc = 0.675000011920929, time = 0.9283149242401123\n",
      "Training at step=1, batch=150, train loss = 0.6475290060043335, train acc = 0.7450000047683716, time = 0.9471018314361572\n",
      "Training at step=1, batch=180, train loss = 0.7375340461730957, train acc = 0.7350000143051147, time = 0.941962480545044\n",
      "Training at step=1, batch=210, train loss = 0.6775838732719421, train acc = 0.7599999904632568, time = 0.9283485412597656\n",
      "Training at step=1, batch=240, train loss = 0.5683263540267944, train acc = 0.7799999713897705, time = 0.9378125667572021\n",
      "Training at step=1, batch=270, train loss = 0.6484802961349487, train acc = 0.75, time = 0.9335026741027832\n",
      "Testing at step=1, batch=0, test loss = 0.6801043152809143, test acc = 0.7549999952316284, time = 0.35442066192626953\n",
      "Testing at step=1, batch=5, test loss = 0.5662970542907715, test acc = 0.7850000262260437, time = 0.34638118743896484\n",
      "Testing at step=1, batch=10, test loss = 0.661701500415802, test acc = 0.7749999761581421, time = 0.33960938453674316\n",
      "Testing at step=1, batch=15, test loss = 0.6583585143089294, test acc = 0.7599999904632568, time = 0.33861756324768066\n",
      "Testing at step=1, batch=20, test loss = 0.5506714582443237, test acc = 0.7900000214576721, time = 0.3425478935241699\n",
      "Testing at step=1, batch=25, test loss = 0.5271157026290894, test acc = 0.8199999928474426, time = 0.3379976749420166\n",
      "Testing at step=1, batch=30, test loss = 0.608951210975647, test acc = 0.7549999952316284, time = 0.3590199947357178\n",
      "Testing at step=1, batch=35, test loss = 0.6015451550483704, test acc = 0.800000011920929, time = 0.35439586639404297\n",
      "Testing at step=1, batch=40, test loss = 0.6270647644996643, test acc = 0.7799999713897705, time = 0.35724973678588867\n",
      "Testing at step=1, batch=45, test loss = 0.6899468302726746, test acc = 0.75, time = 0.33861684799194336\n",
      "Step 1 finished in 307.6617577075958, Train loss = 0.8088569776217143, Test loss = 0.6175058603286743; Train Acc = 0.713400000333786, Test Acc = 0.7769999992847443\n",
      "Training at step=2, batch=0, train loss = 0.5864378213882446, train acc = 0.7749999761581421, time = 0.9549341201782227\n",
      "Training at step=2, batch=30, train loss = 0.6797444820404053, train acc = 0.7649999856948853, time = 0.921292781829834\n",
      "Training at step=2, batch=60, train loss = 0.7019408345222473, train acc = 0.75, time = 0.9365947246551514\n",
      "Training at step=2, batch=90, train loss = 0.6285226345062256, train acc = 0.7850000262260437, time = 0.9470512866973877\n",
      "Training at step=2, batch=120, train loss = 0.6104055643081665, train acc = 0.7599999904632568, time = 0.9366273880004883\n",
      "Training at step=2, batch=150, train loss = 0.4700329601764679, train acc = 0.8450000286102295, time = 0.9505891799926758\n",
      "Training at step=2, batch=180, train loss = 0.6058640480041504, train acc = 0.7950000166893005, time = 0.9251630306243896\n",
      "Training at step=2, batch=210, train loss = 0.7153646349906921, train acc = 0.7749999761581421, time = 0.9299242496490479\n",
      "Training at step=2, batch=240, train loss = 0.6634766459465027, train acc = 0.7799999713897705, time = 0.9338922500610352\n",
      "Training at step=2, batch=270, train loss = 0.46847155690193176, train acc = 0.8500000238418579, time = 0.9297370910644531\n",
      "Testing at step=2, batch=0, test loss = 0.49200230836868286, test acc = 0.8149999976158142, time = 0.34009289741516113\n",
      "Testing at step=2, batch=5, test loss = 0.4819304645061493, test acc = 0.8199999928474426, time = 0.34291696548461914\n",
      "Testing at step=2, batch=10, test loss = 0.6328757405281067, test acc = 0.7850000262260437, time = 0.34381651878356934\n",
      "Testing at step=2, batch=15, test loss = 0.523841142654419, test acc = 0.7900000214576721, time = 0.35250234603881836\n",
      "Testing at step=2, batch=20, test loss = 0.47848397493362427, test acc = 0.8100000023841858, time = 0.3377864360809326\n",
      "Testing at step=2, batch=25, test loss = 0.538916289806366, test acc = 0.7950000166893005, time = 0.34536027908325195\n",
      "Testing at step=2, batch=30, test loss = 0.5316864252090454, test acc = 0.8100000023841858, time = 0.3364567756652832\n",
      "Testing at step=2, batch=35, test loss = 0.5990573763847351, test acc = 0.800000011920929, time = 0.35259532928466797\n",
      "Testing at step=2, batch=40, test loss = 0.6923640370368958, test acc = 0.7400000095367432, time = 0.3550119400024414\n",
      "Testing at step=2, batch=45, test loss = 0.5766544938087463, test acc = 0.7699999809265137, time = 0.34300875663757324\n",
      "Step 2 finished in 306.82078218460083, Train loss = 0.5489970763524373, Test loss = 0.6041200667619705; Train Acc = 0.8002000008026758, Test Acc = 0.7780000042915344\n",
      "Training at step=3, batch=0, train loss = 0.6248357892036438, train acc = 0.7450000047683716, time = 0.9368243217468262\n",
      "Training at step=3, batch=30, train loss = 0.5774136185646057, train acc = 0.7900000214576721, time = 0.932159423828125\n",
      "Training at step=3, batch=60, train loss = 0.5021598935127258, train acc = 0.8299999833106995, time = 0.9256596565246582\n",
      "Training at step=3, batch=90, train loss = 0.5038403868675232, train acc = 0.8199999928474426, time = 0.9369168281555176\n",
      "Training at step=3, batch=120, train loss = 0.4877021312713623, train acc = 0.8199999928474426, time = 0.9336762428283691\n",
      "Training at step=3, batch=150, train loss = 0.507901132106781, train acc = 0.8050000071525574, time = 0.9217844009399414\n",
      "Training at step=3, batch=180, train loss = 0.4510159194469452, train acc = 0.8450000286102295, time = 0.9373834133148193\n",
      "Training at step=3, batch=210, train loss = 0.5315421223640442, train acc = 0.7950000166893005, time = 0.9284279346466064\n",
      "Training at step=3, batch=240, train loss = 0.5781252980232239, train acc = 0.7850000262260437, time = 0.9382567405700684\n",
      "Training at step=3, batch=270, train loss = 0.42382732033729553, train acc = 0.8600000143051147, time = 0.9349644184112549\n",
      "Testing at step=3, batch=0, test loss = 0.5318158864974976, test acc = 0.8349999785423279, time = 0.34195947647094727\n",
      "Testing at step=3, batch=5, test loss = 0.5113268494606018, test acc = 0.824999988079071, time = 0.3488028049468994\n",
      "Testing at step=3, batch=10, test loss = 0.5872256755828857, test acc = 0.7799999713897705, time = 0.33650851249694824\n",
      "Testing at step=3, batch=15, test loss = 0.4935346841812134, test acc = 0.8199999928474426, time = 0.33765745162963867\n",
      "Testing at step=3, batch=20, test loss = 0.5136545896530151, test acc = 0.8349999785423279, time = 0.33858394622802734\n",
      "Testing at step=3, batch=25, test loss = 0.5938969254493713, test acc = 0.800000011920929, time = 0.343090295791626\n",
      "Testing at step=3, batch=30, test loss = 0.5366051197052002, test acc = 0.7900000214576721, time = 0.3507814407348633\n",
      "Testing at step=3, batch=35, test loss = 0.49045249819755554, test acc = 0.824999988079071, time = 0.363598108291626\n",
      "Testing at step=3, batch=40, test loss = 0.6978700160980225, test acc = 0.7549999952316284, time = 0.3345932960510254\n",
      "Testing at step=3, batch=45, test loss = 0.5322703719139099, test acc = 0.7950000166893005, time = 0.3370530605316162\n",
      "Step 3 finished in 307.3272428512573, Train loss = 0.5044872511426608, Test loss = 0.5491838383674622; Train Acc = 0.8163333348433177, Test Acc = 0.7966999995708466\n",
      "Training at step=4, batch=0, train loss = 0.537350594997406, train acc = 0.800000011920929, time = 0.9445388317108154\n",
      "Training at step=4, batch=30, train loss = 0.5840953588485718, train acc = 0.7699999809265137, time = 0.931210994720459\n",
      "Training at step=4, batch=60, train loss = 0.4427729845046997, train acc = 0.8550000190734863, time = 0.9371185302734375\n",
      "Training at step=4, batch=90, train loss = 0.4227495491504669, train acc = 0.8700000047683716, time = 0.9313218593597412\n",
      "Training at step=4, batch=120, train loss = 0.5012999773025513, train acc = 0.8100000023841858, time = 0.9307067394256592\n",
      "Training at step=4, batch=150, train loss = 0.46143731474876404, train acc = 0.8100000023841858, time = 0.9319913387298584\n",
      "Training at step=4, batch=180, train loss = 0.4891752302646637, train acc = 0.8399999737739563, time = 0.9298760890960693\n",
      "Training at step=4, batch=210, train loss = 0.473970502614975, train acc = 0.8550000190734863, time = 0.9438495635986328\n",
      "Training at step=4, batch=240, train loss = 0.487853467464447, train acc = 0.7950000166893005, time = 0.9333779811859131\n",
      "Training at step=4, batch=270, train loss = 0.5814155340194702, train acc = 0.800000011920929, time = 0.9317378997802734\n",
      "Testing at step=4, batch=0, test loss = 0.5991391539573669, test acc = 0.7749999761581421, time = 0.34780335426330566\n",
      "Testing at step=4, batch=5, test loss = 0.4607207775115967, test acc = 0.8600000143051147, time = 0.3394944667816162\n",
      "Testing at step=4, batch=10, test loss = 0.62893146276474, test acc = 0.7799999713897705, time = 0.3382878303527832\n",
      "Testing at step=4, batch=15, test loss = 0.5907430052757263, test acc = 0.7900000214576721, time = 0.33817553520202637\n",
      "Testing at step=4, batch=20, test loss = 0.50588059425354, test acc = 0.8199999928474426, time = 0.3424825668334961\n",
      "Testing at step=4, batch=25, test loss = 0.4083746671676636, test acc = 0.8199999928474426, time = 0.3401048183441162\n",
      "Testing at step=4, batch=30, test loss = 0.5479294061660767, test acc = 0.8100000023841858, time = 0.3416283130645752\n",
      "Testing at step=4, batch=35, test loss = 0.41830945014953613, test acc = 0.8700000047683716, time = 0.3404850959777832\n",
      "Testing at step=4, batch=40, test loss = 0.46126601099967957, test acc = 0.8349999785423279, time = 0.3380284309387207\n",
      "Testing at step=4, batch=45, test loss = 0.6161686778068542, test acc = 0.7850000262260437, time = 0.34421682357788086\n",
      "Step 4 finished in 307.009624004364, Train loss = 0.47289028922716775, Test loss = 0.5343732243776321; Train Acc = 0.8286999988555909, Test Acc = 0.8136999976634979\n",
      "Training at step=5, batch=0, train loss = 0.5634426474571228, train acc = 0.824999988079071, time = 0.9380857944488525\n",
      "Training at step=5, batch=30, train loss = 0.45028725266456604, train acc = 0.8399999737739563, time = 0.9292762279510498\n",
      "Training at step=5, batch=60, train loss = 0.494974285364151, train acc = 0.7950000166893005, time = 0.9336442947387695\n",
      "Training at step=5, batch=90, train loss = 0.3900342583656311, train acc = 0.8399999737739563, time = 0.9328756332397461\n",
      "Training at step=5, batch=120, train loss = 0.5259581804275513, train acc = 0.8149999976158142, time = 0.928438663482666\n",
      "Training at step=5, batch=150, train loss = 0.46463683247566223, train acc = 0.8149999976158142, time = 0.9338572025299072\n",
      "Training at step=5, batch=180, train loss = 0.479614794254303, train acc = 0.7900000214576721, time = 0.9273011684417725\n",
      "Training at step=5, batch=210, train loss = 0.4682787358760834, train acc = 0.8149999976158142, time = 0.9243009090423584\n",
      "Training at step=5, batch=240, train loss = 0.4904974400997162, train acc = 0.8349999785423279, time = 0.9340658187866211\n",
      "Training at step=5, batch=270, train loss = 0.32988259196281433, train acc = 0.875, time = 0.9262199401855469\n",
      "Testing at step=5, batch=0, test loss = 0.5944109559059143, test acc = 0.7950000166893005, time = 0.3520679473876953\n",
      "Testing at step=5, batch=5, test loss = 0.4190615713596344, test acc = 0.8450000286102295, time = 0.36246824264526367\n",
      "Testing at step=5, batch=10, test loss = 0.3346959054470062, test acc = 0.8700000047683716, time = 0.33672285079956055\n",
      "Testing at step=5, batch=15, test loss = 0.4731779396533966, test acc = 0.8299999833106995, time = 0.339951753616333\n",
      "Testing at step=5, batch=20, test loss = 0.5978025794029236, test acc = 0.8149999976158142, time = 0.3368260860443115\n",
      "Testing at step=5, batch=25, test loss = 0.4956555962562561, test acc = 0.8199999928474426, time = 0.3485291004180908\n",
      "Testing at step=5, batch=30, test loss = 0.5570885539054871, test acc = 0.7649999856948853, time = 0.35512661933898926\n",
      "Testing at step=5, batch=35, test loss = 0.37147387862205505, test acc = 0.8949999809265137, time = 0.36452174186706543\n",
      "Testing at step=5, batch=40, test loss = 0.5354249477386475, test acc = 0.7850000262260437, time = 0.3520686626434326\n",
      "Testing at step=5, batch=45, test loss = 0.49258095026016235, test acc = 0.8299999833106995, time = 0.35477447509765625\n",
      "Step 5 finished in 307.2834119796753, Train loss = 0.44758581707874934, Test loss = 0.4689279896020889; Train Acc = 0.8405166681607564, Test Acc = 0.8318999969959259\n",
      "Training at step=6, batch=0, train loss = 0.37356966733932495, train acc = 0.8600000143051147, time = 0.9395847320556641\n",
      "Training at step=6, batch=30, train loss = 0.34941989183425903, train acc = 0.8500000238418579, time = 0.9357647895812988\n",
      "Training at step=6, batch=60, train loss = 0.40017956495285034, train acc = 0.8600000143051147, time = 0.9305291175842285\n",
      "Training at step=6, batch=90, train loss = 0.45768678188323975, train acc = 0.8349999785423279, time = 0.9276609420776367\n",
      "Training at step=6, batch=120, train loss = 0.3658718764781952, train acc = 0.8550000190734863, time = 0.9412615299224854\n",
      "Training at step=6, batch=150, train loss = 0.4045048952102661, train acc = 0.8700000047683716, time = 0.9393339157104492\n",
      "Training at step=6, batch=180, train loss = 0.4924551248550415, train acc = 0.800000011920929, time = 0.9325828552246094\n",
      "Training at step=6, batch=210, train loss = 0.2607940435409546, train acc = 0.9150000214576721, time = 0.9236257076263428\n",
      "Training at step=6, batch=240, train loss = 0.39243409037590027, train acc = 0.8600000143051147, time = 0.9339449405670166\n",
      "Training at step=6, batch=270, train loss = 0.5131052136421204, train acc = 0.7950000166893005, time = 0.9342477321624756\n",
      "Testing at step=6, batch=0, test loss = 0.5143383741378784, test acc = 0.8199999928474426, time = 0.3371577262878418\n",
      "Testing at step=6, batch=5, test loss = 0.5223510265350342, test acc = 0.800000011920929, time = 0.363480806350708\n",
      "Testing at step=6, batch=10, test loss = 0.36105915904045105, test acc = 0.8550000190734863, time = 0.3388078212738037\n",
      "Testing at step=6, batch=15, test loss = 0.5372439622879028, test acc = 0.7900000214576721, time = 0.3443441390991211\n",
      "Testing at step=6, batch=20, test loss = 0.525912880897522, test acc = 0.8349999785423279, time = 0.3410782814025879\n",
      "Testing at step=6, batch=25, test loss = 0.48021987080574036, test acc = 0.8399999737739563, time = 0.3488130569458008\n",
      "Testing at step=6, batch=30, test loss = 0.5836953520774841, test acc = 0.800000011920929, time = 0.3502919673919678\n",
      "Testing at step=6, batch=35, test loss = 0.47460633516311646, test acc = 0.8450000286102295, time = 0.33791565895080566\n",
      "Testing at step=6, batch=40, test loss = 0.37046799063682556, test acc = 0.8650000095367432, time = 0.3392021656036377\n",
      "Testing at step=6, batch=45, test loss = 0.45934611558914185, test acc = 0.8399999737739563, time = 0.33525681495666504\n",
      "Step 6 finished in 307.43986201286316, Train loss = 0.43452675292889276, Test loss = 0.4878337091207504; Train Acc = 0.8436166689793269, Test Acc = 0.8220999991893768\n",
      "Training at step=7, batch=0, train loss = 0.43872925639152527, train acc = 0.8600000143051147, time = 0.9309868812561035\n",
      "Training at step=7, batch=30, train loss = 0.4571903944015503, train acc = 0.8700000047683716, time = 0.9279494285583496\n",
      "Training at step=7, batch=60, train loss = 0.4185717701911926, train acc = 0.8399999737739563, time = 0.9311668872833252\n",
      "Training at step=7, batch=90, train loss = 0.3915994167327881, train acc = 0.8650000095367432, time = 0.928478479385376\n",
      "Training at step=7, batch=120, train loss = 0.5228777527809143, train acc = 0.8149999976158142, time = 0.9292480945587158\n",
      "Training at step=7, batch=150, train loss = 0.4424373507499695, train acc = 0.824999988079071, time = 0.9329750537872314\n",
      "Training at step=7, batch=180, train loss = 0.49390366673469543, train acc = 0.7950000166893005, time = 0.9346590042114258\n",
      "Training at step=7, batch=210, train loss = 0.3813292384147644, train acc = 0.8899999856948853, time = 0.9455790519714355\n",
      "Training at step=7, batch=240, train loss = 0.41554591059684753, train acc = 0.8600000143051147, time = 0.9246957302093506\n",
      "Training at step=7, batch=270, train loss = 0.3830977976322174, train acc = 0.8650000095367432, time = 0.9280920028686523\n",
      "Testing at step=7, batch=0, test loss = 0.35113784670829773, test acc = 0.8650000095367432, time = 0.3451230525970459\n",
      "Testing at step=7, batch=5, test loss = 0.4645135998725891, test acc = 0.8450000286102295, time = 0.34378981590270996\n",
      "Testing at step=7, batch=10, test loss = 0.4762398600578308, test acc = 0.8299999833106995, time = 0.33622145652770996\n",
      "Testing at step=7, batch=15, test loss = 0.5595568418502808, test acc = 0.7850000262260437, time = 0.35246968269348145\n",
      "Testing at step=7, batch=20, test loss = 0.45999327301979065, test acc = 0.8349999785423279, time = 0.35079002380371094\n",
      "Testing at step=7, batch=25, test loss = 0.47451838850975037, test acc = 0.8399999737739563, time = 0.3366971015930176\n",
      "Testing at step=7, batch=30, test loss = 0.4559875428676605, test acc = 0.8349999785423279, time = 0.33867645263671875\n",
      "Testing at step=7, batch=35, test loss = 0.37984612584114075, test acc = 0.8700000047683716, time = 0.33997130393981934\n",
      "Testing at step=7, batch=40, test loss = 0.42223989963531494, test acc = 0.8600000143051147, time = 0.33692145347595215\n",
      "Testing at step=7, batch=45, test loss = 0.3690585792064667, test acc = 0.8799999952316284, time = 0.35272765159606934\n",
      "Step 7 finished in 307.48521852493286, Train loss = 0.42833946327368416, Test loss = 0.4446519029140472; Train Acc = 0.8453000017007192, Test Acc = 0.8397000014781952\n",
      "Training at step=8, batch=0, train loss = 0.4249233603477478, train acc = 0.8600000143051147, time = 0.9505252838134766\n",
      "Training at step=8, batch=30, train loss = 0.4530143737792969, train acc = 0.8299999833106995, time = 0.9301822185516357\n",
      "Training at step=8, batch=60, train loss = 0.32870379090309143, train acc = 0.8949999809265137, time = 0.9282646179199219\n",
      "Training at step=8, batch=90, train loss = 0.4064846932888031, train acc = 0.8650000095367432, time = 0.9246687889099121\n",
      "Training at step=8, batch=120, train loss = 0.29246610403060913, train acc = 0.875, time = 0.9441463947296143\n",
      "Training at step=8, batch=150, train loss = 0.3815300762653351, train acc = 0.8700000047683716, time = 0.947380781173706\n",
      "Training at step=8, batch=180, train loss = 0.5524840354919434, train acc = 0.8050000071525574, time = 0.9273948669433594\n",
      "Training at step=8, batch=210, train loss = 0.3636670410633087, train acc = 0.875, time = 0.9280989170074463\n",
      "Training at step=8, batch=240, train loss = 0.389840692281723, train acc = 0.8700000047683716, time = 0.9405543804168701\n",
      "Training at step=8, batch=270, train loss = 0.4042898416519165, train acc = 0.8799999952316284, time = 0.9396085739135742\n",
      "Testing at step=8, batch=0, test loss = 0.5173552632331848, test acc = 0.824999988079071, time = 0.341860294342041\n",
      "Testing at step=8, batch=5, test loss = 0.5024318099021912, test acc = 0.8199999928474426, time = 0.33786559104919434\n",
      "Testing at step=8, batch=10, test loss = 0.4614609479904175, test acc = 0.8100000023841858, time = 0.336597204208374\n",
      "Testing at step=8, batch=15, test loss = 0.4094748795032501, test acc = 0.8650000095367432, time = 0.35182952880859375\n",
      "Testing at step=8, batch=20, test loss = 0.5489149689674377, test acc = 0.800000011920929, time = 0.3379654884338379\n",
      "Testing at step=8, batch=25, test loss = 0.46099215745925903, test acc = 0.8399999737739563, time = 0.3326857089996338\n",
      "Testing at step=8, batch=30, test loss = 0.474595308303833, test acc = 0.824999988079071, time = 0.34195804595947266\n",
      "Testing at step=8, batch=35, test loss = 0.4435648024082184, test acc = 0.8399999737739563, time = 0.33916425704956055\n",
      "Testing at step=8, batch=40, test loss = 0.43762749433517456, test acc = 0.8450000286102295, time = 0.3537614345550537\n",
      "Testing at step=8, batch=45, test loss = 0.4259156882762909, test acc = 0.8500000238418579, time = 0.3458065986633301\n",
      "Step 8 finished in 308.5361752510071, Train loss = 0.41352867225805917, Test loss = 0.48942054867744444; Train Acc = 0.8508500005801519, Test Acc = 0.82299999833107\n",
      "Training at step=9, batch=0, train loss = 0.4952884316444397, train acc = 0.8399999737739563, time = 0.9667394161224365\n",
      "Training at step=9, batch=30, train loss = 0.5070008635520935, train acc = 0.8600000143051147, time = 0.9322450160980225\n",
      "Training at step=9, batch=60, train loss = 0.3657212555408478, train acc = 0.8650000095367432, time = 0.9309477806091309\n",
      "Training at step=9, batch=90, train loss = 0.4029735326766968, train acc = 0.8550000190734863, time = 0.9401581287384033\n",
      "Training at step=9, batch=120, train loss = 0.41769734025001526, train acc = 0.8550000190734863, time = 0.927452564239502\n",
      "Training at step=9, batch=150, train loss = 0.29930007457733154, train acc = 0.8949999809265137, time = 0.931208610534668\n",
      "Training at step=9, batch=180, train loss = 0.3298719823360443, train acc = 0.8849999904632568, time = 0.9317882061004639\n",
      "Training at step=9, batch=210, train loss = 0.4888996481895447, train acc = 0.8500000238418579, time = 0.930250883102417\n",
      "Training at step=9, batch=240, train loss = 0.43133050203323364, train acc = 0.8399999737739563, time = 0.9439218044281006\n",
      "Training at step=9, batch=270, train loss = 0.31760653853416443, train acc = 0.8949999809265137, time = 0.9365735054016113\n",
      "Testing at step=9, batch=0, test loss = 0.4796988368034363, test acc = 0.8600000143051147, time = 0.3379950523376465\n",
      "Testing at step=9, batch=5, test loss = 0.474253386259079, test acc = 0.8100000023841858, time = 0.33881068229675293\n",
      "Testing at step=9, batch=10, test loss = 0.3978610932826996, test acc = 0.8700000047683716, time = 0.33905816078186035\n",
      "Testing at step=9, batch=15, test loss = 0.6025837063789368, test acc = 0.800000011920929, time = 0.3400840759277344\n",
      "Testing at step=9, batch=20, test loss = 0.3947415053844452, test acc = 0.8600000143051147, time = 0.34177732467651367\n",
      "Testing at step=9, batch=25, test loss = 0.33778437972068787, test acc = 0.8650000095367432, time = 0.3417789936065674\n",
      "Testing at step=9, batch=30, test loss = 0.39220723509788513, test acc = 0.8500000238418579, time = 0.3393881320953369\n",
      "Testing at step=9, batch=35, test loss = 0.410313218832016, test acc = 0.8450000286102295, time = 0.33972883224487305\n",
      "Testing at step=9, batch=40, test loss = 0.37668707966804504, test acc = 0.8600000143051147, time = 0.3386054039001465\n",
      "Testing at step=9, batch=45, test loss = 0.4219478368759155, test acc = 0.875, time = 0.3408212661743164\n",
      "Step 9 finished in 307.8147060871124, Train loss = 0.40131776124238966, Test loss = 0.4411370670795441; Train Acc = 0.8551333326101304, Test Acc = 0.8437000024318695\n",
      "Training at step=10, batch=0, train loss = 0.45095235109329224, train acc = 0.8500000238418579, time = 0.9385888576507568\n",
      "Training at step=10, batch=30, train loss = 0.41259679198265076, train acc = 0.8650000095367432, time = 0.9308538436889648\n",
      "Training at step=10, batch=60, train loss = 0.4196808934211731, train acc = 0.8700000047683716, time = 0.9328641891479492\n",
      "Training at step=10, batch=90, train loss = 0.3468223810195923, train acc = 0.8650000095367432, time = 0.9352743625640869\n",
      "Training at step=10, batch=120, train loss = 0.3524075448513031, train acc = 0.8799999952316284, time = 0.9267392158508301\n",
      "Training at step=10, batch=150, train loss = 0.4636645019054413, train acc = 0.8450000286102295, time = 0.9374678134918213\n",
      "Training at step=10, batch=180, train loss = 0.3905143439769745, train acc = 0.8450000286102295, time = 0.9332635402679443\n",
      "Training at step=10, batch=210, train loss = 0.41978657245635986, train acc = 0.8600000143051147, time = 0.9265046119689941\n",
      "Training at step=10, batch=240, train loss = 0.3707844018936157, train acc = 0.875, time = 0.9324290752410889\n",
      "Training at step=10, batch=270, train loss = 0.38323062658309937, train acc = 0.8349999785423279, time = 0.9361059665679932\n",
      "Testing at step=10, batch=0, test loss = 0.48581257462501526, test acc = 0.8550000190734863, time = 0.339418888092041\n",
      "Testing at step=10, batch=5, test loss = 0.4404861330986023, test acc = 0.824999988079071, time = 0.3399178981781006\n",
      "Testing at step=10, batch=10, test loss = 0.5259210467338562, test acc = 0.8050000071525574, time = 0.33778905868530273\n",
      "Testing at step=10, batch=15, test loss = 0.36390089988708496, test acc = 0.8600000143051147, time = 0.3374910354614258\n",
      "Testing at step=10, batch=20, test loss = 0.4249964952468872, test acc = 0.8299999833106995, time = 0.3532984256744385\n",
      "Testing at step=10, batch=25, test loss = 0.515057384967804, test acc = 0.8299999833106995, time = 0.34671592712402344\n",
      "Testing at step=10, batch=30, test loss = 0.4724746346473694, test acc = 0.8399999737739563, time = 0.33858442306518555\n",
      "Testing at step=10, batch=35, test loss = 0.43267735838890076, test acc = 0.8600000143051147, time = 0.33525657653808594\n",
      "Testing at step=10, batch=40, test loss = 0.38448187708854675, test acc = 0.8700000047683716, time = 0.34079957008361816\n",
      "Testing at step=10, batch=45, test loss = 0.5076762437820435, test acc = 0.8149999976158142, time = 0.3375740051269531\n",
      "Step 10 finished in 307.81451869010925, Train loss = 0.3932539436717828, Test loss = 0.4569488435983658; Train Acc = 0.857683334549268, Test Acc = 0.8383999979496002\n",
      "Training at step=11, batch=0, train loss = 0.525031566619873, train acc = 0.8199999928474426, time = 0.9345035552978516\n",
      "Training at step=11, batch=30, train loss = 0.3971358835697174, train acc = 0.8999999761581421, time = 0.9366664886474609\n",
      "Training at step=11, batch=60, train loss = 0.4147067368030548, train acc = 0.8650000095367432, time = 0.9316070079803467\n",
      "Training at step=11, batch=90, train loss = 0.3208402693271637, train acc = 0.8849999904632568, time = 0.9447460174560547\n",
      "Training at step=11, batch=120, train loss = 0.5622550249099731, train acc = 0.8349999785423279, time = 0.9381780624389648\n",
      "Training at step=11, batch=150, train loss = 0.4356343448162079, train acc = 0.8600000143051147, time = 0.925184965133667\n",
      "Training at step=11, batch=180, train loss = 0.36208420991897583, train acc = 0.875, time = 0.9264316558837891\n",
      "Training at step=11, batch=210, train loss = 0.41110023856163025, train acc = 0.8550000190734863, time = 0.9342348575592041\n",
      "Training at step=11, batch=240, train loss = 0.3243024945259094, train acc = 0.8799999952316284, time = 0.9223651885986328\n",
      "Training at step=11, batch=270, train loss = 0.4463879466056824, train acc = 0.8149999976158142, time = 0.9341840744018555\n",
      "Testing at step=11, batch=0, test loss = 0.4812721610069275, test acc = 0.8050000071525574, time = 0.33919501304626465\n",
      "Testing at step=11, batch=5, test loss = 0.4584938883781433, test acc = 0.8500000238418579, time = 0.3428683280944824\n",
      "Testing at step=11, batch=10, test loss = 0.35942474007606506, test acc = 0.8700000047683716, time = 0.3394145965576172\n",
      "Testing at step=11, batch=15, test loss = 0.4477642774581909, test acc = 0.8550000190734863, time = 0.34895920753479004\n",
      "Testing at step=11, batch=20, test loss = 0.4347440004348755, test acc = 0.8500000238418579, time = 0.3439819812774658\n",
      "Testing at step=11, batch=25, test loss = 0.3302968144416809, test acc = 0.8799999952316284, time = 0.34427952766418457\n",
      "Testing at step=11, batch=30, test loss = 0.3788459897041321, test acc = 0.8650000095367432, time = 0.3352205753326416\n",
      "Testing at step=11, batch=35, test loss = 0.3893497586250305, test acc = 0.8550000190734863, time = 0.3397495746612549\n",
      "Testing at step=11, batch=40, test loss = 0.4256807267665863, test acc = 0.8500000238418579, time = 0.3433222770690918\n",
      "Testing at step=11, batch=45, test loss = 0.44804930686950684, test acc = 0.8399999737739563, time = 0.3392181396484375\n",
      "Step 11 finished in 307.7907569408417, Train loss = 0.39280681158105535, Test loss = 0.4135518062114716; Train Acc = 0.8586999992529551, Test Acc = 0.8522000026702881\n",
      "Training at step=12, batch=0, train loss = 0.337538480758667, train acc = 0.875, time = 0.965489387512207\n",
      "Training at step=12, batch=30, train loss = 0.3600521385669708, train acc = 0.8600000143051147, time = 0.9326467514038086\n",
      "Training at step=12, batch=60, train loss = 0.29475265741348267, train acc = 0.9100000262260437, time = 0.9407947063446045\n",
      "Training at step=12, batch=90, train loss = 0.32244303822517395, train acc = 0.9100000262260437, time = 0.9282219409942627\n",
      "Training at step=12, batch=120, train loss = 0.35810086131095886, train acc = 0.8700000047683716, time = 0.9402074813842773\n",
      "Training at step=12, batch=150, train loss = 0.3868931531906128, train acc = 0.8450000286102295, time = 0.9320688247680664\n",
      "Training at step=12, batch=180, train loss = 0.32106277346611023, train acc = 0.8700000047683716, time = 0.9328737258911133\n",
      "Training at step=12, batch=210, train loss = 0.4148126244544983, train acc = 0.8550000190734863, time = 0.9231820106506348\n",
      "Training at step=12, batch=240, train loss = 0.33965519070625305, train acc = 0.8849999904632568, time = 0.928844690322876\n",
      "Training at step=12, batch=270, train loss = 0.3875829577445984, train acc = 0.8600000143051147, time = 0.9293346405029297\n",
      "Testing at step=12, batch=0, test loss = 0.36510032415390015, test acc = 0.8450000286102295, time = 0.3396294116973877\n",
      "Testing at step=12, batch=5, test loss = 0.40988609194755554, test acc = 0.8650000095367432, time = 0.33974695205688477\n",
      "Testing at step=12, batch=10, test loss = 0.5064655542373657, test acc = 0.8199999928474426, time = 0.3538932800292969\n",
      "Testing at step=12, batch=15, test loss = 0.5262196660041809, test acc = 0.8050000071525574, time = 0.35916829109191895\n",
      "Testing at step=12, batch=20, test loss = 0.35518133640289307, test acc = 0.875, time = 0.3502323627471924\n",
      "Testing at step=12, batch=25, test loss = 0.2818392515182495, test acc = 0.8999999761581421, time = 0.35311102867126465\n",
      "Testing at step=12, batch=30, test loss = 0.36446520686149597, test acc = 0.8600000143051147, time = 0.3536703586578369\n",
      "Testing at step=12, batch=35, test loss = 0.3795776665210724, test acc = 0.8700000047683716, time = 0.35337114334106445\n",
      "Testing at step=12, batch=40, test loss = 0.3968967795372009, test acc = 0.8550000190734863, time = 0.3632528781890869\n",
      "Testing at step=12, batch=45, test loss = 0.4613778591156006, test acc = 0.8450000286102295, time = 0.35430026054382324\n",
      "Step 12 finished in 307.9371666908264, Train loss = 0.38246973921855293, Test loss = 0.41300652384757996; Train Acc = 0.8622833345333735, Test Acc = 0.8525\n",
      "Training at step=13, batch=0, train loss = 0.3001781702041626, train acc = 0.8949999809265137, time = 0.9498176574707031\n",
      "Training at step=13, batch=30, train loss = 0.43142199516296387, train acc = 0.8450000286102295, time = 0.9227802753448486\n",
      "Training at step=13, batch=60, train loss = 0.26357975602149963, train acc = 0.8999999761581421, time = 0.9352936744689941\n",
      "Training at step=13, batch=90, train loss = 0.4062100946903229, train acc = 0.8399999737739563, time = 0.9409985542297363\n",
      "Training at step=13, batch=120, train loss = 0.5051947236061096, train acc = 0.8149999976158142, time = 1.2305598258972168\n",
      "Training at step=13, batch=150, train loss = 0.4012298583984375, train acc = 0.8849999904632568, time = 1.362602710723877\n",
      "Training at step=13, batch=180, train loss = 0.3526185154914856, train acc = 0.8949999809265137, time = 0.9331207275390625\n",
      "Training at step=13, batch=210, train loss = 0.3605826497077942, train acc = 0.8450000286102295, time = 0.9482259750366211\n",
      "Training at step=13, batch=240, train loss = 0.31937557458877563, train acc = 0.9100000262260437, time = 0.9218499660491943\n",
      "Training at step=13, batch=270, train loss = 0.3532772362232208, train acc = 0.8899999856948853, time = 0.9271652698516846\n",
      "Testing at step=13, batch=0, test loss = 0.3892909288406372, test acc = 0.8600000143051147, time = 0.34194469451904297\n",
      "Testing at step=13, batch=5, test loss = 0.38830938935279846, test acc = 0.8650000095367432, time = 0.33644652366638184\n",
      "Testing at step=13, batch=10, test loss = 0.4586546719074249, test acc = 0.8149999976158142, time = 0.3365488052368164\n",
      "Testing at step=13, batch=15, test loss = 0.3458520770072937, test acc = 0.8849999904632568, time = 0.33855247497558594\n",
      "Testing at step=13, batch=20, test loss = 0.5099212527275085, test acc = 0.8100000023841858, time = 0.33426976203918457\n",
      "Testing at step=13, batch=25, test loss = 0.5203941464424133, test acc = 0.8100000023841858, time = 0.3333113193511963\n",
      "Testing at step=13, batch=30, test loss = 0.41854867339134216, test acc = 0.8299999833106995, time = 0.35222339630126953\n",
      "Testing at step=13, batch=35, test loss = 0.4951549172401428, test acc = 0.800000011920929, time = 0.3635690212249756\n",
      "Testing at step=13, batch=40, test loss = 0.41878142952919006, test acc = 0.8650000095367432, time = 0.35469865798950195\n",
      "Testing at step=13, batch=45, test loss = 0.36494094133377075, test acc = 0.8849999904632568, time = 0.3511166572570801\n",
      "Step 13 finished in 332.2759418487549, Train loss = 0.38089189315835636, Test loss = 0.433855043053627; Train Acc = 0.8618833351135254, Test Acc = 0.8425000035762786\n",
      "Training at step=14, batch=0, train loss = 0.40473559498786926, train acc = 0.8399999737739563, time = 0.9521403312683105\n",
      "Training at step=14, batch=30, train loss = 0.3499322831630707, train acc = 0.875, time = 0.9477062225341797\n",
      "Training at step=14, batch=60, train loss = 0.32309916615486145, train acc = 0.8799999952316284, time = 0.949002742767334\n",
      "Training at step=14, batch=90, train loss = 0.4126160442829132, train acc = 0.8700000047683716, time = 0.9364182949066162\n",
      "Training at step=14, batch=120, train loss = 0.4787771701812744, train acc = 0.8349999785423279, time = 0.9262833595275879\n",
      "Training at step=14, batch=150, train loss = 0.24032177031040192, train acc = 0.9200000166893005, time = 0.9279553890228271\n",
      "Training at step=14, batch=180, train loss = 0.30070361495018005, train acc = 0.8899999856948853, time = 0.9452419281005859\n",
      "Training at step=14, batch=210, train loss = 0.42707979679107666, train acc = 0.8500000238418579, time = 0.940732479095459\n",
      "Training at step=14, batch=240, train loss = 0.4054366648197174, train acc = 0.8600000143051147, time = 0.920900821685791\n",
      "Training at step=14, batch=270, train loss = 0.38189223408699036, train acc = 0.8650000095367432, time = 0.9297590255737305\n",
      "Testing at step=14, batch=0, test loss = 0.5674417614936829, test acc = 0.8149999976158142, time = 0.33478450775146484\n",
      "Testing at step=14, batch=5, test loss = 0.3972436487674713, test acc = 0.8600000143051147, time = 0.33632612228393555\n",
      "Testing at step=14, batch=10, test loss = 0.3523145020008087, test acc = 0.875, time = 0.35207247734069824\n",
      "Testing at step=14, batch=15, test loss = 0.30316510796546936, test acc = 0.8949999809265137, time = 0.33476877212524414\n",
      "Testing at step=14, batch=20, test loss = 0.4342309534549713, test acc = 0.8349999785423279, time = 0.35116147994995117\n",
      "Testing at step=14, batch=25, test loss = 0.39351847767829895, test acc = 0.8700000047683716, time = 0.33789992332458496\n",
      "Testing at step=14, batch=30, test loss = 0.35933026671409607, test acc = 0.8450000286102295, time = 0.33072376251220703\n",
      "Testing at step=14, batch=35, test loss = 0.5122123956680298, test acc = 0.8199999928474426, time = 0.3366396427154541\n",
      "Testing at step=14, batch=40, test loss = 0.30525171756744385, test acc = 0.8999999761581421, time = 0.35404109954833984\n",
      "Testing at step=14, batch=45, test loss = 0.4570622742176056, test acc = 0.8399999737739563, time = 0.3562037944793701\n",
      "Step 14 finished in 306.9112286567688, Train loss = 0.3734522819519043, Test loss = 0.4083119767904282; Train Acc = 0.8655666673183441, Test Acc = 0.8525\n",
      "Training at step=15, batch=0, train loss = 0.40618082880973816, train acc = 0.8349999785423279, time = 0.9572653770446777\n",
      "Training at step=15, batch=30, train loss = 0.35696619749069214, train acc = 0.8600000143051147, time = 0.9331977367401123\n",
      "Training at step=15, batch=60, train loss = 0.3490943908691406, train acc = 0.8799999952316284, time = 0.9409000873565674\n",
      "Training at step=15, batch=90, train loss = 0.32736802101135254, train acc = 0.8949999809265137, time = 0.930649995803833\n",
      "Training at step=15, batch=120, train loss = 0.25939249992370605, train acc = 0.8999999761581421, time = 0.9912505149841309\n",
      "Training at step=15, batch=150, train loss = 0.2776011526584625, train acc = 0.8999999761581421, time = 0.9448566436767578\n",
      "Training at step=15, batch=180, train loss = 0.2850044369697571, train acc = 0.8899999856948853, time = 0.9419031143188477\n",
      "Training at step=15, batch=210, train loss = 0.34678661823272705, train acc = 0.8799999952316284, time = 0.9484190940856934\n",
      "Training at step=15, batch=240, train loss = 0.29823100566864014, train acc = 0.8899999856948853, time = 0.9539885520935059\n",
      "Training at step=15, batch=270, train loss = 0.32614368200302124, train acc = 0.875, time = 0.950944185256958\n",
      "Testing at step=15, batch=0, test loss = 0.42389586567878723, test acc = 0.8299999833106995, time = 0.3414802551269531\n",
      "Testing at step=15, batch=5, test loss = 0.43748462200164795, test acc = 0.8399999737739563, time = 0.3503603935241699\n",
      "Testing at step=15, batch=10, test loss = 0.33764827251434326, test acc = 0.8899999856948853, time = 0.3467421531677246\n",
      "Testing at step=15, batch=15, test loss = 0.35448959469795227, test acc = 0.8650000095367432, time = 0.36450934410095215\n",
      "Testing at step=15, batch=20, test loss = 0.39615997672080994, test acc = 0.8500000238418579, time = 0.3566734790802002\n",
      "Testing at step=15, batch=25, test loss = 0.3775264620780945, test acc = 0.8399999737739563, time = 0.3524317741394043\n",
      "Testing at step=15, batch=30, test loss = 0.3625946044921875, test acc = 0.8600000143051147, time = 0.3582885265350342\n",
      "Testing at step=15, batch=35, test loss = 0.43445420265197754, test acc = 0.8349999785423279, time = 0.3496112823486328\n",
      "Testing at step=15, batch=40, test loss = 0.47021228075027466, test acc = 0.8650000095367432, time = 0.35585951805114746\n",
      "Testing at step=15, batch=45, test loss = 0.44742080569267273, test acc = 0.8500000238418579, time = 0.34128904342651367\n",
      "Step 15 finished in 311.871862411499, Train loss = 0.3667571790516376, Test loss = 0.4068866539001465; Train Acc = 0.8670166691144308, Test Acc = 0.8516000032424926\n",
      "Training at step=16, batch=0, train loss = 0.3048543632030487, train acc = 0.8899999856948853, time = 0.9630775451660156\n",
      "Training at step=16, batch=30, train loss = 0.4366176724433899, train acc = 0.8450000286102295, time = 0.9446892738342285\n",
      "Training at step=16, batch=60, train loss = 0.3168167173862457, train acc = 0.8999999761581421, time = 0.9496910572052002\n",
      "Training at step=16, batch=90, train loss = 0.31729352474212646, train acc = 0.8949999809265137, time = 0.9371845722198486\n",
      "Training at step=16, batch=120, train loss = 0.39943554997444153, train acc = 0.8600000143051147, time = 0.9517931938171387\n",
      "Training at step=16, batch=150, train loss = 0.3403439223766327, train acc = 0.8700000047683716, time = 0.9366905689239502\n",
      "Training at step=16, batch=180, train loss = 0.3922545611858368, train acc = 0.8550000190734863, time = 0.9543795585632324\n",
      "Training at step=16, batch=210, train loss = 0.3206944167613983, train acc = 0.8650000095367432, time = 0.9485034942626953\n",
      "Training at step=16, batch=240, train loss = 0.3154275417327881, train acc = 0.9100000262260437, time = 0.9534497261047363\n",
      "Training at step=16, batch=270, train loss = 0.3400317430496216, train acc = 0.8949999809265137, time = 0.9498026371002197\n",
      "Testing at step=16, batch=0, test loss = 0.3978854715824127, test acc = 0.8450000286102295, time = 0.34126949310302734\n",
      "Testing at step=16, batch=5, test loss = 0.3614353835582733, test acc = 0.8849999904632568, time = 0.3579585552215576\n",
      "Testing at step=16, batch=10, test loss = 0.32666832208633423, test acc = 0.8799999952316284, time = 0.35343027114868164\n",
      "Testing at step=16, batch=15, test loss = 0.40988409519195557, test acc = 0.824999988079071, time = 0.35850095748901367\n",
      "Testing at step=16, batch=20, test loss = 0.339569091796875, test acc = 0.875, time = 0.34403276443481445\n",
      "Testing at step=16, batch=25, test loss = 0.494652658700943, test acc = 0.8199999928474426, time = 0.35959935188293457\n",
      "Testing at step=16, batch=30, test loss = 0.45453357696533203, test acc = 0.875, time = 0.36191654205322266\n",
      "Testing at step=16, batch=35, test loss = 0.3518582582473755, test acc = 0.8600000143051147, time = 0.34844255447387695\n",
      "Testing at step=16, batch=40, test loss = 0.39710888266563416, test acc = 0.8700000047683716, time = 0.34650397300720215\n",
      "Testing at step=16, batch=45, test loss = 0.3311656713485718, test acc = 0.8799999952316284, time = 0.34483909606933594\n",
      "Step 16 finished in 313.437447309494, Train loss = 0.36514034966627756, Test loss = 0.40650291085243223; Train Acc = 0.8680000013113022, Test Acc = 0.8561999988555908\n",
      "Training at step=17, batch=0, train loss = 0.3182198107242584, train acc = 0.8700000047683716, time = 0.953772783279419\n",
      "Training at step=17, batch=30, train loss = 0.42191392183303833, train acc = 0.8299999833106995, time = 0.9518346786499023\n",
      "Training at step=17, batch=60, train loss = 0.4404664933681488, train acc = 0.8299999833106995, time = 0.9571053981781006\n",
      "Training at step=17, batch=90, train loss = 0.41747403144836426, train acc = 0.8550000190734863, time = 0.9505615234375\n",
      "Training at step=17, batch=120, train loss = 0.32433784008026123, train acc = 0.8799999952316284, time = 0.9280514717102051\n",
      "Training at step=17, batch=150, train loss = 0.4046687185764313, train acc = 0.8550000190734863, time = 0.9455661773681641\n",
      "Training at step=17, batch=180, train loss = 0.3394964635372162, train acc = 0.8799999952316284, time = 0.9515984058380127\n",
      "Training at step=17, batch=210, train loss = 0.45903080701828003, train acc = 0.8399999737739563, time = 0.9559383392333984\n",
      "Training at step=17, batch=240, train loss = 0.372714638710022, train acc = 0.8500000238418579, time = 0.9468908309936523\n",
      "Training at step=17, batch=270, train loss = 0.31073203682899475, train acc = 0.9150000214576721, time = 0.9359574317932129\n",
      "Testing at step=17, batch=0, test loss = 0.39042022824287415, test acc = 0.8899999856948853, time = 0.3614482879638672\n",
      "Testing at step=17, batch=5, test loss = 0.30724087357521057, test acc = 0.8999999761581421, time = 0.3557734489440918\n",
      "Testing at step=17, batch=10, test loss = 0.37183433771133423, test acc = 0.8650000095367432, time = 0.344498872756958\n",
      "Testing at step=17, batch=15, test loss = 0.38559722900390625, test acc = 0.8849999904632568, time = 0.35334348678588867\n",
      "Testing at step=17, batch=20, test loss = 0.3656834661960602, test acc = 0.8600000143051147, time = 0.37268567085266113\n",
      "Testing at step=17, batch=25, test loss = 0.4206210970878601, test acc = 0.8550000190734863, time = 0.3585855960845947\n",
      "Testing at step=17, batch=30, test loss = 0.35315096378326416, test acc = 0.8450000286102295, time = 0.358245849609375\n",
      "Testing at step=17, batch=35, test loss = 0.40279537439346313, test acc = 0.8600000143051147, time = 0.357388973236084\n",
      "Testing at step=17, batch=40, test loss = 0.3767206072807312, test acc = 0.8600000143051147, time = 0.3604161739349365\n",
      "Testing at step=17, batch=45, test loss = 0.4750741720199585, test acc = 0.8600000143051147, time = 0.35471010208129883\n",
      "Step 17 finished in 313.60402822494507, Train loss = 0.3590805585682392, Test loss = 0.3949188524484634; Train Acc = 0.86953333457311, Test Acc = 0.8602000010013581\n",
      "Training at step=18, batch=0, train loss = 0.2361120581626892, train acc = 0.9350000023841858, time = 0.9660894870758057\n",
      "Training at step=18, batch=30, train loss = 0.24000778794288635, train acc = 0.9150000214576721, time = 0.947551965713501\n",
      "Training at step=18, batch=60, train loss = 0.4378964900970459, train acc = 0.8399999737739563, time = 0.954148530960083\n",
      "Training at step=18, batch=90, train loss = 0.3612034320831299, train acc = 0.875, time = 0.9451601505279541\n",
      "Training at step=18, batch=120, train loss = 0.33944445848464966, train acc = 0.8799999952316284, time = 0.9528005123138428\n",
      "Training at step=18, batch=150, train loss = 0.3554559350013733, train acc = 0.8700000047683716, time = 0.9578037261962891\n",
      "Training at step=18, batch=180, train loss = 0.2964347004890442, train acc = 0.9049999713897705, time = 0.9584617614746094\n",
      "Training at step=18, batch=210, train loss = 0.3325524628162384, train acc = 0.875, time = 0.9411463737487793\n",
      "Training at step=18, batch=240, train loss = 0.30512744188308716, train acc = 0.8849999904632568, time = 0.9489941596984863\n",
      "Training at step=18, batch=270, train loss = 0.3408666253089905, train acc = 0.8849999904632568, time = 0.9460575580596924\n",
      "Testing at step=18, batch=0, test loss = 0.36030441522598267, test acc = 0.8899999856948853, time = 0.352642297744751\n",
      "Testing at step=18, batch=5, test loss = 0.4534749686717987, test acc = 0.8450000286102295, time = 0.3616607189178467\n",
      "Testing at step=18, batch=10, test loss = 0.38561561703681946, test acc = 0.8550000190734863, time = 0.3578376770019531\n",
      "Testing at step=18, batch=15, test loss = 0.4121750295162201, test acc = 0.8500000238418579, time = 0.35595130920410156\n",
      "Testing at step=18, batch=20, test loss = 0.4004043638706207, test acc = 0.8550000190734863, time = 0.3499314785003662\n",
      "Testing at step=18, batch=25, test loss = 0.2832488715648651, test acc = 0.8949999809265137, time = 0.3435037136077881\n",
      "Testing at step=18, batch=30, test loss = 0.3898022174835205, test acc = 0.9049999713897705, time = 0.3690462112426758\n",
      "Testing at step=18, batch=35, test loss = 0.5377346277236938, test acc = 0.8100000023841858, time = 0.35539937019348145\n",
      "Testing at step=18, batch=40, test loss = 0.41076821088790894, test acc = 0.8500000238418579, time = 0.3670060634613037\n",
      "Testing at step=18, batch=45, test loss = 0.4137268364429474, test acc = 0.824999988079071, time = 0.3549988269805908\n",
      "Step 18 finished in 313.43034863471985, Train loss = 0.35408128996690114, Test loss = 0.4255604833364487; Train Acc = 0.8701666661103566, Test Acc = 0.8494999980926514\n",
      "Training at step=19, batch=0, train loss = 0.37579646706581116, train acc = 0.8899999856948853, time = 0.9419877529144287\n",
      "Training at step=19, batch=30, train loss = 0.39239969849586487, train acc = 0.8500000238418579, time = 0.9521777629852295\n",
      "Training at step=19, batch=60, train loss = 0.31964007019996643, train acc = 0.8700000047683716, time = 0.9590249061584473\n",
      "Training at step=19, batch=90, train loss = 0.32427772879600525, train acc = 0.8949999809265137, time = 0.9625685214996338\n",
      "Training at step=19, batch=120, train loss = 0.3612300157546997, train acc = 0.8650000095367432, time = 0.9614498615264893\n",
      "Training at step=19, batch=150, train loss = 0.5017253756523132, train acc = 0.8149999976158142, time = 0.9506306648254395\n",
      "Training at step=19, batch=180, train loss = 0.3195510506629944, train acc = 0.9049999713897705, time = 0.9671874046325684\n",
      "Training at step=19, batch=210, train loss = 0.35905590653419495, train acc = 0.8650000095367432, time = 0.9499368667602539\n",
      "Training at step=19, batch=240, train loss = 0.3320121765136719, train acc = 0.8799999952316284, time = 0.9395413398742676\n",
      "Training at step=19, batch=270, train loss = 0.37666159868240356, train acc = 0.875, time = 0.934981107711792\n",
      "Testing at step=19, batch=0, test loss = 0.31355151534080505, test acc = 0.8949999809265137, time = 0.35428881645202637\n",
      "Testing at step=19, batch=5, test loss = 0.39097461104393005, test acc = 0.8349999785423279, time = 0.34681105613708496\n",
      "Testing at step=19, batch=10, test loss = 0.40797221660614014, test acc = 0.8799999952316284, time = 0.3645186424255371\n",
      "Testing at step=19, batch=15, test loss = 0.4269944727420807, test acc = 0.8650000095367432, time = 0.34029459953308105\n",
      "Testing at step=19, batch=20, test loss = 0.46768704056739807, test acc = 0.8450000286102295, time = 0.3399646282196045\n",
      "Testing at step=19, batch=25, test loss = 0.3790868818759918, test acc = 0.8700000047683716, time = 0.3489413261413574\n",
      "Testing at step=19, batch=30, test loss = 0.3274361491203308, test acc = 0.9100000262260437, time = 0.360471248626709\n",
      "Testing at step=19, batch=35, test loss = 0.3714199364185333, test acc = 0.8650000095367432, time = 0.35776209831237793\n",
      "Testing at step=19, batch=40, test loss = 0.38686850666999817, test acc = 0.8899999856948853, time = 0.3684086799621582\n",
      "Testing at step=19, batch=45, test loss = 0.34809234738349915, test acc = 0.8849999904632568, time = 0.3583486080169678\n",
      "Step 19 finished in 313.15847182273865, Train loss = 0.35147034709652264, Test loss = 0.38751808166503904; Train Acc = 0.8711333330472311, Test Acc = 0.8648000025749206\n",
      "Training at step=20, batch=0, train loss = 0.26842406392097473, train acc = 0.9049999713897705, time = 0.9646780490875244\n",
      "Training at step=20, batch=30, train loss = 0.3675498962402344, train acc = 0.8949999809265137, time = 0.9557251930236816\n",
      "Training at step=20, batch=60, train loss = 0.35500195622444153, train acc = 0.8700000047683716, time = 0.9435751438140869\n",
      "Training at step=20, batch=90, train loss = 0.3859440088272095, train acc = 0.8600000143051147, time = 0.9432671070098877\n",
      "Training at step=20, batch=120, train loss = 0.37115591764450073, train acc = 0.8600000143051147, time = 0.9534962177276611\n",
      "Training at step=20, batch=150, train loss = 0.332017183303833, train acc = 0.8700000047683716, time = 0.9404494762420654\n",
      "Training at step=20, batch=180, train loss = 0.3421367406845093, train acc = 0.8799999952316284, time = 0.9475827217102051\n",
      "Training at step=20, batch=210, train loss = 0.39138656854629517, train acc = 0.8550000190734863, time = 0.9528694152832031\n",
      "Training at step=20, batch=240, train loss = 0.3309796452522278, train acc = 0.8899999856948853, time = 0.9583840370178223\n",
      "Training at step=20, batch=270, train loss = 0.37307488918304443, train acc = 0.824999988079071, time = 0.969581127166748\n",
      "Testing at step=20, batch=0, test loss = 0.4126761257648468, test acc = 0.8399999737739563, time = 0.3576362133026123\n",
      "Testing at step=20, batch=5, test loss = 0.4431349039077759, test acc = 0.8299999833106995, time = 0.3699333667755127\n",
      "Testing at step=20, batch=10, test loss = 0.5143716931343079, test acc = 0.824999988079071, time = 0.3900144100189209\n",
      "Testing at step=20, batch=15, test loss = 0.3511470556259155, test acc = 0.8650000095367432, time = 0.361130952835083\n",
      "Testing at step=20, batch=20, test loss = 0.4260382056236267, test acc = 0.8349999785423279, time = 0.3614640235900879\n",
      "Testing at step=20, batch=25, test loss = 0.4031327962875366, test acc = 0.8799999952316284, time = 0.36023521423339844\n",
      "Testing at step=20, batch=30, test loss = 0.454507052898407, test acc = 0.8399999737739563, time = 0.3565962314605713\n",
      "Testing at step=20, batch=35, test loss = 0.3539758622646332, test acc = 0.8700000047683716, time = 0.3374354839324951\n",
      "Testing at step=20, batch=40, test loss = 0.40807217359542847, test acc = 0.8399999737739563, time = 0.345595121383667\n",
      "Testing at step=20, batch=45, test loss = 0.4069058299064636, test acc = 0.8550000190734863, time = 0.34914588928222656\n",
      "Step 20 finished in 313.22783374786377, Train loss = 0.34950936287641526, Test loss = 0.4005784124135971; Train Acc = 0.873700000445048, Test Acc = 0.8561000001430511\n",
      "Training at step=21, batch=0, train loss = 0.3712248206138611, train acc = 0.8799999952316284, time = 0.9423568248748779\n",
      "Training at step=21, batch=30, train loss = 0.30573052167892456, train acc = 0.8999999761581421, time = 0.9550747871398926\n",
      "Training at step=21, batch=60, train loss = 0.29822033643722534, train acc = 0.8799999952316284, time = 0.959484338760376\n",
      "Training at step=21, batch=90, train loss = 0.32714805006980896, train acc = 0.8899999856948853, time = 0.9593465328216553\n",
      "Training at step=21, batch=120, train loss = 0.33184361457824707, train acc = 0.8299999833106995, time = 0.9612855911254883\n",
      "Training at step=21, batch=150, train loss = 0.35325995087623596, train acc = 0.8899999856948853, time = 0.9482545852661133\n",
      "Training at step=21, batch=180, train loss = 0.32588160037994385, train acc = 0.8799999952316284, time = 0.9558093547821045\n",
      "Training at step=21, batch=210, train loss = 0.32900363206863403, train acc = 0.875, time = 0.9502348899841309\n",
      "Training at step=21, batch=240, train loss = 0.36115187406539917, train acc = 0.8349999785423279, time = 0.9461331367492676\n",
      "Training at step=21, batch=270, train loss = 0.3198467493057251, train acc = 0.875, time = 0.951744794845581\n",
      "Testing at step=21, batch=0, test loss = 0.3987639546394348, test acc = 0.8700000047683716, time = 0.36148905754089355\n",
      "Testing at step=21, batch=5, test loss = 0.42907580733299255, test acc = 0.8349999785423279, time = 0.354111909866333\n",
      "Testing at step=21, batch=10, test loss = 0.6768657565116882, test acc = 0.7699999809265137, time = 0.36693310737609863\n",
      "Testing at step=21, batch=15, test loss = 0.3941872715950012, test acc = 0.8700000047683716, time = 0.3794422149658203\n",
      "Testing at step=21, batch=20, test loss = 0.3677792251110077, test acc = 0.875, time = 0.3684682846069336\n",
      "Testing at step=21, batch=25, test loss = 0.42772507667541504, test acc = 0.8849999904632568, time = 0.36205267906188965\n",
      "Testing at step=21, batch=30, test loss = 0.48078662157058716, test acc = 0.8299999833106995, time = 0.35158753395080566\n",
      "Testing at step=21, batch=35, test loss = 0.5528408288955688, test acc = 0.824999988079071, time = 0.35109400749206543\n",
      "Testing at step=21, batch=40, test loss = 0.5093271136283875, test acc = 0.8450000286102295, time = 0.34566664695739746\n",
      "Testing at step=21, batch=45, test loss = 0.3323540985584259, test acc = 0.8799999952316284, time = 0.37118077278137207\n",
      "Step 21 finished in 313.8593318462372, Train loss = 0.34353576133648556, Test loss = 0.39401940882205966; Train Acc = 0.8749333345890045, Test Acc = 0.8630000042915345\n",
      "Training at step=22, batch=0, train loss = 0.3365565538406372, train acc = 0.8899999856948853, time = 0.9467916488647461\n",
      "Training at step=22, batch=30, train loss = 0.38260743021965027, train acc = 0.8600000143051147, time = 0.945380449295044\n",
      "Training at step=22, batch=60, train loss = 0.30924922227859497, train acc = 0.8849999904632568, time = 0.9452545642852783\n",
      "Training at step=22, batch=90, train loss = 0.3205525577068329, train acc = 0.9049999713897705, time = 0.9488017559051514\n",
      "Training at step=22, batch=120, train loss = 0.4372921884059906, train acc = 0.8450000286102295, time = 0.9656496047973633\n",
      "Training at step=22, batch=150, train loss = 0.3576521575450897, train acc = 0.8799999952316284, time = 0.9497737884521484\n",
      "Training at step=22, batch=180, train loss = 0.2845776081085205, train acc = 0.8949999809265137, time = 0.9518969058990479\n",
      "Training at step=22, batch=210, train loss = 0.33247825503349304, train acc = 0.8650000095367432, time = 0.9520432949066162\n",
      "Training at step=22, batch=240, train loss = 0.4015904366970062, train acc = 0.8700000047683716, time = 0.94637131690979\n",
      "Training at step=22, batch=270, train loss = 0.3839915096759796, train acc = 0.8550000190734863, time = 0.947622537612915\n",
      "Testing at step=22, batch=0, test loss = 0.33710426092147827, test acc = 0.8849999904632568, time = 0.35565900802612305\n",
      "Testing at step=22, batch=5, test loss = 0.32039138674736023, test acc = 0.8949999809265137, time = 0.3624227046966553\n",
      "Testing at step=22, batch=10, test loss = 0.4280087649822235, test acc = 0.8600000143051147, time = 0.37339067459106445\n",
      "Testing at step=22, batch=15, test loss = 0.39415037631988525, test acc = 0.8600000143051147, time = 0.3575704097747803\n",
      "Testing at step=22, batch=20, test loss = 0.3882911801338196, test acc = 0.8700000047683716, time = 0.35436439514160156\n",
      "Testing at step=22, batch=25, test loss = 0.40179499983787537, test acc = 0.8550000190734863, time = 0.36559200286865234\n",
      "Testing at step=22, batch=30, test loss = 0.36749228835105896, test acc = 0.8500000238418579, time = 0.36225008964538574\n",
      "Testing at step=22, batch=35, test loss = 0.4371611773967743, test acc = 0.875, time = 0.3622281551361084\n",
      "Testing at step=22, batch=40, test loss = 0.3775408864021301, test acc = 0.8650000095367432, time = 0.34976792335510254\n",
      "Testing at step=22, batch=45, test loss = 0.5202162265777588, test acc = 0.8650000095367432, time = 0.3705482482910156\n",
      "Step 22 finished in 313.60331892967224, Train loss = 0.3402934557199478, Test loss = 0.40067907989025114; Train Acc = 0.8764500008026759, Test Acc = 0.8603000044822693\n",
      "Training at step=23, batch=0, train loss = 0.3230516314506531, train acc = 0.8949999809265137, time = 0.9459173679351807\n",
      "Training at step=23, batch=30, train loss = 0.3317643105983734, train acc = 0.875, time = 0.9433295726776123\n",
      "Training at step=23, batch=60, train loss = 0.38362109661102295, train acc = 0.8700000047683716, time = 0.9572927951812744\n",
      "Training at step=23, batch=90, train loss = 0.3297795057296753, train acc = 0.875, time = 0.9464573860168457\n",
      "Training at step=23, batch=120, train loss = 0.35657694935798645, train acc = 0.8550000190734863, time = 0.9396216869354248\n",
      "Training at step=23, batch=150, train loss = 0.2945021688938141, train acc = 0.8999999761581421, time = 0.9479610919952393\n",
      "Training at step=23, batch=180, train loss = 0.3789258599281311, train acc = 0.8600000143051147, time = 0.9371333122253418\n",
      "Training at step=23, batch=210, train loss = 0.4224868416786194, train acc = 0.8600000143051147, time = 0.9465289115905762\n",
      "Training at step=23, batch=240, train loss = 0.265501469373703, train acc = 0.8899999856948853, time = 0.9549283981323242\n",
      "Training at step=23, batch=270, train loss = 0.3385305404663086, train acc = 0.8849999904632568, time = 0.957608699798584\n",
      "Testing at step=23, batch=0, test loss = 0.46769818663597107, test acc = 0.8199999928474426, time = 0.3547215461730957\n",
      "Testing at step=23, batch=5, test loss = 0.3951907753944397, test acc = 0.8700000047683716, time = 0.381450891494751\n",
      "Testing at step=23, batch=10, test loss = 0.37524595856666565, test acc = 0.8650000095367432, time = 0.3607814311981201\n",
      "Testing at step=23, batch=15, test loss = 0.35081908106803894, test acc = 0.875, time = 0.3653435707092285\n",
      "Testing at step=23, batch=20, test loss = 0.29158109426498413, test acc = 0.8849999904632568, time = 0.35715508460998535\n",
      "Testing at step=23, batch=25, test loss = 0.4547092914581299, test acc = 0.8399999737739563, time = 0.349193811416626\n",
      "Testing at step=23, batch=30, test loss = 0.3173954486846924, test acc = 0.8899999856948853, time = 0.35838747024536133\n",
      "Testing at step=23, batch=35, test loss = 0.36241790652275085, test acc = 0.8799999952316284, time = 0.34917283058166504\n",
      "Testing at step=23, batch=40, test loss = 0.4405292868614197, test acc = 0.8600000143051147, time = 0.33792901039123535\n",
      "Testing at step=23, batch=45, test loss = 0.4183427691459656, test acc = 0.8600000143051147, time = 0.35579538345336914\n",
      "Step 23 finished in 313.9902937412262, Train loss = 0.33487417389949165, Test loss = 0.3820480042695999; Train Acc = 0.8779166648785274, Test Acc = 0.8639999997615814\n",
      "Training at step=24, batch=0, train loss = 0.3079165518283844, train acc = 0.8899999856948853, time = 0.9604501724243164\n",
      "Training at step=24, batch=30, train loss = 0.30396145582199097, train acc = 0.8949999809265137, time = 0.9396204948425293\n",
      "Training at step=24, batch=60, train loss = 0.38095623254776, train acc = 0.8650000095367432, time = 0.9574947357177734\n",
      "Training at step=24, batch=90, train loss = 0.3556387424468994, train acc = 0.875, time = 0.9310176372528076\n",
      "Training at step=24, batch=120, train loss = 0.3485751748085022, train acc = 0.8899999856948853, time = 0.934959888458252\n",
      "Training at step=24, batch=150, train loss = 0.4757615923881531, train acc = 0.8050000071525574, time = 0.9659633636474609\n",
      "Training at step=24, batch=180, train loss = 0.410740464925766, train acc = 0.8700000047683716, time = 0.9651353359222412\n",
      "Training at step=24, batch=210, train loss = 0.32491549849510193, train acc = 0.8899999856948853, time = 0.948483943939209\n",
      "Training at step=24, batch=240, train loss = 0.299797922372818, train acc = 0.8949999809265137, time = 0.9349789619445801\n",
      "Training at step=24, batch=270, train loss = 0.27272936701774597, train acc = 0.8999999761581421, time = 0.9631667137145996\n",
      "Testing at step=24, batch=0, test loss = 0.26693886518478394, test acc = 0.925000011920929, time = 0.3679347038269043\n",
      "Testing at step=24, batch=5, test loss = 0.4842621684074402, test acc = 0.8450000286102295, time = 0.3633096218109131\n",
      "Testing at step=24, batch=10, test loss = 0.3428255319595337, test acc = 0.8550000190734863, time = 0.35997462272644043\n",
      "Testing at step=24, batch=15, test loss = 0.3818356394767761, test acc = 0.8799999952316284, time = 0.34066152572631836\n",
      "Testing at step=24, batch=20, test loss = 0.3820129334926605, test acc = 0.8600000143051147, time = 0.364760160446167\n",
      "Testing at step=24, batch=25, test loss = 0.4108230471611023, test acc = 0.8349999785423279, time = 0.34649014472961426\n",
      "Testing at step=24, batch=30, test loss = 0.29950302839279175, test acc = 0.8999999761581421, time = 0.34639501571655273\n",
      "Testing at step=24, batch=35, test loss = 0.3848239481449127, test acc = 0.8600000143051147, time = 0.3419487476348877\n",
      "Testing at step=24, batch=40, test loss = 0.40627938508987427, test acc = 0.8700000047683716, time = 0.3618490695953369\n",
      "Testing at step=24, batch=45, test loss = 0.4172070324420929, test acc = 0.8849999904632568, time = 0.3630833625793457\n",
      "Step 24 finished in 313.3175320625305, Train loss = 0.3349219997227192, Test loss = 0.3911008697748184; Train Acc = 0.878333331545194, Test Acc = 0.8612999987602233\n",
      "Training at step=25, batch=0, train loss = 0.41455134749412537, train acc = 0.8500000238418579, time = 0.9348187446594238\n",
      "Training at step=25, batch=30, train loss = 0.31017979979515076, train acc = 0.8799999952316284, time = 0.9466667175292969\n",
      "Training at step=25, batch=60, train loss = 0.34303417801856995, train acc = 0.8999999761581421, time = 0.9328558444976807\n",
      "Training at step=25, batch=90, train loss = 0.26568445563316345, train acc = 0.9200000166893005, time = 0.9447658061981201\n",
      "Training at step=25, batch=120, train loss = 0.3613605201244354, train acc = 0.8899999856948853, time = 0.9448256492614746\n",
      "Training at step=25, batch=150, train loss = 0.3569219708442688, train acc = 0.8899999856948853, time = 0.9413101673126221\n",
      "Training at step=25, batch=180, train loss = 0.31702739000320435, train acc = 0.875, time = 0.9606373310089111\n",
      "Training at step=25, batch=210, train loss = 0.30122604966163635, train acc = 0.8799999952316284, time = 0.9716644287109375\n",
      "Training at step=25, batch=240, train loss = 0.2733510732650757, train acc = 0.9150000214576721, time = 0.9339337348937988\n",
      "Training at step=25, batch=270, train loss = 0.3513355255126953, train acc = 0.8949999809265137, time = 0.9652085304260254\n",
      "Testing at step=25, batch=0, test loss = 0.3638419806957245, test acc = 0.8700000047683716, time = 0.3705484867095947\n",
      "Testing at step=25, batch=5, test loss = 0.33100631833076477, test acc = 0.8799999952316284, time = 0.36756467819213867\n",
      "Testing at step=25, batch=10, test loss = 0.4845079779624939, test acc = 0.824999988079071, time = 0.3429126739501953\n",
      "Testing at step=25, batch=15, test loss = 0.4303240180015564, test acc = 0.8500000238418579, time = 0.34841489791870117\n",
      "Testing at step=25, batch=20, test loss = 0.4011145830154419, test acc = 0.8550000190734863, time = 0.3546149730682373\n",
      "Testing at step=25, batch=25, test loss = 0.3486735224723816, test acc = 0.8849999904632568, time = 0.3559999465942383\n",
      "Testing at step=25, batch=30, test loss = 0.41120946407318115, test acc = 0.8399999737739563, time = 0.3567070960998535\n",
      "Testing at step=25, batch=35, test loss = 0.46277862787246704, test acc = 0.8550000190734863, time = 0.35556483268737793\n",
      "Testing at step=25, batch=40, test loss = 0.4640878438949585, test acc = 0.8450000286102295, time = 0.3599965572357178\n",
      "Testing at step=25, batch=45, test loss = 0.4413616955280304, test acc = 0.8299999833106995, time = 0.3708224296569824\n",
      "Step 25 finished in 313.79110860824585, Train loss = 0.33541601806879046, Test loss = 0.3816421601176262; Train Acc = 0.8770499976476034, Test Acc = 0.8620000076293945\n",
      "Training at step=26, batch=0, train loss = 0.32053428888320923, train acc = 0.8849999904632568, time = 0.9560320377349854\n",
      "Training at step=26, batch=30, train loss = 0.2572000026702881, train acc = 0.8999999761581421, time = 0.9547135829925537\n",
      "Training at step=26, batch=60, train loss = 0.39801785349845886, train acc = 0.8600000143051147, time = 0.9688968658447266\n",
      "Training at step=26, batch=90, train loss = 0.3359866440296173, train acc = 0.8849999904632568, time = 0.9332160949707031\n",
      "Training at step=26, batch=120, train loss = 0.35027173161506653, train acc = 0.8849999904632568, time = 0.9464781284332275\n",
      "Training at step=26, batch=150, train loss = 0.3288027048110962, train acc = 0.8899999856948853, time = 0.9471001625061035\n",
      "Training at step=26, batch=180, train loss = 0.348371684551239, train acc = 0.8849999904632568, time = 0.9361991882324219\n",
      "Training at step=26, batch=210, train loss = 0.4247373938560486, train acc = 0.8500000238418579, time = 0.9475398063659668\n",
      "Training at step=26, batch=240, train loss = 0.27379316091537476, train acc = 0.8949999809265137, time = 0.9500138759613037\n",
      "Training at step=26, batch=270, train loss = 0.3881351053714752, train acc = 0.8450000286102295, time = 0.9532861709594727\n",
      "Testing at step=26, batch=0, test loss = 0.3816942572593689, test acc = 0.8550000190734863, time = 0.3526625633239746\n",
      "Testing at step=26, batch=5, test loss = 0.4406157433986664, test acc = 0.8349999785423279, time = 0.3518822193145752\n",
      "Testing at step=26, batch=10, test loss = 0.4195803701877594, test acc = 0.8700000047683716, time = 0.34732484817504883\n",
      "Testing at step=26, batch=15, test loss = 0.36325398087501526, test acc = 0.875, time = 0.3420858383178711\n",
      "Testing at step=26, batch=20, test loss = 0.33097365498542786, test acc = 0.8949999809265137, time = 0.35319089889526367\n",
      "Testing at step=26, batch=25, test loss = 0.3904646933078766, test acc = 0.8650000095367432, time = 0.35927391052246094\n",
      "Testing at step=26, batch=30, test loss = 0.36715686321258545, test acc = 0.8500000238418579, time = 0.34291744232177734\n",
      "Testing at step=26, batch=35, test loss = 0.34107688069343567, test acc = 0.875, time = 0.3529934883117676\n",
      "Testing at step=26, batch=40, test loss = 0.3498641550540924, test acc = 0.8799999952316284, time = 0.35669541358947754\n",
      "Testing at step=26, batch=45, test loss = 0.4290826916694641, test acc = 0.8450000286102295, time = 0.34723639488220215\n",
      "Step 26 finished in 313.43988060951233, Train loss = 0.3260214290519555, Test loss = 0.37806212455034255; Train Acc = 0.8816166647275289, Test Acc = 0.8643000006675721\n",
      "Training at step=27, batch=0, train loss = 0.2844810485839844, train acc = 0.8650000095367432, time = 0.9524238109588623\n",
      "Training at step=27, batch=30, train loss = 0.34417733550071716, train acc = 0.8650000095367432, time = 0.9602644443511963\n",
      "Training at step=27, batch=60, train loss = 0.2806280851364136, train acc = 0.9100000262260437, time = 0.9567210674285889\n",
      "Training at step=27, batch=90, train loss = 0.37411174178123474, train acc = 0.875, time = 0.9638137817382812\n",
      "Training at step=27, batch=120, train loss = 0.3224709630012512, train acc = 0.8999999761581421, time = 0.9703528881072998\n",
      "Training at step=27, batch=150, train loss = 0.36714649200439453, train acc = 0.8600000143051147, time = 0.9583439826965332\n",
      "Training at step=27, batch=180, train loss = 0.3834788501262665, train acc = 0.8450000286102295, time = 0.9686760902404785\n",
      "Training at step=27, batch=210, train loss = 0.298705518245697, train acc = 0.9100000262260437, time = 0.9451310634613037\n",
      "Training at step=27, batch=240, train loss = 0.32436442375183105, train acc = 0.8550000190734863, time = 0.9474086761474609\n",
      "Training at step=27, batch=270, train loss = 0.29517656564712524, train acc = 0.9049999713897705, time = 0.9603710174560547\n",
      "Testing at step=27, batch=0, test loss = 0.2961156964302063, test acc = 0.8849999904632568, time = 0.36119890213012695\n",
      "Testing at step=27, batch=5, test loss = 0.4373939633369446, test acc = 0.8399999737739563, time = 0.35271191596984863\n",
      "Testing at step=27, batch=10, test loss = 0.3493458032608032, test acc = 0.9049999713897705, time = 0.35860276222229004\n",
      "Testing at step=27, batch=15, test loss = 0.3812178373336792, test acc = 0.8650000095367432, time = 0.36324381828308105\n",
      "Testing at step=27, batch=20, test loss = 0.4085770547389984, test acc = 0.8600000143051147, time = 0.35652685165405273\n",
      "Testing at step=27, batch=25, test loss = 0.4861217141151428, test acc = 0.8349999785423279, time = 0.3696107864379883\n",
      "Testing at step=27, batch=30, test loss = 0.4922974407672882, test acc = 0.8199999928474426, time = 0.40181732177734375\n",
      "Testing at step=27, batch=35, test loss = 0.40214282274246216, test acc = 0.8550000190734863, time = 0.34795284271240234\n",
      "Testing at step=27, batch=40, test loss = 0.4639689326286316, test acc = 0.8349999785423279, time = 0.3433802127838135\n",
      "Testing at step=27, batch=45, test loss = 0.40283969044685364, test acc = 0.8199999928474426, time = 0.362473726272583\n",
      "Step 27 finished in 314.9374361038208, Train loss = 0.3257359160979589, Test loss = 0.3973506274819374; Train Acc = 0.8820500006278356, Test Acc = 0.8579000008106231\n",
      "Training at step=28, batch=0, train loss = 0.33893775939941406, train acc = 0.9049999713897705, time = 0.9619603157043457\n",
      "Training at step=28, batch=30, train loss = 0.37612012028694153, train acc = 0.8100000023841858, time = 0.9632115364074707\n",
      "Training at step=28, batch=60, train loss = 0.28752854466438293, train acc = 0.8899999856948853, time = 0.9512240886688232\n",
      "Training at step=28, batch=90, train loss = 0.264200359582901, train acc = 0.9150000214576721, time = 0.9536776542663574\n",
      "Training at step=28, batch=120, train loss = 0.2778715491294861, train acc = 0.9350000023841858, time = 0.9514191150665283\n",
      "Training at step=28, batch=150, train loss = 0.32127419114112854, train acc = 0.8650000095367432, time = 0.9578573703765869\n",
      "Training at step=28, batch=180, train loss = 0.37354323267936707, train acc = 0.8799999952316284, time = 0.9464163780212402\n",
      "Training at step=28, batch=210, train loss = 0.29471123218536377, train acc = 0.8999999761581421, time = 0.9566431045532227\n",
      "Training at step=28, batch=240, train loss = 0.26701661944389343, train acc = 0.8999999761581421, time = 0.9565260410308838\n",
      "Training at step=28, batch=270, train loss = 0.37122753262519836, train acc = 0.8700000047683716, time = 0.938725471496582\n",
      "Testing at step=28, batch=0, test loss = 0.47911158204078674, test acc = 0.8450000286102295, time = 0.36011838912963867\n",
      "Testing at step=28, batch=5, test loss = 0.5348727703094482, test acc = 0.824999988079071, time = 0.35707902908325195\n",
      "Testing at step=28, batch=10, test loss = 0.4215056896209717, test acc = 0.8500000238418579, time = 0.35787367820739746\n",
      "Testing at step=28, batch=15, test loss = 0.4257785677909851, test acc = 0.8600000143051147, time = 0.34453868865966797\n",
      "Testing at step=28, batch=20, test loss = 0.2754993140697479, test acc = 0.8949999809265137, time = 0.3606529235839844\n",
      "Testing at step=28, batch=25, test loss = 0.3322926461696625, test acc = 0.8899999856948853, time = 0.3686821460723877\n",
      "Testing at step=28, batch=30, test loss = 0.4102400839328766, test acc = 0.8899999856948853, time = 0.3557438850402832\n",
      "Testing at step=28, batch=35, test loss = 0.4278443157672882, test acc = 0.8550000190734863, time = 0.3619852066040039\n",
      "Testing at step=28, batch=40, test loss = 0.29398563504219055, test acc = 0.8899999856948853, time = 0.36026716232299805\n",
      "Testing at step=28, batch=45, test loss = 0.45296233892440796, test acc = 0.8500000238418579, time = 0.3602714538574219\n",
      "Step 28 finished in 313.9197037220001, Train loss = 0.3236011382440726, Test loss = 0.3877979892492294; Train Acc = 0.881883331934611, Test Acc = 0.865\n",
      "Training at step=29, batch=0, train loss = 0.28403493762016296, train acc = 0.8849999904632568, time = 0.9618399143218994\n",
      "Training at step=29, batch=30, train loss = 0.3784109950065613, train acc = 0.8550000190734863, time = 0.9657015800476074\n",
      "Training at step=29, batch=60, train loss = 0.23973417282104492, train acc = 0.9049999713897705, time = 0.9397869110107422\n",
      "Training at step=29, batch=90, train loss = 0.24078264832496643, train acc = 0.9399999976158142, time = 0.9429190158843994\n",
      "Training at step=29, batch=120, train loss = 0.36537644267082214, train acc = 0.8399999737739563, time = 0.9567146301269531\n",
      "Training at step=29, batch=150, train loss = 0.3731892704963684, train acc = 0.8700000047683716, time = 0.935230016708374\n",
      "Training at step=29, batch=180, train loss = 0.26880133152008057, train acc = 0.9100000262260437, time = 0.95924973487854\n",
      "Training at step=29, batch=210, train loss = 0.26296189427375793, train acc = 0.8999999761581421, time = 0.9477672576904297\n",
      "Training at step=29, batch=240, train loss = 0.2324354499578476, train acc = 0.9300000071525574, time = 0.9416649341583252\n",
      "Training at step=29, batch=270, train loss = 0.29539117217063904, train acc = 0.8999999761581421, time = 0.9490566253662109\n",
      "Testing at step=29, batch=0, test loss = 0.48965224623680115, test acc = 0.8450000286102295, time = 0.36605191230773926\n",
      "Testing at step=29, batch=5, test loss = 0.3376133441925049, test acc = 0.8899999856948853, time = 0.3473091125488281\n",
      "Testing at step=29, batch=10, test loss = 0.43630871176719666, test acc = 0.8299999833106995, time = 0.3558011054992676\n",
      "Testing at step=29, batch=15, test loss = 0.5014948844909668, test acc = 0.8149999976158142, time = 0.35176658630371094\n",
      "Testing at step=29, batch=20, test loss = 0.4305171072483063, test acc = 0.8349999785423279, time = 0.36544013023376465\n",
      "Testing at step=29, batch=25, test loss = 0.40560203790664673, test acc = 0.8550000190734863, time = 0.35016322135925293\n",
      "Testing at step=29, batch=30, test loss = 0.3921773433685303, test acc = 0.8650000095367432, time = 0.35454678535461426\n",
      "Testing at step=29, batch=35, test loss = 0.2747024595737457, test acc = 0.9049999713897705, time = 0.3571341037750244\n",
      "Testing at step=29, batch=40, test loss = 0.4075486660003662, test acc = 0.8899999856948853, time = 0.3753511905670166\n",
      "Testing at step=29, batch=45, test loss = 0.28911304473876953, test acc = 0.8650000095367432, time = 0.3615877628326416\n",
      "Step 29 finished in 313.44867610931396, Train loss = 0.3210695952177048, Test loss = 0.4005775275826454; Train Acc = 0.8823999985059102, Test Acc = 0.856800000667572\n",
      "Training at step=30, batch=0, train loss = 0.34132900834083557, train acc = 0.8849999904632568, time = 0.9511494636535645\n",
      "Training at step=30, batch=30, train loss = 0.30794277787208557, train acc = 0.8799999952316284, time = 0.9566881656646729\n",
      "Training at step=30, batch=60, train loss = 0.19772066175937653, train acc = 0.9300000071525574, time = 0.9435045719146729\n",
      "Training at step=30, batch=90, train loss = 0.291847825050354, train acc = 0.9049999713897705, time = 0.9591426849365234\n",
      "Training at step=30, batch=120, train loss = 0.26201871037483215, train acc = 0.8999999761581421, time = 0.9710583686828613\n",
      "Training at step=30, batch=150, train loss = 0.1861787587404251, train acc = 0.9300000071525574, time = 0.9581727981567383\n",
      "Training at step=30, batch=180, train loss = 0.30585530400276184, train acc = 0.8849999904632568, time = 0.9533126354217529\n",
      "Training at step=30, batch=210, train loss = 0.2803976535797119, train acc = 0.8949999809265137, time = 0.9408776760101318\n",
      "Training at step=30, batch=240, train loss = 0.35979610681533813, train acc = 0.8799999952316284, time = 0.9632670879364014\n",
      "Training at step=30, batch=270, train loss = 0.33614933490753174, train acc = 0.875, time = 0.9443213939666748\n",
      "Testing at step=30, batch=0, test loss = 0.3802735209465027, test acc = 0.8700000047683716, time = 0.3611729145050049\n",
      "Testing at step=30, batch=5, test loss = 0.38117849826812744, test acc = 0.8650000095367432, time = 0.3638172149658203\n",
      "Testing at step=30, batch=10, test loss = 0.5312320590019226, test acc = 0.8149999976158142, time = 0.3447439670562744\n",
      "Testing at step=30, batch=15, test loss = 0.39531493186950684, test acc = 0.875, time = 0.3530454635620117\n",
      "Testing at step=30, batch=20, test loss = 0.3696098327636719, test acc = 0.8700000047683716, time = 0.366835355758667\n",
      "Testing at step=30, batch=25, test loss = 0.39482712745666504, test acc = 0.8700000047683716, time = 0.3481130599975586\n",
      "Testing at step=30, batch=30, test loss = 0.34031009674072266, test acc = 0.8550000190734863, time = 0.35700058937072754\n",
      "Testing at step=30, batch=35, test loss = 0.41495242714881897, test acc = 0.8500000238418579, time = 0.36164331436157227\n",
      "Testing at step=30, batch=40, test loss = 0.2516460120677948, test acc = 0.925000011920929, time = 0.35474348068237305\n",
      "Testing at step=30, batch=45, test loss = 0.3880510628223419, test acc = 0.8600000143051147, time = 0.35343050956726074\n",
      "Step 30 finished in 313.6925070285797, Train loss = 0.31532163297136623, Test loss = 0.38614450007677076; Train Acc = 0.8861499977111816, Test Acc = 0.8637000024318695\n",
      "Training at step=31, batch=0, train loss = 0.360330730676651, train acc = 0.8899999856948853, time = 0.9504845142364502\n",
      "Training at step=31, batch=30, train loss = 0.297358900308609, train acc = 0.8849999904632568, time = 0.9596221446990967\n",
      "Training at step=31, batch=60, train loss = 0.37947794795036316, train acc = 0.8600000143051147, time = 0.9333031177520752\n",
      "Training at step=31, batch=90, train loss = 0.2969462275505066, train acc = 0.9049999713897705, time = 0.9520120620727539\n",
      "Training at step=31, batch=120, train loss = 0.279900461435318, train acc = 0.8899999856948853, time = 0.9461479187011719\n",
      "Training at step=31, batch=150, train loss = 0.32392293214797974, train acc = 0.9150000214576721, time = 0.9609081745147705\n",
      "Training at step=31, batch=180, train loss = 0.36786437034606934, train acc = 0.8600000143051147, time = 0.9500613212585449\n",
      "Training at step=31, batch=210, train loss = 0.38523152470588684, train acc = 0.8650000095367432, time = 0.9572861194610596\n",
      "Training at step=31, batch=240, train loss = 0.29824939370155334, train acc = 0.8849999904632568, time = 0.9567224979400635\n",
      "Training at step=31, batch=270, train loss = 0.29302167892456055, train acc = 0.8849999904632568, time = 0.9509012699127197\n",
      "Testing at step=31, batch=0, test loss = 0.4264635443687439, test acc = 0.8450000286102295, time = 0.34800100326538086\n",
      "Testing at step=31, batch=5, test loss = 0.3424362540245056, test acc = 0.8899999856948853, time = 0.35388898849487305\n",
      "Testing at step=31, batch=10, test loss = 0.4124119281768799, test acc = 0.8399999737739563, time = 0.3502976894378662\n",
      "Testing at step=31, batch=15, test loss = 0.4028085768222809, test acc = 0.875, time = 0.3515629768371582\n",
      "Testing at step=31, batch=20, test loss = 0.3762933611869812, test acc = 0.8500000238418579, time = 0.34516119956970215\n",
      "Testing at step=31, batch=25, test loss = 0.2727760076522827, test acc = 0.8949999809265137, time = 0.3425281047821045\n",
      "Testing at step=31, batch=30, test loss = 0.44374722242355347, test acc = 0.8299999833106995, time = 0.35411977767944336\n",
      "Testing at step=31, batch=35, test loss = 0.3030868470668793, test acc = 0.8700000047683716, time = 0.3373088836669922\n",
      "Testing at step=31, batch=40, test loss = 0.29089629650115967, test acc = 0.9150000214576721, time = 0.34655070304870605\n",
      "Testing at step=31, batch=45, test loss = 0.42348578572273254, test acc = 0.8199999928474426, time = 0.35523152351379395\n",
      "Step 31 finished in 313.354248046875, Train loss = 0.31667129213611284, Test loss = 0.3699572294950485; Train Acc = 0.8834833323955535, Test Acc = 0.8675999999046325\n",
      "Training at step=32, batch=0, train loss = 0.29881882667541504, train acc = 0.8899999856948853, time = 0.9383087158203125\n",
      "Training at step=32, batch=30, train loss = 0.29234999418258667, train acc = 0.8999999761581421, time = 0.9588322639465332\n",
      "Training at step=32, batch=60, train loss = 0.327057421207428, train acc = 0.8949999809265137, time = 0.948828935623169\n",
      "Training at step=32, batch=90, train loss = 0.3903537690639496, train acc = 0.8700000047683716, time = 0.9491217136383057\n",
      "Training at step=32, batch=120, train loss = 0.26201894879341125, train acc = 0.8899999856948853, time = 0.9493319988250732\n",
      "Training at step=32, batch=150, train loss = 0.3527950942516327, train acc = 0.8650000095367432, time = 0.940049409866333\n",
      "Training at step=32, batch=180, train loss = 0.3534306287765503, train acc = 0.8700000047683716, time = 0.969071626663208\n",
      "Training at step=32, batch=210, train loss = 0.23823308944702148, train acc = 0.9100000262260437, time = 0.9371919631958008\n",
      "Training at step=32, batch=240, train loss = 0.4079383611679077, train acc = 0.8799999952316284, time = 0.9481439590454102\n",
      "Training at step=32, batch=270, train loss = 0.2747197449207306, train acc = 0.8949999809265137, time = 0.9502029418945312\n",
      "Testing at step=32, batch=0, test loss = 0.3609439432621002, test acc = 0.8799999952316284, time = 0.3453803062438965\n",
      "Testing at step=32, batch=5, test loss = 0.33529049158096313, test acc = 0.875, time = 0.3511638641357422\n",
      "Testing at step=32, batch=10, test loss = 0.3768438994884491, test acc = 0.8650000095367432, time = 0.43166565895080566\n",
      "Testing at step=32, batch=15, test loss = 0.5674644708633423, test acc = 0.8149999976158142, time = 0.3534374237060547\n",
      "Testing at step=32, batch=20, test loss = 0.3547675609588623, test acc = 0.8849999904632568, time = 0.368117094039917\n",
      "Testing at step=32, batch=25, test loss = 0.44476157426834106, test acc = 0.8349999785423279, time = 0.35245466232299805\n",
      "Testing at step=32, batch=30, test loss = 0.355257123708725, test acc = 0.8650000095367432, time = 0.3490593433380127\n",
      "Testing at step=32, batch=35, test loss = 0.38169965147972107, test acc = 0.8500000238418579, time = 0.3475914001464844\n",
      "Testing at step=32, batch=40, test loss = 0.38128966093063354, test acc = 0.8700000047683716, time = 0.353287935256958\n",
      "Testing at step=32, batch=45, test loss = 0.4258773922920227, test acc = 0.824999988079071, time = 0.34015798568725586\n",
      "Step 32 finished in 313.327623128891, Train loss = 0.31592180743813514, Test loss = 0.3927775079011917; Train Acc = 0.8837999989589055, Test Acc = 0.8601999962329865\n",
      "Training at step=33, batch=0, train loss = 0.30673322081565857, train acc = 0.8999999761581421, time = 0.9430601596832275\n",
      "Training at step=33, batch=30, train loss = 0.23494727909564972, train acc = 0.9300000071525574, time = 0.9583694934844971\n",
      "Training at step=33, batch=60, train loss = 0.2559904158115387, train acc = 0.9049999713897705, time = 0.9440388679504395\n",
      "Training at step=33, batch=90, train loss = 0.2800772488117218, train acc = 0.8849999904632568, time = 0.9528522491455078\n",
      "Training at step=33, batch=120, train loss = 0.30769863724708557, train acc = 0.875, time = 0.9534592628479004\n",
      "Training at step=33, batch=150, train loss = 0.35649800300598145, train acc = 0.8700000047683716, time = 0.9435365200042725\n",
      "Training at step=33, batch=180, train loss = 0.28999418020248413, train acc = 0.9049999713897705, time = 0.9374587535858154\n",
      "Training at step=33, batch=210, train loss = 0.2511158883571625, train acc = 0.9049999713897705, time = 0.9453446865081787\n",
      "Training at step=33, batch=240, train loss = 0.3110660910606384, train acc = 0.8999999761581421, time = 0.965038537979126\n",
      "Training at step=33, batch=270, train loss = 0.30646729469299316, train acc = 0.8849999904632568, time = 0.9609019756317139\n",
      "Testing at step=33, batch=0, test loss = 0.4327053129673004, test acc = 0.8349999785423279, time = 0.3504941463470459\n",
      "Testing at step=33, batch=5, test loss = 0.3663560450077057, test acc = 0.8650000095367432, time = 0.3533916473388672\n",
      "Testing at step=33, batch=10, test loss = 0.3792443573474884, test acc = 0.8600000143051147, time = 0.36070966720581055\n",
      "Testing at step=33, batch=15, test loss = 0.27462923526763916, test acc = 0.9049999713897705, time = 0.35387110710144043\n",
      "Testing at step=33, batch=20, test loss = 0.30740079283714294, test acc = 0.9049999713897705, time = 0.3491196632385254\n",
      "Testing at step=33, batch=25, test loss = 0.313798725605011, test acc = 0.8700000047683716, time = 0.34844303131103516\n",
      "Testing at step=33, batch=30, test loss = 0.33765357732772827, test acc = 0.8700000047683716, time = 0.37105488777160645\n",
      "Testing at step=33, batch=35, test loss = 0.3262313902378082, test acc = 0.8799999952316284, time = 0.3616151809692383\n",
      "Testing at step=33, batch=40, test loss = 0.37837010622024536, test acc = 0.8600000143051147, time = 0.3671078681945801\n",
      "Testing at step=33, batch=45, test loss = 0.45871371030807495, test acc = 0.8700000047683716, time = 0.3711566925048828\n",
      "Step 33 finished in 313.5423300266266, Train loss = 0.311849530339241, Test loss = 0.3566139751672745; Train Acc = 0.886433331767718, Test Acc = 0.8732999968528747\n",
      "Training at step=34, batch=0, train loss = 0.21869175136089325, train acc = 0.9049999713897705, time = 0.9646549224853516\n",
      "Training at step=34, batch=30, train loss = 0.38095447421073914, train acc = 0.8700000047683716, time = 0.9372270107269287\n",
      "Training at step=34, batch=60, train loss = 0.25662386417388916, train acc = 0.8799999952316284, time = 0.9498827457427979\n",
      "Training at step=34, batch=90, train loss = 0.24096691608428955, train acc = 0.9150000214576721, time = 0.9519803524017334\n",
      "Training at step=34, batch=120, train loss = 0.26907795667648315, train acc = 0.8949999809265137, time = 0.9429514408111572\n",
      "Training at step=34, batch=150, train loss = 0.27753517031669617, train acc = 0.8849999904632568, time = 0.9503295421600342\n",
      "Training at step=34, batch=180, train loss = 0.22159670293331146, train acc = 0.9150000214576721, time = 0.958763837814331\n",
      "Training at step=34, batch=210, train loss = 0.27950820326805115, train acc = 0.9049999713897705, time = 0.9352462291717529\n",
      "Training at step=34, batch=240, train loss = 0.35310012102127075, train acc = 0.8799999952316284, time = 0.9436342716217041\n",
      "Training at step=34, batch=270, train loss = 0.22387905418872833, train acc = 0.9100000262260437, time = 0.9622492790222168\n",
      "Testing at step=34, batch=0, test loss = 0.3356321454048157, test acc = 0.8999999761581421, time = 0.37287378311157227\n",
      "Testing at step=34, batch=5, test loss = 0.3663432002067566, test acc = 0.8650000095367432, time = 0.3505682945251465\n",
      "Testing at step=34, batch=10, test loss = 0.3276776969432831, test acc = 0.8799999952316284, time = 0.35213375091552734\n",
      "Testing at step=34, batch=15, test loss = 0.4442157745361328, test acc = 0.824999988079071, time = 0.35747742652893066\n",
      "Testing at step=34, batch=20, test loss = 0.3988156020641327, test acc = 0.8399999737739563, time = 0.34892868995666504\n",
      "Testing at step=34, batch=25, test loss = 0.45178279280662537, test acc = 0.8450000286102295, time = 0.35283613204956055\n",
      "Testing at step=34, batch=30, test loss = 0.38099387288093567, test acc = 0.8799999952316284, time = 0.35724878311157227\n",
      "Testing at step=34, batch=35, test loss = 0.35134357213974, test acc = 0.8949999809265137, time = 0.3520958423614502\n",
      "Testing at step=34, batch=40, test loss = 0.3292822241783142, test acc = 0.875, time = 0.3410801887512207\n",
      "Testing at step=34, batch=45, test loss = 0.29423561692237854, test acc = 0.9049999713897705, time = 0.3545382022857666\n",
      "Step 34 finished in 313.54612851142883, Train loss = 0.3127827531099319, Test loss = 0.36490608036518096; Train Acc = 0.8852833292881648, Test Acc = 0.8722999978065491\n",
      "Training at step=35, batch=0, train loss = 0.2766411006450653, train acc = 0.8999999761581421, time = 0.9411101341247559\n",
      "Training at step=35, batch=30, train loss = 0.3144073188304901, train acc = 0.8700000047683716, time = 0.9414858818054199\n",
      "Training at step=35, batch=60, train loss = 0.2203000783920288, train acc = 0.9200000166893005, time = 0.948317289352417\n",
      "Training at step=35, batch=90, train loss = 0.3228594660758972, train acc = 0.8700000047683716, time = 0.9522459506988525\n",
      "Training at step=35, batch=120, train loss = 0.25252169370651245, train acc = 0.9049999713897705, time = 0.9340963363647461\n",
      "Training at step=35, batch=150, train loss = 0.2742525637149811, train acc = 0.8849999904632568, time = 0.9590024948120117\n",
      "Training at step=35, batch=180, train loss = 0.38866832852363586, train acc = 0.8600000143051147, time = 0.9604861736297607\n",
      "Training at step=35, batch=210, train loss = 0.3082498610019684, train acc = 0.875, time = 0.9618053436279297\n",
      "Training at step=35, batch=240, train loss = 0.3195172846317291, train acc = 0.8949999809265137, time = 0.9446728229522705\n",
      "Training at step=35, batch=270, train loss = 0.36985138058662415, train acc = 0.8550000190734863, time = 0.9591562747955322\n",
      "Testing at step=35, batch=0, test loss = 0.3382086455821991, test acc = 0.875, time = 0.35840797424316406\n",
      "Testing at step=35, batch=5, test loss = 0.3417700231075287, test acc = 0.875, time = 0.35427021980285645\n",
      "Testing at step=35, batch=10, test loss = 0.34074482321739197, test acc = 0.875, time = 0.3591899871826172\n",
      "Testing at step=35, batch=15, test loss = 0.3286522626876831, test acc = 0.8799999952316284, time = 0.3693678379058838\n",
      "Testing at step=35, batch=20, test loss = 0.33088603615760803, test acc = 0.8899999856948853, time = 0.3587989807128906\n",
      "Testing at step=35, batch=25, test loss = 0.41981762647628784, test acc = 0.8399999737739563, time = 0.3595304489135742\n",
      "Testing at step=35, batch=30, test loss = 0.3211215138435364, test acc = 0.8899999856948853, time = 0.34976649284362793\n",
      "Testing at step=35, batch=35, test loss = 0.3919571042060852, test acc = 0.8450000286102295, time = 0.33928632736206055\n",
      "Testing at step=35, batch=40, test loss = 0.341318815946579, test acc = 0.8899999856948853, time = 0.3560478687286377\n",
      "Testing at step=35, batch=45, test loss = 0.368958055973053, test acc = 0.8450000286102295, time = 0.3606271743774414\n",
      "Step 35 finished in 312.83180713653564, Train loss = 0.30525479798515637, Test loss = 0.36142565310001373; Train Acc = 0.8875666644175847, Test Acc = 0.8725999987125397\n",
      "Training at step=36, batch=0, train loss = 0.25290369987487793, train acc = 0.8949999809265137, time = 0.9448635578155518\n",
      "Training at step=36, batch=30, train loss = 0.3270191550254822, train acc = 0.8500000238418579, time = 0.9308886528015137\n",
      "Training at step=36, batch=60, train loss = 0.23045490682125092, train acc = 0.9200000166893005, time = 0.9492530822753906\n",
      "Training at step=36, batch=90, train loss = 0.28482258319854736, train acc = 0.9100000262260437, time = 0.9464547634124756\n",
      "Training at step=36, batch=120, train loss = 0.28049710392951965, train acc = 0.875, time = 0.9482147693634033\n",
      "Training at step=36, batch=150, train loss = 0.30770087242126465, train acc = 0.8949999809265137, time = 0.9473748207092285\n",
      "Training at step=36, batch=180, train loss = 0.34190791845321655, train acc = 0.8899999856948853, time = 0.9329977035522461\n",
      "Training at step=36, batch=210, train loss = 0.29317721724510193, train acc = 0.9100000262260437, time = 0.9388859272003174\n",
      "Training at step=36, batch=240, train loss = 0.25891709327697754, train acc = 0.9150000214576721, time = 0.9412734508514404\n",
      "Training at step=36, batch=270, train loss = 0.3978307247161865, train acc = 0.8349999785423279, time = 0.9349632263183594\n",
      "Testing at step=36, batch=0, test loss = 0.25866198539733887, test acc = 0.8949999809265137, time = 0.3453695774078369\n",
      "Testing at step=36, batch=5, test loss = 0.41566070914268494, test acc = 0.8399999737739563, time = 0.36023759841918945\n",
      "Testing at step=36, batch=10, test loss = 0.2806544601917267, test acc = 0.8949999809265137, time = 0.3410196304321289\n",
      "Testing at step=36, batch=15, test loss = 0.47981083393096924, test acc = 0.8550000190734863, time = 0.3617362976074219\n",
      "Testing at step=36, batch=20, test loss = 0.37895214557647705, test acc = 0.8600000143051147, time = 0.35005688667297363\n",
      "Testing at step=36, batch=25, test loss = 0.41031280159950256, test acc = 0.8500000238418579, time = 0.3370668888092041\n",
      "Testing at step=36, batch=30, test loss = 0.3334248661994934, test acc = 0.8849999904632568, time = 0.34390950202941895\n",
      "Testing at step=36, batch=35, test loss = 0.3652189373970032, test acc = 0.8700000047683716, time = 0.36415529251098633\n",
      "Testing at step=36, batch=40, test loss = 0.3185363709926605, test acc = 0.8949999809265137, time = 0.3637247085571289\n",
      "Testing at step=36, batch=45, test loss = 0.25928622484207153, test acc = 0.8999999761581421, time = 0.3556251525878906\n",
      "Step 36 finished in 312.7463550567627, Train loss = 0.3050857449074586, Test loss = 0.37114024788141253; Train Acc = 0.8881999981403351, Test Acc = 0.8648000013828278\n",
      "Training at step=37, batch=0, train loss = 0.3113834261894226, train acc = 0.8899999856948853, time = 0.9496150016784668\n",
      "Training at step=37, batch=30, train loss = 0.2795807123184204, train acc = 0.9150000214576721, time = 0.9757184982299805\n",
      "Training at step=37, batch=60, train loss = 0.27025869488716125, train acc = 0.9100000262260437, time = 0.9290766716003418\n",
      "Training at step=37, batch=90, train loss = 0.2582213282585144, train acc = 0.8899999856948853, time = 0.9711055755615234\n",
      "Training at step=37, batch=120, train loss = 0.31839263439178467, train acc = 0.8700000047683716, time = 0.9602193832397461\n",
      "Training at step=37, batch=150, train loss = 0.2843729257583618, train acc = 0.8849999904632568, time = 0.9624707698822021\n",
      "Training at step=37, batch=180, train loss = 0.23366336524486542, train acc = 0.9350000023841858, time = 0.9531033039093018\n",
      "Training at step=37, batch=210, train loss = 0.33932340145111084, train acc = 0.9100000262260437, time = 0.9558124542236328\n",
      "Training at step=37, batch=240, train loss = 0.30836355686187744, train acc = 0.8650000095367432, time = 0.9438180923461914\n",
      "Training at step=37, batch=270, train loss = 0.24930325150489807, train acc = 0.925000011920929, time = 0.9472196102142334\n",
      "Testing at step=37, batch=0, test loss = 0.2686344385147095, test acc = 0.8899999856948853, time = 0.35865116119384766\n",
      "Testing at step=37, batch=5, test loss = 0.3605844974517822, test acc = 0.875, time = 0.36487722396850586\n",
      "Testing at step=37, batch=10, test loss = 0.3121105432510376, test acc = 0.8849999904632568, time = 0.36675047874450684\n",
      "Testing at step=37, batch=15, test loss = 0.4123944342136383, test acc = 0.8500000238418579, time = 0.36312127113342285\n",
      "Testing at step=37, batch=20, test loss = 0.35100382566452026, test acc = 0.8700000047683716, time = 0.3524153232574463\n",
      "Testing at step=37, batch=25, test loss = 0.4181238114833832, test acc = 0.8799999952316284, time = 0.3405039310455322\n",
      "Testing at step=37, batch=30, test loss = 0.3386877775192261, test acc = 0.8700000047683716, time = 0.3566594123840332\n",
      "Testing at step=37, batch=35, test loss = 0.3783649206161499, test acc = 0.8650000095367432, time = 0.36714601516723633\n",
      "Testing at step=37, batch=40, test loss = 0.29647380113601685, test acc = 0.8949999809265137, time = 0.351367712020874\n",
      "Testing at step=37, batch=45, test loss = 0.39949092268943787, test acc = 0.8600000143051147, time = 0.3493926525115967\n",
      "Step 37 finished in 313.54427552223206, Train loss = 0.30130065366625786, Test loss = 0.3573355108499527; Train Acc = 0.8893666634956996, Test Acc = 0.8726999986171723\n",
      "Training at step=38, batch=0, train loss = 0.4331377446651459, train acc = 0.8500000238418579, time = 0.9541926383972168\n",
      "Training at step=38, batch=30, train loss = 0.30821889638900757, train acc = 0.8899999856948853, time = 0.934110164642334\n",
      "Training at step=38, batch=60, train loss = 0.2639341354370117, train acc = 0.8849999904632568, time = 0.9538586139678955\n",
      "Training at step=38, batch=90, train loss = 0.2929198145866394, train acc = 0.8999999761581421, time = 0.955613374710083\n",
      "Training at step=38, batch=120, train loss = 0.34932079911231995, train acc = 0.8650000095367432, time = 0.9503333568572998\n",
      "Training at step=38, batch=150, train loss = 0.31265029311180115, train acc = 0.8799999952316284, time = 0.9288678169250488\n",
      "Training at step=38, batch=180, train loss = 0.22775417566299438, train acc = 0.9150000214576721, time = 0.9542982578277588\n",
      "Training at step=38, batch=210, train loss = 0.34491512179374695, train acc = 0.8799999952316284, time = 0.9505276679992676\n",
      "Training at step=38, batch=240, train loss = 0.22010096907615662, train acc = 0.9150000214576721, time = 0.9508495330810547\n",
      "Training at step=38, batch=270, train loss = 0.3405572175979614, train acc = 0.8500000238418579, time = 0.9473798274993896\n",
      "Testing at step=38, batch=0, test loss = 0.507203996181488, test acc = 0.8399999737739563, time = 0.35984134674072266\n",
      "Testing at step=38, batch=5, test loss = 0.3067835569381714, test acc = 0.9200000166893005, time = 0.3473834991455078\n",
      "Testing at step=38, batch=10, test loss = 0.22932344675064087, test acc = 0.9049999713897705, time = 0.3580644130706787\n",
      "Testing at step=38, batch=15, test loss = 0.35967132449150085, test acc = 0.875, time = 0.3544626235961914\n",
      "Testing at step=38, batch=20, test loss = 0.2801276743412018, test acc = 0.8949999809265137, time = 0.35912609100341797\n",
      "Testing at step=38, batch=25, test loss = 0.30936864018440247, test acc = 0.9150000214576721, time = 0.3583955764770508\n",
      "Testing at step=38, batch=30, test loss = 0.35463643074035645, test acc = 0.9150000214576721, time = 0.3469994068145752\n",
      "Testing at step=38, batch=35, test loss = 0.35626286268234253, test acc = 0.8700000047683716, time = 0.35577869415283203\n",
      "Testing at step=38, batch=40, test loss = 0.3772864043712616, test acc = 0.8700000047683716, time = 0.34845519065856934\n",
      "Testing at step=38, batch=45, test loss = 0.38081812858581543, test acc = 0.875, time = 0.35892653465270996\n",
      "Step 38 finished in 313.2941732406616, Train loss = 0.29955636863907176, Test loss = 0.36000752210617065; Train Acc = 0.8911999986569087, Test Acc = 0.8736000001430512\n",
      "Training at step=39, batch=0, train loss = 0.3106929659843445, train acc = 0.8849999904632568, time = 0.9492931365966797\n",
      "Training at step=39, batch=30, train loss = 0.3574240803718567, train acc = 0.8849999904632568, time = 0.9606494903564453\n",
      "Training at step=39, batch=60, train loss = 0.30998697876930237, train acc = 0.8899999856948853, time = 0.9346158504486084\n",
      "Training at step=39, batch=90, train loss = 0.2935671806335449, train acc = 0.8899999856948853, time = 0.9511733055114746\n",
      "Training at step=39, batch=120, train loss = 0.2397657036781311, train acc = 0.8949999809265137, time = 0.9477064609527588\n",
      "Training at step=39, batch=150, train loss = 0.27690762281417847, train acc = 0.8949999809265137, time = 0.9614887237548828\n",
      "Training at step=39, batch=180, train loss = 0.22934618592262268, train acc = 0.9049999713897705, time = 0.943134069442749\n",
      "Training at step=39, batch=210, train loss = 0.31102055311203003, train acc = 0.875, time = 0.9405767917633057\n",
      "Training at step=39, batch=240, train loss = 0.34819865226745605, train acc = 0.8600000143051147, time = 0.9509682655334473\n",
      "Training at step=39, batch=270, train loss = 0.32520735263824463, train acc = 0.8849999904632568, time = 0.9244160652160645\n",
      "Testing at step=39, batch=0, test loss = 0.5183383822441101, test acc = 0.8700000047683716, time = 0.35938024520874023\n",
      "Testing at step=39, batch=5, test loss = 0.37242671847343445, test acc = 0.8450000286102295, time = 0.35617542266845703\n",
      "Testing at step=39, batch=10, test loss = 0.2669679820537567, test acc = 0.8949999809265137, time = 0.34082913398742676\n",
      "Testing at step=39, batch=15, test loss = 0.2931324541568756, test acc = 0.8949999809265137, time = 0.359086275100708\n",
      "Testing at step=39, batch=20, test loss = 0.355141818523407, test acc = 0.8500000238418579, time = 0.3735034465789795\n",
      "Testing at step=39, batch=25, test loss = 0.3413696587085724, test acc = 0.8650000095367432, time = 0.36237144470214844\n",
      "Testing at step=39, batch=30, test loss = 0.34693199396133423, test acc = 0.8899999856948853, time = 0.35692477226257324\n",
      "Testing at step=39, batch=35, test loss = 0.38043278455734253, test acc = 0.8600000143051147, time = 0.3530440330505371\n",
      "Testing at step=39, batch=40, test loss = 0.445965051651001, test acc = 0.8600000143051147, time = 0.3493616580963135\n",
      "Testing at step=39, batch=45, test loss = 0.3923182785511017, test acc = 0.8550000190734863, time = 0.3521454334259033\n",
      "Step 39 finished in 313.6266510486603, Train loss = 0.30202253073453905, Test loss = 0.37170723468065264; Train Acc = 0.8885333295663198, Test Acc = 0.8670000052452087\n",
      "Training at step=40, batch=0, train loss = 0.2767707109451294, train acc = 0.8849999904632568, time = 0.9493191242218018\n",
      "Training at step=40, batch=30, train loss = 0.2756887674331665, train acc = 0.8949999809265137, time = 0.9590976238250732\n",
      "Training at step=40, batch=60, train loss = 0.2956761419773102, train acc = 0.8799999952316284, time = 0.9495649337768555\n",
      "Training at step=40, batch=90, train loss = 0.27476704120635986, train acc = 0.9100000262260437, time = 0.9501914978027344\n",
      "Training at step=40, batch=120, train loss = 0.30133679509162903, train acc = 0.8799999952316284, time = 0.949230432510376\n",
      "Training at step=40, batch=150, train loss = 0.3421154022216797, train acc = 0.8600000143051147, time = 0.9474995136260986\n",
      "Training at step=40, batch=180, train loss = 0.3671043813228607, train acc = 0.8550000190734863, time = 0.9342617988586426\n",
      "Training at step=40, batch=210, train loss = 0.25137099623680115, train acc = 0.9150000214576721, time = 0.9513163566589355\n",
      "Training at step=40, batch=240, train loss = 0.35210052132606506, train acc = 0.8799999952316284, time = 0.9634160995483398\n",
      "Training at step=40, batch=270, train loss = 0.28857219219207764, train acc = 0.9150000214576721, time = 0.9536488056182861\n",
      "Testing at step=40, batch=0, test loss = 0.2801307737827301, test acc = 0.8949999809265137, time = 0.35900402069091797\n",
      "Testing at step=40, batch=5, test loss = 0.32365357875823975, test acc = 0.8700000047683716, time = 0.3542025089263916\n",
      "Testing at step=40, batch=10, test loss = 0.3225746154785156, test acc = 0.8899999856948853, time = 0.3419804573059082\n",
      "Testing at step=40, batch=15, test loss = 0.3633614778518677, test acc = 0.8500000238418579, time = 0.38489627838134766\n",
      "Testing at step=40, batch=20, test loss = 0.34921514987945557, test acc = 0.8949999809265137, time = 0.36426281929016113\n",
      "Testing at step=40, batch=25, test loss = 0.4209567606449127, test acc = 0.8650000095367432, time = 0.3564262390136719\n",
      "Testing at step=40, batch=30, test loss = 0.3314341604709625, test acc = 0.8949999809265137, time = 0.3603355884552002\n",
      "Testing at step=40, batch=35, test loss = 0.310507595539093, test acc = 0.8799999952316284, time = 0.3532242774963379\n",
      "Testing at step=40, batch=40, test loss = 0.3926711976528168, test acc = 0.8550000190734863, time = 0.3586008548736572\n",
      "Testing at step=40, batch=45, test loss = 0.2975722551345825, test acc = 0.8849999904632568, time = 0.34641456604003906\n",
      "Step 40 finished in 313.36045241355896, Train loss = 0.2988900942603747, Test loss = 0.3671241569519043; Train Acc = 0.890083331267039, Test Acc = 0.8677999973297119\n",
      "Training at step=41, batch=0, train loss = 0.3325713872909546, train acc = 0.8849999904632568, time = 0.9452862739562988\n",
      "Training at step=41, batch=30, train loss = 0.31858447194099426, train acc = 0.8799999952316284, time = 0.958925724029541\n",
      "Training at step=41, batch=60, train loss = 0.3099816143512726, train acc = 0.8700000047683716, time = 0.9630773067474365\n",
      "Training at step=41, batch=90, train loss = 0.27502182126045227, train acc = 0.9049999713897705, time = 0.9613010883331299\n",
      "Training at step=41, batch=120, train loss = 0.2513256072998047, train acc = 0.8949999809265137, time = 0.9512956142425537\n",
      "Training at step=41, batch=150, train loss = 0.2923496961593628, train acc = 0.9049999713897705, time = 0.9534685611724854\n",
      "Training at step=41, batch=180, train loss = 0.3881644010543823, train acc = 0.8550000190734863, time = 0.9519667625427246\n",
      "Training at step=41, batch=210, train loss = 0.2136523425579071, train acc = 0.925000011920929, time = 0.9507870674133301\n",
      "Training at step=41, batch=240, train loss = 0.3124271333217621, train acc = 0.8849999904632568, time = 0.9563994407653809\n",
      "Training at step=41, batch=270, train loss = 0.20889243483543396, train acc = 0.9300000071525574, time = 0.9575996398925781\n",
      "Testing at step=41, batch=0, test loss = 0.3474139869213104, test acc = 0.8849999904632568, time = 0.37384629249572754\n",
      "Testing at step=41, batch=5, test loss = 0.3191338777542114, test acc = 0.8799999952316284, time = 0.35208988189697266\n",
      "Testing at step=41, batch=10, test loss = 0.35090935230255127, test acc = 0.8399999737739563, time = 0.36310863494873047\n",
      "Testing at step=41, batch=15, test loss = 0.27664077281951904, test acc = 0.8799999952316284, time = 0.35616374015808105\n",
      "Testing at step=41, batch=20, test loss = 0.40391549468040466, test acc = 0.8500000238418579, time = 0.3582644462585449\n",
      "Testing at step=41, batch=25, test loss = 0.27583032846450806, test acc = 0.9100000262260437, time = 0.35376882553100586\n",
      "Testing at step=41, batch=30, test loss = 0.37939557433128357, test acc = 0.8650000095367432, time = 0.33989429473876953\n",
      "Testing at step=41, batch=35, test loss = 0.5036107897758484, test acc = 0.8500000238418579, time = 0.34937357902526855\n",
      "Testing at step=41, batch=40, test loss = 0.3106862008571625, test acc = 0.8849999904632568, time = 0.37534666061401367\n",
      "Testing at step=41, batch=45, test loss = 0.40411388874053955, test acc = 0.8799999952316284, time = 0.3409099578857422\n",
      "Step 41 finished in 313.43543815612793, Train loss = 0.2976649769147237, Test loss = 0.3647757524251938; Train Acc = 0.8909999976555506, Test Acc = 0.8685000002384186\n",
      "Training at step=42, batch=0, train loss = 0.24818967282772064, train acc = 0.9049999713897705, time = 0.9498181343078613\n",
      "Training at step=42, batch=30, train loss = 0.3277578055858612, train acc = 0.875, time = 0.9550583362579346\n",
      "Training at step=42, batch=60, train loss = 0.2899368107318878, train acc = 0.8949999809265137, time = 0.9515295028686523\n",
      "Training at step=42, batch=90, train loss = 0.3092944622039795, train acc = 0.8899999856948853, time = 0.959503173828125\n",
      "Training at step=42, batch=120, train loss = 0.3175511658191681, train acc = 0.8949999809265137, time = 0.9431130886077881\n",
      "Training at step=42, batch=150, train loss = 0.38716575503349304, train acc = 0.875, time = 0.956650972366333\n",
      "Training at step=42, batch=180, train loss = 0.22307850420475006, train acc = 0.9100000262260437, time = 0.9607524871826172\n",
      "Training at step=42, batch=210, train loss = 0.290809690952301, train acc = 0.8949999809265137, time = 0.95322585105896\n",
      "Training at step=42, batch=240, train loss = 0.20734888315200806, train acc = 0.925000011920929, time = 0.9514272212982178\n",
      "Training at step=42, batch=270, train loss = 0.34485071897506714, train acc = 0.875, time = 0.9572749137878418\n",
      "Testing at step=42, batch=0, test loss = 0.3052595257759094, test acc = 0.8999999761581421, time = 0.35157203674316406\n",
      "Testing at step=42, batch=5, test loss = 0.30724602937698364, test acc = 0.9200000166893005, time = 0.35405564308166504\n",
      "Testing at step=42, batch=10, test loss = 0.36711302399635315, test acc = 0.8799999952316284, time = 0.358536958694458\n",
      "Testing at step=42, batch=15, test loss = 0.3193965554237366, test acc = 0.8700000047683716, time = 0.37210774421691895\n",
      "Testing at step=42, batch=20, test loss = 0.3444876968860626, test acc = 0.8550000190734863, time = 0.35582733154296875\n",
      "Testing at step=42, batch=25, test loss = 0.3393535912036896, test acc = 0.8899999856948853, time = 0.34121108055114746\n",
      "Testing at step=42, batch=30, test loss = 0.3563116490840912, test acc = 0.8799999952316284, time = 0.3464851379394531\n",
      "Testing at step=42, batch=35, test loss = 0.33648043870925903, test acc = 0.8949999809265137, time = 0.36699891090393066\n",
      "Testing at step=42, batch=40, test loss = 0.3528822064399719, test acc = 0.8399999737739563, time = 0.3619372844696045\n",
      "Testing at step=42, batch=45, test loss = 0.3918724060058594, test acc = 0.875, time = 0.35605645179748535\n",
      "Step 42 finished in 313.0391981601715, Train loss = 0.29506385599573454, Test loss = 0.35602866142988204; Train Acc = 0.8917666639884313, Test Acc = 0.8732000017166137\n",
      "Training at step=43, batch=0, train loss = 0.29491519927978516, train acc = 0.8799999952316284, time = 0.9522597789764404\n",
      "Training at step=43, batch=30, train loss = 0.2682414650917053, train acc = 0.9100000262260437, time = 0.9404375553131104\n",
      "Training at step=43, batch=60, train loss = 0.2843756079673767, train acc = 0.9049999713897705, time = 0.9388313293457031\n",
      "Training at step=43, batch=90, train loss = 0.26326146721839905, train acc = 0.8999999761581421, time = 0.9579904079437256\n",
      "Training at step=43, batch=120, train loss = 0.35327446460723877, train acc = 0.8799999952316284, time = 0.9579269886016846\n",
      "Training at step=43, batch=150, train loss = 0.33834466338157654, train acc = 0.8500000238418579, time = 0.9527435302734375\n",
      "Training at step=43, batch=180, train loss = 0.2690460681915283, train acc = 0.9100000262260437, time = 0.9629471302032471\n",
      "Training at step=43, batch=210, train loss = 0.22266969084739685, train acc = 0.925000011920929, time = 0.9611921310424805\n",
      "Training at step=43, batch=240, train loss = 0.35499241948127747, train acc = 0.875, time = 0.9592084884643555\n",
      "Training at step=43, batch=270, train loss = 0.248447984457016, train acc = 0.9150000214576721, time = 0.9378499984741211\n",
      "Testing at step=43, batch=0, test loss = 0.3241567313671112, test acc = 0.8450000286102295, time = 0.35918498039245605\n",
      "Testing at step=43, batch=5, test loss = 0.21015943586826324, test acc = 0.9049999713897705, time = 0.3558773994445801\n",
      "Testing at step=43, batch=10, test loss = 0.3956127166748047, test acc = 0.8700000047683716, time = 0.3467888832092285\n",
      "Testing at step=43, batch=15, test loss = 0.46477916836738586, test acc = 0.8500000238418579, time = 0.3381032943725586\n",
      "Testing at step=43, batch=20, test loss = 0.3516994118690491, test acc = 0.8700000047683716, time = 0.3550221920013428\n",
      "Testing at step=43, batch=25, test loss = 0.4295486509799957, test acc = 0.8500000238418579, time = 0.3550586700439453\n",
      "Testing at step=43, batch=30, test loss = 0.3715405762195587, test acc = 0.8849999904632568, time = 0.36884331703186035\n",
      "Testing at step=43, batch=35, test loss = 0.3095923662185669, test acc = 0.8949999809265137, time = 0.3610959053039551\n",
      "Testing at step=43, batch=40, test loss = 0.3765645921230316, test acc = 0.8799999952316284, time = 0.3518695831298828\n",
      "Testing at step=43, batch=45, test loss = 0.3839420676231384, test acc = 0.8799999952316284, time = 0.37244486808776855\n",
      "Step 43 finished in 313.3057496547699, Train loss = 0.29295140773057937, Test loss = 0.3561394992470741; Train Acc = 0.8929999977350235, Test Acc = 0.8732999992370606\n",
      "Training at step=44, batch=0, train loss = 0.29498612880706787, train acc = 0.8849999904632568, time = 0.9570200443267822\n",
      "Training at step=44, batch=30, train loss = 0.24926874041557312, train acc = 0.9150000214576721, time = 0.9599709510803223\n",
      "Training at step=44, batch=60, train loss = 0.3434542417526245, train acc = 0.8949999809265137, time = 0.9442055225372314\n",
      "Training at step=44, batch=90, train loss = 0.2542285621166229, train acc = 0.9150000214576721, time = 0.9575014114379883\n",
      "Training at step=44, batch=120, train loss = 0.3274889886379242, train acc = 0.9049999713897705, time = 0.9408361911773682\n",
      "Training at step=44, batch=150, train loss = 0.24173003435134888, train acc = 0.9100000262260437, time = 0.9498276710510254\n",
      "Training at step=44, batch=180, train loss = 0.2750012278556824, train acc = 0.875, time = 0.9643535614013672\n",
      "Training at step=44, batch=210, train loss = 0.21754197776317596, train acc = 0.9049999713897705, time = 0.9398822784423828\n",
      "Training at step=44, batch=240, train loss = 0.29019153118133545, train acc = 0.8799999952316284, time = 0.9397587776184082\n",
      "Training at step=44, batch=270, train loss = 0.3987928628921509, train acc = 0.8949999809265137, time = 0.9425439834594727\n",
      "Testing at step=44, batch=0, test loss = 0.3960120379924774, test acc = 0.8450000286102295, time = 0.35692691802978516\n",
      "Testing at step=44, batch=5, test loss = 0.3098398447036743, test acc = 0.8650000095367432, time = 0.36339735984802246\n",
      "Testing at step=44, batch=10, test loss = 0.35819435119628906, test acc = 0.8799999952316284, time = 0.3581876754760742\n",
      "Testing at step=44, batch=15, test loss = 0.4422972500324249, test acc = 0.8399999737739563, time = 0.35534024238586426\n",
      "Testing at step=44, batch=20, test loss = 0.40696942806243896, test acc = 0.8650000095367432, time = 0.3553769588470459\n",
      "Testing at step=44, batch=25, test loss = 0.33828744292259216, test acc = 0.8700000047683716, time = 0.3496835231781006\n",
      "Testing at step=44, batch=30, test loss = 0.40629351139068604, test acc = 0.8650000095367432, time = 0.3614048957824707\n",
      "Testing at step=44, batch=35, test loss = 0.324726939201355, test acc = 0.8899999856948853, time = 0.3426680564880371\n",
      "Testing at step=44, batch=40, test loss = 0.49091339111328125, test acc = 0.8550000190734863, time = 0.35398340225219727\n",
      "Testing at step=44, batch=45, test loss = 0.4505569040775299, test acc = 0.8650000095367432, time = 0.3597390651702881\n",
      "Step 44 finished in 313.4167146682739, Train loss = 0.29251703634858134, Test loss = 0.36211892336606977; Train Acc = 0.8921666649977366, Test Acc = 0.8755999994277954\n",
      "Training at step=45, batch=0, train loss = 0.33065181970596313, train acc = 0.8849999904632568, time = 0.9465420246124268\n",
      "Training at step=45, batch=30, train loss = 0.2386055737733841, train acc = 0.9100000262260437, time = 0.9539074897766113\n",
      "Training at step=45, batch=60, train loss = 0.28976136445999146, train acc = 0.8849999904632568, time = 0.9470677375793457\n",
      "Training at step=45, batch=90, train loss = 0.3751167058944702, train acc = 0.8450000286102295, time = 0.9526991844177246\n",
      "Training at step=45, batch=120, train loss = 0.27911055088043213, train acc = 0.8899999856948853, time = 0.9604547023773193\n",
      "Training at step=45, batch=150, train loss = 0.43226858973503113, train acc = 0.8100000023841858, time = 0.9552743434906006\n",
      "Training at step=45, batch=180, train loss = 0.21217207610607147, train acc = 0.9200000166893005, time = 0.9519624710083008\n",
      "Training at step=45, batch=210, train loss = 0.2640961706638336, train acc = 0.9049999713897705, time = 0.9465527534484863\n",
      "Training at step=45, batch=240, train loss = 0.2892715334892273, train acc = 0.9049999713897705, time = 0.9425954818725586\n",
      "Training at step=45, batch=270, train loss = 0.28391745686531067, train acc = 0.8600000143051147, time = 0.9507830142974854\n",
      "Testing at step=45, batch=0, test loss = 0.35412827134132385, test acc = 0.875, time = 0.3447990417480469\n",
      "Testing at step=45, batch=5, test loss = 0.3538205027580261, test acc = 0.8849999904632568, time = 0.35453128814697266\n",
      "Testing at step=45, batch=10, test loss = 0.30233675241470337, test acc = 0.875, time = 0.34816575050354004\n",
      "Testing at step=45, batch=15, test loss = 0.2790790796279907, test acc = 0.8949999809265137, time = 0.3594985008239746\n",
      "Testing at step=45, batch=20, test loss = 0.38660645484924316, test acc = 0.8550000190734863, time = 0.35371828079223633\n",
      "Testing at step=45, batch=25, test loss = 0.3817136883735657, test acc = 0.8799999952316284, time = 0.35157346725463867\n",
      "Testing at step=45, batch=30, test loss = 0.2554146945476532, test acc = 0.8899999856948853, time = 0.3593783378601074\n",
      "Testing at step=45, batch=35, test loss = 0.3826921880245209, test acc = 0.8899999856948853, time = 0.3483903408050537\n",
      "Testing at step=45, batch=40, test loss = 0.39578965306282043, test acc = 0.8500000238418579, time = 0.34695959091186523\n",
      "Testing at step=45, batch=45, test loss = 0.3874408006668091, test acc = 0.8799999952316284, time = 0.3501160144805908\n",
      "Step 45 finished in 313.26065325737, Train loss = 0.29012783989310265, Test loss = 0.3616571393609047; Train Acc = 0.89293332974116, Test Acc = 0.8714000058174133\n",
      "Training at step=46, batch=0, train loss = 0.3548939526081085, train acc = 0.8700000047683716, time = 0.9481501579284668\n",
      "Training at step=46, batch=30, train loss = 0.1874454915523529, train acc = 0.9350000023841858, time = 0.9550814628601074\n",
      "Training at step=46, batch=60, train loss = 0.30369681119918823, train acc = 0.9150000214576721, time = 0.9513132572174072\n",
      "Training at step=46, batch=90, train loss = 0.3531688451766968, train acc = 0.8700000047683716, time = 0.9457387924194336\n",
      "Training at step=46, batch=120, train loss = 0.28698620200157166, train acc = 0.8899999856948853, time = 0.9291326999664307\n",
      "Training at step=46, batch=150, train loss = 0.12583991885185242, train acc = 0.9549999833106995, time = 0.968172550201416\n",
      "Training at step=46, batch=180, train loss = 0.3576301634311676, train acc = 0.8849999904632568, time = 0.9646315574645996\n",
      "Training at step=46, batch=210, train loss = 0.29410994052886963, train acc = 0.8849999904632568, time = 0.9537789821624756\n",
      "Training at step=46, batch=240, train loss = 0.4054041802883148, train acc = 0.8500000238418579, time = 0.9429891109466553\n",
      "Training at step=46, batch=270, train loss = 0.33050835132598877, train acc = 0.8550000190734863, time = 0.9476027488708496\n",
      "Testing at step=46, batch=0, test loss = 0.29362010955810547, test acc = 0.8799999952316284, time = 0.34372591972351074\n",
      "Testing at step=46, batch=5, test loss = 0.33983132243156433, test acc = 0.8899999856948853, time = 0.3444850444793701\n",
      "Testing at step=46, batch=10, test loss = 0.37447044253349304, test acc = 0.8500000238418579, time = 0.34966278076171875\n",
      "Testing at step=46, batch=15, test loss = 0.43247130513191223, test acc = 0.8199999928474426, time = 0.35327720642089844\n",
      "Testing at step=46, batch=20, test loss = 0.3131379187107086, test acc = 0.8949999809265137, time = 0.343325138092041\n",
      "Testing at step=46, batch=25, test loss = 0.42442965507507324, test acc = 0.8600000143051147, time = 0.3490726947784424\n",
      "Testing at step=46, batch=30, test loss = 0.38457560539245605, test acc = 0.8500000238418579, time = 0.35216522216796875\n",
      "Testing at step=46, batch=35, test loss = 0.3443899154663086, test acc = 0.8949999809265137, time = 0.35409045219421387\n",
      "Testing at step=46, batch=40, test loss = 0.3790059983730316, test acc = 0.875, time = 0.34272217750549316\n",
      "Testing at step=46, batch=45, test loss = 0.3945915102958679, test acc = 0.8500000238418579, time = 0.3614223003387451\n",
      "Step 46 finished in 313.3141393661499, Train loss = 0.28819092084964115, Test loss = 0.36504311859607697; Train Acc = 0.8946166654427846, Test Acc = 0.8682000005245208\n",
      "Training at step=47, batch=0, train loss = 0.3885321021080017, train acc = 0.8849999904632568, time = 0.9478976726531982\n",
      "Training at step=47, batch=30, train loss = 0.24300003051757812, train acc = 0.9049999713897705, time = 0.945432186126709\n",
      "Training at step=47, batch=60, train loss = 0.2497214525938034, train acc = 0.9049999713897705, time = 0.9540479183197021\n",
      "Training at step=47, batch=90, train loss = 0.39456623792648315, train acc = 0.8399999737739563, time = 0.9490652084350586\n",
      "Training at step=47, batch=120, train loss = 0.31635674834251404, train acc = 0.8799999952316284, time = 0.9572873115539551\n",
      "Training at step=47, batch=150, train loss = 0.32667091488838196, train acc = 0.8799999952316284, time = 0.9517672061920166\n",
      "Training at step=47, batch=180, train loss = 0.27758878469467163, train acc = 0.8949999809265137, time = 0.951012134552002\n",
      "Training at step=47, batch=210, train loss = 0.2729080617427826, train acc = 0.9049999713897705, time = 0.9462718963623047\n",
      "Training at step=47, batch=240, train loss = 0.38995102047920227, train acc = 0.8700000047683716, time = 0.9560506343841553\n",
      "Training at step=47, batch=270, train loss = 0.24231737852096558, train acc = 0.9200000166893005, time = 0.9445419311523438\n",
      "Testing at step=47, batch=0, test loss = 0.5324980020523071, test acc = 0.8199999928474426, time = 0.35535311698913574\n",
      "Testing at step=47, batch=5, test loss = 0.3873445391654968, test acc = 0.8700000047683716, time = 0.35524559020996094\n",
      "Testing at step=47, batch=10, test loss = 0.24821913242340088, test acc = 0.9200000166893005, time = 0.35056614875793457\n",
      "Testing at step=47, batch=15, test loss = 0.41911768913269043, test acc = 0.8700000047683716, time = 0.3570702075958252\n",
      "Testing at step=47, batch=20, test loss = 0.41556259989738464, test acc = 0.8799999952316284, time = 0.35611844062805176\n",
      "Testing at step=47, batch=25, test loss = 0.3642198443412781, test acc = 0.8999999761581421, time = 0.3569040298461914\n",
      "Testing at step=47, batch=30, test loss = 0.409236878156662, test acc = 0.8500000238418579, time = 0.3601500988006592\n",
      "Testing at step=47, batch=35, test loss = 0.35055553913116455, test acc = 0.8849999904632568, time = 0.34466028213500977\n",
      "Testing at step=47, batch=40, test loss = 0.34124821424484253, test acc = 0.8399999737739563, time = 0.35094666481018066\n",
      "Testing at step=47, batch=45, test loss = 0.31973111629486084, test acc = 0.8500000238418579, time = 0.37288808822631836\n",
      "Step 47 finished in 313.4307110309601, Train loss = 0.2880259437362353, Test loss = 0.3531069931387901; Train Acc = 0.8945333309968313, Test Acc = 0.8764000010490417\n",
      "Training at step=48, batch=0, train loss = 0.27310729026794434, train acc = 0.8949999809265137, time = 0.9618110656738281\n",
      "Training at step=48, batch=30, train loss = 0.2655174136161804, train acc = 0.925000011920929, time = 0.9299890995025635\n",
      "Training at step=48, batch=60, train loss = 0.2846698462963104, train acc = 0.8849999904632568, time = 0.9394252300262451\n",
      "Training at step=48, batch=90, train loss = 0.2783394753932953, train acc = 0.875, time = 0.9510829448699951\n",
      "Training at step=48, batch=120, train loss = 0.26962167024612427, train acc = 0.9100000262260437, time = 0.9416260719299316\n",
      "Training at step=48, batch=150, train loss = 0.3380950093269348, train acc = 0.8799999952316284, time = 0.9573354721069336\n",
      "Training at step=48, batch=180, train loss = 0.21447964012622833, train acc = 0.9100000262260437, time = 0.9652056694030762\n",
      "Training at step=48, batch=210, train loss = 0.26032376289367676, train acc = 0.9150000214576721, time = 0.9596717357635498\n",
      "Training at step=48, batch=240, train loss = 0.3430696427822113, train acc = 0.875, time = 0.9768712520599365\n",
      "Training at step=48, batch=270, train loss = 0.3138749301433563, train acc = 0.8849999904632568, time = 0.9473159313201904\n",
      "Testing at step=48, batch=0, test loss = 0.3642365634441376, test acc = 0.8799999952316284, time = 0.3557448387145996\n",
      "Testing at step=48, batch=5, test loss = 0.28886380791664124, test acc = 0.8849999904632568, time = 0.3391876220703125\n",
      "Testing at step=48, batch=10, test loss = 0.3048902750015259, test acc = 0.875, time = 0.35639166831970215\n",
      "Testing at step=48, batch=15, test loss = 0.33087262511253357, test acc = 0.9150000214576721, time = 0.3450965881347656\n",
      "Testing at step=48, batch=20, test loss = 0.3138929307460785, test acc = 0.8650000095367432, time = 0.35178661346435547\n",
      "Testing at step=48, batch=25, test loss = 0.4516730010509491, test acc = 0.875, time = 0.36083507537841797\n",
      "Testing at step=48, batch=30, test loss = 0.4299105703830719, test acc = 0.8450000286102295, time = 0.34455132484436035\n",
      "Testing at step=48, batch=35, test loss = 0.3596753776073456, test acc = 0.8600000143051147, time = 0.3687310218811035\n",
      "Testing at step=48, batch=40, test loss = 0.4013153910636902, test acc = 0.8450000286102295, time = 0.3549802303314209\n",
      "Testing at step=48, batch=45, test loss = 0.3921844959259033, test acc = 0.8650000095367432, time = 0.3381061553955078\n",
      "Step 48 finished in 312.9208433628082, Train loss = 0.2863515750567118, Test loss = 0.3556913688778877; Train Acc = 0.8950666630268097, Test Acc = 0.8767000007629394\n",
      "Training at step=49, batch=0, train loss = 0.30982741713523865, train acc = 0.8949999809265137, time = 0.9421582221984863\n",
      "Training at step=49, batch=30, train loss = 0.29800719022750854, train acc = 0.9100000262260437, time = 0.9922769069671631\n",
      "Training at step=49, batch=60, train loss = 0.3090834319591522, train acc = 0.8999999761581421, time = 0.9519751071929932\n",
      "Training at step=49, batch=90, train loss = 0.313091903924942, train acc = 0.8849999904632568, time = 0.9471480846405029\n",
      "Training at step=49, batch=120, train loss = 0.2876511514186859, train acc = 0.8949999809265137, time = 0.9651708602905273\n",
      "Training at step=49, batch=150, train loss = 0.21919286251068115, train acc = 0.9100000262260437, time = 0.9479572772979736\n",
      "Training at step=49, batch=180, train loss = 0.24446655809879303, train acc = 0.9049999713897705, time = 0.9736063480377197\n",
      "Training at step=49, batch=210, train loss = 0.25287970900535583, train acc = 0.8899999856948853, time = 0.9335248470306396\n",
      "Training at step=49, batch=240, train loss = 0.23980456590652466, train acc = 0.8999999761581421, time = 0.9472596645355225\n",
      "Training at step=49, batch=270, train loss = 0.27853715419769287, train acc = 0.9100000262260437, time = 0.9518158435821533\n",
      "Testing at step=49, batch=0, test loss = 0.37504300475120544, test acc = 0.8700000047683716, time = 0.35359954833984375\n",
      "Testing at step=49, batch=5, test loss = 0.31106406450271606, test acc = 0.875, time = 0.35105228424072266\n",
      "Testing at step=49, batch=10, test loss = 0.3160178065299988, test acc = 0.8849999904632568, time = 0.36805248260498047\n",
      "Testing at step=49, batch=15, test loss = 0.38649046421051025, test acc = 0.8650000095367432, time = 0.34973645210266113\n",
      "Testing at step=49, batch=20, test loss = 0.3352939486503601, test acc = 0.8600000143051147, time = 0.34882235527038574\n",
      "Testing at step=49, batch=25, test loss = 0.26220282912254333, test acc = 0.8949999809265137, time = 0.3537416458129883\n",
      "Testing at step=49, batch=30, test loss = 0.31078240275382996, test acc = 0.8949999809265137, time = 0.3571023941040039\n",
      "Testing at step=49, batch=35, test loss = 0.401620477437973, test acc = 0.8700000047683716, time = 0.35228490829467773\n",
      "Testing at step=49, batch=40, test loss = 0.25910013914108276, test acc = 0.8949999809265137, time = 0.36922240257263184\n",
      "Testing at step=49, batch=45, test loss = 0.2838379740715027, test acc = 0.9100000262260437, time = 0.37180137634277344\n",
      "Step 49 finished in 314.0945999622345, Train loss = 0.28667092020312945, Test loss = 0.35067598164081576; Train Acc = 0.8951166631778081, Test Acc = 0.8761999976634979\n",
      "Training at step=50, batch=0, train loss = 0.31915003061294556, train acc = 0.8949999809265137, time = 0.9462897777557373\n",
      "Training at step=50, batch=30, train loss = 0.27087661623954773, train acc = 0.9200000166893005, time = 0.962242841720581\n",
      "Training at step=50, batch=60, train loss = 0.3632168173789978, train acc = 0.8550000190734863, time = 0.9364824295043945\n",
      "Training at step=50, batch=90, train loss = 0.2237529158592224, train acc = 0.9049999713897705, time = 0.9454278945922852\n",
      "Training at step=50, batch=120, train loss = 0.42917731404304504, train acc = 0.824999988079071, time = 0.9440994262695312\n",
      "Training at step=50, batch=150, train loss = 0.2832511365413666, train acc = 0.9100000262260437, time = 0.9456024169921875\n",
      "Training at step=50, batch=180, train loss = 0.25583764910697937, train acc = 0.9049999713897705, time = 0.9487504959106445\n",
      "Training at step=50, batch=210, train loss = 0.25293073058128357, train acc = 0.8999999761581421, time = 0.9485008716583252\n",
      "Training at step=50, batch=240, train loss = 0.2785031199455261, train acc = 0.9100000262260437, time = 0.9504649639129639\n",
      "Training at step=50, batch=270, train loss = 0.34690606594085693, train acc = 0.8650000095367432, time = 0.9481735229492188\n",
      "Testing at step=50, batch=0, test loss = 0.3387433886528015, test acc = 0.8799999952316284, time = 0.35867857933044434\n",
      "Testing at step=50, batch=5, test loss = 0.37328431010246277, test acc = 0.875, time = 0.35700249671936035\n",
      "Testing at step=50, batch=10, test loss = 0.33557403087615967, test acc = 0.9049999713897705, time = 0.3549201488494873\n",
      "Testing at step=50, batch=15, test loss = 0.35894349217414856, test acc = 0.8700000047683716, time = 0.333665132522583\n",
      "Testing at step=50, batch=20, test loss = 0.3123817443847656, test acc = 0.8949999809265137, time = 0.356198787689209\n",
      "Testing at step=50, batch=25, test loss = 0.380756139755249, test acc = 0.8799999952316284, time = 0.3615596294403076\n",
      "Testing at step=50, batch=30, test loss = 0.4437696039676666, test acc = 0.8650000095367432, time = 0.3539583683013916\n",
      "Testing at step=50, batch=35, test loss = 0.3160040080547333, test acc = 0.8650000095367432, time = 0.3681144714355469\n",
      "Testing at step=50, batch=40, test loss = 0.29888251423835754, test acc = 0.8899999856948853, time = 0.3634002208709717\n",
      "Testing at step=50, batch=45, test loss = 0.36109885573387146, test acc = 0.8550000190734863, time = 0.35700297355651855\n",
      "Step 50 finished in 313.3825581073761, Train loss = 0.28454462269941966, Test loss = 0.34505467504262927; Train Acc = 0.8950333291292191, Test Acc = 0.8780999994277954\n",
      "Training at step=51, batch=0, train loss = 0.32519295811653137, train acc = 0.8799999952316284, time = 0.9468069076538086\n",
      "Training at step=51, batch=30, train loss = 0.39212340116500854, train acc = 0.8500000238418579, time = 0.9522173404693604\n",
      "Training at step=51, batch=60, train loss = 0.2071746289730072, train acc = 0.9150000214576721, time = 0.9579319953918457\n",
      "Training at step=51, batch=90, train loss = 0.23155853152275085, train acc = 0.9049999713897705, time = 0.9706668853759766\n",
      "Training at step=51, batch=120, train loss = 0.20826385915279388, train acc = 0.9150000214576721, time = 0.9500889778137207\n",
      "Training at step=51, batch=150, train loss = 0.3516998291015625, train acc = 0.8799999952316284, time = 0.9566566944122314\n",
      "Training at step=51, batch=180, train loss = 0.2947404086589813, train acc = 0.875, time = 0.947411298751831\n",
      "Training at step=51, batch=210, train loss = 0.22418886423110962, train acc = 0.9200000166893005, time = 0.9518594741821289\n",
      "Training at step=51, batch=240, train loss = 0.17656473815441132, train acc = 0.949999988079071, time = 0.9505324363708496\n",
      "Training at step=51, batch=270, train loss = 0.34174808859825134, train acc = 0.875, time = 0.9551663398742676\n",
      "Testing at step=51, batch=0, test loss = 0.25851500034332275, test acc = 0.9200000166893005, time = 0.3588831424713135\n",
      "Testing at step=51, batch=5, test loss = 0.3673733174800873, test acc = 0.875, time = 0.34303832054138184\n",
      "Testing at step=51, batch=10, test loss = 0.4231759309768677, test acc = 0.8299999833106995, time = 0.3562791347503662\n",
      "Testing at step=51, batch=15, test loss = 0.2353559285402298, test acc = 0.9200000166893005, time = 0.34757137298583984\n",
      "Testing at step=51, batch=20, test loss = 0.3142983913421631, test acc = 0.8849999904632568, time = 0.35785841941833496\n",
      "Testing at step=51, batch=25, test loss = 0.3830752968788147, test acc = 0.8700000047683716, time = 0.359661340713501\n",
      "Testing at step=51, batch=30, test loss = 0.3743796944618225, test acc = 0.8799999952316284, time = 0.3604738712310791\n",
      "Testing at step=51, batch=35, test loss = 0.36471474170684814, test acc = 0.8450000286102295, time = 0.37766480445861816\n",
      "Testing at step=51, batch=40, test loss = 0.4236675500869751, test acc = 0.8450000286102295, time = 0.3483011722564697\n",
      "Testing at step=51, batch=45, test loss = 0.47118040919303894, test acc = 0.8500000238418579, time = 0.354811429977417\n",
      "Step 51 finished in 313.51969814300537, Train loss = 0.2800903641184171, Test loss = 0.3600807556509972; Train Acc = 0.8965666657686233, Test Acc = 0.8756000006198883\n",
      "Training at step=52, batch=0, train loss = 0.2047695368528366, train acc = 0.9300000071525574, time = 0.932258129119873\n",
      "Training at step=52, batch=30, train loss = 0.3621664047241211, train acc = 0.8550000190734863, time = 0.930809497833252\n",
      "Training at step=52, batch=60, train loss = 0.2012425810098648, train acc = 0.925000011920929, time = 0.9327366352081299\n",
      "Training at step=52, batch=90, train loss = 0.232835590839386, train acc = 0.8949999809265137, time = 0.9695248603820801\n",
      "Training at step=52, batch=120, train loss = 0.3194892704486847, train acc = 0.9049999713897705, time = 0.9420411586761475\n",
      "Training at step=52, batch=150, train loss = 0.3391578793525696, train acc = 0.8550000190734863, time = 0.9359002113342285\n",
      "Training at step=52, batch=180, train loss = 0.29235464334487915, train acc = 0.8999999761581421, time = 0.9282927513122559\n",
      "Training at step=52, batch=210, train loss = 0.2518349587917328, train acc = 0.8999999761581421, time = 0.9340627193450928\n",
      "Training at step=52, batch=240, train loss = 0.29923829436302185, train acc = 0.8899999856948853, time = 0.9347796440124512\n",
      "Training at step=52, batch=270, train loss = 0.3256644904613495, train acc = 0.8849999904632568, time = 0.9435510635375977\n",
      "Testing at step=52, batch=0, test loss = 0.359000563621521, test acc = 0.8700000047683716, time = 0.337543249130249\n",
      "Testing at step=52, batch=5, test loss = 0.38184162974357605, test acc = 0.875, time = 0.3369584083557129\n",
      "Testing at step=52, batch=10, test loss = 0.33130955696105957, test acc = 0.8450000286102295, time = 0.3360590934753418\n",
      "Testing at step=52, batch=15, test loss = 0.3615744709968567, test acc = 0.8799999952316284, time = 0.3380725383758545\n",
      "Testing at step=52, batch=20, test loss = 0.3700588643550873, test acc = 0.8700000047683716, time = 0.33718204498291016\n",
      "Testing at step=52, batch=25, test loss = 0.31753861904144287, test acc = 0.8949999809265137, time = 0.34641265869140625\n",
      "Testing at step=52, batch=30, test loss = 0.27589237689971924, test acc = 0.8949999809265137, time = 0.34601855278015137\n",
      "Testing at step=52, batch=35, test loss = 0.28764334321022034, test acc = 0.8899999856948853, time = 0.34069275856018066\n",
      "Testing at step=52, batch=40, test loss = 0.41916465759277344, test acc = 0.8450000286102295, time = 0.3409998416900635\n",
      "Testing at step=52, batch=45, test loss = 0.34852561354637146, test acc = 0.8799999952316284, time = 0.3382894992828369\n",
      "Step 52 finished in 308.6053366661072, Train loss = 0.28129039898514746, Test loss = 0.3578565636277199; Train Acc = 0.8958999969561895, Test Acc = 0.8741000008583069\n",
      "Training at step=53, batch=0, train loss = 0.2653507888317108, train acc = 0.8899999856948853, time = 0.9517881870269775\n",
      "Training at step=53, batch=30, train loss = 0.328354150056839, train acc = 0.8849999904632568, time = 0.9246270656585693\n",
      "Training at step=53, batch=60, train loss = 0.24447961151599884, train acc = 0.8949999809265137, time = 0.924811840057373\n",
      "Training at step=53, batch=90, train loss = 0.22633551061153412, train acc = 0.9100000262260437, time = 0.9281527996063232\n",
      "Training at step=53, batch=120, train loss = 0.25245270133018494, train acc = 0.9100000262260437, time = 0.9277641773223877\n",
      "Training at step=53, batch=150, train loss = 0.26021987199783325, train acc = 0.8949999809265137, time = 0.9324469566345215\n",
      "Training at step=53, batch=180, train loss = 0.24515481293201447, train acc = 0.9100000262260437, time = 0.9335730075836182\n",
      "Training at step=53, batch=210, train loss = 0.262759804725647, train acc = 0.8899999856948853, time = 0.9288773536682129\n",
      "Training at step=53, batch=240, train loss = 0.294155091047287, train acc = 0.8849999904632568, time = 0.9230554103851318\n",
      "Training at step=53, batch=270, train loss = 0.40907448530197144, train acc = 0.8550000190734863, time = 0.9498672485351562\n",
      "Testing at step=53, batch=0, test loss = 0.41117390990257263, test acc = 0.8550000190734863, time = 0.3428761959075928\n",
      "Testing at step=53, batch=5, test loss = 0.2847362756729126, test acc = 0.8799999952316284, time = 0.33736538887023926\n",
      "Testing at step=53, batch=10, test loss = 0.31522834300994873, test acc = 0.8999999761581421, time = 0.3350522518157959\n",
      "Testing at step=53, batch=15, test loss = 0.3212971091270447, test acc = 0.8849999904632568, time = 0.3437221050262451\n",
      "Testing at step=53, batch=20, test loss = 0.3721161186695099, test acc = 0.8500000238418579, time = 0.33580613136291504\n",
      "Testing at step=53, batch=25, test loss = 0.38318324089050293, test acc = 0.8899999856948853, time = 0.3403141498565674\n",
      "Testing at step=53, batch=30, test loss = 0.32707110047340393, test acc = 0.8799999952316284, time = 0.3434762954711914\n",
      "Testing at step=53, batch=35, test loss = 0.5057884454727173, test acc = 0.8700000047683716, time = 0.3343386650085449\n",
      "Testing at step=53, batch=40, test loss = 0.2830989360809326, test acc = 0.9049999713897705, time = 0.33289146423339844\n",
      "Testing at step=53, batch=45, test loss = 0.2749650180339813, test acc = 0.9049999713897705, time = 0.33650708198547363\n",
      "Step 53 finished in 306.0379137992859, Train loss = 0.27833713601032894, Test loss = 0.3472431117296219; Train Acc = 0.8981166646877925, Test Acc = 0.8794999980926513\n",
      "Training at step=54, batch=0, train loss = 0.2785388231277466, train acc = 0.8999999761581421, time = 0.9289493560791016\n",
      "Training at step=54, batch=30, train loss = 0.2776740789413452, train acc = 0.8849999904632568, time = 0.9286916255950928\n",
      "Training at step=54, batch=60, train loss = 0.25291258096694946, train acc = 0.8999999761581421, time = 0.9359526634216309\n",
      "Training at step=54, batch=90, train loss = 0.3079492449760437, train acc = 0.8799999952316284, time = 0.9255905151367188\n",
      "Training at step=54, batch=120, train loss = 0.29880332946777344, train acc = 0.8849999904632568, time = 0.9219379425048828\n",
      "Training at step=54, batch=150, train loss = 0.2467639148235321, train acc = 0.9200000166893005, time = 0.9411261081695557\n",
      "Training at step=54, batch=180, train loss = 0.23250748217105865, train acc = 0.9350000023841858, time = 0.9420640468597412\n",
      "Training at step=54, batch=210, train loss = 0.2663065195083618, train acc = 0.8899999856948853, time = 0.9309790134429932\n",
      "Training at step=54, batch=240, train loss = 0.2984263300895691, train acc = 0.8899999856948853, time = 0.940561056137085\n",
      "Training at step=54, batch=270, train loss = 0.2548476755619049, train acc = 0.9350000023841858, time = 0.9457380771636963\n",
      "Testing at step=54, batch=0, test loss = 0.29236069321632385, test acc = 0.8999999761581421, time = 0.34647345542907715\n",
      "Testing at step=54, batch=5, test loss = 0.3886989653110504, test acc = 0.8700000047683716, time = 0.33904361724853516\n",
      "Testing at step=54, batch=10, test loss = 0.38694241642951965, test acc = 0.8450000286102295, time = 0.35248756408691406\n",
      "Testing at step=54, batch=15, test loss = 0.4479432702064514, test acc = 0.8799999952316284, time = 0.3518071174621582\n",
      "Testing at step=54, batch=20, test loss = 0.28629618883132935, test acc = 0.8949999809265137, time = 0.3535895347595215\n",
      "Testing at step=54, batch=25, test loss = 0.3208337724208832, test acc = 0.8849999904632568, time = 0.36316418647766113\n",
      "Testing at step=54, batch=30, test loss = 0.3479255437850952, test acc = 0.8700000047683716, time = 0.35465049743652344\n",
      "Testing at step=54, batch=35, test loss = 0.3567999303340912, test acc = 0.8600000143051147, time = 0.35606813430786133\n",
      "Testing at step=54, batch=40, test loss = 0.31606370210647583, test acc = 0.8799999952316284, time = 0.3546290397644043\n",
      "Testing at step=54, batch=45, test loss = 0.27718326449394226, test acc = 0.9049999713897705, time = 0.3487098217010498\n",
      "Step 54 finished in 307.99701833724976, Train loss = 0.2768667444835107, Test loss = 0.3543854355812073; Train Acc = 0.8987666648626328, Test Acc = 0.8758000004291534\n",
      "Training at step=55, batch=0, train loss = 0.2874680459499359, train acc = 0.8799999952316284, time = 0.9580819606781006\n",
      "Training at step=55, batch=30, train loss = 0.25830012559890747, train acc = 0.9049999713897705, time = 0.9289789199829102\n",
      "Training at step=55, batch=60, train loss = 0.24331778287887573, train acc = 0.9200000166893005, time = 0.9232516288757324\n",
      "Training at step=55, batch=90, train loss = 0.23934286832809448, train acc = 0.9399999976158142, time = 0.9399433135986328\n",
      "Training at step=55, batch=120, train loss = 0.24559080600738525, train acc = 0.9049999713897705, time = 0.927149772644043\n",
      "Training at step=55, batch=150, train loss = 0.29398584365844727, train acc = 0.8799999952316284, time = 0.9258430004119873\n",
      "Training at step=55, batch=180, train loss = 0.2227974385023117, train acc = 0.9100000262260437, time = 0.9259696006774902\n",
      "Training at step=55, batch=210, train loss = 0.21791456639766693, train acc = 0.9449999928474426, time = 0.9279820919036865\n",
      "Training at step=55, batch=240, train loss = 0.30943557620048523, train acc = 0.8700000047683716, time = 0.9314723014831543\n",
      "Training at step=55, batch=270, train loss = 0.219610333442688, train acc = 0.9049999713897705, time = 0.9281013011932373\n",
      "Testing at step=55, batch=0, test loss = 0.2724955976009369, test acc = 0.8949999809265137, time = 0.340742826461792\n",
      "Testing at step=55, batch=5, test loss = 0.3855741620063782, test acc = 0.875, time = 0.3502655029296875\n",
      "Testing at step=55, batch=10, test loss = 0.3332017660140991, test acc = 0.875, time = 0.34581708908081055\n",
      "Testing at step=55, batch=15, test loss = 0.45012715458869934, test acc = 0.8550000190734863, time = 0.3395979404449463\n",
      "Testing at step=55, batch=20, test loss = 0.20789800584316254, test acc = 0.9200000166893005, time = 0.3400132656097412\n",
      "Testing at step=55, batch=25, test loss = 0.31453627347946167, test acc = 0.875, time = 0.3417537212371826\n",
      "Testing at step=55, batch=30, test loss = 0.43343573808670044, test acc = 0.8600000143051147, time = 0.3460581302642822\n",
      "Testing at step=55, batch=35, test loss = 0.3839677572250366, test acc = 0.8450000286102295, time = 0.3451364040374756\n",
      "Testing at step=55, batch=40, test loss = 0.34362873435020447, test acc = 0.8700000047683716, time = 0.4560225009918213\n",
      "Testing at step=55, batch=45, test loss = 0.5298288464546204, test acc = 0.8399999737739563, time = 0.6018764972686768\n",
      "Step 55 finished in 312.2565839290619, Train loss = 0.27453413501381874, Test loss = 0.38604826241731643; Train Acc = 0.8985499978065491, Test Acc = 0.8658000016212464\n",
      "Training at step=56, batch=0, train loss = 0.3469017446041107, train acc = 0.8650000095367432, time = 1.337890625\n",
      "Training at step=56, batch=30, train loss = 0.2830393612384796, train acc = 0.8949999809265137, time = 1.2710113525390625\n",
      "Training at step=56, batch=60, train loss = 0.3610191345214844, train acc = 0.8700000047683716, time = 0.9295086860656738\n",
      "Training at step=56, batch=90, train loss = 0.3141797184944153, train acc = 0.8949999809265137, time = 0.9363951683044434\n",
      "Training at step=56, batch=120, train loss = 0.33650362491607666, train acc = 0.8949999809265137, time = 0.9323351383209229\n",
      "Training at step=56, batch=150, train loss = 0.2754417955875397, train acc = 0.8799999952316284, time = 0.9289686679840088\n",
      "Training at step=56, batch=180, train loss = 0.32007521390914917, train acc = 0.8949999809265137, time = 0.939965009689331\n",
      "Training at step=56, batch=210, train loss = 0.20116853713989258, train acc = 0.925000011920929, time = 0.9300825595855713\n",
      "Training at step=56, batch=240, train loss = 0.21335917711257935, train acc = 0.9150000214576721, time = 0.9276556968688965\n",
      "Training at step=56, batch=270, train loss = 0.2486734390258789, train acc = 0.8999999761581421, time = 0.9454960823059082\n",
      "Testing at step=56, batch=0, test loss = 0.32855814695358276, test acc = 0.8799999952316284, time = 0.3453030586242676\n",
      "Testing at step=56, batch=5, test loss = 0.297892689704895, test acc = 0.9100000262260437, time = 0.3395543098449707\n",
      "Testing at step=56, batch=10, test loss = 0.28016430139541626, test acc = 0.8999999761581421, time = 0.33343005180358887\n",
      "Testing at step=56, batch=15, test loss = 0.2924797236919403, test acc = 0.9049999713897705, time = 0.3366854190826416\n",
      "Testing at step=56, batch=20, test loss = 0.36368945240974426, test acc = 0.8899999856948853, time = 0.3358619213104248\n",
      "Testing at step=56, batch=25, test loss = 0.3506510257720947, test acc = 0.8899999856948853, time = 0.33716249465942383\n",
      "Testing at step=56, batch=30, test loss = 0.29338163137435913, test acc = 0.8899999856948853, time = 0.33563899993896484\n",
      "Testing at step=56, batch=35, test loss = 0.3296060562133789, test acc = 0.8849999904632568, time = 0.3343026638031006\n",
      "Testing at step=56, batch=40, test loss = 0.3352324366569519, test acc = 0.8849999904632568, time = 0.33599352836608887\n",
      "Testing at step=56, batch=45, test loss = 0.3373596966266632, test acc = 0.8849999904632568, time = 0.34622693061828613\n",
      "Step 56 finished in 327.79501605033875, Train loss = 0.27634786928693456, Test loss = 0.34887776225805284; Train Acc = 0.8971666644016901, Test Acc = 0.8786999988555908\n",
      "Training at step=57, batch=0, train loss = 0.3592601418495178, train acc = 0.8849999904632568, time = 0.9258735179901123\n",
      "Training at step=57, batch=30, train loss = 0.35998424887657166, train acc = 0.8550000190734863, time = 0.9612390995025635\n",
      "Training at step=57, batch=60, train loss = 0.30381080508232117, train acc = 0.9049999713897705, time = 0.9335434436798096\n",
      "Training at step=57, batch=90, train loss = 0.3007519841194153, train acc = 0.9150000214576721, time = 0.9332523345947266\n",
      "Training at step=57, batch=120, train loss = 0.3424985408782959, train acc = 0.8849999904632568, time = 0.9466822147369385\n",
      "Training at step=57, batch=150, train loss = 0.28625547885894775, train acc = 0.9100000262260437, time = 0.9333758354187012\n",
      "Training at step=57, batch=180, train loss = 0.2095165103673935, train acc = 0.9150000214576721, time = 0.9414730072021484\n",
      "Training at step=57, batch=210, train loss = 0.22524602711200714, train acc = 0.9200000166893005, time = 0.9463186264038086\n",
      "Training at step=57, batch=240, train loss = 0.2628500759601593, train acc = 0.9150000214576721, time = 0.948859691619873\n",
      "Training at step=57, batch=270, train loss = 0.24490320682525635, train acc = 0.8999999761581421, time = 0.9397847652435303\n",
      "Testing at step=57, batch=0, test loss = 0.37673428654670715, test acc = 0.8799999952316284, time = 0.35434889793395996\n",
      "Testing at step=57, batch=5, test loss = 0.3800700902938843, test acc = 0.9049999713897705, time = 0.35433506965637207\n",
      "Testing at step=57, batch=10, test loss = 0.38054797053337097, test acc = 0.8650000095367432, time = 0.3453192710876465\n",
      "Testing at step=57, batch=15, test loss = 0.34112417697906494, test acc = 0.8700000047683716, time = 0.35114383697509766\n",
      "Testing at step=57, batch=20, test loss = 0.23175619542598724, test acc = 0.9150000214576721, time = 0.3574244976043701\n",
      "Testing at step=57, batch=25, test loss = 0.24765312671661377, test acc = 0.8999999761581421, time = 0.3438386917114258\n",
      "Testing at step=57, batch=30, test loss = 0.2786865532398224, test acc = 0.9049999713897705, time = 0.35279178619384766\n",
      "Testing at step=57, batch=35, test loss = 0.4259239137172699, test acc = 0.8349999785423279, time = 0.3656480312347412\n",
      "Testing at step=57, batch=40, test loss = 0.3043217957019806, test acc = 0.8949999809265137, time = 0.3493921756744385\n",
      "Testing at step=57, batch=45, test loss = 0.33624276518821716, test acc = 0.9100000262260437, time = 0.3500492572784424\n",
      "Step 57 finished in 310.25307631492615, Train loss = 0.27401812598109243, Test loss = 0.34816202878952024; Train Acc = 0.9001833313703537, Test Acc = 0.8776999986171723\n",
      "Training at step=58, batch=0, train loss = 0.2934871017932892, train acc = 0.8849999904632568, time = 0.9491727352142334\n",
      "Training at step=58, batch=30, train loss = 0.3156481385231018, train acc = 0.8899999856948853, time = 0.9421193599700928\n",
      "Training at step=58, batch=60, train loss = 0.20945009589195251, train acc = 0.9150000214576721, time = 0.9417803287506104\n",
      "Training at step=58, batch=90, train loss = 0.18718889355659485, train acc = 0.9350000023841858, time = 0.9260797500610352\n",
      "Training at step=58, batch=120, train loss = 0.32454192638397217, train acc = 0.8399999737739563, time = 0.9352233409881592\n",
      "Training at step=58, batch=150, train loss = 0.2906375229358673, train acc = 0.8999999761581421, time = 0.9477095603942871\n",
      "Training at step=58, batch=180, train loss = 0.2923436164855957, train acc = 0.8899999856948853, time = 0.9387798309326172\n",
      "Training at step=58, batch=210, train loss = 0.32399100065231323, train acc = 0.8500000238418579, time = 0.9391298294067383\n",
      "Training at step=58, batch=240, train loss = 0.287993460893631, train acc = 0.8849999904632568, time = 0.9472978115081787\n",
      "Training at step=58, batch=270, train loss = 0.39482638239860535, train acc = 0.8500000238418579, time = 0.9425280094146729\n",
      "Testing at step=58, batch=0, test loss = 0.23857381939888, test acc = 0.9100000262260437, time = 0.3501410484313965\n",
      "Testing at step=58, batch=5, test loss = 0.33890673518180847, test acc = 0.875, time = 0.3614356517791748\n",
      "Testing at step=58, batch=10, test loss = 0.42754077911376953, test acc = 0.875, time = 0.3472309112548828\n",
      "Testing at step=58, batch=15, test loss = 0.3787844479084015, test acc = 0.8650000095367432, time = 0.3594202995300293\n",
      "Testing at step=58, batch=20, test loss = 0.43768611550331116, test acc = 0.8550000190734863, time = 0.34160494804382324\n",
      "Testing at step=58, batch=25, test loss = 0.27448731660842896, test acc = 0.8949999809265137, time = 0.35947108268737793\n",
      "Testing at step=58, batch=30, test loss = 0.3048815429210663, test acc = 0.8999999761581421, time = 0.3474304676055908\n",
      "Testing at step=58, batch=35, test loss = 0.30088794231414795, test acc = 0.8949999809265137, time = 0.3501579761505127\n",
      "Testing at step=58, batch=40, test loss = 0.4040610194206238, test acc = 0.8849999904632568, time = 0.342756986618042\n",
      "Testing at step=58, batch=45, test loss = 0.3899804353713989, test acc = 0.8550000190734863, time = 0.3394911289215088\n",
      "Step 58 finished in 310.2519385814667, Train loss = 0.2715507491926352, Test loss = 0.35664448857307435; Train Acc = 0.9000333325068156, Test Acc = 0.8749000024795532\n",
      "Training at step=59, batch=0, train loss = 0.36241447925567627, train acc = 0.8899999856948853, time = 0.942650318145752\n",
      "Training at step=59, batch=30, train loss = 0.32262688875198364, train acc = 0.8999999761581421, time = 0.9366977214813232\n",
      "Training at step=59, batch=60, train loss = 0.22437608242034912, train acc = 0.9150000214576721, time = 0.9367949962615967\n",
      "Training at step=59, batch=90, train loss = 0.27531689405441284, train acc = 0.8899999856948853, time = 0.9439589977264404\n",
      "Training at step=59, batch=120, train loss = 0.2101910412311554, train acc = 0.925000011920929, time = 0.9373824596405029\n",
      "Training at step=59, batch=150, train loss = 0.2732171416282654, train acc = 0.8999999761581421, time = 0.9440147876739502\n",
      "Training at step=59, batch=180, train loss = 0.22846665978431702, train acc = 0.9049999713897705, time = 0.9382014274597168\n",
      "Training at step=59, batch=210, train loss = 0.2942524254322052, train acc = 0.8949999809265137, time = 0.950486421585083\n",
      "Training at step=59, batch=240, train loss = 0.39375028014183044, train acc = 0.8650000095367432, time = 0.9446165561676025\n",
      "Training at step=59, batch=270, train loss = 0.25965622067451477, train acc = 0.9300000071525574, time = 0.9469916820526123\n",
      "Testing at step=59, batch=0, test loss = 0.33668050169944763, test acc = 0.8899999856948853, time = 0.34610438346862793\n",
      "Testing at step=59, batch=5, test loss = 0.25919532775878906, test acc = 0.9100000262260437, time = 0.3483726978302002\n",
      "Testing at step=59, batch=10, test loss = 0.35940349102020264, test acc = 0.8999999761581421, time = 0.34345579147338867\n",
      "Testing at step=59, batch=15, test loss = 0.33678969740867615, test acc = 0.8799999952316284, time = 0.34134721755981445\n",
      "Testing at step=59, batch=20, test loss = 0.3165854513645172, test acc = 0.8999999761581421, time = 0.34856677055358887\n",
      "Testing at step=59, batch=25, test loss = 0.364844411611557, test acc = 0.8650000095367432, time = 0.34597325325012207\n",
      "Testing at step=59, batch=30, test loss = 0.4393242299556732, test acc = 0.8450000286102295, time = 0.36562418937683105\n",
      "Testing at step=59, batch=35, test loss = 0.3528733551502228, test acc = 0.8650000095367432, time = 0.351046085357666\n",
      "Testing at step=59, batch=40, test loss = 0.41108667850494385, test acc = 0.875, time = 0.3475916385650635\n",
      "Testing at step=59, batch=45, test loss = 0.46931400895118713, test acc = 0.8399999737739563, time = 0.3418252468109131\n",
      "Step 59 finished in 309.95080733299255, Train loss = 0.2706326673924923, Test loss = 0.3613764786720276; Train Acc = 0.9007166641950607, Test Acc = 0.8751999962329865\n",
      "Training at step=60, batch=0, train loss = 0.2347245067358017, train acc = 0.9200000166893005, time = 0.9451746940612793\n",
      "Training at step=60, batch=30, train loss = 0.2561691999435425, train acc = 0.8949999809265137, time = 0.955115556716919\n",
      "Training at step=60, batch=60, train loss = 0.24610988795757294, train acc = 0.9100000262260437, time = 0.9384169578552246\n",
      "Training at step=60, batch=90, train loss = 0.3337370753288269, train acc = 0.8650000095367432, time = 0.9615468978881836\n",
      "Training at step=60, batch=120, train loss = 0.20673872530460358, train acc = 0.9300000071525574, time = 0.9383163452148438\n",
      "Training at step=60, batch=150, train loss = 0.23016856610774994, train acc = 0.9150000214576721, time = 0.9370501041412354\n",
      "Training at step=60, batch=180, train loss = 0.2969246506690979, train acc = 0.8799999952316284, time = 0.9378354549407959\n",
      "Training at step=60, batch=210, train loss = 0.22978201508522034, train acc = 0.9049999713897705, time = 0.9330971240997314\n",
      "Training at step=60, batch=240, train loss = 0.2593919038772583, train acc = 0.8899999856948853, time = 0.9307150840759277\n",
      "Training at step=60, batch=270, train loss = 0.2836376428604126, train acc = 0.8849999904632568, time = 0.9463026523590088\n",
      "Testing at step=60, batch=0, test loss = 0.3104808032512665, test acc = 0.9049999713897705, time = 0.3491852283477783\n",
      "Testing at step=60, batch=5, test loss = 0.3519081175327301, test acc = 0.8550000190734863, time = 0.3472118377685547\n",
      "Testing at step=60, batch=10, test loss = 0.35190021991729736, test acc = 0.8600000143051147, time = 0.3558769226074219\n",
      "Testing at step=60, batch=15, test loss = 0.23718996345996857, test acc = 0.9350000023841858, time = 0.353503942489624\n",
      "Testing at step=60, batch=20, test loss = 0.3678084909915924, test acc = 0.8799999952316284, time = 0.3642404079437256\n",
      "Testing at step=60, batch=25, test loss = 0.43085986375808716, test acc = 0.8700000047683716, time = 0.36025071144104004\n",
      "Testing at step=60, batch=30, test loss = 0.31664252281188965, test acc = 0.875, time = 0.3629879951477051\n",
      "Testing at step=60, batch=35, test loss = 0.3190051019191742, test acc = 0.8999999761581421, time = 0.34659337997436523\n",
      "Testing at step=60, batch=40, test loss = 0.2913523316383362, test acc = 0.8899999856948853, time = 0.3516080379486084\n",
      "Testing at step=60, batch=45, test loss = 0.32187098264694214, test acc = 0.8600000143051147, time = 0.342958927154541\n",
      "Step 60 finished in 310.6840009689331, Train loss = 0.2708596543967724, Test loss = 0.3547947835922241; Train Acc = 0.9006666664282481, Test Acc = 0.8757000017166138\n",
      "Training at step=61, batch=0, train loss = 0.3354993462562561, train acc = 0.8799999952316284, time = 0.9393224716186523\n",
      "Training at step=61, batch=30, train loss = 0.19563879072666168, train acc = 0.8949999809265137, time = 0.9352877140045166\n",
      "Training at step=61, batch=60, train loss = 0.2708390951156616, train acc = 0.9049999713897705, time = 0.93849778175354\n",
      "Training at step=61, batch=90, train loss = 0.23214712738990784, train acc = 0.9049999713897705, time = 0.9509313106536865\n",
      "Training at step=61, batch=120, train loss = 0.28270137310028076, train acc = 0.8899999856948853, time = 0.9513876438140869\n",
      "Training at step=61, batch=150, train loss = 0.34798189997673035, train acc = 0.8849999904632568, time = 0.9504351615905762\n",
      "Training at step=61, batch=180, train loss = 0.2624465227127075, train acc = 0.8949999809265137, time = 0.9446077346801758\n",
      "Training at step=61, batch=210, train loss = 0.19801856577396393, train acc = 0.925000011920929, time = 0.9382085800170898\n",
      "Training at step=61, batch=240, train loss = 0.2869943380355835, train acc = 0.8899999856948853, time = 0.949713945388794\n",
      "Training at step=61, batch=270, train loss = 0.27534493803977966, train acc = 0.8949999809265137, time = 0.9464602470397949\n",
      "Testing at step=61, batch=0, test loss = 0.4286632537841797, test acc = 0.8349999785423279, time = 0.35146665573120117\n",
      "Testing at step=61, batch=5, test loss = 0.3850604295730591, test acc = 0.8450000286102295, time = 0.3462221622467041\n",
      "Testing at step=61, batch=10, test loss = 0.40252041816711426, test acc = 0.8700000047683716, time = 0.3418614864349365\n",
      "Testing at step=61, batch=15, test loss = 0.31130990386009216, test acc = 0.8799999952316284, time = 0.34975171089172363\n",
      "Testing at step=61, batch=20, test loss = 0.3278411030769348, test acc = 0.8700000047683716, time = 0.345456600189209\n",
      "Testing at step=61, batch=25, test loss = 0.426018625497818, test acc = 0.8700000047683716, time = 0.34470248222351074\n",
      "Testing at step=61, batch=30, test loss = 0.2839778661727905, test acc = 0.8999999761581421, time = 0.34682488441467285\n",
      "Testing at step=61, batch=35, test loss = 0.31688278913497925, test acc = 0.8899999856948853, time = 0.34378623962402344\n",
      "Testing at step=61, batch=40, test loss = 0.27489134669303894, test acc = 0.8949999809265137, time = 0.34265708923339844\n",
      "Testing at step=61, batch=45, test loss = 0.29457223415374756, test acc = 0.8849999904632568, time = 0.35041236877441406\n",
      "Step 61 finished in 310.2213604450226, Train loss = 0.26967269301414487, Test loss = 0.35231791287660597; Train Acc = 0.9005666651328404, Test Acc = 0.8763000011444092\n",
      "Training at step=62, batch=0, train loss = 0.2707807421684265, train acc = 0.9049999713897705, time = 0.954606294631958\n",
      "Training at step=62, batch=30, train loss = 0.33021971583366394, train acc = 0.8799999952316284, time = 0.9427084922790527\n",
      "Training at step=62, batch=60, train loss = 0.2579137086868286, train acc = 0.8999999761581421, time = 0.9406774044036865\n",
      "Training at step=62, batch=90, train loss = 0.27096474170684814, train acc = 0.8849999904632568, time = 0.9544520378112793\n",
      "Training at step=62, batch=120, train loss = 0.30261126160621643, train acc = 0.8650000095367432, time = 0.9426188468933105\n",
      "Training at step=62, batch=150, train loss = 0.3111337423324585, train acc = 0.9150000214576721, time = 0.9397742748260498\n",
      "Training at step=62, batch=180, train loss = 0.3242165744304657, train acc = 0.8999999761581421, time = 0.9492027759552002\n",
      "Training at step=62, batch=210, train loss = 0.1983824223279953, train acc = 0.925000011920929, time = 0.9669857025146484\n",
      "Training at step=62, batch=240, train loss = 0.23825706541538239, train acc = 0.9150000214576721, time = 0.9384489059448242\n",
      "Training at step=62, batch=270, train loss = 0.36985546350479126, train acc = 0.8600000143051147, time = 0.9364550113677979\n",
      "Testing at step=62, batch=0, test loss = 0.24055662751197815, test acc = 0.9150000214576721, time = 0.34779977798461914\n",
      "Testing at step=62, batch=5, test loss = 0.4388171136379242, test acc = 0.8399999737739563, time = 0.3458366394042969\n",
      "Testing at step=62, batch=10, test loss = 0.2911033630371094, test acc = 0.8999999761581421, time = 0.3502824306488037\n",
      "Testing at step=62, batch=15, test loss = 0.31526488065719604, test acc = 0.8799999952316284, time = 0.34396791458129883\n",
      "Testing at step=62, batch=20, test loss = 0.3299822211265564, test acc = 0.8899999856948853, time = 0.3529808521270752\n",
      "Testing at step=62, batch=25, test loss = 0.40295085310935974, test acc = 0.8600000143051147, time = 0.3469052314758301\n",
      "Testing at step=62, batch=30, test loss = 0.28626418113708496, test acc = 0.8949999809265137, time = 0.33994460105895996\n",
      "Testing at step=62, batch=35, test loss = 0.384476900100708, test acc = 0.9049999713897705, time = 0.3474619388580322\n",
      "Testing at step=62, batch=40, test loss = 0.48497775197029114, test acc = 0.8349999785423279, time = 0.350078821182251\n",
      "Testing at step=62, batch=45, test loss = 0.2802273631095886, test acc = 0.9049999713897705, time = 0.3420572280883789\n",
      "Step 62 finished in 310.3249514102936, Train loss = 0.27132803430159885, Test loss = 0.34349482029676437; Train Acc = 0.9000999989112218, Test Acc = 0.8800999975204468\n",
      "Training at step=63, batch=0, train loss = 0.24626664817333221, train acc = 0.8949999809265137, time = 0.9405045509338379\n",
      "Training at step=63, batch=30, train loss = 0.16824640333652496, train acc = 0.949999988079071, time = 0.9496285915374756\n",
      "Training at step=63, batch=60, train loss = 0.2640734314918518, train acc = 0.9100000262260437, time = 0.9393610954284668\n",
      "Training at step=63, batch=90, train loss = 0.23988871276378632, train acc = 0.9200000166893005, time = 0.9424867630004883\n",
      "Training at step=63, batch=120, train loss = 0.27330437302589417, train acc = 0.8849999904632568, time = 0.9317052364349365\n",
      "Training at step=63, batch=150, train loss = 0.35056450963020325, train acc = 0.8799999952316284, time = 0.9356286525726318\n",
      "Training at step=63, batch=180, train loss = 0.31491386890411377, train acc = 0.875, time = 0.9577980041503906\n",
      "Training at step=63, batch=210, train loss = 0.2039458155632019, train acc = 0.9150000214576721, time = 0.9394583702087402\n",
      "Training at step=63, batch=240, train loss = 0.2619123160839081, train acc = 0.9150000214576721, time = 0.9382667541503906\n",
      "Training at step=63, batch=270, train loss = 0.23974266648292542, train acc = 0.9049999713897705, time = 0.9468095302581787\n",
      "Testing at step=63, batch=0, test loss = 0.4355721175670624, test acc = 0.8550000190734863, time = 0.34310483932495117\n",
      "Testing at step=63, batch=5, test loss = 0.33354026079177856, test acc = 0.8849999904632568, time = 0.36625027656555176\n",
      "Testing at step=63, batch=10, test loss = 0.46300143003463745, test acc = 0.8450000286102295, time = 0.34992504119873047\n",
      "Testing at step=63, batch=15, test loss = 0.44204795360565186, test acc = 0.8600000143051147, time = 0.3424973487854004\n",
      "Testing at step=63, batch=20, test loss = 0.3628039062023163, test acc = 0.875, time = 0.3598806858062744\n",
      "Testing at step=63, batch=25, test loss = 0.35532334446907043, test acc = 0.8949999809265137, time = 0.35890865325927734\n",
      "Testing at step=63, batch=30, test loss = 0.390121728181839, test acc = 0.8600000143051147, time = 0.346726655960083\n",
      "Testing at step=63, batch=35, test loss = 0.39029738306999207, test acc = 0.8600000143051147, time = 0.3656768798828125\n",
      "Testing at step=63, batch=40, test loss = 0.3049090504646301, test acc = 0.9150000214576721, time = 0.36359596252441406\n",
      "Testing at step=63, batch=45, test loss = 0.3198806345462799, test acc = 0.8600000143051147, time = 0.356372594833374\n",
      "Step 63 finished in 310.3345549106598, Train loss = 0.26737662265698114, Test loss = 0.3567217919230461; Train Acc = 0.9007166647911071, Test Acc = 0.8774000000953674\n",
      "Training at step=64, batch=0, train loss = 0.29204362630844116, train acc = 0.8949999809265137, time = 0.9448316097259521\n",
      "Training at step=64, batch=30, train loss = 0.2486213743686676, train acc = 0.8999999761581421, time = 0.9475448131561279\n",
      "Training at step=64, batch=60, train loss = 0.16043484210968018, train acc = 0.9399999976158142, time = 0.9353384971618652\n",
      "Training at step=64, batch=90, train loss = 0.22840125858783722, train acc = 0.8949999809265137, time = 0.948521614074707\n",
      "Training at step=64, batch=120, train loss = 0.2125694900751114, train acc = 0.9399999976158142, time = 0.93149733543396\n",
      "Training at step=64, batch=150, train loss = 0.3439464867115021, train acc = 0.875, time = 0.939349889755249\n",
      "Training at step=64, batch=180, train loss = 0.3415992856025696, train acc = 0.875, time = 0.9466099739074707\n",
      "Training at step=64, batch=210, train loss = 0.2329493761062622, train acc = 0.9150000214576721, time = 0.9517066478729248\n",
      "Training at step=64, batch=240, train loss = 0.2565157413482666, train acc = 0.9049999713897705, time = 0.9416928291320801\n",
      "Training at step=64, batch=270, train loss = 0.20854195952415466, train acc = 0.9350000023841858, time = 0.94026780128479\n",
      "Testing at step=64, batch=0, test loss = 0.2633226215839386, test acc = 0.8949999809265137, time = 0.35799598693847656\n",
      "Testing at step=64, batch=5, test loss = 0.29442402720451355, test acc = 0.8799999952316284, time = 0.3450589179992676\n",
      "Testing at step=64, batch=10, test loss = 0.3651348352432251, test acc = 0.8700000047683716, time = 0.34708380699157715\n",
      "Testing at step=64, batch=15, test loss = 0.3059115707874298, test acc = 0.8949999809265137, time = 0.35713791847229004\n",
      "Testing at step=64, batch=20, test loss = 0.34130218625068665, test acc = 0.8550000190734863, time = 0.35873866081237793\n",
      "Testing at step=64, batch=25, test loss = 0.3669187128543854, test acc = 0.8299999833106995, time = 0.3473954200744629\n",
      "Testing at step=64, batch=30, test loss = 0.3912166953086853, test acc = 0.8799999952316284, time = 0.351210355758667\n",
      "Testing at step=64, batch=35, test loss = 0.4808608293533325, test acc = 0.875, time = 0.344158411026001\n",
      "Testing at step=64, batch=40, test loss = 0.3865297734737396, test acc = 0.875, time = 0.35798072814941406\n",
      "Testing at step=64, batch=45, test loss = 0.2937577962875366, test acc = 0.8999999761581421, time = 0.3482692241668701\n",
      "Step 64 finished in 310.82505202293396, Train loss = 0.26645362878839174, Test loss = 0.3580314368009567; Train Acc = 0.9013499983151754, Test Acc = 0.8712999999523163\n",
      "Training at step=65, batch=0, train loss = 0.2911626994609833, train acc = 0.8999999761581421, time = 0.9371747970581055\n",
      "Training at step=65, batch=30, train loss = 0.24233871698379517, train acc = 0.9100000262260437, time = 0.9478542804718018\n",
      "Training at step=65, batch=60, train loss = 0.3046669661998749, train acc = 0.8799999952316284, time = 0.9424746036529541\n",
      "Training at step=65, batch=90, train loss = 0.26678773760795593, train acc = 0.8949999809265137, time = 0.9414877891540527\n",
      "Training at step=65, batch=120, train loss = 0.262827068567276, train acc = 0.8849999904632568, time = 0.944782018661499\n",
      "Training at step=65, batch=150, train loss = 0.2510988116264343, train acc = 0.9049999713897705, time = 0.9420485496520996\n",
      "Training at step=65, batch=180, train loss = 0.2128325253725052, train acc = 0.9150000214576721, time = 0.9366269111633301\n",
      "Training at step=65, batch=210, train loss = 0.18796144425868988, train acc = 0.9300000071525574, time = 0.9450469017028809\n",
      "Training at step=65, batch=240, train loss = 0.3471654951572418, train acc = 0.8799999952316284, time = 0.9370391368865967\n",
      "Training at step=65, batch=270, train loss = 0.3393631875514984, train acc = 0.8849999904632568, time = 0.9356615543365479\n",
      "Testing at step=65, batch=0, test loss = 0.3291673958301544, test acc = 0.8600000143051147, time = 0.3510549068450928\n",
      "Testing at step=65, batch=5, test loss = 0.32602038979530334, test acc = 0.8650000095367432, time = 0.3415985107421875\n",
      "Testing at step=65, batch=10, test loss = 0.23380230367183685, test acc = 0.9049999713897705, time = 0.35058069229125977\n",
      "Testing at step=65, batch=15, test loss = 0.4291134178638458, test acc = 0.8349999785423279, time = 0.3582127094268799\n",
      "Testing at step=65, batch=20, test loss = 0.34566208720207214, test acc = 0.8700000047683716, time = 0.34415745735168457\n",
      "Testing at step=65, batch=25, test loss = 0.3364677131175995, test acc = 0.8799999952316284, time = 0.3465080261230469\n",
      "Testing at step=65, batch=30, test loss = 0.3352782130241394, test acc = 0.8849999904632568, time = 0.36066389083862305\n",
      "Testing at step=65, batch=35, test loss = 0.2671312689781189, test acc = 0.9049999713897705, time = 0.36434006690979004\n",
      "Testing at step=65, batch=40, test loss = 0.31940239667892456, test acc = 0.875, time = 0.3486180305480957\n",
      "Testing at step=65, batch=45, test loss = 0.4513944685459137, test acc = 0.8550000190734863, time = 0.3457448482513428\n",
      "Step 65 finished in 310.4693372249603, Train loss = 0.26518931696812315, Test loss = 0.35821649461984634; Train Acc = 0.9019833316405614, Test Acc = 0.8719999992847443\n",
      "Training at step=66, batch=0, train loss = 0.29952624440193176, train acc = 0.875, time = 0.9551892280578613\n",
      "Training at step=66, batch=30, train loss = 0.2889895737171173, train acc = 0.875, time = 0.9344329833984375\n",
      "Training at step=66, batch=60, train loss = 0.2225457727909088, train acc = 0.9150000214576721, time = 0.943781852722168\n",
      "Training at step=66, batch=90, train loss = 0.23829467594623566, train acc = 0.9049999713897705, time = 0.9367961883544922\n",
      "Training at step=66, batch=120, train loss = 0.25478479266166687, train acc = 0.8899999856948853, time = 0.9440321922302246\n",
      "Training at step=66, batch=150, train loss = 0.1906309276819229, train acc = 0.9200000166893005, time = 0.941969633102417\n",
      "Training at step=66, batch=180, train loss = 0.2838335931301117, train acc = 0.875, time = 0.9481773376464844\n",
      "Training at step=66, batch=210, train loss = 0.28715670108795166, train acc = 0.8849999904632568, time = 0.9512546062469482\n",
      "Training at step=66, batch=240, train loss = 0.2562512159347534, train acc = 0.9100000262260437, time = 0.9356598854064941\n",
      "Training at step=66, batch=270, train loss = 0.24592065811157227, train acc = 0.8999999761581421, time = 0.9475502967834473\n",
      "Testing at step=66, batch=0, test loss = 0.3264530301094055, test acc = 0.8600000143051147, time = 0.34200501441955566\n",
      "Testing at step=66, batch=5, test loss = 0.39460089802742004, test acc = 0.8650000095367432, time = 0.3448679447174072\n",
      "Testing at step=66, batch=10, test loss = 0.31488391757011414, test acc = 0.9049999713897705, time = 0.34775400161743164\n",
      "Testing at step=66, batch=15, test loss = 0.3441077470779419, test acc = 0.8849999904632568, time = 0.34767842292785645\n",
      "Testing at step=66, batch=20, test loss = 0.4678718149662018, test acc = 0.8650000095367432, time = 0.350477933883667\n",
      "Testing at step=66, batch=25, test loss = 0.445890873670578, test acc = 0.8700000047683716, time = 0.3468022346496582\n",
      "Testing at step=66, batch=30, test loss = 0.3113287091255188, test acc = 0.8849999904632568, time = 0.33726978302001953\n",
      "Testing at step=66, batch=35, test loss = 0.269746333360672, test acc = 0.8949999809265137, time = 0.34595799446105957\n",
      "Testing at step=66, batch=40, test loss = 0.4209868609905243, test acc = 0.8600000143051147, time = 0.35193681716918945\n",
      "Testing at step=66, batch=45, test loss = 0.36161744594573975, test acc = 0.8650000095367432, time = 0.353348970413208\n",
      "Step 66 finished in 310.76532888412476, Train loss = 0.26190585280458134, Test loss = 0.3600101882219315; Train Acc = 0.9038999990622203, Test Acc = 0.8756000006198883\n",
      "Training at step=67, batch=0, train loss = 0.3452186584472656, train acc = 0.8650000095367432, time = 0.9386608600616455\n",
      "Training at step=67, batch=30, train loss = 0.22408132255077362, train acc = 0.925000011920929, time = 0.9446189403533936\n",
      "Training at step=67, batch=60, train loss = 0.23538623750209808, train acc = 0.9049999713897705, time = 0.9483768939971924\n",
      "Training at step=67, batch=90, train loss = 0.24929502606391907, train acc = 0.8999999761581421, time = 0.94051194190979\n",
      "Training at step=67, batch=120, train loss = 0.19198770821094513, train acc = 0.925000011920929, time = 0.9492323398590088\n",
      "Training at step=67, batch=150, train loss = 0.23908674716949463, train acc = 0.9049999713897705, time = 0.9285264015197754\n",
      "Training at step=67, batch=180, train loss = 0.23573391139507294, train acc = 0.9100000262260437, time = 0.9344208240509033\n",
      "Training at step=67, batch=210, train loss = 0.27442002296447754, train acc = 0.9200000166893005, time = 0.9341740608215332\n",
      "Training at step=67, batch=240, train loss = 0.25557172298431396, train acc = 0.925000011920929, time = 0.9512207508087158\n",
      "Training at step=67, batch=270, train loss = 0.21138860285282135, train acc = 0.9150000214576721, time = 0.9408950805664062\n",
      "Testing at step=67, batch=0, test loss = 0.35223740339279175, test acc = 0.8650000095367432, time = 0.35822033882141113\n",
      "Testing at step=67, batch=5, test loss = 0.43954041600227356, test acc = 0.875, time = 0.3485121726989746\n",
      "Testing at step=67, batch=10, test loss = 0.3014957010746002, test acc = 0.9150000214576721, time = 0.3440821170806885\n",
      "Testing at step=67, batch=15, test loss = 0.4201376736164093, test acc = 0.8500000238418579, time = 0.35077452659606934\n",
      "Testing at step=67, batch=20, test loss = 0.38271602988243103, test acc = 0.8550000190734863, time = 0.36366701126098633\n",
      "Testing at step=67, batch=25, test loss = 0.400309294462204, test acc = 0.8600000143051147, time = 0.34197187423706055\n",
      "Testing at step=67, batch=30, test loss = 0.31884053349494934, test acc = 0.8849999904632568, time = 0.3481934070587158\n",
      "Testing at step=67, batch=35, test loss = 0.3654239773750305, test acc = 0.8399999737739563, time = 0.3559854030609131\n",
      "Testing at step=67, batch=40, test loss = 0.2980354428291321, test acc = 0.8999999761581421, time = 0.3473358154296875\n",
      "Testing at step=67, batch=45, test loss = 0.2930552661418915, test acc = 0.8949999809265137, time = 0.3421018123626709\n",
      "Step 67 finished in 310.5146851539612, Train loss = 0.26028294652700423, Test loss = 0.35057260721921923; Train Acc = 0.9031333337227504, Test Acc = 0.8774999988079071\n",
      "Training at step=68, batch=0, train loss = 0.24000579118728638, train acc = 0.9150000214576721, time = 1.037222146987915\n",
      "Training at step=68, batch=30, train loss = 0.26895174384117126, train acc = 0.8999999761581421, time = 0.9388828277587891\n",
      "Training at step=68, batch=60, train loss = 0.24220043420791626, train acc = 0.8999999761581421, time = 0.9520308971405029\n",
      "Training at step=68, batch=90, train loss = 0.26218682527542114, train acc = 0.8899999856948853, time = 0.9394087791442871\n",
      "Training at step=68, batch=120, train loss = 0.2802654802799225, train acc = 0.9049999713897705, time = 0.9336845874786377\n",
      "Training at step=68, batch=150, train loss = 0.22476564347743988, train acc = 0.9399999976158142, time = 0.9381670951843262\n",
      "Training at step=68, batch=180, train loss = 0.35589146614074707, train acc = 0.8799999952316284, time = 0.9359428882598877\n",
      "Training at step=68, batch=210, train loss = 0.23244673013687134, train acc = 0.9100000262260437, time = 0.9352712631225586\n",
      "Training at step=68, batch=240, train loss = 0.2074890285730362, train acc = 0.9300000071525574, time = 0.9394741058349609\n",
      "Training at step=68, batch=270, train loss = 0.28365135192871094, train acc = 0.9100000262260437, time = 0.9404988288879395\n",
      "Testing at step=68, batch=0, test loss = 0.29243940114974976, test acc = 0.9200000166893005, time = 0.3413715362548828\n",
      "Testing at step=68, batch=5, test loss = 0.2746613025665283, test acc = 0.8999999761581421, time = 0.3451666831970215\n",
      "Testing at step=68, batch=10, test loss = 0.3931960165500641, test acc = 0.8799999952316284, time = 0.3488798141479492\n",
      "Testing at step=68, batch=15, test loss = 0.3411675691604614, test acc = 0.8999999761581421, time = 0.3421196937561035\n",
      "Testing at step=68, batch=20, test loss = 0.5142585635185242, test acc = 0.8299999833106995, time = 0.34711623191833496\n",
      "Testing at step=68, batch=25, test loss = 0.2557719647884369, test acc = 0.9049999713897705, time = 0.34483885765075684\n",
      "Testing at step=68, batch=30, test loss = 0.5195981860160828, test acc = 0.8199999928474426, time = 0.3479795455932617\n",
      "Testing at step=68, batch=35, test loss = 0.3446040749549866, test acc = 0.8849999904632568, time = 0.36407470703125\n",
      "Testing at step=68, batch=40, test loss = 0.36877402663230896, test acc = 0.8650000095367432, time = 0.35712170600891113\n",
      "Testing at step=68, batch=45, test loss = 0.3512840270996094, test acc = 0.875, time = 0.3569645881652832\n",
      "Step 68 finished in 310.33719301223755, Train loss = 0.26060725197196005, Test loss = 0.3449887147545814; Train Acc = 0.9035166652997335, Test Acc = 0.8787999975681305\n",
      "Training at step=69, batch=0, train loss = 0.1937638521194458, train acc = 0.9300000071525574, time = 0.9386997222900391\n",
      "Training at step=69, batch=30, train loss = 0.22652454674243927, train acc = 0.9150000214576721, time = 0.9448444843292236\n",
      "Training at step=69, batch=60, train loss = 0.2712562680244446, train acc = 0.8899999856948853, time = 0.9407434463500977\n",
      "Training at step=69, batch=90, train loss = 0.18773750960826874, train acc = 0.9300000071525574, time = 0.9501404762268066\n",
      "Training at step=69, batch=120, train loss = 0.30805355310440063, train acc = 0.8700000047683716, time = 0.9380323886871338\n",
      "Training at step=69, batch=150, train loss = 0.3145354092121124, train acc = 0.8600000143051147, time = 0.9477131366729736\n",
      "Training at step=69, batch=180, train loss = 0.24450740218162537, train acc = 0.8949999809265137, time = 0.9315235614776611\n",
      "Training at step=69, batch=210, train loss = 0.2616659104824066, train acc = 0.925000011920929, time = 0.9362094402313232\n",
      "Training at step=69, batch=240, train loss = 0.22498983144760132, train acc = 0.925000011920929, time = 0.9360561370849609\n",
      "Training at step=69, batch=270, train loss = 0.2702936828136444, train acc = 0.8899999856948853, time = 0.9534199237823486\n",
      "Testing at step=69, batch=0, test loss = 0.37053367495536804, test acc = 0.8700000047683716, time = 0.3510730266571045\n",
      "Testing at step=69, batch=5, test loss = 0.47858989238739014, test acc = 0.8399999737739563, time = 0.344433069229126\n",
      "Testing at step=69, batch=10, test loss = 0.41344842314720154, test acc = 0.8650000095367432, time = 0.3569056987762451\n",
      "Testing at step=69, batch=15, test loss = 0.32572242617607117, test acc = 0.8949999809265137, time = 0.3470027446746826\n",
      "Testing at step=69, batch=20, test loss = 0.34898844361305237, test acc = 0.9049999713897705, time = 0.3480238914489746\n",
      "Testing at step=69, batch=25, test loss = 0.5308353900909424, test acc = 0.8299999833106995, time = 0.34166622161865234\n",
      "Testing at step=69, batch=30, test loss = 0.2491275817155838, test acc = 0.8899999856948853, time = 0.3468203544616699\n",
      "Testing at step=69, batch=35, test loss = 0.279157429933548, test acc = 0.9049999713897705, time = 0.34923768043518066\n",
      "Testing at step=69, batch=40, test loss = 0.3692864179611206, test acc = 0.8600000143051147, time = 0.34997057914733887\n",
      "Testing at step=69, batch=45, test loss = 0.5451791286468506, test acc = 0.8199999928474426, time = 0.34603238105773926\n",
      "Step 69 finished in 309.8840308189392, Train loss = 0.2617894312242667, Test loss = 0.36178658574819567; Train Acc = 0.903433334628741, Test Acc = 0.8751999950408935\n",
      "Training at step=70, batch=0, train loss = 0.30310314893722534, train acc = 0.8999999761581421, time = 0.9436831474304199\n",
      "Training at step=70, batch=30, train loss = 0.33746373653411865, train acc = 0.875, time = 0.947728157043457\n",
      "Training at step=70, batch=60, train loss = 0.26101091504096985, train acc = 0.8799999952316284, time = 0.9443376064300537\n",
      "Training at step=70, batch=90, train loss = 0.27041110396385193, train acc = 0.9100000262260437, time = 0.946833610534668\n",
      "Training at step=70, batch=120, train loss = 0.22164201736450195, train acc = 0.9300000071525574, time = 0.93843674659729\n",
      "Training at step=70, batch=150, train loss = 0.2661084532737732, train acc = 0.9049999713897705, time = 0.9416971206665039\n",
      "Training at step=70, batch=180, train loss = 0.25843334197998047, train acc = 0.8999999761581421, time = 0.9337050914764404\n",
      "Training at step=70, batch=210, train loss = 0.29361066222190857, train acc = 0.8899999856948853, time = 0.9376943111419678\n",
      "Training at step=70, batch=240, train loss = 0.29089054465293884, train acc = 0.9049999713897705, time = 0.9425952434539795\n",
      "Training at step=70, batch=270, train loss = 0.27842751145362854, train acc = 0.8849999904632568, time = 0.9449508190155029\n",
      "Testing at step=70, batch=0, test loss = 0.3717760145664215, test acc = 0.875, time = 0.3510243892669678\n",
      "Testing at step=70, batch=5, test loss = 0.3831941485404968, test acc = 0.8550000190734863, time = 0.3551604747772217\n",
      "Testing at step=70, batch=10, test loss = 0.44921383261680603, test acc = 0.8700000047683716, time = 0.36019062995910645\n",
      "Testing at step=70, batch=15, test loss = 0.3328937888145447, test acc = 0.875, time = 0.3463451862335205\n",
      "Testing at step=70, batch=20, test loss = 0.49208858609199524, test acc = 0.8550000190734863, time = 0.35603857040405273\n",
      "Testing at step=70, batch=25, test loss = 0.42676910758018494, test acc = 0.8650000095367432, time = 0.35915231704711914\n",
      "Testing at step=70, batch=30, test loss = 0.3445814549922943, test acc = 0.8899999856948853, time = 0.36048150062561035\n",
      "Testing at step=70, batch=35, test loss = 0.31003981828689575, test acc = 0.8849999904632568, time = 0.3446378707885742\n",
      "Testing at step=70, batch=40, test loss = 0.378622442483902, test acc = 0.8849999904632568, time = 0.34737372398376465\n",
      "Testing at step=70, batch=45, test loss = 0.33818337321281433, test acc = 0.8849999904632568, time = 0.3494727611541748\n",
      "Step 70 finished in 310.36921525001526, Train loss = 0.25947033137083053, Test loss = 0.351611530482769; Train Acc = 0.9050166654586792, Test Acc = 0.8802999973297119\n",
      "Training at step=71, batch=0, train loss = 0.26553091406822205, train acc = 0.8899999856948853, time = 0.9358952045440674\n",
      "Training at step=71, batch=30, train loss = 0.20416881144046783, train acc = 0.9049999713897705, time = 0.9421217441558838\n",
      "Training at step=71, batch=60, train loss = 0.24206075072288513, train acc = 0.9150000214576721, time = 0.9335887432098389\n",
      "Training at step=71, batch=90, train loss = 0.21971628069877625, train acc = 0.9100000262260437, time = 0.9438180923461914\n",
      "Training at step=71, batch=120, train loss = 0.23933525383472443, train acc = 0.9049999713897705, time = 0.9557104110717773\n",
      "Training at step=71, batch=150, train loss = 0.267619788646698, train acc = 0.9049999713897705, time = 0.9360020160675049\n",
      "Training at step=71, batch=180, train loss = 0.17787308990955353, train acc = 0.9399999976158142, time = 0.93682861328125\n",
      "Training at step=71, batch=210, train loss = 0.1460665464401245, train acc = 0.9350000023841858, time = 0.9444146156311035\n",
      "Training at step=71, batch=240, train loss = 0.2627521753311157, train acc = 0.8849999904632568, time = 0.9379651546478271\n",
      "Training at step=71, batch=270, train loss = 0.26160070300102234, train acc = 0.8849999904632568, time = 0.9367811679840088\n",
      "Testing at step=71, batch=0, test loss = 0.3886040449142456, test acc = 0.8700000047683716, time = 0.3470792770385742\n",
      "Testing at step=71, batch=5, test loss = 0.31436505913734436, test acc = 0.8849999904632568, time = 0.34396839141845703\n",
      "Testing at step=71, batch=10, test loss = 0.31259337067604065, test acc = 0.8949999809265137, time = 0.35269594192504883\n",
      "Testing at step=71, batch=15, test loss = 0.33906757831573486, test acc = 0.8799999952316284, time = 0.34232378005981445\n",
      "Testing at step=71, batch=20, test loss = 0.369086354970932, test acc = 0.8550000190734863, time = 0.3526132106781006\n",
      "Testing at step=71, batch=25, test loss = 0.2571870982646942, test acc = 0.9150000214576721, time = 0.346362829208374\n",
      "Testing at step=71, batch=30, test loss = 0.33191463351249695, test acc = 0.8899999856948853, time = 0.3450915813446045\n",
      "Testing at step=71, batch=35, test loss = 0.2927018105983734, test acc = 0.875, time = 0.35019779205322266\n",
      "Testing at step=71, batch=40, test loss = 0.31110066175460815, test acc = 0.8799999952316284, time = 0.34625697135925293\n",
      "Testing at step=71, batch=45, test loss = 0.3412666618824005, test acc = 0.8999999761581421, time = 0.3466622829437256\n",
      "Step 71 finished in 309.9029235839844, Train loss = 0.2551029238353173, Test loss = 0.34621949136257174; Train Acc = 0.9061333318551381, Test Acc = 0.8796999967098236\n",
      "Training at step=72, batch=0, train loss = 0.18282857537269592, train acc = 0.949999988079071, time = 0.9469358921051025\n",
      "Training at step=72, batch=30, train loss = 0.3387378752231598, train acc = 0.875, time = 0.9418308734893799\n",
      "Training at step=72, batch=60, train loss = 0.24650664627552032, train acc = 0.8999999761581421, time = 0.9450364112854004\n",
      "Training at step=72, batch=90, train loss = 0.22778618335723877, train acc = 0.8849999904632568, time = 0.9288465976715088\n",
      "Training at step=72, batch=120, train loss = 0.16024862229824066, train acc = 0.9599999785423279, time = 0.9419493675231934\n",
      "Training at step=72, batch=150, train loss = 0.2294521927833557, train acc = 0.9300000071525574, time = 0.9522359371185303\n",
      "Training at step=72, batch=180, train loss = 0.2400606870651245, train acc = 0.9150000214576721, time = 0.9410650730133057\n",
      "Training at step=72, batch=210, train loss = 0.20143380761146545, train acc = 0.9350000023841858, time = 0.9364759922027588\n",
      "Training at step=72, batch=240, train loss = 0.1792183667421341, train acc = 0.949999988079071, time = 0.939232349395752\n",
      "Training at step=72, batch=270, train loss = 0.19018328189849854, train acc = 0.9350000023841858, time = 0.9347496032714844\n",
      "Testing at step=72, batch=0, test loss = 0.39462172985076904, test acc = 0.8999999761581421, time = 0.34642744064331055\n",
      "Testing at step=72, batch=5, test loss = 0.34467777609825134, test acc = 0.8849999904632568, time = 0.34109973907470703\n",
      "Testing at step=72, batch=10, test loss = 0.3374679684638977, test acc = 0.8700000047683716, time = 0.3446815013885498\n",
      "Testing at step=72, batch=15, test loss = 0.3212924599647522, test acc = 0.8700000047683716, time = 0.35327577590942383\n",
      "Testing at step=72, batch=20, test loss = 0.28377366065979004, test acc = 0.8999999761581421, time = 0.3477132320404053\n",
      "Testing at step=72, batch=25, test loss = 0.337062269449234, test acc = 0.8999999761581421, time = 0.3414266109466553\n",
      "Testing at step=72, batch=30, test loss = 0.3343551754951477, test acc = 0.875, time = 0.34929919242858887\n",
      "Testing at step=72, batch=35, test loss = 0.36069047451019287, test acc = 0.8550000190734863, time = 0.34003591537475586\n",
      "Testing at step=72, batch=40, test loss = 0.33288052678108215, test acc = 0.8700000047683716, time = 0.34597039222717285\n",
      "Testing at step=72, batch=45, test loss = 0.34946390986442566, test acc = 0.8650000095367432, time = 0.34828853607177734\n",
      "Step 72 finished in 310.11721563339233, Train loss = 0.25479623839259147, Test loss = 0.3609954378008842; Train Acc = 0.9058833322922388, Test Acc = 0.8740999984741211\n",
      "Training at step=73, batch=0, train loss = 0.26265814900398254, train acc = 0.9049999713897705, time = 0.9433836936950684\n",
      "Training at step=73, batch=30, train loss = 0.1924283355474472, train acc = 0.925000011920929, time = 0.9410338401794434\n",
      "Training at step=73, batch=60, train loss = 0.26541611552238464, train acc = 0.8899999856948853, time = 0.9426684379577637\n",
      "Training at step=73, batch=90, train loss = 0.30211329460144043, train acc = 0.8949999809265137, time = 0.933495283126831\n",
      "Training at step=73, batch=120, train loss = 0.26775145530700684, train acc = 0.8849999904632568, time = 0.9389331340789795\n",
      "Training at step=73, batch=150, train loss = 0.2808194160461426, train acc = 0.8999999761581421, time = 0.9424688816070557\n",
      "Training at step=73, batch=180, train loss = 0.20958587527275085, train acc = 0.9150000214576721, time = 0.9354157447814941\n",
      "Training at step=73, batch=210, train loss = 0.23738184571266174, train acc = 0.9100000262260437, time = 0.9508240222930908\n",
      "Training at step=73, batch=240, train loss = 0.35385075211524963, train acc = 0.8899999856948853, time = 0.9406962394714355\n",
      "Training at step=73, batch=270, train loss = 0.2315337359905243, train acc = 0.9200000166893005, time = 0.9344155788421631\n",
      "Testing at step=73, batch=0, test loss = 0.28945192694664, test acc = 0.8999999761581421, time = 0.3444972038269043\n",
      "Testing at step=73, batch=5, test loss = 0.23149321973323822, test acc = 0.8999999761581421, time = 0.3420255184173584\n",
      "Testing at step=73, batch=10, test loss = 0.2756692171096802, test acc = 0.925000011920929, time = 0.35344958305358887\n",
      "Testing at step=73, batch=15, test loss = 0.34237948060035706, test acc = 0.8849999904632568, time = 0.34416913986206055\n",
      "Testing at step=73, batch=20, test loss = 0.3637920320034027, test acc = 0.8550000190734863, time = 0.3453404903411865\n",
      "Testing at step=73, batch=25, test loss = 0.3779556155204773, test acc = 0.875, time = 0.3493361473083496\n",
      "Testing at step=73, batch=30, test loss = 0.33341968059539795, test acc = 0.8949999809265137, time = 0.34807372093200684\n",
      "Testing at step=73, batch=35, test loss = 0.2018168568611145, test acc = 0.9200000166893005, time = 0.34817981719970703\n",
      "Testing at step=73, batch=40, test loss = 0.4408102035522461, test acc = 0.875, time = 0.3589491844177246\n",
      "Testing at step=73, batch=45, test loss = 0.31488919258117676, test acc = 0.8949999809265137, time = 0.35805320739746094\n",
      "Step 73 finished in 310.96037316322327, Train loss = 0.2574270152052244, Test loss = 0.3490659454464912; Train Acc = 0.9033833328882853, Test Acc = 0.8795999979972839\n",
      "Training at step=74, batch=0, train loss = 0.33002594113349915, train acc = 0.8949999809265137, time = 0.9416112899780273\n",
      "Training at step=74, batch=30, train loss = 0.27946293354034424, train acc = 0.8999999761581421, time = 0.947824239730835\n",
      "Training at step=74, batch=60, train loss = 0.2943486273288727, train acc = 0.875, time = 0.9421000480651855\n",
      "Training at step=74, batch=90, train loss = 0.20789934694766998, train acc = 0.9449999928474426, time = 0.948495626449585\n",
      "Training at step=74, batch=120, train loss = 0.29981765151023865, train acc = 0.875, time = 0.9471909999847412\n",
      "Training at step=74, batch=150, train loss = 0.17838677763938904, train acc = 0.925000011920929, time = 0.9354214668273926\n",
      "Training at step=74, batch=180, train loss = 0.21755893528461456, train acc = 0.9200000166893005, time = 0.9396274089813232\n",
      "Training at step=74, batch=210, train loss = 0.2540583908557892, train acc = 0.8949999809265137, time = 0.9375331401824951\n",
      "Training at step=74, batch=240, train loss = 0.2879626154899597, train acc = 0.9100000262260437, time = 0.9451160430908203\n",
      "Training at step=74, batch=270, train loss = 0.24845951795578003, train acc = 0.9200000166893005, time = 0.9385263919830322\n",
      "Testing at step=74, batch=0, test loss = 0.29704150557518005, test acc = 0.8849999904632568, time = 0.3606545925140381\n",
      "Testing at step=74, batch=5, test loss = 0.41340380907058716, test acc = 0.8600000143051147, time = 0.34462833404541016\n",
      "Testing at step=74, batch=10, test loss = 0.3513490557670593, test acc = 0.8949999809265137, time = 0.36170315742492676\n",
      "Testing at step=74, batch=15, test loss = 0.46589264273643494, test acc = 0.824999988079071, time = 0.3498659133911133\n",
      "Testing at step=74, batch=20, test loss = 0.40023764967918396, test acc = 0.8500000238418579, time = 0.3510894775390625\n",
      "Testing at step=74, batch=25, test loss = 0.3291749656200409, test acc = 0.9049999713897705, time = 0.35286498069763184\n",
      "Testing at step=74, batch=30, test loss = 0.34755760431289673, test acc = 0.8999999761581421, time = 0.360368013381958\n",
      "Testing at step=74, batch=35, test loss = 0.35295069217681885, test acc = 0.8700000047683716, time = 0.3477766513824463\n",
      "Testing at step=74, batch=40, test loss = 0.39093920588493347, test acc = 0.8550000190734863, time = 0.3621516227722168\n",
      "Testing at step=74, batch=45, test loss = 0.43726956844329834, test acc = 0.8550000190734863, time = 0.3590726852416992\n",
      "Step 74 finished in 310.6374137401581, Train loss = 0.2533765410383542, Test loss = 0.36209674924612045; Train Acc = 0.9064833301305771, Test Acc = 0.8760999989509582\n",
      "Training at step=75, batch=0, train loss = 0.30113112926483154, train acc = 0.8799999952316284, time = 0.9357943534851074\n",
      "Training at step=75, batch=30, train loss = 0.23470638692378998, train acc = 0.9100000262260437, time = 0.9451422691345215\n",
      "Training at step=75, batch=60, train loss = 0.19617822766304016, train acc = 0.9350000023841858, time = 0.937903881072998\n",
      "Training at step=75, batch=90, train loss = 0.21614427864551544, train acc = 0.9100000262260437, time = 0.935920000076294\n",
      "Training at step=75, batch=120, train loss = 0.3288331925868988, train acc = 0.8799999952316284, time = 0.9414582252502441\n",
      "Training at step=75, batch=150, train loss = 0.2435695230960846, train acc = 0.925000011920929, time = 0.9349396228790283\n",
      "Training at step=75, batch=180, train loss = 0.2949429750442505, train acc = 0.8849999904632568, time = 0.9425575733184814\n",
      "Training at step=75, batch=210, train loss = 0.31327155232429504, train acc = 0.8849999904632568, time = 0.9460616111755371\n",
      "Training at step=75, batch=240, train loss = 0.3588188588619232, train acc = 0.8949999809265137, time = 0.9511005878448486\n",
      "Training at step=75, batch=270, train loss = 0.3319547176361084, train acc = 0.8899999856948853, time = 0.9550859928131104\n",
      "Testing at step=75, batch=0, test loss = 0.3876454830169678, test acc = 0.8550000190734863, time = 0.36034488677978516\n",
      "Testing at step=75, batch=5, test loss = 0.3361091613769531, test acc = 0.9049999713897705, time = 0.35712766647338867\n",
      "Testing at step=75, batch=10, test loss = 0.38120952248573303, test acc = 0.8600000143051147, time = 0.3612837791442871\n",
      "Testing at step=75, batch=15, test loss = 0.27888309955596924, test acc = 0.8899999856948853, time = 0.3565549850463867\n",
      "Testing at step=75, batch=20, test loss = 0.42493751645088196, test acc = 0.8500000238418579, time = 0.344409704208374\n",
      "Testing at step=75, batch=25, test loss = 0.3245149850845337, test acc = 0.8849999904632568, time = 0.33957862854003906\n",
      "Testing at step=75, batch=30, test loss = 0.29958638548851013, test acc = 0.8849999904632568, time = 0.35921716690063477\n",
      "Testing at step=75, batch=35, test loss = 0.30143389105796814, test acc = 0.875, time = 0.361116886138916\n",
      "Testing at step=75, batch=40, test loss = 0.3732817471027374, test acc = 0.8600000143051147, time = 0.3434152603149414\n",
      "Testing at step=75, batch=45, test loss = 0.31578439474105835, test acc = 0.8799999952316284, time = 0.34223008155822754\n",
      "Step 75 finished in 310.53839445114136, Train loss = 0.25552820359667144, Test loss = 0.3543082639575005; Train Acc = 0.9051999992132187, Test Acc = 0.8768999981880188\n",
      "Training at step=76, batch=0, train loss = 0.21842752397060394, train acc = 0.9100000262260437, time = 0.9373564720153809\n",
      "Training at step=76, batch=30, train loss = 0.18968087434768677, train acc = 0.9200000166893005, time = 0.9326333999633789\n",
      "Training at step=76, batch=60, train loss = 0.22822101414203644, train acc = 0.9200000166893005, time = 0.9446988105773926\n",
      "Training at step=76, batch=90, train loss = 0.19719882309436798, train acc = 0.9350000023841858, time = 0.9524474143981934\n",
      "Training at step=76, batch=120, train loss = 0.2727763056755066, train acc = 0.8999999761581421, time = 0.9408679008483887\n",
      "Training at step=76, batch=150, train loss = 0.2602294087409973, train acc = 0.8999999761581421, time = 0.9349138736724854\n",
      "Training at step=76, batch=180, train loss = 0.25359928607940674, train acc = 0.8999999761581421, time = 0.9373574256896973\n",
      "Training at step=76, batch=210, train loss = 0.21234014630317688, train acc = 0.925000011920929, time = 0.9377973079681396\n",
      "Training at step=76, batch=240, train loss = 0.3526807427406311, train acc = 0.8650000095367432, time = 0.9439353942871094\n",
      "Training at step=76, batch=270, train loss = 0.2052120566368103, train acc = 0.9100000262260437, time = 0.9617230892181396\n",
      "Testing at step=76, batch=0, test loss = 0.4615200459957123, test acc = 0.8600000143051147, time = 0.35603952407836914\n",
      "Testing at step=76, batch=5, test loss = 0.29227781295776367, test acc = 0.9049999713897705, time = 0.35858607292175293\n",
      "Testing at step=76, batch=10, test loss = 0.4731382727622986, test acc = 0.8299999833106995, time = 0.3463606834411621\n",
      "Testing at step=76, batch=15, test loss = 0.34135276079177856, test acc = 0.8899999856948853, time = 0.3576624393463135\n",
      "Testing at step=76, batch=20, test loss = 0.33967429399490356, test acc = 0.8650000095367432, time = 0.34108614921569824\n",
      "Testing at step=76, batch=25, test loss = 0.3516358435153961, test acc = 0.8700000047683716, time = 0.3500211238861084\n",
      "Testing at step=76, batch=30, test loss = 0.24167652428150177, test acc = 0.8999999761581421, time = 0.3483874797821045\n",
      "Testing at step=76, batch=35, test loss = 0.3543461561203003, test acc = 0.8899999856948853, time = 0.3609042167663574\n",
      "Testing at step=76, batch=40, test loss = 0.295246422290802, test acc = 0.9049999713897705, time = 0.347337007522583\n",
      "Testing at step=76, batch=45, test loss = 0.46900084614753723, test acc = 0.8500000238418579, time = 0.35373783111572266\n",
      "Step 76 finished in 310.94253873825073, Train loss = 0.25226957152287166, Test loss = 0.3652080434560776; Train Acc = 0.9065666651725769, Test Acc = 0.8748999977111817\n",
      "Training at step=77, batch=0, train loss = 0.22770079970359802, train acc = 0.925000011920929, time = 0.9412531852722168\n",
      "Training at step=77, batch=30, train loss = 0.2925375699996948, train acc = 0.8899999856948853, time = 0.9337146282196045\n",
      "Training at step=77, batch=60, train loss = 0.1667155772447586, train acc = 0.9300000071525574, time = 0.9452705383300781\n",
      "Training at step=77, batch=90, train loss = 0.22718818485736847, train acc = 0.925000011920929, time = 0.9334964752197266\n",
      "Training at step=77, batch=120, train loss = 0.3078485131263733, train acc = 0.8799999952316284, time = 0.9390392303466797\n",
      "Training at step=77, batch=150, train loss = 0.2401394098997116, train acc = 0.8949999809265137, time = 0.9431207180023193\n",
      "Training at step=77, batch=180, train loss = 0.30879807472229004, train acc = 0.8899999856948853, time = 0.9500036239624023\n",
      "Training at step=77, batch=210, train loss = 0.27287328243255615, train acc = 0.9100000262260437, time = 0.9389162063598633\n",
      "Training at step=77, batch=240, train loss = 0.27266165614128113, train acc = 0.8899999856948853, time = 0.9575636386871338\n",
      "Training at step=77, batch=270, train loss = 0.32813510298728943, train acc = 0.875, time = 0.9404945373535156\n",
      "Testing at step=77, batch=0, test loss = 0.3205874562263489, test acc = 0.8600000143051147, time = 0.34252333641052246\n",
      "Testing at step=77, batch=5, test loss = 0.3242834508419037, test acc = 0.8899999856948853, time = 0.34532713890075684\n",
      "Testing at step=77, batch=10, test loss = 0.4159854054450989, test acc = 0.8700000047683716, time = 0.34859633445739746\n",
      "Testing at step=77, batch=15, test loss = 0.44889310002326965, test acc = 0.8650000095367432, time = 0.34955859184265137\n",
      "Testing at step=77, batch=20, test loss = 0.34779202938079834, test acc = 0.8849999904632568, time = 0.36226725578308105\n",
      "Testing at step=77, batch=25, test loss = 0.2829045057296753, test acc = 0.9100000262260437, time = 0.3495299816131592\n",
      "Testing at step=77, batch=30, test loss = 0.3138287663459778, test acc = 0.9049999713897705, time = 0.35011982917785645\n",
      "Testing at step=77, batch=35, test loss = 0.2526347041130066, test acc = 0.9049999713897705, time = 0.35318756103515625\n",
      "Testing at step=77, batch=40, test loss = 0.32566696405410767, test acc = 0.8949999809265137, time = 0.35007548332214355\n",
      "Testing at step=77, batch=45, test loss = 0.3716019093990326, test acc = 0.8849999904632568, time = 0.3423800468444824\n",
      "Step 77 finished in 310.42871260643005, Train loss = 0.25052619566520057, Test loss = 0.3483204621076584; Train Acc = 0.9074333322048187, Test Acc = 0.8768999993801116\n",
      "Training at step=78, batch=0, train loss = 0.18311548233032227, train acc = 0.9449999928474426, time = 0.9409408569335938\n",
      "Training at step=78, batch=30, train loss = 0.25164663791656494, train acc = 0.9150000214576721, time = 0.9513318538665771\n",
      "Training at step=78, batch=60, train loss = 0.24889902770519257, train acc = 0.9100000262260437, time = 0.9471626281738281\n",
      "Training at step=78, batch=90, train loss = 0.24651235342025757, train acc = 0.9049999713897705, time = 0.9478180408477783\n",
      "Training at step=78, batch=120, train loss = 0.2514054775238037, train acc = 0.8999999761581421, time = 0.9355592727661133\n",
      "Training at step=78, batch=150, train loss = 0.2851509749889374, train acc = 0.8949999809265137, time = 0.9470207691192627\n",
      "Training at step=78, batch=180, train loss = 0.1983157992362976, train acc = 0.9399999976158142, time = 0.9553852081298828\n",
      "Training at step=78, batch=210, train loss = 0.26767125725746155, train acc = 0.8999999761581421, time = 0.9524576663970947\n",
      "Training at step=78, batch=240, train loss = 0.2899722158908844, train acc = 0.9100000262260437, time = 0.9379100799560547\n",
      "Training at step=78, batch=270, train loss = 0.27630025148391724, train acc = 0.925000011920929, time = 0.9378547668457031\n",
      "Testing at step=78, batch=0, test loss = 0.32933616638183594, test acc = 0.8799999952316284, time = 0.3423647880554199\n",
      "Testing at step=78, batch=5, test loss = 0.3273125886917114, test acc = 0.8849999904632568, time = 0.34286999702453613\n",
      "Testing at step=78, batch=10, test loss = 0.3428242802619934, test acc = 0.8899999856948853, time = 0.3465101718902588\n",
      "Testing at step=78, batch=15, test loss = 0.2713264226913452, test acc = 0.9100000262260437, time = 0.3428199291229248\n",
      "Testing at step=78, batch=20, test loss = 0.32261136174201965, test acc = 0.8799999952316284, time = 0.34288811683654785\n",
      "Testing at step=78, batch=25, test loss = 0.43014276027679443, test acc = 0.8500000238418579, time = 0.3496379852294922\n",
      "Testing at step=78, batch=30, test loss = 0.3974718451499939, test acc = 0.8650000095367432, time = 0.34589672088623047\n",
      "Testing at step=78, batch=35, test loss = 0.3069472014904022, test acc = 0.8799999952316284, time = 0.34792304039001465\n",
      "Testing at step=78, batch=40, test loss = 0.3831300735473633, test acc = 0.8949999809265137, time = 0.3422222137451172\n",
      "Testing at step=78, batch=45, test loss = 0.43637165427207947, test acc = 0.8450000286102295, time = 0.34705090522766113\n",
      "Step 78 finished in 310.5300221443176, Train loss = 0.2507234446704388, Test loss = 0.35024738222360613; Train Acc = 0.9073666667938233, Test Acc = 0.8778000020980835\n",
      "Training at step=79, batch=0, train loss = 0.17963670194149017, train acc = 0.9150000214576721, time = 0.9450702667236328\n",
      "Training at step=79, batch=30, train loss = 0.21667176485061646, train acc = 0.8949999809265137, time = 0.9365668296813965\n",
      "Training at step=79, batch=60, train loss = 0.27280527353286743, train acc = 0.8949999809265137, time = 0.9412565231323242\n",
      "Training at step=79, batch=90, train loss = 0.17647594213485718, train acc = 0.9350000023841858, time = 0.9307870864868164\n",
      "Training at step=79, batch=120, train loss = 0.23888713121414185, train acc = 0.9100000262260437, time = 0.9398672580718994\n",
      "Training at step=79, batch=150, train loss = 0.2082173377275467, train acc = 0.9049999713897705, time = 0.937328577041626\n",
      "Training at step=79, batch=180, train loss = 0.28929436206817627, train acc = 0.8899999856948853, time = 0.9445803165435791\n",
      "Training at step=79, batch=210, train loss = 0.3036613464355469, train acc = 0.8849999904632568, time = 0.94134521484375\n",
      "Training at step=79, batch=240, train loss = 0.25274142622947693, train acc = 0.8999999761581421, time = 0.9330868721008301\n",
      "Training at step=79, batch=270, train loss = 0.35727423429489136, train acc = 0.8899999856948853, time = 0.9347860813140869\n",
      "Testing at step=79, batch=0, test loss = 0.3229534924030304, test acc = 0.875, time = 0.34375762939453125\n",
      "Testing at step=79, batch=5, test loss = 0.33267146348953247, test acc = 0.8849999904632568, time = 0.3473498821258545\n",
      "Testing at step=79, batch=10, test loss = 0.3792002499103546, test acc = 0.8899999856948853, time = 0.3492414951324463\n",
      "Testing at step=79, batch=15, test loss = 0.342195063829422, test acc = 0.8899999856948853, time = 0.35124707221984863\n",
      "Testing at step=79, batch=20, test loss = 0.3636974096298218, test acc = 0.8650000095367432, time = 0.3493311405181885\n",
      "Testing at step=79, batch=25, test loss = 0.4603421092033386, test acc = 0.8299999833106995, time = 0.3433268070220947\n",
      "Testing at step=79, batch=30, test loss = 0.4223652780056, test acc = 0.8399999737739563, time = 0.34899067878723145\n",
      "Testing at step=79, batch=35, test loss = 0.34865638613700867, test acc = 0.8700000047683716, time = 0.34800028800964355\n",
      "Testing at step=79, batch=40, test loss = 0.3635294735431671, test acc = 0.8849999904632568, time = 0.35822081565856934\n",
      "Testing at step=79, batch=45, test loss = 0.31770801544189453, test acc = 0.8650000095367432, time = 0.35111069679260254\n",
      "Step 79 finished in 310.2443037033081, Train loss = 0.24988698961834113, Test loss = 0.35070867359638214; Train Acc = 0.9073333334922791, Test Acc = 0.8787000012397767\n",
      "Training at step=80, batch=0, train loss = 0.29841306805610657, train acc = 0.8799999952316284, time = 0.9564049243927002\n",
      "Training at step=80, batch=30, train loss = 0.18495997786521912, train acc = 0.9449999928474426, time = 0.9353530406951904\n",
      "Training at step=80, batch=60, train loss = 0.23943102359771729, train acc = 0.9150000214576721, time = 0.9534437656402588\n",
      "Training at step=80, batch=90, train loss = 0.23170314729213715, train acc = 0.8949999809265137, time = 0.9331386089324951\n",
      "Training at step=80, batch=120, train loss = 0.19422543048858643, train acc = 0.9150000214576721, time = 0.9484703540802002\n",
      "Training at step=80, batch=150, train loss = 0.25587236881256104, train acc = 0.8949999809265137, time = 0.9385392665863037\n",
      "Training at step=80, batch=180, train loss = 0.17687390744686127, train acc = 0.9200000166893005, time = 0.9387905597686768\n",
      "Training at step=80, batch=210, train loss = 0.3760639429092407, train acc = 0.8899999856948853, time = 0.9513423442840576\n",
      "Training at step=80, batch=240, train loss = 0.34779879450798035, train acc = 0.8550000190734863, time = 0.9514889717102051\n",
      "Training at step=80, batch=270, train loss = 0.28261029720306396, train acc = 0.8949999809265137, time = 0.9367775917053223\n",
      "Testing at step=80, batch=0, test loss = 0.3506949245929718, test acc = 0.8650000095367432, time = 0.3557734489440918\n",
      "Testing at step=80, batch=5, test loss = 0.2552453279495239, test acc = 0.9150000214576721, time = 0.35504817962646484\n",
      "Testing at step=80, batch=10, test loss = 0.4542754292488098, test acc = 0.8299999833106995, time = 0.3611776828765869\n",
      "Testing at step=80, batch=15, test loss = 0.39599528908729553, test acc = 0.8999999761581421, time = 0.36533308029174805\n",
      "Testing at step=80, batch=20, test loss = 0.30961111187934875, test acc = 0.8899999856948853, time = 0.34952831268310547\n",
      "Testing at step=80, batch=25, test loss = 0.4725196957588196, test acc = 0.8650000095367432, time = 0.35232114791870117\n",
      "Testing at step=80, batch=30, test loss = 0.33683452010154724, test acc = 0.8849999904632568, time = 0.3492412567138672\n",
      "Testing at step=80, batch=35, test loss = 0.29092276096343994, test acc = 0.9150000214576721, time = 0.3436439037322998\n",
      "Testing at step=80, batch=40, test loss = 0.3346634805202484, test acc = 0.875, time = 0.34839415550231934\n",
      "Testing at step=80, batch=45, test loss = 0.3000149130821228, test acc = 0.8999999761581421, time = 0.3508632183074951\n",
      "Step 80 finished in 310.92657470703125, Train loss = 0.2502220905323823, Test loss = 0.3666371750831604; Train Acc = 0.9072833339373271, Test Acc = 0.8748999977111817\n",
      "Training at step=81, batch=0, train loss = 0.22846662998199463, train acc = 0.9100000262260437, time = 0.9488973617553711\n",
      "Training at step=81, batch=30, train loss = 0.22742228209972382, train acc = 0.9150000214576721, time = 0.9397392272949219\n",
      "Training at step=81, batch=60, train loss = 0.23759090900421143, train acc = 0.9200000166893005, time = 0.9361550807952881\n",
      "Training at step=81, batch=90, train loss = 0.20588958263397217, train acc = 0.8949999809265137, time = 0.9476485252380371\n",
      "Training at step=81, batch=120, train loss = 0.3472427725791931, train acc = 0.8550000190734863, time = 0.9580535888671875\n",
      "Training at step=81, batch=150, train loss = 0.20954178273677826, train acc = 0.9449999928474426, time = 0.9409940242767334\n",
      "Training at step=81, batch=180, train loss = 0.30722567439079285, train acc = 0.8799999952316284, time = 0.9364104270935059\n",
      "Training at step=81, batch=210, train loss = 0.2885093092918396, train acc = 0.8949999809265137, time = 0.9516146183013916\n",
      "Training at step=81, batch=240, train loss = 0.3044711947441101, train acc = 0.8949999809265137, time = 0.9345641136169434\n",
      "Training at step=81, batch=270, train loss = 0.30024290084838867, train acc = 0.8949999809265137, time = 0.9368042945861816\n",
      "Testing at step=81, batch=0, test loss = 0.28793996572494507, test acc = 0.9150000214576721, time = 0.3457510471343994\n",
      "Testing at step=81, batch=5, test loss = 0.38036099076271057, test acc = 0.8550000190734863, time = 0.3453996181488037\n",
      "Testing at step=81, batch=10, test loss = 0.3505571484565735, test acc = 0.9150000214576721, time = 0.35542821884155273\n",
      "Testing at step=81, batch=15, test loss = 0.4435044229030609, test acc = 0.8500000238418579, time = 0.3426949977874756\n",
      "Testing at step=81, batch=20, test loss = 0.3451980650424957, test acc = 0.8999999761581421, time = 0.34885668754577637\n",
      "Testing at step=81, batch=25, test loss = 0.2798628509044647, test acc = 0.8949999809265137, time = 0.34358882904052734\n",
      "Testing at step=81, batch=30, test loss = 0.25577589869499207, test acc = 0.8899999856948853, time = 0.3473052978515625\n",
      "Testing at step=81, batch=35, test loss = 0.24300144612789154, test acc = 0.9049999713897705, time = 0.3435349464416504\n",
      "Testing at step=81, batch=40, test loss = 0.34175464510917664, test acc = 0.8600000143051147, time = 0.3455326557159424\n",
      "Testing at step=81, batch=45, test loss = 0.41452479362487793, test acc = 0.8500000238418579, time = 0.3481118679046631\n",
      "Step 81 finished in 310.08652687072754, Train loss = 0.24803344508012135, Test loss = 0.34898538887500763; Train Acc = 0.9071500007311503, Test Acc = 0.8832000005245209\n",
      "Training at step=82, batch=0, train loss = 0.2742427885532379, train acc = 0.925000011920929, time = 0.9385998249053955\n",
      "Training at step=82, batch=30, train loss = 0.23377186059951782, train acc = 0.8999999761581421, time = 0.9392642974853516\n",
      "Training at step=82, batch=60, train loss = 0.28060856461524963, train acc = 0.8650000095367432, time = 0.9474856853485107\n",
      "Training at step=82, batch=90, train loss = 0.19535726308822632, train acc = 0.9200000166893005, time = 0.9256000518798828\n",
      "Training at step=82, batch=120, train loss = 0.2292855978012085, train acc = 0.9150000214576721, time = 0.9245538711547852\n",
      "Training at step=82, batch=150, train loss = 0.23248344659805298, train acc = 0.9200000166893005, time = 0.9326763153076172\n",
      "Training at step=82, batch=180, train loss = 0.2388090342283249, train acc = 0.9049999713897705, time = 0.9528036117553711\n",
      "Training at step=82, batch=210, train loss = 0.20969977974891663, train acc = 0.925000011920929, time = 0.9373331069946289\n",
      "Training at step=82, batch=240, train loss = 0.22640018165111542, train acc = 0.925000011920929, time = 0.9492220878601074\n",
      "Training at step=82, batch=270, train loss = 0.19447502493858337, train acc = 0.9300000071525574, time = 0.942488431930542\n",
      "Testing at step=82, batch=0, test loss = 0.3281891345977783, test acc = 0.875, time = 0.35147595405578613\n",
      "Testing at step=82, batch=5, test loss = 0.3001512289047241, test acc = 0.8700000047683716, time = 0.3465607166290283\n",
      "Testing at step=82, batch=10, test loss = 0.4422336518764496, test acc = 0.8550000190734863, time = 0.343585729598999\n",
      "Testing at step=82, batch=15, test loss = 0.36660727858543396, test acc = 0.8600000143051147, time = 0.3506941795349121\n",
      "Testing at step=82, batch=20, test loss = 0.3537672460079193, test acc = 0.875, time = 0.34395933151245117\n",
      "Testing at step=82, batch=25, test loss = 0.320966899394989, test acc = 0.8999999761581421, time = 0.34508657455444336\n",
      "Testing at step=82, batch=30, test loss = 0.3005307912826538, test acc = 0.8650000095367432, time = 0.34575629234313965\n",
      "Testing at step=82, batch=35, test loss = 0.32568037509918213, test acc = 0.8849999904632568, time = 0.34206676483154297\n",
      "Testing at step=82, batch=40, test loss = 0.43599167466163635, test acc = 0.8600000143051147, time = 0.3420846462249756\n",
      "Testing at step=82, batch=45, test loss = 0.47067123651504517, test acc = 0.824999988079071, time = 0.34132909774780273\n",
      "Step 82 finished in 309.2546739578247, Train loss = 0.24585561588406563, Test loss = 0.3571888503432274; Train Acc = 0.9082333326339722, Test Acc = 0.8741999995708466\n",
      "Training at step=83, batch=0, train loss = 0.2220321148633957, train acc = 0.925000011920929, time = 0.9444026947021484\n",
      "Training at step=83, batch=30, train loss = 0.2201000154018402, train acc = 0.9150000214576721, time = 0.9420435428619385\n",
      "Training at step=83, batch=60, train loss = 0.25130248069763184, train acc = 0.9100000262260437, time = 0.9420592784881592\n",
      "Training at step=83, batch=90, train loss = 0.1800127625465393, train acc = 0.9350000023841858, time = 0.9360146522521973\n",
      "Training at step=83, batch=120, train loss = 0.16290563344955444, train acc = 0.9350000023841858, time = 0.9465372562408447\n",
      "Training at step=83, batch=150, train loss = 0.21453985571861267, train acc = 0.9350000023841858, time = 0.9467833042144775\n",
      "Training at step=83, batch=180, train loss = 0.2675515115261078, train acc = 0.8949999809265137, time = 0.9413175582885742\n",
      "Training at step=83, batch=210, train loss = 0.22838488221168518, train acc = 0.9200000166893005, time = 0.9371378421783447\n",
      "Training at step=83, batch=240, train loss = 0.20744414627552032, train acc = 0.9200000166893005, time = 0.9460413455963135\n",
      "Training at step=83, batch=270, train loss = 0.25875455141067505, train acc = 0.8999999761581421, time = 0.9336161613464355\n",
      "Testing at step=83, batch=0, test loss = 0.43201950192451477, test acc = 0.8399999737739563, time = 0.34831857681274414\n",
      "Testing at step=83, batch=5, test loss = 0.31152158975601196, test acc = 0.8849999904632568, time = 0.3494696617126465\n",
      "Testing at step=83, batch=10, test loss = 0.33172711730003357, test acc = 0.8650000095367432, time = 0.34847116470336914\n",
      "Testing at step=83, batch=15, test loss = 0.3600350320339203, test acc = 0.8500000238418579, time = 0.34795093536376953\n",
      "Testing at step=83, batch=20, test loss = 0.33268192410469055, test acc = 0.875, time = 0.34403300285339355\n",
      "Testing at step=83, batch=25, test loss = 0.41082626581192017, test acc = 0.8700000047683716, time = 0.3460366725921631\n",
      "Testing at step=83, batch=30, test loss = 0.3813728094100952, test acc = 0.875, time = 0.35019516944885254\n",
      "Testing at step=83, batch=35, test loss = 0.24728095531463623, test acc = 0.925000011920929, time = 0.341874361038208\n",
      "Testing at step=83, batch=40, test loss = 0.3528806269168854, test acc = 0.8849999904632568, time = 0.34192824363708496\n",
      "Testing at step=83, batch=45, test loss = 0.4040033221244812, test acc = 0.8450000286102295, time = 0.3514218330383301\n",
      "Step 83 finished in 310.1585786342621, Train loss = 0.24577059745788574, Test loss = 0.3615789583325386; Train Acc = 0.9090000009536743, Test Acc = 0.8749999988079071\n",
      "Training at step=84, batch=0, train loss = 0.2207534909248352, train acc = 0.9200000166893005, time = 0.9561030864715576\n",
      "Training at step=84, batch=30, train loss = 0.19246019423007965, train acc = 0.9399999976158142, time = 0.9632048606872559\n",
      "Training at step=84, batch=60, train loss = 0.22977428138256073, train acc = 0.9100000262260437, time = 0.9523978233337402\n",
      "Training at step=84, batch=90, train loss = 0.2724328637123108, train acc = 0.8999999761581421, time = 0.9402759075164795\n",
      "Training at step=84, batch=120, train loss = 0.22234433889389038, train acc = 0.925000011920929, time = 0.9565784931182861\n",
      "Training at step=84, batch=150, train loss = 0.2698439359664917, train acc = 0.9049999713897705, time = 0.9355316162109375\n",
      "Training at step=84, batch=180, train loss = 0.22439983487129211, train acc = 0.9200000166893005, time = 0.9381914138793945\n",
      "Training at step=84, batch=210, train loss = 0.19641482830047607, train acc = 0.925000011920929, time = 0.9481146335601807\n",
      "Training at step=84, batch=240, train loss = 0.3436235189437866, train acc = 0.8799999952316284, time = 0.9378170967102051\n",
      "Training at step=84, batch=270, train loss = 0.2523968517780304, train acc = 0.9100000262260437, time = 0.9472453594207764\n",
      "Testing at step=84, batch=0, test loss = 0.39916637539863586, test acc = 0.8550000190734863, time = 0.3450593948364258\n",
      "Testing at step=84, batch=5, test loss = 0.3898099958896637, test acc = 0.8700000047683716, time = 0.3505527973175049\n",
      "Testing at step=84, batch=10, test loss = 0.3188193738460541, test acc = 0.875, time = 0.36923766136169434\n",
      "Testing at step=84, batch=15, test loss = 0.3782377243041992, test acc = 0.8500000238418579, time = 0.3513202667236328\n",
      "Testing at step=84, batch=20, test loss = 0.38408592343330383, test acc = 0.8849999904632568, time = 0.3413505554199219\n",
      "Testing at step=84, batch=25, test loss = 0.4278492033481598, test acc = 0.8550000190734863, time = 0.35968613624572754\n",
      "Testing at step=84, batch=30, test loss = 0.40319985151290894, test acc = 0.8650000095367432, time = 0.3446519374847412\n",
      "Testing at step=84, batch=35, test loss = 0.47362220287323, test acc = 0.8299999833106995, time = 0.34899210929870605\n",
      "Testing at step=84, batch=40, test loss = 0.36614787578582764, test acc = 0.8949999809265137, time = 0.34749794006347656\n",
      "Testing at step=84, batch=45, test loss = 0.3522188663482666, test acc = 0.9049999713897705, time = 0.34713268280029297\n",
      "Step 84 finished in 310.3170621395111, Train loss = 0.24464779218037924, Test loss = 0.3718508929014206; Train Acc = 0.909066666563352, Test Acc = 0.876399998664856\n",
      "Training at step=85, batch=0, train loss = 0.23790611326694489, train acc = 0.9300000071525574, time = 0.9449257850646973\n",
      "Training at step=85, batch=30, train loss = 0.3413645625114441, train acc = 0.8600000143051147, time = 0.9466953277587891\n",
      "Training at step=85, batch=60, train loss = 0.19113671779632568, train acc = 0.9300000071525574, time = 0.9406735897064209\n",
      "Training at step=85, batch=90, train loss = 0.2165423184633255, train acc = 0.9399999976158142, time = 0.943885087966919\n",
      "Training at step=85, batch=120, train loss = 0.1955384463071823, train acc = 0.9449999928474426, time = 0.9384799003601074\n",
      "Training at step=85, batch=150, train loss = 0.2709609568119049, train acc = 0.8999999761581421, time = 0.9340603351593018\n",
      "Training at step=85, batch=180, train loss = 0.29004359245300293, train acc = 0.8799999952316284, time = 0.9460349082946777\n",
      "Training at step=85, batch=210, train loss = 0.1590748280286789, train acc = 0.9399999976158142, time = 0.9382679462432861\n",
      "Training at step=85, batch=240, train loss = 0.2581191658973694, train acc = 0.8849999904632568, time = 0.9436044692993164\n",
      "Training at step=85, batch=270, train loss = 0.2255088835954666, train acc = 0.9200000166893005, time = 0.9498107433319092\n",
      "Testing at step=85, batch=0, test loss = 0.3831036388874054, test acc = 0.8799999952316284, time = 0.35283756256103516\n",
      "Testing at step=85, batch=5, test loss = 0.3698369860649109, test acc = 0.8999999761581421, time = 0.3468048572540283\n",
      "Testing at step=85, batch=10, test loss = 0.2888094484806061, test acc = 0.9100000262260437, time = 0.3502473831176758\n",
      "Testing at step=85, batch=15, test loss = 0.3245694637298584, test acc = 0.8949999809265137, time = 0.35532045364379883\n",
      "Testing at step=85, batch=20, test loss = 0.4211503267288208, test acc = 0.875, time = 0.34923315048217773\n",
      "Testing at step=85, batch=25, test loss = 0.38636040687561035, test acc = 0.875, time = 0.34276843070983887\n",
      "Testing at step=85, batch=30, test loss = 0.3757416605949402, test acc = 0.8600000143051147, time = 0.3410451412200928\n",
      "Testing at step=85, batch=35, test loss = 0.31640806794166565, test acc = 0.8550000190734863, time = 0.3508005142211914\n",
      "Testing at step=85, batch=40, test loss = 0.2778801918029785, test acc = 0.8899999856948853, time = 0.3528022766113281\n",
      "Testing at step=85, batch=45, test loss = 0.33392414450645447, test acc = 0.875, time = 0.34392547607421875\n",
      "Step 85 finished in 310.81398725509644, Train loss = 0.24500166644652685, Test loss = 0.3485498321056366; Train Acc = 0.9099000014861425, Test Acc = 0.8810999965667725\n",
      "Training at step=86, batch=0, train loss = 0.3481638431549072, train acc = 0.875, time = 0.9355101585388184\n",
      "Training at step=86, batch=30, train loss = 0.242888405919075, train acc = 0.925000011920929, time = 0.9413723945617676\n",
      "Training at step=86, batch=60, train loss = 0.27455681562423706, train acc = 0.8799999952316284, time = 0.9311423301696777\n",
      "Training at step=86, batch=90, train loss = 0.16415733098983765, train acc = 0.9350000023841858, time = 0.9351439476013184\n",
      "Training at step=86, batch=120, train loss = 0.3331933915615082, train acc = 0.8899999856948853, time = 0.9405415058135986\n",
      "Training at step=86, batch=150, train loss = 0.2059669941663742, train acc = 0.9150000214576721, time = 0.9449360370635986\n",
      "Training at step=86, batch=180, train loss = 0.28995993733406067, train acc = 0.8949999809265137, time = 0.9421296119689941\n",
      "Training at step=86, batch=210, train loss = 0.23791836202144623, train acc = 0.9150000214576721, time = 0.9362735748291016\n",
      "Training at step=86, batch=240, train loss = 0.26988014578819275, train acc = 0.9100000262260437, time = 0.9538629055023193\n",
      "Training at step=86, batch=270, train loss = 0.24764680862426758, train acc = 0.9100000262260437, time = 0.939457893371582\n",
      "Testing at step=86, batch=0, test loss = 0.41794657707214355, test acc = 0.8550000190734863, time = 0.34432530403137207\n",
      "Testing at step=86, batch=5, test loss = 0.3718856871128082, test acc = 0.8799999952316284, time = 0.34391140937805176\n",
      "Testing at step=86, batch=10, test loss = 0.41719910502433777, test acc = 0.8500000238418579, time = 0.35732007026672363\n",
      "Testing at step=86, batch=15, test loss = 0.4160464406013489, test acc = 0.8899999856948853, time = 0.35468459129333496\n",
      "Testing at step=86, batch=20, test loss = 0.29757851362228394, test acc = 0.8949999809265137, time = 0.3601503372192383\n",
      "Testing at step=86, batch=25, test loss = 0.38799992203712463, test acc = 0.8500000238418579, time = 0.35741353034973145\n",
      "Testing at step=86, batch=30, test loss = 0.39130523800849915, test acc = 0.875, time = 0.3533060550689697\n",
      "Testing at step=86, batch=35, test loss = 0.3566357493400574, test acc = 0.875, time = 0.3473789691925049\n",
      "Testing at step=86, batch=40, test loss = 0.3989015221595764, test acc = 0.875, time = 0.35041332244873047\n",
      "Testing at step=86, batch=45, test loss = 0.5448562502861023, test acc = 0.8600000143051147, time = 0.3581817150115967\n",
      "Step 86 finished in 310.57124066352844, Train loss = 0.24684626509745916, Test loss = 0.35600104987621306; Train Acc = 0.9083833336830139, Test Acc = 0.8779000008106231\n",
      "Training at step=87, batch=0, train loss = 0.2986714541912079, train acc = 0.8899999856948853, time = 0.9362874031066895\n",
      "Training at step=87, batch=30, train loss = 0.2672916650772095, train acc = 0.8949999809265137, time = 0.9499614238739014\n",
      "Training at step=87, batch=60, train loss = 0.20079277455806732, train acc = 0.9350000023841858, time = 0.9497756958007812\n",
      "Training at step=87, batch=90, train loss = 0.24338321387767792, train acc = 0.9300000071525574, time = 0.9512138366699219\n",
      "Training at step=87, batch=120, train loss = 0.28529518842697144, train acc = 0.9150000214576721, time = 0.9354305267333984\n",
      "Training at step=87, batch=150, train loss = 0.24397599697113037, train acc = 0.9150000214576721, time = 0.9338665008544922\n",
      "Training at step=87, batch=180, train loss = 0.2694147229194641, train acc = 0.8999999761581421, time = 0.942249059677124\n",
      "Training at step=87, batch=210, train loss = 0.19540922343730927, train acc = 0.8999999761581421, time = 0.9417159557342529\n",
      "Training at step=87, batch=240, train loss = 0.28253090381622314, train acc = 0.8999999761581421, time = 0.9374258518218994\n",
      "Training at step=87, batch=270, train loss = 0.28811824321746826, train acc = 0.9150000214576721, time = 0.9367868900299072\n",
      "Testing at step=87, batch=0, test loss = 0.33562171459198, test acc = 0.8849999904632568, time = 0.3503403663635254\n",
      "Testing at step=87, batch=5, test loss = 0.3654251992702484, test acc = 0.8700000047683716, time = 0.3568744659423828\n",
      "Testing at step=87, batch=10, test loss = 0.4470534026622772, test acc = 0.8399999737739563, time = 0.35869479179382324\n",
      "Testing at step=87, batch=15, test loss = 0.4043402075767517, test acc = 0.8849999904632568, time = 0.3568410873413086\n",
      "Testing at step=87, batch=20, test loss = 0.3330841064453125, test acc = 0.875, time = 0.3565964698791504\n",
      "Testing at step=87, batch=25, test loss = 0.37267693877220154, test acc = 0.875, time = 0.3546919822692871\n",
      "Testing at step=87, batch=30, test loss = 0.2555968463420868, test acc = 0.8999999761581421, time = 0.3602118492126465\n",
      "Testing at step=87, batch=35, test loss = 0.36893853545188904, test acc = 0.8799999952316284, time = 0.35680103302001953\n",
      "Testing at step=87, batch=40, test loss = 0.28058791160583496, test acc = 0.9200000166893005, time = 0.36139369010925293\n",
      "Testing at step=87, batch=45, test loss = 0.40692055225372314, test acc = 0.8450000286102295, time = 0.35254979133605957\n",
      "Step 87 finished in 310.80012130737305, Train loss = 0.24139316494266191, Test loss = 0.35522410362958906; Train Acc = 0.9099999990065892, Test Acc = 0.8798999977111817\n",
      "Training at step=88, batch=0, train loss = 0.25361984968185425, train acc = 0.9049999713897705, time = 0.9498085975646973\n",
      "Training at step=88, batch=30, train loss = 0.317699670791626, train acc = 0.8700000047683716, time = 0.9425303936004639\n",
      "Training at step=88, batch=60, train loss = 0.23248359560966492, train acc = 0.9100000262260437, time = 0.9365382194519043\n",
      "Training at step=88, batch=90, train loss = 0.10702667385339737, train acc = 0.9649999737739563, time = 0.9338734149932861\n",
      "Training at step=88, batch=120, train loss = 0.22492992877960205, train acc = 0.9049999713897705, time = 0.9530198574066162\n",
      "Training at step=88, batch=150, train loss = 0.25771573185920715, train acc = 0.925000011920929, time = 0.9486982822418213\n",
      "Training at step=88, batch=180, train loss = 0.23585183918476105, train acc = 0.9049999713897705, time = 0.9431049823760986\n",
      "Training at step=88, batch=210, train loss = 0.23717136681079865, train acc = 0.9150000214576721, time = 0.9501874446868896\n",
      "Training at step=88, batch=240, train loss = 0.1848999410867691, train acc = 0.9449999928474426, time = 0.9387447834014893\n",
      "Training at step=88, batch=270, train loss = 0.26281169056892395, train acc = 0.8999999761581421, time = 0.9421060085296631\n",
      "Testing at step=88, batch=0, test loss = 0.399748831987381, test acc = 0.8299999833106995, time = 0.34640026092529297\n",
      "Testing at step=88, batch=5, test loss = 0.3401627838611603, test acc = 0.8949999809265137, time = 0.3479595184326172\n",
      "Testing at step=88, batch=10, test loss = 0.3498205244541168, test acc = 0.8600000143051147, time = 0.3572366237640381\n",
      "Testing at step=88, batch=15, test loss = 0.3326510488986969, test acc = 0.875, time = 0.3424203395843506\n",
      "Testing at step=88, batch=20, test loss = 0.3047329783439636, test acc = 0.9100000262260437, time = 0.3448605537414551\n",
      "Testing at step=88, batch=25, test loss = 0.2842194736003876, test acc = 0.8849999904632568, time = 0.34708309173583984\n",
      "Testing at step=88, batch=30, test loss = 0.2945650815963745, test acc = 0.9200000166893005, time = 0.3460078239440918\n",
      "Testing at step=88, batch=35, test loss = 0.40507563948631287, test acc = 0.8600000143051147, time = 0.360001802444458\n",
      "Testing at step=88, batch=40, test loss = 0.42788830399513245, test acc = 0.8349999785423279, time = 0.3612058162689209\n",
      "Testing at step=88, batch=45, test loss = 0.4586212933063507, test acc = 0.8550000190734863, time = 0.3638877868652344\n",
      "Step 88 finished in 310.77563309669495, Train loss = 0.24269477414588134, Test loss = 0.35781792253255845; Train Acc = 0.9096999990940094, Test Acc = 0.8751999974250794\n",
      "Training at step=89, batch=0, train loss = 0.27891165018081665, train acc = 0.9300000071525574, time = 0.9400393962860107\n",
      "Training at step=89, batch=30, train loss = 0.21539361774921417, train acc = 0.9049999713897705, time = 0.940885066986084\n",
      "Training at step=89, batch=60, train loss = 0.22092698514461517, train acc = 0.9049999713897705, time = 0.9366304874420166\n",
      "Training at step=89, batch=90, train loss = 0.22047485411167145, train acc = 0.9150000214576721, time = 0.941439151763916\n",
      "Training at step=89, batch=120, train loss = 0.2087881863117218, train acc = 0.9200000166893005, time = 0.9397544860839844\n",
      "Training at step=89, batch=150, train loss = 0.19522151350975037, train acc = 0.9350000023841858, time = 0.9648659229278564\n",
      "Training at step=89, batch=180, train loss = 0.27580034732818604, train acc = 0.9150000214576721, time = 0.94952392578125\n",
      "Training at step=89, batch=210, train loss = 0.17658936977386475, train acc = 0.9350000023841858, time = 0.9395458698272705\n",
      "Training at step=89, batch=240, train loss = 0.2291841357946396, train acc = 0.9100000262260437, time = 0.944704532623291\n",
      "Training at step=89, batch=270, train loss = 0.16342617571353912, train acc = 0.9399999976158142, time = 0.9354681968688965\n",
      "Testing at step=89, batch=0, test loss = 0.3592096269130707, test acc = 0.8949999809265137, time = 0.3449137210845947\n",
      "Testing at step=89, batch=5, test loss = 0.33322086930274963, test acc = 0.8650000095367432, time = 0.342604398727417\n",
      "Testing at step=89, batch=10, test loss = 0.4407638609409332, test acc = 0.8199999928474426, time = 0.35033440589904785\n",
      "Testing at step=89, batch=15, test loss = 0.4175952970981598, test acc = 0.8849999904632568, time = 0.34119296073913574\n",
      "Testing at step=89, batch=20, test loss = 0.44088008999824524, test acc = 0.8500000238418579, time = 0.3582632541656494\n",
      "Testing at step=89, batch=25, test loss = 0.269935667514801, test acc = 0.8799999952316284, time = 0.3574972152709961\n",
      "Testing at step=89, batch=30, test loss = 0.5255000591278076, test acc = 0.8100000023841858, time = 0.35531091690063477\n",
      "Testing at step=89, batch=35, test loss = 0.30858445167541504, test acc = 0.8700000047683716, time = 0.35681796073913574\n",
      "Testing at step=89, batch=40, test loss = 0.43485918641090393, test acc = 0.8399999737739563, time = 0.36637115478515625\n",
      "Testing at step=89, batch=45, test loss = 0.38407257199287415, test acc = 0.8399999737739563, time = 0.3545351028442383\n",
      "Step 89 finished in 310.6357123851776, Train loss = 0.24166938910881677, Test loss = 0.36522257298231126; Train Acc = 0.9110666676362356, Test Acc = 0.8736999976634979\n",
      "Training at step=90, batch=0, train loss = 0.285447359085083, train acc = 0.8999999761581421, time = 0.9611496925354004\n",
      "Training at step=90, batch=30, train loss = 0.2879743278026581, train acc = 0.875, time = 0.9368071556091309\n",
      "Training at step=90, batch=60, train loss = 0.30610617995262146, train acc = 0.8949999809265137, time = 0.9420895576477051\n",
      "Training at step=90, batch=90, train loss = 0.2345975637435913, train acc = 0.9200000166893005, time = 0.9424781799316406\n",
      "Training at step=90, batch=120, train loss = 0.19782231748104095, train acc = 0.925000011920929, time = 0.9478662014007568\n",
      "Training at step=90, batch=150, train loss = 0.24450674653053284, train acc = 0.9049999713897705, time = 0.9394123554229736\n",
      "Training at step=90, batch=180, train loss = 0.18471069633960724, train acc = 0.9399999976158142, time = 0.9345197677612305\n",
      "Training at step=90, batch=210, train loss = 0.2590528428554535, train acc = 0.9200000166893005, time = 0.937915563583374\n",
      "Training at step=90, batch=240, train loss = 0.2360917031764984, train acc = 0.9150000214576721, time = 0.9324502944946289\n",
      "Training at step=90, batch=270, train loss = 0.1955433040857315, train acc = 0.9350000023841858, time = 0.9491198062896729\n",
      "Testing at step=90, batch=0, test loss = 0.4959311783313751, test acc = 0.8650000095367432, time = 0.34259486198425293\n",
      "Testing at step=90, batch=5, test loss = 0.34841689467430115, test acc = 0.8500000238418579, time = 0.34626126289367676\n",
      "Testing at step=90, batch=10, test loss = 0.30635866522789, test acc = 0.8899999856948853, time = 0.34340858459472656\n",
      "Testing at step=90, batch=15, test loss = 0.3209812641143799, test acc = 0.8700000047683716, time = 0.3434295654296875\n",
      "Testing at step=90, batch=20, test loss = 0.3150092661380768, test acc = 0.9049999713897705, time = 0.34418797492980957\n",
      "Testing at step=90, batch=25, test loss = 0.24265140295028687, test acc = 0.8999999761581421, time = 0.34840869903564453\n",
      "Testing at step=90, batch=30, test loss = 0.2378271222114563, test acc = 0.9150000214576721, time = 0.3406507968902588\n",
      "Testing at step=90, batch=35, test loss = 0.42774245142936707, test acc = 0.8700000047683716, time = 0.3461301326751709\n",
      "Testing at step=90, batch=40, test loss = 0.43213585019111633, test acc = 0.8550000190734863, time = 0.3469226360321045\n",
      "Testing at step=90, batch=45, test loss = 0.32027870416641235, test acc = 0.8899999856948853, time = 0.34771084785461426\n",
      "Step 90 finished in 310.2696523666382, Train loss = 0.2402179982761542, Test loss = 0.36324226170778273; Train Acc = 0.9107833351691564, Test Acc = 0.8747000002861023\n",
      "Training at step=91, batch=0, train loss = 0.17795638740062714, train acc = 0.9449999928474426, time = 0.9329936504364014\n",
      "Training at step=91, batch=30, train loss = 0.2227039784193039, train acc = 0.925000011920929, time = 0.936901330947876\n",
      "Training at step=91, batch=60, train loss = 0.2753644287586212, train acc = 0.8949999809265137, time = 0.9982116222381592\n",
      "Training at step=91, batch=90, train loss = 0.22717095911502838, train acc = 0.9150000214576721, time = 0.9489603042602539\n",
      "Training at step=91, batch=120, train loss = 0.1989988088607788, train acc = 0.9300000071525574, time = 0.9316656589508057\n",
      "Training at step=91, batch=150, train loss = 0.30671510100364685, train acc = 0.8799999952316284, time = 0.9484913349151611\n",
      "Training at step=91, batch=180, train loss = 0.26902416348457336, train acc = 0.9100000262260437, time = 0.9492218494415283\n",
      "Training at step=91, batch=210, train loss = 0.357059121131897, train acc = 0.8650000095367432, time = 1.004601240158081\n",
      "Training at step=91, batch=240, train loss = 0.24987675249576569, train acc = 0.9049999713897705, time = 0.9383442401885986\n",
      "Training at step=91, batch=270, train loss = 0.2054705172777176, train acc = 0.9350000023841858, time = 0.9459640979766846\n",
      "Testing at step=91, batch=0, test loss = 0.323525607585907, test acc = 0.8899999856948853, time = 0.35001492500305176\n",
      "Testing at step=91, batch=5, test loss = 0.44783562421798706, test acc = 0.8650000095367432, time = 0.3427009582519531\n",
      "Testing at step=91, batch=10, test loss = 0.29530811309814453, test acc = 0.9100000262260437, time = 0.34844255447387695\n",
      "Testing at step=91, batch=15, test loss = 0.46957194805145264, test acc = 0.8949999809265137, time = 0.3423435688018799\n",
      "Testing at step=91, batch=20, test loss = 0.4885711669921875, test acc = 0.8450000286102295, time = 0.350278377532959\n",
      "Testing at step=91, batch=25, test loss = 0.34559738636016846, test acc = 0.8949999809265137, time = 0.345531702041626\n",
      "Testing at step=91, batch=30, test loss = 0.389566570520401, test acc = 0.8700000047683716, time = 0.34398412704467773\n",
      "Testing at step=91, batch=35, test loss = 0.3329462707042694, test acc = 0.8999999761581421, time = 0.34151196479797363\n",
      "Testing at step=91, batch=40, test loss = 0.47245919704437256, test acc = 0.8899999856948853, time = 0.3557753562927246\n",
      "Testing at step=91, batch=45, test loss = 0.21654671430587769, test acc = 0.8899999856948853, time = 0.34635114669799805\n",
      "Step 91 finished in 310.5829246044159, Train loss = 0.24073122471570968, Test loss = 0.3522971972823143; Train Acc = 0.9107333346207936, Test Acc = 0.8825999987125397\n",
      "Training at step=92, batch=0, train loss = 0.2660902440547943, train acc = 0.8949999809265137, time = 0.9457135200500488\n",
      "Training at step=92, batch=30, train loss = 0.35084277391433716, train acc = 0.8999999761581421, time = 0.9415090084075928\n",
      "Training at step=92, batch=60, train loss = 0.22182446718215942, train acc = 0.9049999713897705, time = 0.9606289863586426\n",
      "Training at step=92, batch=90, train loss = 0.18950939178466797, train acc = 0.9150000214576721, time = 0.9513833522796631\n",
      "Training at step=92, batch=120, train loss = 0.2595823407173157, train acc = 0.9150000214576721, time = 0.9369826316833496\n",
      "Training at step=92, batch=150, train loss = 0.2323351800441742, train acc = 0.8999999761581421, time = 0.9316647052764893\n",
      "Training at step=92, batch=180, train loss = 0.17427560687065125, train acc = 0.9350000023841858, time = 0.934577226638794\n",
      "Training at step=92, batch=210, train loss = 0.1749298870563507, train acc = 0.925000011920929, time = 0.9326012134552002\n",
      "Training at step=92, batch=240, train loss = 0.1587277054786682, train acc = 0.949999988079071, time = 0.9393162727355957\n",
      "Training at step=92, batch=270, train loss = 0.18612192571163177, train acc = 0.9350000023841858, time = 0.9373831748962402\n",
      "Testing at step=92, batch=0, test loss = 0.4018312692642212, test acc = 0.8600000143051147, time = 0.34891772270202637\n",
      "Testing at step=92, batch=5, test loss = 0.4503623843193054, test acc = 0.8550000190734863, time = 0.35462403297424316\n",
      "Testing at step=92, batch=10, test loss = 0.28119394183158875, test acc = 0.8949999809265137, time = 0.3466320037841797\n",
      "Testing at step=92, batch=15, test loss = 0.3297896087169647, test acc = 0.8999999761581421, time = 0.3478837013244629\n",
      "Testing at step=92, batch=20, test loss = 0.35698533058166504, test acc = 0.8650000095367432, time = 0.34087491035461426\n",
      "Testing at step=92, batch=25, test loss = 0.28547203540802, test acc = 0.8999999761581421, time = 0.36482977867126465\n",
      "Testing at step=92, batch=30, test loss = 0.40015289187431335, test acc = 0.8600000143051147, time = 0.36188602447509766\n",
      "Testing at step=92, batch=35, test loss = 0.4146597981452942, test acc = 0.8450000286102295, time = 0.3660738468170166\n",
      "Testing at step=92, batch=40, test loss = 0.3758390545845032, test acc = 0.8849999904632568, time = 0.34900498390197754\n",
      "Testing at step=92, batch=45, test loss = 0.2591478228569031, test acc = 0.9049999713897705, time = 0.34500622749328613\n",
      "Step 92 finished in 310.2341253757477, Train loss = 0.23827380046248436, Test loss = 0.35691183656454084; Train Acc = 0.9114166649182638, Test Acc = 0.8767999970912933\n",
      "Training at step=93, batch=0, train loss = 0.203227698802948, train acc = 0.9150000214576721, time = 0.9541871547698975\n",
      "Training at step=93, batch=30, train loss = 0.22213679552078247, train acc = 0.8999999761581421, time = 0.9413976669311523\n",
      "Training at step=93, batch=60, train loss = 0.22615253925323486, train acc = 0.9200000166893005, time = 0.9367763996124268\n",
      "Training at step=93, batch=90, train loss = 0.29759323596954346, train acc = 0.9150000214576721, time = 0.9384884834289551\n",
      "Training at step=93, batch=120, train loss = 0.23329603672027588, train acc = 0.9300000071525574, time = 0.9376943111419678\n",
      "Training at step=93, batch=150, train loss = 0.27458029985427856, train acc = 0.8999999761581421, time = 0.931248664855957\n",
      "Training at step=93, batch=180, train loss = 0.24198368191719055, train acc = 0.8899999856948853, time = 0.9378242492675781\n",
      "Training at step=93, batch=210, train loss = 0.2255125492811203, train acc = 0.8949999809265137, time = 0.9400815963745117\n",
      "Training at step=93, batch=240, train loss = 0.2549844980239868, train acc = 0.9150000214576721, time = 0.9427049160003662\n",
      "Training at step=93, batch=270, train loss = 0.28348010778427124, train acc = 0.9049999713897705, time = 0.9462060928344727\n",
      "Testing at step=93, batch=0, test loss = 0.48917779326438904, test acc = 0.8299999833106995, time = 0.349139928817749\n",
      "Testing at step=93, batch=5, test loss = 0.32181987166404724, test acc = 0.8600000143051147, time = 0.35826802253723145\n",
      "Testing at step=93, batch=10, test loss = 0.4045301079750061, test acc = 0.8550000190734863, time = 0.3466634750366211\n",
      "Testing at step=93, batch=15, test loss = 0.518164336681366, test acc = 0.8399999737739563, time = 0.3502795696258545\n",
      "Testing at step=93, batch=20, test loss = 0.4623798429965973, test acc = 0.875, time = 0.353954553604126\n",
      "Testing at step=93, batch=25, test loss = 0.3421962857246399, test acc = 0.8600000143051147, time = 0.3638160228729248\n",
      "Testing at step=93, batch=30, test loss = 0.6090681552886963, test acc = 0.8399999737739563, time = 0.34368038177490234\n",
      "Testing at step=93, batch=35, test loss = 0.26141464710235596, test acc = 0.9150000214576721, time = 0.3566102981567383\n",
      "Testing at step=93, batch=40, test loss = 0.2476959377527237, test acc = 0.8999999761581421, time = 0.3570108413696289\n",
      "Testing at step=93, batch=45, test loss = 0.3579227030277252, test acc = 0.8849999904632568, time = 0.37192511558532715\n",
      "Step 93 finished in 310.49642181396484, Train loss = 0.2390045581261317, Test loss = 0.35268562465906145; Train Acc = 0.9120500018199285, Test Acc = 0.8789999997615814\n",
      "Training at step=94, batch=0, train loss = 0.15560382604599, train acc = 0.949999988079071, time = 0.9577548503875732\n",
      "Training at step=94, batch=30, train loss = 0.16953207552433014, train acc = 0.9399999976158142, time = 0.9597210884094238\n",
      "Training at step=94, batch=60, train loss = 0.17754745483398438, train acc = 0.9350000023841858, time = 0.9358186721801758\n",
      "Training at step=94, batch=90, train loss = 0.20618243515491486, train acc = 0.9300000071525574, time = 0.9506340026855469\n",
      "Training at step=94, batch=120, train loss = 0.25668272376060486, train acc = 0.8999999761581421, time = 0.9331462383270264\n",
      "Training at step=94, batch=150, train loss = 0.22370734810829163, train acc = 0.925000011920929, time = 0.9388678073883057\n",
      "Training at step=94, batch=180, train loss = 0.2111624777317047, train acc = 0.8999999761581421, time = 0.9384076595306396\n",
      "Training at step=94, batch=210, train loss = 0.3032209873199463, train acc = 0.8500000238418579, time = 0.9430270195007324\n",
      "Training at step=94, batch=240, train loss = 0.2308143824338913, train acc = 0.9200000166893005, time = 0.9530737400054932\n",
      "Training at step=94, batch=270, train loss = 0.2629837095737457, train acc = 0.8999999761581421, time = 0.9517488479614258\n",
      "Testing at step=94, batch=0, test loss = 0.3093852698802948, test acc = 0.8949999809265137, time = 0.34771108627319336\n",
      "Testing at step=94, batch=5, test loss = 0.29755499958992004, test acc = 0.8949999809265137, time = 0.3435096740722656\n",
      "Testing at step=94, batch=10, test loss = 0.43577539920806885, test acc = 0.8500000238418579, time = 0.3417642116546631\n",
      "Testing at step=94, batch=15, test loss = 0.3135809600353241, test acc = 0.8849999904632568, time = 0.34198451042175293\n",
      "Testing at step=94, batch=20, test loss = 0.3984452188014984, test acc = 0.8450000286102295, time = 0.3476235866546631\n",
      "Testing at step=94, batch=25, test loss = 0.35164394974708557, test acc = 0.8700000047683716, time = 0.35471463203430176\n",
      "Testing at step=94, batch=30, test loss = 0.40274548530578613, test acc = 0.875, time = 0.3448328971862793\n",
      "Testing at step=94, batch=35, test loss = 0.2638920843601227, test acc = 0.8849999904632568, time = 0.3519711494445801\n",
      "Testing at step=94, batch=40, test loss = 0.28719040751457214, test acc = 0.8949999809265137, time = 0.3400301933288574\n",
      "Testing at step=94, batch=45, test loss = 0.26270875334739685, test acc = 0.9100000262260437, time = 0.34125638008117676\n",
      "Step 94 finished in 309.9367251396179, Train loss = 0.23691640953222912, Test loss = 0.3505648168921471; Train Acc = 0.9121500015258789, Test Acc = 0.8808999991416931\n",
      "Training at step=95, batch=0, train loss = 0.14497758448123932, train acc = 0.9549999833106995, time = 0.9419748783111572\n",
      "Training at step=95, batch=30, train loss = 0.22532856464385986, train acc = 0.8899999856948853, time = 0.9546787738800049\n",
      "Training at step=95, batch=60, train loss = 0.28987425565719604, train acc = 0.9150000214576721, time = 0.9319469928741455\n",
      "Training at step=95, batch=90, train loss = 0.21498937904834747, train acc = 0.9300000071525574, time = 0.951340913772583\n",
      "Training at step=95, batch=120, train loss = 0.1779676228761673, train acc = 0.9300000071525574, time = 0.9342126846313477\n",
      "Training at step=95, batch=150, train loss = 0.18603667616844177, train acc = 0.9449999928474426, time = 0.9557685852050781\n",
      "Training at step=95, batch=180, train loss = 0.21307064592838287, train acc = 0.9350000023841858, time = 0.9457495212554932\n",
      "Training at step=95, batch=210, train loss = 0.18694017827510834, train acc = 0.9200000166893005, time = 0.9358189105987549\n",
      "Training at step=95, batch=240, train loss = 0.2483329176902771, train acc = 0.925000011920929, time = 0.9408664703369141\n",
      "Training at step=95, batch=270, train loss = 0.1981310248374939, train acc = 0.9350000023841858, time = 0.9337344169616699\n",
      "Testing at step=95, batch=0, test loss = 0.4236900210380554, test acc = 0.8700000047683716, time = 0.3465917110443115\n",
      "Testing at step=95, batch=5, test loss = 0.21989460289478302, test acc = 0.9049999713897705, time = 0.34830594062805176\n",
      "Testing at step=95, batch=10, test loss = 0.46464332938194275, test acc = 0.875, time = 0.3425929546356201\n",
      "Testing at step=95, batch=15, test loss = 0.3699222803115845, test acc = 0.8550000190734863, time = 0.34674596786499023\n",
      "Testing at step=95, batch=20, test loss = 0.4658265709877014, test acc = 0.8500000238418579, time = 0.3457605838775635\n",
      "Testing at step=95, batch=25, test loss = 0.34132203459739685, test acc = 0.8799999952316284, time = 0.3412485122680664\n",
      "Testing at step=95, batch=30, test loss = 0.2763119041919708, test acc = 0.8999999761581421, time = 0.3436737060546875\n",
      "Testing at step=95, batch=35, test loss = 0.3685642182826996, test acc = 0.8700000047683716, time = 0.33917760848999023\n",
      "Testing at step=95, batch=40, test loss = 0.4105442464351654, test acc = 0.8550000190734863, time = 0.3452749252319336\n",
      "Testing at step=95, batch=45, test loss = 0.35526737570762634, test acc = 0.8500000238418579, time = 0.34163522720336914\n",
      "Step 95 finished in 310.08564352989197, Train loss = 0.23555160894989968, Test loss = 0.3648610219359398; Train Acc = 0.9139166686932246, Test Acc = 0.8792999970912934\n",
      "Training at step=96, batch=0, train loss = 0.20431111752986908, train acc = 0.949999988079071, time = 0.9443085193634033\n",
      "Training at step=96, batch=30, train loss = 0.21602009236812592, train acc = 0.925000011920929, time = 0.9346456527709961\n",
      "Training at step=96, batch=60, train loss = 0.3059298098087311, train acc = 0.9150000214576721, time = 0.9463484287261963\n",
      "Training at step=96, batch=90, train loss = 0.15745753049850464, train acc = 0.9350000023841858, time = 0.9515390396118164\n",
      "Training at step=96, batch=120, train loss = 0.2740265727043152, train acc = 0.8999999761581421, time = 0.9380581378936768\n",
      "Training at step=96, batch=150, train loss = 0.23245204985141754, train acc = 0.9200000166893005, time = 0.938513994216919\n",
      "Training at step=96, batch=180, train loss = 0.26877841353416443, train acc = 0.8700000047683716, time = 0.9318194389343262\n",
      "Training at step=96, batch=210, train loss = 0.23184700310230255, train acc = 0.9150000214576721, time = 0.9474284648895264\n",
      "Training at step=96, batch=240, train loss = 0.30109912157058716, train acc = 0.8799999952316284, time = 0.9388692378997803\n",
      "Training at step=96, batch=270, train loss = 0.23569753766059875, train acc = 0.9100000262260437, time = 0.9425044059753418\n",
      "Testing at step=96, batch=0, test loss = 0.35404032468795776, test acc = 0.8899999856948853, time = 0.3514559268951416\n",
      "Testing at step=96, batch=5, test loss = 0.3163052201271057, test acc = 0.8899999856948853, time = 0.3398728370666504\n",
      "Testing at step=96, batch=10, test loss = 0.36830997467041016, test acc = 0.8700000047683716, time = 0.3430337905883789\n",
      "Testing at step=96, batch=15, test loss = 0.4042350649833679, test acc = 0.8700000047683716, time = 0.3485245704650879\n",
      "Testing at step=96, batch=20, test loss = 0.37766820192337036, test acc = 0.8899999856948853, time = 0.34439611434936523\n",
      "Testing at step=96, batch=25, test loss = 0.2677161395549774, test acc = 0.8999999761581421, time = 0.34720635414123535\n",
      "Testing at step=96, batch=30, test loss = 0.2229139506816864, test acc = 0.9150000214576721, time = 0.34697484970092773\n",
      "Testing at step=96, batch=35, test loss = 0.35405048727989197, test acc = 0.8600000143051147, time = 0.3452463150024414\n",
      "Testing at step=96, batch=40, test loss = 0.39524033665657043, test acc = 0.8550000190734863, time = 0.34746813774108887\n",
      "Testing at step=96, batch=45, test loss = 0.37211140990257263, test acc = 0.8650000095367432, time = 0.34343910217285156\n",
      "Step 96 finished in 310.6585969924927, Train loss = 0.23586914052565894, Test loss = 0.3636771947145462; Train Acc = 0.912483334938685, Test Acc = 0.8793999993801117\n",
      "Training at step=97, batch=0, train loss = 0.25170329213142395, train acc = 0.9049999713897705, time = 0.9355363845825195\n",
      "Training at step=97, batch=30, train loss = 0.277018666267395, train acc = 0.9049999713897705, time = 0.9382932186126709\n",
      "Training at step=97, batch=60, train loss = 0.3174974322319031, train acc = 0.8849999904632568, time = 0.9353296756744385\n",
      "Training at step=97, batch=90, train loss = 0.3098585605621338, train acc = 0.8700000047683716, time = 0.9447791576385498\n",
      "Training at step=97, batch=120, train loss = 0.2838340401649475, train acc = 0.8999999761581421, time = 0.9359302520751953\n",
      "Training at step=97, batch=150, train loss = 0.2876793146133423, train acc = 0.875, time = 0.9361884593963623\n",
      "Training at step=97, batch=180, train loss = 0.24086874723434448, train acc = 0.9100000262260437, time = 0.9297451972961426\n",
      "Training at step=97, batch=210, train loss = 0.26078876852989197, train acc = 0.8999999761581421, time = 0.941399335861206\n",
      "Training at step=97, batch=240, train loss = 0.34104758501052856, train acc = 0.8899999856948853, time = 0.9401073455810547\n",
      "Training at step=97, batch=270, train loss = 0.2864404320716858, train acc = 0.9100000262260437, time = 0.9355940818786621\n",
      "Testing at step=97, batch=0, test loss = 0.32741454243659973, test acc = 0.8799999952316284, time = 0.3470311164855957\n",
      "Testing at step=97, batch=5, test loss = 0.4387282431125641, test acc = 0.8050000071525574, time = 0.3472940921783447\n",
      "Testing at step=97, batch=10, test loss = 0.30535611510276794, test acc = 0.875, time = 0.3611416816711426\n",
      "Testing at step=97, batch=15, test loss = 0.3519315719604492, test acc = 0.8949999809265137, time = 0.34530162811279297\n",
      "Testing at step=97, batch=20, test loss = 0.44153153896331787, test acc = 0.8550000190734863, time = 0.3506016731262207\n",
      "Testing at step=97, batch=25, test loss = 0.3145574629306793, test acc = 0.875, time = 0.3509056568145752\n",
      "Testing at step=97, batch=30, test loss = 0.30455872416496277, test acc = 0.9150000214576721, time = 0.3468480110168457\n",
      "Testing at step=97, batch=35, test loss = 0.3789072036743164, test acc = 0.8550000190734863, time = 0.35227131843566895\n",
      "Testing at step=97, batch=40, test loss = 0.3026924729347229, test acc = 0.875, time = 0.3548409938812256\n",
      "Testing at step=97, batch=45, test loss = 0.32949259877204895, test acc = 0.9049999713897705, time = 0.3496408462524414\n",
      "Step 97 finished in 309.7344446182251, Train loss = 0.23448260359466075, Test loss = 0.3681489700078964; Train Acc = 0.9130999998251597, Test Acc = 0.8755999970436096\n",
      "Training at step=98, batch=0, train loss = 0.21418802440166473, train acc = 0.8999999761581421, time = 0.9391593933105469\n",
      "Training at step=98, batch=30, train loss = 0.20638227462768555, train acc = 0.9200000166893005, time = 0.9378790855407715\n",
      "Training at step=98, batch=60, train loss = 0.2072332203388214, train acc = 0.9200000166893005, time = 0.943016767501831\n",
      "Training at step=98, batch=90, train loss = 0.2040288746356964, train acc = 0.9150000214576721, time = 0.9330759048461914\n",
      "Training at step=98, batch=120, train loss = 0.1878395974636078, train acc = 0.9599999785423279, time = 0.9399752616882324\n",
      "Training at step=98, batch=150, train loss = 0.1780332326889038, train acc = 0.949999988079071, time = 0.9513895511627197\n",
      "Training at step=98, batch=180, train loss = 0.23209109902381897, train acc = 0.8849999904632568, time = 0.941490650177002\n",
      "Training at step=98, batch=210, train loss = 0.3486264944076538, train acc = 0.8600000143051147, time = 0.9411778450012207\n",
      "Training at step=98, batch=240, train loss = 0.16383346915245056, train acc = 0.9549999833106995, time = 0.9491510391235352\n",
      "Training at step=98, batch=270, train loss = 0.23864568769931793, train acc = 0.8999999761581421, time = 0.9382438659667969\n",
      "Testing at step=98, batch=0, test loss = 0.4909977614879608, test acc = 0.8450000286102295, time = 0.3580434322357178\n",
      "Testing at step=98, batch=5, test loss = 0.26406821608543396, test acc = 0.9100000262260437, time = 0.3536567687988281\n",
      "Testing at step=98, batch=10, test loss = 0.36118006706237793, test acc = 0.9049999713897705, time = 0.359813928604126\n",
      "Testing at step=98, batch=15, test loss = 0.3850418031215668, test acc = 0.875, time = 0.34932637214660645\n",
      "Testing at step=98, batch=20, test loss = 0.2794657051563263, test acc = 0.9150000214576721, time = 0.3617546558380127\n",
      "Testing at step=98, batch=25, test loss = 0.42543333768844604, test acc = 0.8650000095367432, time = 0.3522007465362549\n",
      "Testing at step=98, batch=30, test loss = 0.3401547968387604, test acc = 0.8949999809265137, time = 0.34627747535705566\n",
      "Testing at step=98, batch=35, test loss = 0.4959146976470947, test acc = 0.8399999737739563, time = 0.3436744213104248\n",
      "Testing at step=98, batch=40, test loss = 0.36878377199172974, test acc = 0.8399999737739563, time = 0.34403014183044434\n",
      "Testing at step=98, batch=45, test loss = 0.3406691253185272, test acc = 0.9049999713897705, time = 0.34551215171813965\n",
      "Step 98 finished in 310.671875, Train loss = 0.23333561822772025, Test loss = 0.36288513779640197; Train Acc = 0.912900000611941, Test Acc = 0.8795999968051911\n",
      "Training at step=99, batch=0, train loss = 0.24835357069969177, train acc = 0.9150000214576721, time = 0.9494271278381348\n",
      "Training at step=99, batch=30, train loss = 0.2503511309623718, train acc = 0.9150000214576721, time = 0.9452364444732666\n",
      "Training at step=99, batch=60, train loss = 0.24201013147830963, train acc = 0.925000011920929, time = 0.9344396591186523\n",
      "Training at step=99, batch=90, train loss = 0.22617307305335999, train acc = 0.8999999761581421, time = 0.944486141204834\n",
      "Training at step=99, batch=120, train loss = 0.18300995230674744, train acc = 0.925000011920929, time = 0.9283568859100342\n",
      "Training at step=99, batch=150, train loss = 0.2441725730895996, train acc = 0.8849999904632568, time = 0.9401118755340576\n",
      "Training at step=99, batch=180, train loss = 0.1731211543083191, train acc = 0.9449999928474426, time = 0.9333508014678955\n",
      "Training at step=99, batch=210, train loss = 0.2491137534379959, train acc = 0.8999999761581421, time = 0.9459702968597412\n",
      "Training at step=99, batch=240, train loss = 0.24053817987442017, train acc = 0.9200000166893005, time = 0.940014123916626\n",
      "Training at step=99, batch=270, train loss = 0.23889243602752686, train acc = 0.9150000214576721, time = 0.9406082630157471\n",
      "Testing at step=99, batch=0, test loss = 0.36552155017852783, test acc = 0.8349999785423279, time = 0.34534168243408203\n",
      "Testing at step=99, batch=5, test loss = 0.2652474343776703, test acc = 0.9200000166893005, time = 0.3448066711425781\n",
      "Testing at step=99, batch=10, test loss = 0.30840763449668884, test acc = 0.8949999809265137, time = 0.34581613540649414\n",
      "Testing at step=99, batch=15, test loss = 0.27936550974845886, test acc = 0.8999999761581421, time = 0.34534454345703125\n",
      "Testing at step=99, batch=20, test loss = 0.3177761435508728, test acc = 0.8600000143051147, time = 0.3392984867095947\n",
      "Testing at step=99, batch=25, test loss = 0.3478063941001892, test acc = 0.8600000143051147, time = 0.3397834300994873\n",
      "Testing at step=99, batch=30, test loss = 0.33773380517959595, test acc = 0.875, time = 0.3443155288696289\n",
      "Testing at step=99, batch=35, test loss = 0.34990638494491577, test acc = 0.875, time = 0.34653472900390625\n",
      "Testing at step=99, batch=40, test loss = 0.4488687813282013, test acc = 0.824999988079071, time = 0.33742499351501465\n",
      "Testing at step=99, batch=45, test loss = 0.37878403067588806, test acc = 0.8700000047683716, time = 0.34893298149108887\n",
      "Step 99 finished in 309.821129322052, Train loss = 0.2335457620024681, Test loss = 0.3566420865058899; Train Acc = 0.9138500010967254, Test Acc = 0.8762999987602234\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "BATCH_SIZE = 200\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.1\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-03T17:06:02.381493Z",
     "iopub.status.busy": "2024-04-03T17:06:02.381278Z",
     "iopub.status.idle": "2024-04-03T17:06:02.758413Z",
     "shell.execute_reply": "2024-04-03T17:06:02.757799Z",
     "shell.execute_reply.started": "2024-04-03T17:06:02.381473Z"
    },
    "id": "U0Q0vFm7B6cg"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
