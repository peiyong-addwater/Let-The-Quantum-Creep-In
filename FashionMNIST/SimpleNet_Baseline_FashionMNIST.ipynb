{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 618301,
     "status": "ok",
     "timestamp": 1712091541763,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "acHJk38WrBOi",
    "outputId": "ba1ec555-f0d0-45fd-df87-2f537a902cab"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pennylane\n",
      "  Downloading PennyLane-0.35.1-py3-none-any.whl (1.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m7.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting cotengra\n",
      "  Downloading cotengra-0.5.6-py3-none-any.whl (148 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m148.0/148.0 kB\u001B[0m \u001B[31m17.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting quimb\n",
      "  Downloading quimb-1.7.3-py3-none-any.whl (500 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m500.7/500.7 kB\u001B[0m \u001B[31m39.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting torchmetrics\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m841.5/841.5 kB\u001B[0m \u001B[31m46.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.25.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.11.4)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.2.1)\n",
      "Collecting rustworkx (from pennylane)\n",
      "  Downloading rustworkx-0.14.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.1/2.1 MB\u001B[0m \u001B[31m34.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.6.2)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from pennylane) (0.10.2)\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.4.4)\n",
      "Collecting semantic-version>=2.7 (from pennylane)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting autoray>=0.6.1 (from pennylane)\n",
      "  Downloading autoray-0.6.9-py3-none-any.whl (49 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m49.8/49.8 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.3.3)\n",
      "Collecting pennylane-lightning>=0.35 (from pennylane)\n",
      "  Downloading PennyLane_Lightning-0.35.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.5/18.5 MB\u001B[0m \u001B[31m46.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.10.0)\n",
      "Collecting cytoolz>=0.8.0 (from quimb)\n",
      "  Downloading cytoolz-0.12.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.0/2.0 MB\u001B[0m \u001B[31m67.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numba>=0.39 in /usr/local/lib/python3.10/dist-packages (from quimb) (0.58.1)\n",
      "Requirement already satisfied: psutil>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from quimb) (5.9.5)\n",
      "Requirement already satisfied: tqdm>=4 in /usr/local/lib/python3.10/dist-packages (from quimb) (4.66.2)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.8.0->quimb) (0.12.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.39->quimb) (0.41.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.3)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m23.7/23.7 MB\u001B[0m \u001B[31m36.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m823.6/823.6 kB\u001B[0m \u001B[31m40.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m14.1/14.1 MB\u001B[0m \u001B[31m41.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m731.7/731.7 MB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m410.6/410.6 MB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.6/121.6 MB\u001B[0m \u001B[31m1.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.5/56.5 MB\u001B[0m \u001B[31m3.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m124.2/124.2 MB\u001B[0m \u001B[31m2.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.0/196.0 MB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m166.0/166.0 MB\u001B[0m \u001B[31m2.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.1/99.1 kB\u001B[0m \u001B[31m2.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m2.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.10/dist-packages (from autograd->pennylane) (0.18.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Installing collected packages: semantic-version, rustworkx, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, cytoolz, autoray, nvidia-cusparse-cu12, nvidia-cudnn-cu12, cotengra, quimb, nvidia-cusolver-cu12, torchmetrics, pennylane-lightning, pennylane\n",
      "Successfully installed autoray-0.6.9 cotengra-0.5.6 cytoolz-0.12.3 lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pennylane-0.35.1 pennylane-lightning-0.35.1 quimb-1.7.3 rustworkx-0.14.2 semantic-version-2.10.0 torchmetrics-1.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio pennylane cotengra quimb torchmetrics --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8020,
     "status": "ok",
     "timestamp": 1712091549780,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "FB_BuVzCrMNk",
    "outputId": "235fd337-f4c4-4ba7-9547-7b96570eac2e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "#np.set_printoptions(threshold=sys.maxsize)\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "#import jax\n",
    "import time\n",
    "\n",
    "import functools\n",
    "\n",
    "from typing import List, Union, Tuple, Dict, Optional, Any\n",
    "from typing import Callable\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", True)\n",
    "#jax.config.update(\"jax_debug_nans\", True)\n",
    "#import jax.numpy as jnp\n",
    "\n",
    "#import optax  # optimization using jax\n",
    "\n",
    "import torch  # https://pytorch.org\n",
    "import torchvision  # https://pytorch.org\n",
    "#torch.set_printoptions(profile=\"full\")\n",
    "#import torch_xla\n",
    "#import torch_xla.core.xla_model as xm\n",
    "\n",
    "\n",
    "\n",
    "import pennylane as qml\n",
    "import pennylane.numpy as pnp\n",
    "\n",
    "import os, cv2, itertools # cv2 -- OpenCV\n",
    "import shutil\n",
    "import zipfile\n",
    "%matplotlib inline\n",
    "\n",
    "#from jax.lib import xla_bridge\n",
    "\n",
    "\n",
    "sns.set()\n",
    "\n",
    "seed = 1701\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "prng = pnp.random.default_rng(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#COMPLEX_DTYPE = torch.cfloat #torch.cdouble\n",
    "#REAL_DTYPE = torch.float\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5513,
     "status": "ok",
     "timestamp": 1712091555290,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "K7wJnOiEralR",
    "outputId": "a21dab31-4609-4119-cbdd-0b84f74ac35f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 26421880/26421880 [00:01<00:00, 15959281.87it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/train-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 271707.62it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/train-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4422102/4422102 [00:00<00:00, 5005675.65it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 8185093.63it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extracting FashionMNIST/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to FashionMNIST/FashionMNIST/raw\n",
      "\n",
      "torch.Size([64, 1, 32, 32])\n",
      "torch.Size([64])\n",
      "tensor([4, 1, 7, 3, 5, 8, 2, 6, 4, 6, 6, 3, 6, 9, 9, 7, 8, 7, 7, 7, 2, 7, 3, 8,\n",
      "        8, 8, 2, 2, 8, 2, 1, 5, 8, 7, 8, 2, 1, 4, 8, 7, 0, 0, 8, 9, 7, 5, 3, 6,\n",
      "        9, 3, 9, 8, 7, 1, 8, 8, 0, 7, 3, 0, 6, 4, 5, 8])\n",
      "tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.7176,\n",
      "         0.5686,  0.8588,  0.2235,  0.0902,  0.9059,  0.5529,  0.6000,  0.6078,\n",
      "         0.6392,  0.6314,  0.6157,  0.8039,  0.4039, -0.0039,  0.9922,  0.6314,\n",
      "         0.7490, -0.6235, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000])\n"
     ]
    }
   ],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Pad(2),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #torchvision.transforms.Lambda(lambda x: torch.squeeze(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x / torch.trace(x)),\n",
    "    #torchvision.transforms.Lambda(lambda x: (x+torch.t(x))/2)\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "    #torchvision.transforms.Lambda(lambda x: x.type(COMPLEX_DTYPE))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"FashionMNIST\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=preprocess,\n",
    ")\n",
    "dummy_trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "dummy_testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "dummy_x, dummy_y = next(iter(dummy_trainloader))\n",
    "\n",
    "print(dummy_x.shape)  # 64x32x32\n",
    "print(dummy_y.shape)  # 64\n",
    "print(dummy_y)\n",
    "print(dummy_x[0,0,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1712091555780,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     },
     "user_tz": -660
    },
    "id": "t6wDzOaJrmZ4",
    "outputId": "e216a519-beac-46f4-de29-cd0e9e280630"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([64, 1, 32, 32])\n",
      "SimpleNet(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Flatten(start_dim=1, end_dim=-1)\n",
      "    (3): Linear(in_features=12544, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class SimpleNet(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SimpleNet, self).__init__()\n",
    "\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(1, 32, kernel_size=3),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Conv2d(32, 16, kernel_size=3),\n",
    "        #torch.nn.ReLU(),\n",
    "        #torch.nn.MaxPool2d(kernel_size=2, stride=1),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(16*28*28, 10),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "net = SimpleNet().to(device)\n",
    "test_img = dummy_x.to(device)\n",
    "print(test_img.shape)\n",
    "print(net)\n",
    "test_out = net(test_img)\n",
    "print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDNHjKy4sSWZ",
    "outputId": "a0ef5fa2-455a-4017-d32f-bc0e38c7cdab",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712093844984,
     "user_tz": -660,
     "elapsed": 2289206,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of train batches = 600, Number of test batches = 100\n",
      "Print every train batch = 120, Print every test batch = 20\n",
      "Training at step=0, batch=0, train loss = 2.306792765239672, train acc = 0.10999999940395355, time = 0.38187241554260254\n",
      "Training at step=0, batch=120, train loss = 0.7192115776557847, train acc = 0.75, time = 0.008966684341430664\n",
      "Training at step=0, batch=240, train loss = 0.7989091330869031, train acc = 0.75, time = 0.008941650390625\n",
      "Training at step=0, batch=360, train loss = 0.4937506498953356, train acc = 0.7799999713897705, time = 0.009010553359985352\n",
      "Training at step=0, batch=480, train loss = 0.5181211140449858, train acc = 0.8399999737739563, time = 0.009001970291137695\n",
      "Testing at step=0, batch=0, test loss = 0.6360847186959647, test acc = 0.75, time = 0.0019049644470214844\n",
      "Testing at step=0, batch=20, test loss = 0.5886414608914923, test acc = 0.7599999904632568, time = 0.0019016265869140625\n",
      "Testing at step=0, batch=40, test loss = 0.45389199524904805, test acc = 0.8199999928474426, time = 0.0019049644470214844\n",
      "Testing at step=0, batch=60, test loss = 0.640431102497422, test acc = 0.7699999809265137, time = 0.0018842220306396484\n",
      "Testing at step=0, batch=80, test loss = 0.5357739669468177, test acc = 0.800000011920929, time = 0.001888275146484375\n",
      "Step 0 finished in 23.367867469787598, Train loss = 0.7241927799136988, Test loss = 0.5520994171337232; Train Acc = 0.7505166640629372, Test Acc = 0.7998999989032746\n",
      "Training at step=1, batch=0, train loss = 0.46399548580780836, train acc = 0.8299999833106995, time = 0.008932828903198242\n",
      "Training at step=1, batch=120, train loss = 0.40264857414024574, train acc = 0.8899999856948853, time = 0.00903630256652832\n",
      "Training at step=1, batch=240, train loss = 0.4986857496317564, train acc = 0.8299999833106995, time = 0.009136676788330078\n",
      "Training at step=1, batch=360, train loss = 0.6168165248964934, train acc = 0.7900000214576721, time = 0.008953332901000977\n",
      "Training at step=1, batch=480, train loss = 0.46922470297759106, train acc = 0.8100000023841858, time = 0.008922576904296875\n",
      "Testing at step=1, batch=0, test loss = 0.3711550374113527, test acc = 0.8600000143051147, time = 0.0019466876983642578\n",
      "Testing at step=1, batch=20, test loss = 0.5452622015875415, test acc = 0.7900000214576721, time = 0.0019447803497314453\n",
      "Testing at step=1, batch=40, test loss = 0.3866054170755167, test acc = 0.8399999737739563, time = 0.0020356178283691406\n",
      "Testing at step=1, batch=60, test loss = 0.45919201236977725, test acc = 0.8299999833106995, time = 0.0019669532775878906\n",
      "Testing at step=1, batch=80, test loss = 0.36857745237536577, test acc = 0.8799999952316284, time = 0.0019099712371826172\n",
      "Step 1 finished in 22.927559852600098, Train loss = 0.49692725900398776, Test loss = 0.5064217838536715; Train Acc = 0.8246166641513507, Test Acc = 0.8216999971866608\n",
      "Training at step=2, batch=0, train loss = 0.3371824946261796, train acc = 0.8799999952316284, time = 0.009082555770874023\n",
      "Training at step=2, batch=120, train loss = 0.4848131618693196, train acc = 0.8500000238418579, time = 0.009166240692138672\n",
      "Training at step=2, batch=240, train loss = 0.3111420333890567, train acc = 0.8999999761581421, time = 0.008991479873657227\n",
      "Training at step=2, batch=360, train loss = 0.48018303521439276, train acc = 0.8299999833106995, time = 0.008905649185180664\n",
      "Training at step=2, batch=480, train loss = 0.4668734893780955, train acc = 0.8399999737739563, time = 0.009028196334838867\n",
      "Testing at step=2, batch=0, test loss = 0.42121981288444943, test acc = 0.8399999737739563, time = 0.0019249916076660156\n",
      "Testing at step=2, batch=20, test loss = 0.3508443852217542, test acc = 0.8399999737739563, time = 0.001886606216430664\n",
      "Testing at step=2, batch=40, test loss = 0.4436583686933249, test acc = 0.8199999928474426, time = 0.0019121170043945312\n",
      "Testing at step=2, batch=60, test loss = 0.39005118180216813, test acc = 0.8799999952316284, time = 0.0018851757049560547\n",
      "Testing at step=2, batch=80, test loss = 0.64570032695551, test acc = 0.7599999904632568, time = 0.001898050308227539\n",
      "Step 2 finished in 22.832574605941772, Train loss = 0.46781972211485234, Test loss = 0.48556077941750453; Train Acc = 0.8363000000516574, Test Acc = 0.8260999971628189\n",
      "Training at step=3, batch=0, train loss = 0.4243443713220897, train acc = 0.8600000143051147, time = 0.009073495864868164\n",
      "Training at step=3, batch=120, train loss = 0.3715240127980108, train acc = 0.8899999856948853, time = 0.009057760238647461\n",
      "Training at step=3, batch=240, train loss = 0.5189557425643915, train acc = 0.8500000238418579, time = 0.009015798568725586\n",
      "Training at step=3, batch=360, train loss = 0.42981895552585053, train acc = 0.8299999833106995, time = 0.008959054946899414\n",
      "Training at step=3, batch=480, train loss = 0.43814758601955867, train acc = 0.8299999833106995, time = 0.008902311325073242\n",
      "Testing at step=3, batch=0, test loss = 0.5603308217612496, test acc = 0.8100000023841858, time = 0.0019195079803466797\n",
      "Testing at step=3, batch=20, test loss = 0.4667419089405675, test acc = 0.8700000047683716, time = 0.0019164085388183594\n",
      "Testing at step=3, batch=40, test loss = 0.4473227062178122, test acc = 0.8500000238418579, time = 0.0019004344940185547\n",
      "Testing at step=3, batch=60, test loss = 0.5504358674905664, test acc = 0.7900000214576721, time = 0.0019066333770751953\n",
      "Testing at step=3, batch=80, test loss = 0.3684473062859876, test acc = 0.8899999856948853, time = 0.0018916130065917969\n",
      "Step 3 finished in 22.84438419342041, Train loss = 0.4504732604781617, Test loss = 0.4715339074227788; Train Acc = 0.8435833312074343, Test Acc = 0.8336999982595443\n",
      "Training at step=4, batch=0, train loss = 0.43633851297081594, train acc = 0.8299999833106995, time = 0.008991241455078125\n",
      "Training at step=4, batch=120, train loss = 0.5314069536940423, train acc = 0.8399999737739563, time = 0.008942127227783203\n",
      "Training at step=4, batch=240, train loss = 0.4863975285878008, train acc = 0.8600000143051147, time = 0.008938074111938477\n",
      "Training at step=4, batch=360, train loss = 0.3824281683829121, train acc = 0.8299999833106995, time = 0.00902867317199707\n",
      "Training at step=4, batch=480, train loss = 0.338136588849379, train acc = 0.8600000143051147, time = 0.009004831314086914\n",
      "Testing at step=4, batch=0, test loss = 0.3083508542660185, test acc = 0.8999999761581421, time = 0.0019092559814453125\n",
      "Testing at step=4, batch=20, test loss = 0.5597006779470769, test acc = 0.8100000023841858, time = 0.0019073486328125\n",
      "Testing at step=4, batch=40, test loss = 0.49512180789750415, test acc = 0.8100000023841858, time = 0.001916646957397461\n",
      "Testing at step=4, batch=60, test loss = 0.4563012213848768, test acc = 0.8500000238418579, time = 0.0018928050994873047\n",
      "Testing at step=4, batch=80, test loss = 0.41605439336514005, test acc = 0.8600000143051147, time = 0.0018992424011230469\n",
      "Step 4 finished in 22.84795331954956, Train loss = 0.438984516942279, Test loss = 0.4662736931406332; Train Acc = 0.8485499987999598, Test Acc = 0.836800001859665\n",
      "Training at step=5, batch=0, train loss = 0.41948765694591794, train acc = 0.8700000047683716, time = 0.009040117263793945\n",
      "Training at step=5, batch=120, train loss = 0.6085427798412801, train acc = 0.7799999713897705, time = 0.008892059326171875\n",
      "Training at step=5, batch=240, train loss = 0.3744132394386543, train acc = 0.8700000047683716, time = 0.008914947509765625\n",
      "Training at step=5, batch=360, train loss = 0.4300533373282562, train acc = 0.8299999833106995, time = 0.008920907974243164\n",
      "Training at step=5, batch=480, train loss = 0.4087986574662619, train acc = 0.8399999737739563, time = 0.008926630020141602\n",
      "Testing at step=5, batch=0, test loss = 0.4695604929186263, test acc = 0.8199999928474426, time = 0.0019104480743408203\n",
      "Testing at step=5, batch=20, test loss = 0.4860410174948722, test acc = 0.8199999928474426, time = 0.0018887519836425781\n",
      "Testing at step=5, batch=40, test loss = 0.35736931296642516, test acc = 0.8600000143051147, time = 0.0018873214721679688\n",
      "Testing at step=5, batch=60, test loss = 0.44521422310437175, test acc = 0.8399999737739563, time = 0.0019257068634033203\n",
      "Testing at step=5, batch=80, test loss = 0.30912244567380576, test acc = 0.8799999952316284, time = 0.0019061565399169922\n",
      "Step 5 finished in 22.786092519760132, Train loss = 0.4327017824841274, Test loss = 0.46753177408702895; Train Acc = 0.8490333315730095, Test Acc = 0.8341999977827073\n",
      "Training at step=6, batch=0, train loss = 0.45890517201631253, train acc = 0.8500000238418579, time = 0.009036540985107422\n",
      "Training at step=6, batch=120, train loss = 0.41506585721568084, train acc = 0.7900000214576721, time = 0.008923530578613281\n",
      "Training at step=6, batch=240, train loss = 0.288410373670708, train acc = 0.9100000262260437, time = 0.008911848068237305\n",
      "Training at step=6, batch=360, train loss = 0.453539878902758, train acc = 0.8399999737739563, time = 0.008922338485717773\n",
      "Training at step=6, batch=480, train loss = 0.39554968045110195, train acc = 0.8199999928474426, time = 0.009056329727172852\n",
      "Testing at step=6, batch=0, test loss = 0.6872105906942013, test acc = 0.7400000095367432, time = 0.0019006729125976562\n",
      "Testing at step=6, batch=20, test loss = 0.3344630602418485, test acc = 0.9300000071525574, time = 0.0018868446350097656\n",
      "Testing at step=6, batch=40, test loss = 0.5501747292698543, test acc = 0.8299999833106995, time = 0.0018930435180664062\n",
      "Testing at step=6, batch=60, test loss = 0.45642097206697935, test acc = 0.8600000143051147, time = 0.0018887519836425781\n",
      "Testing at step=6, batch=80, test loss = 0.48577676394970276, test acc = 0.8199999928474426, time = 0.0019273757934570312\n",
      "Step 6 finished in 22.826085090637207, Train loss = 0.42687215068410805, Test loss = 0.45957655937722336; Train Acc = 0.8525833331545194, Test Acc = 0.8395999950170517\n",
      "Training at step=7, batch=0, train loss = 0.48803011375398747, train acc = 0.8899999856948853, time = 0.008987903594970703\n",
      "Training at step=7, batch=120, train loss = 0.39368673787946884, train acc = 0.8299999833106995, time = 0.008975744247436523\n",
      "Training at step=7, batch=240, train loss = 0.4045472905608606, train acc = 0.8500000238418579, time = 0.008896589279174805\n",
      "Training at step=7, batch=360, train loss = 0.323732231754448, train acc = 0.8899999856948853, time = 0.008913040161132812\n",
      "Training at step=7, batch=480, train loss = 0.5212297319156919, train acc = 0.8100000023841858, time = 0.008870363235473633\n",
      "Testing at step=7, batch=0, test loss = 0.4203555527071906, test acc = 0.8399999737739563, time = 0.0018999576568603516\n",
      "Testing at step=7, batch=20, test loss = 0.5692979740058498, test acc = 0.7799999713897705, time = 0.002115011215209961\n",
      "Testing at step=7, batch=40, test loss = 0.4101740081641169, test acc = 0.8999999761581421, time = 0.0019168853759765625\n",
      "Testing at step=7, batch=60, test loss = 0.429156236784378, test acc = 0.8299999833106995, time = 0.001947641372680664\n",
      "Testing at step=7, batch=80, test loss = 0.5106044828717562, test acc = 0.8399999737739563, time = 0.0019097328186035156\n",
      "Step 7 finished in 22.740169763565063, Train loss = 0.4228418993531209, Test loss = 0.47277055483167; Train Acc = 0.8538999994595845, Test Acc = 0.8327999979257583\n",
      "Training at step=8, batch=0, train loss = 0.3655015256848981, train acc = 0.8500000238418579, time = 0.009028911590576172\n",
      "Training at step=8, batch=120, train loss = 0.5165872559478756, train acc = 0.8500000238418579, time = 0.008932828903198242\n",
      "Training at step=8, batch=240, train loss = 0.33491016210869423, train acc = 0.8899999856948853, time = 0.008938789367675781\n",
      "Training at step=8, batch=360, train loss = 0.4273836706980078, train acc = 0.8700000047683716, time = 0.009061336517333984\n",
      "Training at step=8, batch=480, train loss = 0.46053264686629874, train acc = 0.8399999737739563, time = 0.00889730453491211\n",
      "Testing at step=8, batch=0, test loss = 0.34516584883616624, test acc = 0.8500000238418579, time = 0.0020294189453125\n",
      "Testing at step=8, batch=20, test loss = 0.5111371089419474, test acc = 0.8700000047683716, time = 0.0019116401672363281\n",
      "Testing at step=8, batch=40, test loss = 0.6002053788307389, test acc = 0.7799999713897705, time = 0.0018811225891113281\n",
      "Testing at step=8, batch=60, test loss = 0.5211527804278253, test acc = 0.8199999928474426, time = 0.001890420913696289\n",
      "Testing at step=8, batch=80, test loss = 0.5368436106821857, test acc = 0.8199999928474426, time = 0.0019297599792480469\n",
      "Step 8 finished in 22.767349243164062, Train loss = 0.4180452636112983, Test loss = 0.44983546665118446; Train Acc = 0.8549333331982295, Test Acc = 0.8423000007867814\n",
      "Training at step=9, batch=0, train loss = 0.284836047433183, train acc = 0.8999999761581421, time = 0.008907794952392578\n",
      "Training at step=9, batch=120, train loss = 0.3203747075433533, train acc = 0.8799999952316284, time = 0.008968353271484375\n",
      "Training at step=9, batch=240, train loss = 0.28958465714803217, train acc = 0.8799999952316284, time = 0.008957862854003906\n",
      "Training at step=9, batch=360, train loss = 0.4391540291618157, train acc = 0.8500000238418579, time = 0.008952856063842773\n",
      "Training at step=9, batch=480, train loss = 0.5192846415216762, train acc = 0.8299999833106995, time = 0.010175943374633789\n",
      "Testing at step=9, batch=0, test loss = 0.4722557688829273, test acc = 0.8500000238418579, time = 0.0019249916076660156\n",
      "Testing at step=9, batch=20, test loss = 0.4780916940466632, test acc = 0.8500000238418579, time = 0.0019054412841796875\n",
      "Testing at step=9, batch=40, test loss = 0.31681198792598436, test acc = 0.8600000143051147, time = 0.0019199848175048828\n",
      "Testing at step=9, batch=60, test loss = 0.4597937025537878, test acc = 0.8399999737739563, time = 0.0019006729125976562\n",
      "Testing at step=9, batch=80, test loss = 0.38033511866320274, test acc = 0.8399999737739563, time = 0.0019071102142333984\n",
      "Step 9 finished in 22.826473236083984, Train loss = 0.41488398308193497, Test loss = 0.45123776578624586; Train Acc = 0.8559500005841255, Test Acc = 0.8421999996900559\n",
      "Training at step=10, batch=0, train loss = 0.4448896218884789, train acc = 0.8500000238418579, time = 0.009113073348999023\n",
      "Training at step=10, batch=120, train loss = 0.6128450514883766, train acc = 0.7799999713897705, time = 0.008878469467163086\n",
      "Training at step=10, batch=240, train loss = 0.3893809247290785, train acc = 0.8600000143051147, time = 0.009011507034301758\n",
      "Training at step=10, batch=360, train loss = 0.5860890376629868, train acc = 0.7599999904632568, time = 0.00894021987915039\n",
      "Training at step=10, batch=480, train loss = 0.5100624522889672, train acc = 0.8199999928474426, time = 0.008982658386230469\n",
      "Testing at step=10, batch=0, test loss = 0.47611722599001843, test acc = 0.8399999737739563, time = 0.0019288063049316406\n",
      "Testing at step=10, batch=20, test loss = 0.47637549676419455, test acc = 0.8100000023841858, time = 0.0018868446350097656\n",
      "Testing at step=10, batch=40, test loss = 0.3998145778027437, test acc = 0.8299999833106995, time = 0.0018999576568603516\n",
      "Testing at step=10, batch=60, test loss = 0.3155515988989814, test acc = 0.8700000047683716, time = 0.0018897056579589844\n",
      "Testing at step=10, batch=80, test loss = 0.34972161958485, test acc = 0.8999999761581421, time = 0.0018992424011230469\n",
      "Step 10 finished in 22.803027153015137, Train loss = 0.4135324983605394, Test loss = 0.44585788796957615; Train Acc = 0.8566833313306172, Test Acc = 0.8432999956607818\n",
      "Training at step=11, batch=0, train loss = 0.4821007034546614, train acc = 0.8600000143051147, time = 0.009043693542480469\n",
      "Training at step=11, batch=120, train loss = 0.3468526387872102, train acc = 0.8899999856948853, time = 0.008986234664916992\n",
      "Training at step=11, batch=240, train loss = 0.41412007472087564, train acc = 0.8299999833106995, time = 0.009261131286621094\n",
      "Training at step=11, batch=360, train loss = 0.5051528069638243, train acc = 0.8500000238418579, time = 0.009066343307495117\n",
      "Training at step=11, batch=480, train loss = 0.42167626833945543, train acc = 0.8600000143051147, time = 0.008887290954589844\n",
      "Testing at step=11, batch=0, test loss = 0.3421301291738657, test acc = 0.8700000047683716, time = 0.0019197463989257812\n",
      "Testing at step=11, batch=20, test loss = 0.37371721284741627, test acc = 0.8700000047683716, time = 0.0018923282623291016\n",
      "Testing at step=11, batch=40, test loss = 0.31619276197422813, test acc = 0.8999999761581421, time = 0.001924753189086914\n",
      "Testing at step=11, batch=60, test loss = 0.40520569964733, test acc = 0.8500000238418579, time = 0.0018837451934814453\n",
      "Testing at step=11, batch=80, test loss = 0.374953082679357, test acc = 0.8500000238418579, time = 0.0018963813781738281\n",
      "Step 11 finished in 22.752376556396484, Train loss = 0.4088233507591385, Test loss = 0.44709694612122874; Train Acc = 0.8583833326896032, Test Acc = 0.8416000020503998\n",
      "Training at step=12, batch=0, train loss = 0.33083304548813663, train acc = 0.9200000166893005, time = 0.009042978286743164\n",
      "Training at step=12, batch=120, train loss = 0.5652058466304524, train acc = 0.7799999713897705, time = 0.00892329216003418\n",
      "Training at step=12, batch=240, train loss = 0.4974552925307536, train acc = 0.800000011920929, time = 0.008955001831054688\n",
      "Training at step=12, batch=360, train loss = 0.33683023947903123, train acc = 0.8700000047683716, time = 0.008867263793945312\n",
      "Training at step=12, batch=480, train loss = 0.4847595413968698, train acc = 0.8399999737739563, time = 0.008888483047485352\n",
      "Testing at step=12, batch=0, test loss = 0.4078337783096074, test acc = 0.8100000023841858, time = 0.001932382583618164\n",
      "Testing at step=12, batch=20, test loss = 0.4740462692307281, test acc = 0.8299999833106995, time = 0.0019102096557617188\n",
      "Testing at step=12, batch=40, test loss = 0.4117555361433478, test acc = 0.8500000238418579, time = 0.001922607421875\n",
      "Testing at step=12, batch=60, test loss = 0.5209946514625112, test acc = 0.8299999833106995, time = 0.0019230842590332031\n",
      "Testing at step=12, batch=80, test loss = 0.3863402984029285, test acc = 0.8600000143051147, time = 0.0019221305847167969\n",
      "Step 12 finished in 22.79743528366089, Train loss = 0.4058498867475246, Test loss = 0.45087096249492165; Train Acc = 0.8594999985893568, Test Acc = 0.839699998497963\n",
      "Training at step=13, batch=0, train loss = 0.5956236837774095, train acc = 0.8100000023841858, time = 0.009028434753417969\n",
      "Training at step=13, batch=120, train loss = 0.35834605363553707, train acc = 0.8799999952316284, time = 0.008923053741455078\n",
      "Training at step=13, batch=240, train loss = 0.3866697607009337, train acc = 0.8500000238418579, time = 0.008909940719604492\n",
      "Training at step=13, batch=360, train loss = 0.4536201599269788, train acc = 0.8600000143051147, time = 0.008868217468261719\n",
      "Training at step=13, batch=480, train loss = 0.2936629612917795, train acc = 0.9100000262260437, time = 0.008943319320678711\n",
      "Testing at step=13, batch=0, test loss = 0.4036594382073329, test acc = 0.8299999833106995, time = 0.0019118785858154297\n",
      "Testing at step=13, batch=20, test loss = 0.5522390850724407, test acc = 0.8399999737739563, time = 0.0019080638885498047\n",
      "Testing at step=13, batch=40, test loss = 0.4757083633044592, test acc = 0.8500000238418579, time = 0.0019021034240722656\n",
      "Testing at step=13, batch=60, test loss = 0.41975036743787136, test acc = 0.8600000143051147, time = 0.001895904541015625\n",
      "Testing at step=13, batch=80, test loss = 0.5031920692146967, test acc = 0.8299999833106995, time = 0.0019080638885498047\n",
      "Step 13 finished in 22.743794679641724, Train loss = 0.4033505810142347, Test loss = 0.4463709409901185; Train Acc = 0.8614666668574015, Test Acc = 0.8415999990701676\n",
      "Training at step=14, batch=0, train loss = 0.36056805565617805, train acc = 0.8700000047683716, time = 0.009020090103149414\n",
      "Training at step=14, batch=120, train loss = 0.2824052397874905, train acc = 0.8999999761581421, time = 0.008995532989501953\n",
      "Training at step=14, batch=240, train loss = 0.45738037003284854, train acc = 0.8399999737739563, time = 0.008913993835449219\n",
      "Training at step=14, batch=360, train loss = 0.5241879203362414, train acc = 0.8399999737739563, time = 0.008899450302124023\n",
      "Training at step=14, batch=480, train loss = 0.3447960377470436, train acc = 0.8999999761581421, time = 0.008986234664916992\n",
      "Testing at step=14, batch=0, test loss = 0.3929341815207007, test acc = 0.8799999952316284, time = 0.0019261837005615234\n",
      "Testing at step=14, batch=20, test loss = 0.3471005221551555, test acc = 0.8899999856948853, time = 0.0019104480743408203\n",
      "Testing at step=14, batch=40, test loss = 0.38933346383259093, test acc = 0.8799999952316284, time = 0.0019087791442871094\n",
      "Testing at step=14, batch=60, test loss = 0.5327985521853443, test acc = 0.8500000238418579, time = 0.00189971923828125\n",
      "Testing at step=14, batch=80, test loss = 0.41633485805965625, test acc = 0.8299999833106995, time = 0.0019147396087646484\n",
      "Step 14 finished in 22.81611466407776, Train loss = 0.4020389145885073, Test loss = 0.44616095016774054; Train Acc = 0.8613166657090187, Test Acc = 0.841199997663498\n",
      "Training at step=15, batch=0, train loss = 0.4855686250394938, train acc = 0.8299999833106995, time = 0.009101629257202148\n",
      "Training at step=15, batch=120, train loss = 0.4993573969455206, train acc = 0.8100000023841858, time = 0.008867025375366211\n",
      "Training at step=15, batch=240, train loss = 0.25002890439318504, train acc = 0.9100000262260437, time = 0.008903026580810547\n",
      "Training at step=15, batch=360, train loss = 0.6214149320785647, train acc = 0.8100000023841858, time = 0.008864879608154297\n",
      "Training at step=15, batch=480, train loss = 0.29903756442929524, train acc = 0.8999999761581421, time = 0.008917570114135742\n",
      "Testing at step=15, batch=0, test loss = 0.36812939325279215, test acc = 0.8600000143051147, time = 0.0018978118896484375\n",
      "Testing at step=15, batch=20, test loss = 0.6758527647618862, test acc = 0.8100000023841858, time = 0.0019097328186035156\n",
      "Testing at step=15, batch=40, test loss = 0.48277268429833997, test acc = 0.7900000214576721, time = 0.0019309520721435547\n",
      "Testing at step=15, batch=60, test loss = 0.4238993058553234, test acc = 0.8799999952316284, time = 0.0018830299377441406\n",
      "Testing at step=15, batch=80, test loss = 0.5841290857043293, test acc = 0.7799999713897705, time = 0.0019371509552001953\n",
      "Step 15 finished in 22.808196783065796, Train loss = 0.39930673672730693, Test loss = 0.44276150800827396; Train Acc = 0.8619499995311102, Test Acc = 0.8433000004291534\n",
      "Training at step=16, batch=0, train loss = 0.3630646164069074, train acc = 0.8899999856948853, time = 0.009040117263793945\n",
      "Training at step=16, batch=120, train loss = 0.3536447150864214, train acc = 0.8899999856948853, time = 0.008949995040893555\n",
      "Training at step=16, batch=240, train loss = 0.3449266499953378, train acc = 0.8799999952316284, time = 0.00886392593383789\n",
      "Training at step=16, batch=360, train loss = 0.3342819975368064, train acc = 0.9100000262260437, time = 0.008933544158935547\n",
      "Training at step=16, batch=480, train loss = 0.3539657967316694, train acc = 0.8500000238418579, time = 0.008914947509765625\n",
      "Testing at step=16, batch=0, test loss = 0.5558324523431785, test acc = 0.8199999928474426, time = 0.0019152164459228516\n",
      "Testing at step=16, batch=20, test loss = 0.5041871688051116, test acc = 0.7900000214576721, time = 0.0018970966339111328\n",
      "Testing at step=16, batch=40, test loss = 0.42145479685069404, test acc = 0.8600000143051147, time = 0.0019068717956542969\n",
      "Testing at step=16, batch=60, test loss = 0.3927164252353928, test acc = 0.8700000047683716, time = 0.0018908977508544922\n",
      "Testing at step=16, batch=80, test loss = 0.41366925811288857, test acc = 0.8500000238418579, time = 0.0019040107727050781\n",
      "Step 16 finished in 22.809603452682495, Train loss = 0.3981798736447315, Test loss = 0.4408723513357787; Train Acc = 0.8622166670362155, Test Acc = 0.8437999999523162\n",
      "Training at step=17, batch=0, train loss = 0.5139728070842458, train acc = 0.8299999833106995, time = 0.009047508239746094\n",
      "Training at step=17, batch=120, train loss = 0.3194454647976629, train acc = 0.8999999761581421, time = 0.00894474983215332\n",
      "Training at step=17, batch=240, train loss = 0.38174202940903046, train acc = 0.9200000166893005, time = 0.008914947509765625\n",
      "Training at step=17, batch=360, train loss = 0.5931338243154356, train acc = 0.800000011920929, time = 0.009051322937011719\n",
      "Training at step=17, batch=480, train loss = 0.6380872652805769, train acc = 0.8500000238418579, time = 0.009217262268066406\n",
      "Testing at step=17, batch=0, test loss = 0.4402019290433264, test acc = 0.8500000238418579, time = 0.0019257068634033203\n",
      "Testing at step=17, batch=20, test loss = 0.35607234727271053, test acc = 0.9100000262260437, time = 0.0019183158874511719\n",
      "Testing at step=17, batch=40, test loss = 0.5684928337918398, test acc = 0.7599999904632568, time = 0.0019004344940185547\n",
      "Testing at step=17, batch=60, test loss = 0.3370845955410504, test acc = 0.8799999952316284, time = 0.0019154548645019531\n",
      "Testing at step=17, batch=80, test loss = 0.46587929505595455, test acc = 0.8100000023841858, time = 0.0019459724426269531\n",
      "Step 17 finished in 22.819299459457397, Train loss = 0.3953990972129022, Test loss = 0.4485337268648547; Train Acc = 0.8636666651566823, Test Acc = 0.8423999983072281\n",
      "Training at step=18, batch=0, train loss = 0.4013833428609928, train acc = 0.8700000047683716, time = 0.00900125503540039\n",
      "Training at step=18, batch=120, train loss = 0.4873671718701445, train acc = 0.8600000143051147, time = 0.008978605270385742\n",
      "Training at step=18, batch=240, train loss = 0.4911610077247277, train acc = 0.8100000023841858, time = 0.008850574493408203\n",
      "Training at step=18, batch=360, train loss = 0.5200733072238457, train acc = 0.8399999737739563, time = 0.00896310806274414\n",
      "Training at step=18, batch=480, train loss = 0.3310161140609796, train acc = 0.9300000071525574, time = 0.008855342864990234\n",
      "Testing at step=18, batch=0, test loss = 0.34741173154000626, test acc = 0.8600000143051147, time = 0.0018947124481201172\n",
      "Testing at step=18, batch=20, test loss = 0.4038755415892959, test acc = 0.8299999833106995, time = 0.0019063949584960938\n",
      "Testing at step=18, batch=40, test loss = 0.5264257478351202, test acc = 0.8600000143051147, time = 0.0019068717956542969\n",
      "Testing at step=18, batch=60, test loss = 0.3084997599124523, test acc = 0.9200000166893005, time = 0.0018987655639648438\n",
      "Testing at step=18, batch=80, test loss = 0.37219821669490394, test acc = 0.8600000143051147, time = 0.0018930435180664062\n",
      "Step 18 finished in 22.78958535194397, Train loss = 0.39489482967001066, Test loss = 0.4439812703938793; Train Acc = 0.8632666664322217, Test Acc = 0.8397000002861023\n",
      "Training at step=19, batch=0, train loss = 0.40006046496394915, train acc = 0.8999999761581421, time = 0.008988618850708008\n",
      "Training at step=19, batch=120, train loss = 0.31513850016853534, train acc = 0.8799999952316284, time = 0.008936166763305664\n",
      "Training at step=19, batch=240, train loss = 0.4701616022012159, train acc = 0.8100000023841858, time = 0.00898289680480957\n",
      "Training at step=19, batch=360, train loss = 0.33449754910620655, train acc = 0.8399999737739563, time = 0.008966445922851562\n",
      "Training at step=19, batch=480, train loss = 0.38072123053849233, train acc = 0.8799999952316284, time = 0.008922815322875977\n",
      "Testing at step=19, batch=0, test loss = 0.3191269919862804, test acc = 0.8799999952316284, time = 0.0019245147705078125\n",
      "Testing at step=19, batch=20, test loss = 0.7127772024510928, test acc = 0.7699999809265137, time = 0.0019207000732421875\n",
      "Testing at step=19, batch=40, test loss = 0.4433202936000248, test acc = 0.8199999928474426, time = 0.001909017562866211\n",
      "Testing at step=19, batch=60, test loss = 0.44309912704858073, test acc = 0.8600000143051147, time = 0.0019059181213378906\n",
      "Testing at step=19, batch=80, test loss = 0.4109788940597319, test acc = 0.8399999737739563, time = 0.0019135475158691406\n",
      "Step 19 finished in 22.76970624923706, Train loss = 0.3929343544997563, Test loss = 0.4446033300533724; Train Acc = 0.8640833325187365, Test Acc = 0.841499999165535\n",
      "Training at step=20, batch=0, train loss = 0.24854419857518043, train acc = 0.9300000071525574, time = 0.009046316146850586\n",
      "Training at step=20, batch=120, train loss = 0.32730533471015166, train acc = 0.8600000143051147, time = 0.008887767791748047\n",
      "Training at step=20, batch=240, train loss = 0.2940426179554855, train acc = 0.8899999856948853, time = 0.008995532989501953\n",
      "Training at step=20, batch=360, train loss = 0.3985652509517667, train acc = 0.8299999833106995, time = 0.008915901184082031\n",
      "Training at step=20, batch=480, train loss = 0.37872407262875546, train acc = 0.8700000047683716, time = 0.00890040397644043\n",
      "Testing at step=20, batch=0, test loss = 0.4991193159254351, test acc = 0.8299999833106995, time = 0.0019009113311767578\n",
      "Testing at step=20, batch=20, test loss = 0.46917301929707017, test acc = 0.8299999833106995, time = 0.0018928050994873047\n",
      "Testing at step=20, batch=40, test loss = 0.45798743936757513, test acc = 0.8399999737739563, time = 0.0019207000732421875\n",
      "Testing at step=20, batch=60, test loss = 0.34296276878622706, test acc = 0.8999999761581421, time = 0.0019154548645019531\n",
      "Testing at step=20, batch=80, test loss = 0.4351763120441285, test acc = 0.8899999856948853, time = 0.0019049644470214844\n",
      "Step 20 finished in 22.798768997192383, Train loss = 0.39202740234704125, Test loss = 0.4382791899872641; Train Acc = 0.8638999985655149, Test Acc = 0.8446999973058701\n",
      "Training at step=21, batch=0, train loss = 0.34377269592131815, train acc = 0.8600000143051147, time = 0.00903177261352539\n",
      "Training at step=21, batch=120, train loss = 0.5222656551552047, train acc = 0.8399999737739563, time = 0.008976459503173828\n",
      "Training at step=21, batch=240, train loss = 0.4025953652786732, train acc = 0.8600000143051147, time = 0.00899815559387207\n",
      "Training at step=21, batch=360, train loss = 0.3270247422161621, train acc = 0.8999999761581421, time = 0.008920431137084961\n",
      "Training at step=21, batch=480, train loss = 0.37078024892935685, train acc = 0.8700000047683716, time = 0.008968114852905273\n",
      "Testing at step=21, batch=0, test loss = 0.4049365625958392, test acc = 0.8999999761581421, time = 0.001924276351928711\n",
      "Testing at step=21, batch=20, test loss = 0.4300774526082858, test acc = 0.8299999833106995, time = 0.0019176006317138672\n",
      "Testing at step=21, batch=40, test loss = 0.2410858604117632, test acc = 0.9300000071525574, time = 0.002006053924560547\n",
      "Testing at step=21, batch=60, test loss = 0.3048619665851156, test acc = 0.9100000262260437, time = 0.001874685287475586\n",
      "Testing at step=21, batch=80, test loss = 0.5012550075864387, test acc = 0.8299999833106995, time = 0.002095937728881836\n",
      "Step 21 finished in 22.782532930374146, Train loss = 0.3909713845312867, Test loss = 0.4424454932222705; Train Acc = 0.8638500000039736, Test Acc = 0.8428999978303909\n",
      "Training at step=22, batch=0, train loss = 0.39784736102750445, train acc = 0.8399999737739563, time = 0.009042978286743164\n",
      "Training at step=22, batch=120, train loss = 0.4344529823501919, train acc = 0.7900000214576721, time = 0.008893013000488281\n",
      "Training at step=22, batch=240, train loss = 0.3453159724733081, train acc = 0.8999999761581421, time = 0.008963346481323242\n",
      "Training at step=22, batch=360, train loss = 0.3529886137970288, train acc = 0.8600000143051147, time = 0.008932828903198242\n",
      "Training at step=22, batch=480, train loss = 0.5068128506596618, train acc = 0.8799999952316284, time = 0.008889198303222656\n",
      "Testing at step=22, batch=0, test loss = 0.5378922967686429, test acc = 0.800000011920929, time = 0.0019168853759765625\n",
      "Testing at step=22, batch=20, test loss = 0.3387782667611453, test acc = 0.8899999856948853, time = 0.001894235610961914\n",
      "Testing at step=22, batch=40, test loss = 0.3300899943407937, test acc = 0.8600000143051147, time = 0.0019156932830810547\n",
      "Testing at step=22, batch=60, test loss = 0.39610678057139104, test acc = 0.8799999952316284, time = 0.0019474029541015625\n",
      "Testing at step=22, batch=80, test loss = 0.5761657193420694, test acc = 0.7799999713897705, time = 0.0019228458404541016\n",
      "Step 22 finished in 22.8089599609375, Train loss = 0.38971470383321666, Test loss = 0.44415084990656484; Train Acc = 0.8640666660666466, Test Acc = 0.842399999499321\n",
      "Training at step=23, batch=0, train loss = 0.41087246555737195, train acc = 0.8799999952316284, time = 0.009096384048461914\n",
      "Training at step=23, batch=120, train loss = 0.3639501481318648, train acc = 0.8799999952316284, time = 0.00896596908569336\n",
      "Training at step=23, batch=240, train loss = 0.4550788368501941, train acc = 0.8399999737739563, time = 0.008981704711914062\n",
      "Training at step=23, batch=360, train loss = 0.47143635560722513, train acc = 0.8199999928474426, time = 0.009039878845214844\n",
      "Training at step=23, batch=480, train loss = 0.27557018454322596, train acc = 0.9100000262260437, time = 0.008921146392822266\n",
      "Testing at step=23, batch=0, test loss = 0.3915101607607496, test acc = 0.8199999928474426, time = 0.0019257068634033203\n",
      "Testing at step=23, batch=20, test loss = 0.3152396660230897, test acc = 0.8999999761581421, time = 0.0019245147705078125\n",
      "Testing at step=23, batch=40, test loss = 0.30888517205744664, test acc = 0.8799999952316284, time = 0.0019071102142333984\n",
      "Testing at step=23, batch=60, test loss = 0.694828731817648, test acc = 0.800000011920929, time = 0.0018966197967529297\n",
      "Testing at step=23, batch=80, test loss = 0.5507325925683434, test acc = 0.8199999928474426, time = 0.0020322799682617188\n",
      "Step 23 finished in 22.820406913757324, Train loss = 0.38867765984135844, Test loss = 0.44860068835458533; Train Acc = 0.8644166669249534, Test Acc = 0.8418999993801117\n",
      "Training at step=24, batch=0, train loss = 0.3684869289830532, train acc = 0.8399999737739563, time = 0.009018182754516602\n",
      "Training at step=24, batch=120, train loss = 0.3407595817217647, train acc = 0.8700000047683716, time = 0.009013891220092773\n",
      "Training at step=24, batch=240, train loss = 0.3612578585201273, train acc = 0.8700000047683716, time = 0.009011268615722656\n",
      "Training at step=24, batch=360, train loss = 0.38375825020397725, train acc = 0.8799999952316284, time = 0.008917093276977539\n",
      "Training at step=24, batch=480, train loss = 0.39288777604553404, train acc = 0.8700000047683716, time = 0.008951425552368164\n",
      "Testing at step=24, batch=0, test loss = 0.4252098766805757, test acc = 0.8399999737739563, time = 0.0019047260284423828\n",
      "Testing at step=24, batch=20, test loss = 0.4822243509450115, test acc = 0.8299999833106995, time = 0.0019233226776123047\n",
      "Testing at step=24, batch=40, test loss = 0.43373749776371817, test acc = 0.8399999737739563, time = 0.0019159317016601562\n",
      "Testing at step=24, batch=60, test loss = 0.32238382421171946, test acc = 0.8999999761581421, time = 0.0019161701202392578\n",
      "Testing at step=24, batch=80, test loss = 0.3560518715419013, test acc = 0.8600000143051147, time = 0.0019106864929199219\n",
      "Step 24 finished in 22.79592490196228, Train loss = 0.3868514841211871, Test loss = 0.4446956141143987; Train Acc = 0.8657999992370605, Test Acc = 0.8412999975681305\n",
      "Training at step=25, batch=0, train loss = 0.28691833561123714, train acc = 0.9100000262260437, time = 0.00904226303100586\n",
      "Training at step=25, batch=120, train loss = 0.5086924941701413, train acc = 0.800000011920929, time = 0.008908510208129883\n",
      "Training at step=25, batch=240, train loss = 0.36691319753376817, train acc = 0.8299999833106995, time = 0.008976936340332031\n",
      "Training at step=25, batch=360, train loss = 0.4521081481054637, train acc = 0.8199999928474426, time = 0.008945703506469727\n",
      "Training at step=25, batch=480, train loss = 0.4671981981324879, train acc = 0.8500000238418579, time = 0.008878231048583984\n",
      "Testing at step=25, batch=0, test loss = 0.4978175213554855, test acc = 0.7699999809265137, time = 0.0020322799682617188\n",
      "Testing at step=25, batch=20, test loss = 0.3781040190861198, test acc = 0.8899999856948853, time = 0.0019237995147705078\n",
      "Testing at step=25, batch=40, test loss = 0.35857797404428693, test acc = 0.8799999952316284, time = 0.0019047260284423828\n",
      "Testing at step=25, batch=60, test loss = 0.5653842304498893, test acc = 0.7900000214576721, time = 0.0019044876098632812\n",
      "Testing at step=25, batch=80, test loss = 0.6052845037992826, test acc = 0.8100000023841858, time = 0.0019142627716064453\n",
      "Step 25 finished in 22.809194087982178, Train loss = 0.38614010188607406, Test loss = 0.44302855552838133; Train Acc = 0.8666333319743474, Test Acc = 0.8440000027418136\n",
      "Training at step=26, batch=0, train loss = 0.40713634662359305, train acc = 0.8899999856948853, time = 0.00901937484741211\n",
      "Training at step=26, batch=120, train loss = 0.43727321784825635, train acc = 0.8500000238418579, time = 0.008998632431030273\n",
      "Training at step=26, batch=240, train loss = 0.47906373556606313, train acc = 0.8600000143051147, time = 0.009052276611328125\n",
      "Training at step=26, batch=360, train loss = 0.34888409179069674, train acc = 0.8799999952316284, time = 0.008905410766601562\n",
      "Training at step=26, batch=480, train loss = 0.29547362954469614, train acc = 0.8700000047683716, time = 0.00888681411743164\n",
      "Testing at step=26, batch=0, test loss = 0.5188165884315269, test acc = 0.8199999928474426, time = 0.0021011829376220703\n",
      "Testing at step=26, batch=20, test loss = 0.37384260805496333, test acc = 0.8899999856948853, time = 0.0018949508666992188\n",
      "Testing at step=26, batch=40, test loss = 0.36530871419399646, test acc = 0.8899999856948853, time = 0.0019178390502929688\n",
      "Testing at step=26, batch=60, test loss = 0.41194234243499767, test acc = 0.8700000047683716, time = 0.0019097328186035156\n",
      "Testing at step=26, batch=80, test loss = 0.34476253284227504, test acc = 0.8700000047683716, time = 0.0019016265869140625\n",
      "Step 26 finished in 22.867855310440063, Train loss = 0.3862648052921272, Test loss = 0.4406540916937186; Train Acc = 0.8664000006516774, Test Acc = 0.8423999983072281\n",
      "Training at step=27, batch=0, train loss = 0.3083842117066469, train acc = 0.8999999761581421, time = 0.009140968322753906\n",
      "Training at step=27, batch=120, train loss = 0.41979377154749414, train acc = 0.8399999737739563, time = 0.009012222290039062\n",
      "Training at step=27, batch=240, train loss = 0.3284509067749612, train acc = 0.8700000047683716, time = 0.00892496109008789\n",
      "Training at step=27, batch=360, train loss = 0.45672493469509107, train acc = 0.8500000238418579, time = 0.009151935577392578\n",
      "Training at step=27, batch=480, train loss = 0.32239415163934815, train acc = 0.8899999856948853, time = 0.008945465087890625\n",
      "Testing at step=27, batch=0, test loss = 0.3848776938084606, test acc = 0.8700000047683716, time = 0.0020105838775634766\n",
      "Testing at step=27, batch=20, test loss = 0.5318200796981288, test acc = 0.8299999833106995, time = 0.0019490718841552734\n",
      "Testing at step=27, batch=40, test loss = 0.36671669947709923, test acc = 0.8899999856948853, time = 0.0019030570983886719\n",
      "Testing at step=27, batch=60, test loss = 0.5334957057079621, test acc = 0.8299999833106995, time = 0.0019080638885498047\n",
      "Testing at step=27, batch=80, test loss = 0.3742825010916486, test acc = 0.8399999737739563, time = 0.0019888877868652344\n",
      "Step 27 finished in 22.86324977874756, Train loss = 0.3848183766074416, Test loss = 0.44217450055998264; Train Acc = 0.8662333339452744, Test Acc = 0.8437000000476838\n",
      "Training at step=28, batch=0, train loss = 0.3794528470194883, train acc = 0.8799999952316284, time = 0.009221553802490234\n",
      "Training at step=28, batch=120, train loss = 0.4539035541598216, train acc = 0.8500000238418579, time = 0.008920431137084961\n",
      "Training at step=28, batch=240, train loss = 0.48064373140709293, train acc = 0.800000011920929, time = 0.008947610855102539\n",
      "Training at step=28, batch=360, train loss = 0.45130573642609684, train acc = 0.8299999833106995, time = 0.008973360061645508\n",
      "Training at step=28, batch=480, train loss = 0.31264383781379534, train acc = 0.8999999761581421, time = 0.00905919075012207\n",
      "Testing at step=28, batch=0, test loss = 0.312680342953189, test acc = 0.8899999856948853, time = 0.0019164085388183594\n",
      "Testing at step=28, batch=20, test loss = 0.5774617453806067, test acc = 0.8100000023841858, time = 0.0019097328186035156\n",
      "Testing at step=28, batch=40, test loss = 0.4638256556259053, test acc = 0.8500000238418579, time = 0.0019099712371826172\n",
      "Testing at step=28, batch=60, test loss = 0.4541359474093973, test acc = 0.7799999713897705, time = 0.0019125938415527344\n",
      "Testing at step=28, batch=80, test loss = 0.46499808028078027, test acc = 0.8500000238418579, time = 0.0019443035125732422\n",
      "Step 28 finished in 22.876160621643066, Train loss = 0.3834550898684994, Test loss = 0.4391943531927337; Train Acc = 0.866099999845028, Test Acc = 0.8410999995470047\n",
      "Training at step=29, batch=0, train loss = 0.239570504118728, train acc = 0.8999999761581421, time = 0.009092092514038086\n",
      "Training at step=29, batch=120, train loss = 0.4296058761561413, train acc = 0.8600000143051147, time = 0.008920907974243164\n",
      "Training at step=29, batch=240, train loss = 0.2476015592665924, train acc = 0.9100000262260437, time = 0.008933544158935547\n",
      "Training at step=29, batch=360, train loss = 0.3025946870836262, train acc = 0.9200000166893005, time = 0.008936405181884766\n",
      "Training at step=29, batch=480, train loss = 0.38787828831399707, train acc = 0.8500000238418579, time = 0.008931159973144531\n",
      "Testing at step=29, batch=0, test loss = 0.47364490996443254, test acc = 0.8500000238418579, time = 0.0020172595977783203\n",
      "Testing at step=29, batch=20, test loss = 0.4901404776130888, test acc = 0.8600000143051147, time = 0.0018956661224365234\n",
      "Testing at step=29, batch=40, test loss = 0.5832767869620917, test acc = 0.7900000214576721, time = 0.0019063949584960938\n",
      "Testing at step=29, batch=60, test loss = 0.3015448029483923, test acc = 0.8700000047683716, time = 0.001996278762817383\n",
      "Testing at step=29, batch=80, test loss = 0.5244008791680088, test acc = 0.8199999928474426, time = 0.0019261837005615234\n",
      "Step 29 finished in 22.85547113418579, Train loss = 0.38206512736210807, Test loss = 0.4458890277168032; Train Acc = 0.8669666661818822, Test Acc = 0.8429000008106232\n",
      "Training at step=30, batch=0, train loss = 0.39774827684581526, train acc = 0.8600000143051147, time = 0.009077072143554688\n",
      "Training at step=30, batch=120, train loss = 0.37970702948020096, train acc = 0.8500000238418579, time = 0.009972095489501953\n",
      "Training at step=30, batch=240, train loss = 0.305160855896149, train acc = 0.8700000047683716, time = 0.008873462677001953\n",
      "Training at step=30, batch=360, train loss = 0.5009933821117063, train acc = 0.8100000023841858, time = 0.008991718292236328\n",
      "Training at step=30, batch=480, train loss = 0.38529247896773755, train acc = 0.8299999833106995, time = 0.008897542953491211\n",
      "Testing at step=30, batch=0, test loss = 0.5788780901193644, test acc = 0.7799999713897705, time = 0.001971006393432617\n",
      "Testing at step=30, batch=20, test loss = 0.4162908751831597, test acc = 0.8500000238418579, time = 0.0018820762634277344\n",
      "Testing at step=30, batch=40, test loss = 0.3682302351440554, test acc = 0.8500000238418579, time = 0.001889944076538086\n",
      "Testing at step=30, batch=60, test loss = 0.4647455741888749, test acc = 0.8500000238418579, time = 0.0019347667694091797\n",
      "Testing at step=30, batch=80, test loss = 0.3262173797663306, test acc = 0.8899999856948853, time = 0.0020279884338378906\n",
      "Step 30 finished in 22.870360851287842, Train loss = 0.38160090938986563, Test loss = 0.44265690210940895; Train Acc = 0.8671666660904884, Test Acc = 0.8420999991893768\n",
      "Training at step=31, batch=0, train loss = 0.26799523034360706, train acc = 0.8899999856948853, time = 0.009129762649536133\n",
      "Training at step=31, batch=120, train loss = 0.5659002426324835, train acc = 0.8299999833106995, time = 0.008908271789550781\n",
      "Training at step=31, batch=240, train loss = 0.21643081327217914, train acc = 0.8999999761581421, time = 0.008903264999389648\n",
      "Training at step=31, batch=360, train loss = 0.34489619936025373, train acc = 0.8899999856948853, time = 0.008881092071533203\n",
      "Training at step=31, batch=480, train loss = 0.25172874244989596, train acc = 0.8899999856948853, time = 0.009013652801513672\n",
      "Testing at step=31, batch=0, test loss = 0.5468016775828455, test acc = 0.7799999713897705, time = 0.0019388198852539062\n",
      "Testing at step=31, batch=20, test loss = 0.6037459902888103, test acc = 0.800000011920929, time = 0.0019245147705078125\n",
      "Testing at step=31, batch=40, test loss = 0.48728478623765265, test acc = 0.8100000023841858, time = 0.002208232879638672\n",
      "Testing at step=31, batch=60, test loss = 0.3877805915037759, test acc = 0.8399999737739563, time = 0.0018877983093261719\n",
      "Testing at step=31, batch=80, test loss = 0.3922524141641918, test acc = 0.8100000023841858, time = 0.0019369125366210938\n",
      "Step 31 finished in 22.83654284477234, Train loss = 0.3807634447561893, Test loss = 0.44243547639436337; Train Acc = 0.8676333334048589, Test Acc = 0.8434999990463257\n",
      "Training at step=32, batch=0, train loss = 0.2986992774742526, train acc = 0.8999999761581421, time = 0.009039878845214844\n",
      "Training at step=32, batch=120, train loss = 0.33824672502165287, train acc = 0.8700000047683716, time = 0.009084224700927734\n",
      "Training at step=32, batch=240, train loss = 0.5155192756942053, train acc = 0.800000011920929, time = 0.00893402099609375\n",
      "Training at step=32, batch=360, train loss = 0.4306442653793865, train acc = 0.8600000143051147, time = 0.00886845588684082\n",
      "Training at step=32, batch=480, train loss = 0.17053745278225757, train acc = 0.949999988079071, time = 0.00887608528137207\n",
      "Testing at step=32, batch=0, test loss = 0.396213718483786, test acc = 0.8799999952316284, time = 0.0019202232360839844\n",
      "Testing at step=32, batch=20, test loss = 0.3697348852544027, test acc = 0.8399999737739563, time = 0.0019059181213378906\n",
      "Testing at step=32, batch=40, test loss = 0.436417780553573, test acc = 0.8100000023841858, time = 0.0019185543060302734\n",
      "Testing at step=32, batch=60, test loss = 0.3808326889729289, test acc = 0.8600000143051147, time = 0.001901865005493164\n",
      "Testing at step=32, batch=80, test loss = 0.4849324873968407, test acc = 0.8399999737739563, time = 0.0018858909606933594\n",
      "Step 32 finished in 22.76347804069519, Train loss = 0.3810019425682988, Test loss = 0.43980076299260984; Train Acc = 0.866433334449927, Test Acc = 0.8427999985218048\n",
      "Training at step=33, batch=0, train loss = 0.32704726583615346, train acc = 0.8799999952316284, time = 0.008960962295532227\n",
      "Training at step=33, batch=120, train loss = 0.27926806558654355, train acc = 0.9200000166893005, time = 0.008908271789550781\n",
      "Training at step=33, batch=240, train loss = 0.3842032655588125, train acc = 0.8500000238418579, time = 0.008907079696655273\n",
      "Training at step=33, batch=360, train loss = 0.26946012941810216, train acc = 0.9200000166893005, time = 0.008976936340332031\n",
      "Training at step=33, batch=480, train loss = 0.306200149523537, train acc = 0.8700000047683716, time = 0.0089263916015625\n",
      "Testing at step=33, batch=0, test loss = 0.3214072447578182, test acc = 0.8999999761581421, time = 0.0019106864929199219\n",
      "Testing at step=33, batch=20, test loss = 0.4667848962636579, test acc = 0.8199999928474426, time = 0.0020477771759033203\n",
      "Testing at step=33, batch=40, test loss = 0.3889241479864121, test acc = 0.8399999737739563, time = 0.0019023418426513672\n",
      "Testing at step=33, batch=60, test loss = 0.3573053737826835, test acc = 0.8799999952316284, time = 0.0019352436065673828\n",
      "Testing at step=33, batch=80, test loss = 0.48152139042355424, test acc = 0.8299999833106995, time = 0.0019180774688720703\n",
      "Step 33 finished in 22.81901979446411, Train loss = 0.378944651344464, Test loss = 0.44077238884776576; Train Acc = 0.8679999991257985, Test Acc = 0.842399999499321\n",
      "Training at step=34, batch=0, train loss = 0.39360996869236403, train acc = 0.8399999737739563, time = 0.00901484489440918\n",
      "Training at step=34, batch=120, train loss = 0.44877782159069335, train acc = 0.8399999737739563, time = 0.008813142776489258\n",
      "Training at step=34, batch=240, train loss = 0.5667654472096899, train acc = 0.8399999737739563, time = 0.008917093276977539\n",
      "Training at step=34, batch=360, train loss = 0.423749400022074, train acc = 0.8600000143051147, time = 0.00893855094909668\n",
      "Training at step=34, batch=480, train loss = 0.3755459315407142, train acc = 0.8899999856948853, time = 0.008944511413574219\n",
      "Testing at step=34, batch=0, test loss = 0.3870505404446272, test acc = 0.8700000047683716, time = 0.0018990039825439453\n",
      "Testing at step=34, batch=20, test loss = 0.3914841737761928, test acc = 0.8399999737739563, time = 0.0019414424896240234\n",
      "Testing at step=34, batch=40, test loss = 0.49047463873631036, test acc = 0.8299999833106995, time = 0.0019087791442871094\n",
      "Testing at step=34, batch=60, test loss = 0.622086193492431, test acc = 0.8100000023841858, time = 0.0019197463989257812\n",
      "Testing at step=34, batch=80, test loss = 0.45732509358405266, test acc = 0.8600000143051147, time = 0.0019359588623046875\n",
      "Step 34 finished in 22.847097635269165, Train loss = 0.3791005369108216, Test loss = 0.4419238629703002; Train Acc = 0.8672666655977567, Test Acc = 0.8420999991893768\n",
      "Training at step=35, batch=0, train loss = 0.3669408496723022, train acc = 0.8700000047683716, time = 0.009093523025512695\n",
      "Training at step=35, batch=120, train loss = 0.3996927618197239, train acc = 0.8399999737739563, time = 0.008909225463867188\n",
      "Training at step=35, batch=240, train loss = 0.359102871646791, train acc = 0.8600000143051147, time = 0.009060859680175781\n",
      "Training at step=35, batch=360, train loss = 0.4577389783116353, train acc = 0.8500000238418579, time = 0.009048700332641602\n",
      "Training at step=35, batch=480, train loss = 0.33535177865973415, train acc = 0.8700000047683716, time = 0.008934497833251953\n",
      "Testing at step=35, batch=0, test loss = 0.3365691589166758, test acc = 0.8999999761581421, time = 0.002005338668823242\n",
      "Testing at step=35, batch=20, test loss = 0.5198072816329649, test acc = 0.75, time = 0.001893758773803711\n",
      "Testing at step=35, batch=40, test loss = 0.4873547854887195, test acc = 0.8399999737739563, time = 0.0019428730010986328\n",
      "Testing at step=35, batch=60, test loss = 0.36981758687179567, test acc = 0.8799999952316284, time = 0.0019116401672363281\n",
      "Testing at step=35, batch=80, test loss = 0.46291615340549586, test acc = 0.800000011920929, time = 0.0018947124481201172\n",
      "Step 35 finished in 22.87422823905945, Train loss = 0.3778101001685021, Test loss = 0.442416402493248; Train Acc = 0.868116666773955, Test Acc = 0.8424000012874603\n",
      "Training at step=36, batch=0, train loss = 0.37944750673841704, train acc = 0.8500000238418579, time = 0.009009361267089844\n",
      "Training at step=36, batch=120, train loss = 0.42777239980247195, train acc = 0.8199999928474426, time = 0.008909940719604492\n",
      "Training at step=36, batch=240, train loss = 0.42539381607671367, train acc = 0.8500000238418579, time = 0.009021759033203125\n",
      "Training at step=36, batch=360, train loss = 0.4672897577157846, train acc = 0.8100000023841858, time = 0.00892782211303711\n",
      "Training at step=36, batch=480, train loss = 0.3552097593557994, train acc = 0.8500000238418579, time = 0.008901596069335938\n",
      "Testing at step=36, batch=0, test loss = 0.6172464663331542, test acc = 0.7400000095367432, time = 0.0019307136535644531\n",
      "Testing at step=36, batch=20, test loss = 0.382916405178608, test acc = 0.8799999952316284, time = 0.001993417739868164\n",
      "Testing at step=36, batch=40, test loss = 0.3935853892174396, test acc = 0.8199999928474426, time = 0.0019116401672363281\n",
      "Testing at step=36, batch=60, test loss = 0.6325232038076362, test acc = 0.7900000214576721, time = 0.001934051513671875\n",
      "Testing at step=36, batch=80, test loss = 0.35684599475382156, test acc = 0.8500000238418579, time = 0.0018951892852783203\n",
      "Step 36 finished in 22.864227056503296, Train loss = 0.3768275849588019, Test loss = 0.4471381425568164; Train Acc = 0.868983334004879, Test Acc = 0.8387999987602234\n",
      "Training at step=37, batch=0, train loss = 0.28561521413498636, train acc = 0.9100000262260437, time = 0.008968353271484375\n",
      "Training at step=37, batch=120, train loss = 0.46077107899423014, train acc = 0.8100000023841858, time = 0.00892186164855957\n",
      "Training at step=37, batch=240, train loss = 0.30471795280685443, train acc = 0.8899999856948853, time = 0.008998632431030273\n",
      "Training at step=37, batch=360, train loss = 0.3585465082604752, train acc = 0.8700000047683716, time = 0.00899815559387207\n",
      "Training at step=37, batch=480, train loss = 0.34693321266606925, train acc = 0.8600000143051147, time = 0.008983850479125977\n",
      "Testing at step=37, batch=0, test loss = 0.2740060437091732, test acc = 0.8899999856948853, time = 0.001981019973754883\n",
      "Testing at step=37, batch=20, test loss = 0.4325455457443879, test acc = 0.8700000047683716, time = 0.0018901824951171875\n",
      "Testing at step=37, batch=40, test loss = 0.39105932081073214, test acc = 0.8299999833106995, time = 0.0019061565399169922\n",
      "Testing at step=37, batch=60, test loss = 0.49328299637108136, test acc = 0.8700000047683716, time = 0.001890420913696289\n",
      "Testing at step=37, batch=80, test loss = 0.3852781073222065, test acc = 0.8600000143051147, time = 0.0019304752349853516\n",
      "Step 37 finished in 22.853713035583496, Train loss = 0.37695693279788506, Test loss = 0.44063130031348186; Train Acc = 0.8689499991138776, Test Acc = 0.8430999994277955\n",
      "Training at step=38, batch=0, train loss = 0.30415933283266, train acc = 0.8899999856948853, time = 0.008995771408081055\n",
      "Training at step=38, batch=120, train loss = 0.5378427301002937, train acc = 0.800000011920929, time = 0.009020328521728516\n",
      "Training at step=38, batch=240, train loss = 0.3483865082661864, train acc = 0.9100000262260437, time = 0.008968591690063477\n",
      "Training at step=38, batch=360, train loss = 0.28752700519088603, train acc = 0.9100000262260437, time = 0.008882999420166016\n",
      "Training at step=38, batch=480, train loss = 0.3411189047795925, train acc = 0.8700000047683716, time = 0.008916139602661133\n",
      "Testing at step=38, batch=0, test loss = 0.5242366115363777, test acc = 0.800000011920929, time = 0.0019512176513671875\n",
      "Testing at step=38, batch=20, test loss = 0.48833987930952105, test acc = 0.7900000214576721, time = 0.0018870830535888672\n",
      "Testing at step=38, batch=40, test loss = 0.44634317991343975, test acc = 0.8100000023841858, time = 0.0023708343505859375\n",
      "Testing at step=38, batch=60, test loss = 0.34044688131896456, test acc = 0.8899999856948853, time = 0.001901865005493164\n",
      "Testing at step=38, batch=80, test loss = 0.4524425301079695, test acc = 0.8299999833106995, time = 0.0019185543060302734\n",
      "Step 38 finished in 22.88980269432068, Train loss = 0.37603396002362094, Test loss = 0.44453846517515244; Train Acc = 0.8686166658004125, Test Acc = 0.8385999983549118\n",
      "Training at step=39, batch=0, train loss = 0.3759484909411748, train acc = 0.8799999952316284, time = 0.009045839309692383\n",
      "Training at step=39, batch=120, train loss = 0.35277034823099024, train acc = 0.8500000238418579, time = 0.00894784927368164\n",
      "Training at step=39, batch=240, train loss = 0.3753220167086444, train acc = 0.8799999952316284, time = 0.008928775787353516\n",
      "Training at step=39, batch=360, train loss = 0.44321586616531716, train acc = 0.8600000143051147, time = 0.008948087692260742\n",
      "Training at step=39, batch=480, train loss = 0.32918664047920615, train acc = 0.8700000047683716, time = 0.00890207290649414\n",
      "Testing at step=39, batch=0, test loss = 0.4897679933271539, test acc = 0.7900000214576721, time = 0.0019342899322509766\n",
      "Testing at step=39, batch=20, test loss = 0.3551601518001348, test acc = 0.8600000143051147, time = 0.0018990039825439453\n",
      "Testing at step=39, batch=40, test loss = 0.3421191393404376, test acc = 0.8600000143051147, time = 0.0018849372863769531\n",
      "Testing at step=39, batch=60, test loss = 0.4735281891976026, test acc = 0.8100000023841858, time = 0.0019054412841796875\n",
      "Testing at step=39, batch=80, test loss = 0.3861517793464491, test acc = 0.8799999952316284, time = 0.0018773078918457031\n",
      "Step 39 finished in 22.788236379623413, Train loss = 0.3756390555689761, Test loss = 0.4413372682010047; Train Acc = 0.8697000004847845, Test Acc = 0.8435999977588654\n",
      "Training at step=40, batch=0, train loss = 0.47573772158167765, train acc = 0.8799999952316284, time = 0.009204626083374023\n",
      "Training at step=40, batch=120, train loss = 0.30908693872238346, train acc = 0.8700000047683716, time = 0.008978843688964844\n",
      "Training at step=40, batch=240, train loss = 0.3956678506318849, train acc = 0.8700000047683716, time = 0.008984088897705078\n",
      "Training at step=40, batch=360, train loss = 0.48251755107113176, train acc = 0.8500000238418579, time = 0.009001731872558594\n",
      "Training at step=40, batch=480, train loss = 0.28617765946101115, train acc = 0.8799999952316284, time = 0.008960723876953125\n",
      "Testing at step=40, batch=0, test loss = 0.44133580781518583, test acc = 0.8500000238418579, time = 0.0019190311431884766\n",
      "Testing at step=40, batch=20, test loss = 0.3902433813604824, test acc = 0.8600000143051147, time = 0.0019121170043945312\n",
      "Testing at step=40, batch=40, test loss = 0.49834510988905484, test acc = 0.8199999928474426, time = 0.0018992424011230469\n",
      "Testing at step=40, batch=60, test loss = 0.41034417468431417, test acc = 0.8299999833106995, time = 0.0019152164459228516\n",
      "Testing at step=40, batch=80, test loss = 0.3758970955322107, test acc = 0.8600000143051147, time = 0.001911163330078125\n",
      "Step 40 finished in 22.874968767166138, Train loss = 0.37433468967951256, Test loss = 0.44344591316575044; Train Acc = 0.86948333243529, Test Acc = 0.8419999998807907\n",
      "Training at step=41, batch=0, train loss = 0.3631301036971303, train acc = 0.9100000262260437, time = 0.009160041809082031\n",
      "Training at step=41, batch=120, train loss = 0.43693816280555065, train acc = 0.8500000238418579, time = 0.009310483932495117\n",
      "Training at step=41, batch=240, train loss = 0.3642461755536706, train acc = 0.8600000143051147, time = 0.009006738662719727\n",
      "Training at step=41, batch=360, train loss = 0.44809708079028404, train acc = 0.8899999856948853, time = 0.00889730453491211\n",
      "Training at step=41, batch=480, train loss = 0.510635749258653, train acc = 0.8500000238418579, time = 0.008985519409179688\n",
      "Testing at step=41, batch=0, test loss = 0.4787815482165929, test acc = 0.8299999833106995, time = 0.0019185543060302734\n",
      "Testing at step=41, batch=20, test loss = 0.39306608689966943, test acc = 0.8700000047683716, time = 0.0018820762634277344\n",
      "Testing at step=41, batch=40, test loss = 0.4885572526571864, test acc = 0.8600000143051147, time = 0.0019214153289794922\n",
      "Testing at step=41, batch=60, test loss = 0.5266530610175062, test acc = 0.8700000047683716, time = 0.0019373893737792969\n",
      "Testing at step=41, batch=80, test loss = 0.5324758808533304, test acc = 0.8100000023841858, time = 0.0019447803497314453\n",
      "Step 41 finished in 22.89778184890747, Train loss = 0.3750061170984969, Test loss = 0.4408636946266748; Train Acc = 0.8685666671395302, Test Acc = 0.8426999992132187\n",
      "Training at step=42, batch=0, train loss = 0.3974325779686276, train acc = 0.8799999952316284, time = 0.009075403213500977\n",
      "Training at step=42, batch=120, train loss = 0.29656941434574546, train acc = 0.8600000143051147, time = 0.00887298583984375\n",
      "Training at step=42, batch=240, train loss = 0.2630648340179392, train acc = 0.8700000047683716, time = 0.008965492248535156\n",
      "Training at step=42, batch=360, train loss = 0.390189271534909, train acc = 0.8500000238418579, time = 0.008921623229980469\n",
      "Training at step=42, batch=480, train loss = 0.2783684509781603, train acc = 0.9200000166893005, time = 0.008942604064941406\n",
      "Testing at step=42, batch=0, test loss = 0.4050318415509176, test acc = 0.8600000143051147, time = 0.0019359588623046875\n",
      "Testing at step=42, batch=20, test loss = 0.46922330270603807, test acc = 0.8399999737739563, time = 0.001897573471069336\n",
      "Testing at step=42, batch=40, test loss = 0.49870647128653617, test acc = 0.8299999833106995, time = 0.0018939971923828125\n",
      "Testing at step=42, batch=60, test loss = 0.4898156955061596, test acc = 0.8399999737739563, time = 0.0019009113311767578\n",
      "Testing at step=42, batch=80, test loss = 0.37604561242341994, test acc = 0.8600000143051147, time = 0.0019731521606445312\n",
      "Step 42 finished in 22.90228772163391, Train loss = 0.3743157312207771, Test loss = 0.4405557132865521; Train Acc = 0.8685166669885317, Test Acc = 0.8435000002384185\n",
      "Training at step=43, batch=0, train loss = 0.4763917820620112, train acc = 0.8399999737739563, time = 0.009053230285644531\n",
      "Training at step=43, batch=120, train loss = 0.387261380884087, train acc = 0.8399999737739563, time = 0.008928537368774414\n",
      "Training at step=43, batch=240, train loss = 0.28079175294505526, train acc = 0.8999999761581421, time = 0.008980989456176758\n",
      "Training at step=43, batch=360, train loss = 0.4930400117893252, train acc = 0.8399999737739563, time = 0.009162664413452148\n",
      "Training at step=43, batch=480, train loss = 0.31963104219582056, train acc = 0.8700000047683716, time = 0.008945703506469727\n",
      "Testing at step=43, batch=0, test loss = 0.5866598315181716, test acc = 0.7799999713897705, time = 0.0019254684448242188\n",
      "Testing at step=43, batch=20, test loss = 0.3924400801193434, test acc = 0.8399999737739563, time = 0.0018858909606933594\n",
      "Testing at step=43, batch=40, test loss = 0.5274108587601832, test acc = 0.8399999737739563, time = 0.0019538402557373047\n",
      "Testing at step=43, batch=60, test loss = 0.24568754143401159, test acc = 0.9100000262260437, time = 0.0019137859344482422\n",
      "Testing at step=43, batch=80, test loss = 0.3081301475034645, test acc = 0.8600000143051147, time = 0.0018837451934814453\n",
      "Step 43 finished in 22.855623483657837, Train loss = 0.3731839015530573, Test loss = 0.44167460816319676; Train Acc = 0.8689999980727832, Test Acc = 0.8414000004529953\n",
      "Training at step=44, batch=0, train loss = 0.245520943799573, train acc = 0.8999999761581421, time = 0.00902414321899414\n",
      "Training at step=44, batch=120, train loss = 0.4813094421426524, train acc = 0.8399999737739563, time = 0.0089263916015625\n",
      "Training at step=44, batch=240, train loss = 0.5499666013428397, train acc = 0.8199999928474426, time = 0.008979320526123047\n",
      "Training at step=44, batch=360, train loss = 0.28737937393996604, train acc = 0.8600000143051147, time = 0.008951902389526367\n",
      "Training at step=44, batch=480, train loss = 0.3207495611087408, train acc = 0.8799999952316284, time = 0.008957147598266602\n",
      "Testing at step=44, batch=0, test loss = 0.3319150493225908, test acc = 0.9300000071525574, time = 0.0019464492797851562\n",
      "Testing at step=44, batch=20, test loss = 0.6037039721520929, test acc = 0.800000011920929, time = 0.0019063949584960938\n",
      "Testing at step=44, batch=40, test loss = 0.42347764399971155, test acc = 0.8199999928474426, time = 0.001924276351928711\n",
      "Testing at step=44, batch=60, test loss = 0.32853163709826494, test acc = 0.8899999856948853, time = 0.0019078254699707031\n",
      "Testing at step=44, batch=80, test loss = 0.34369374846864464, test acc = 0.9100000262260437, time = 0.0019278526306152344\n",
      "Step 44 finished in 22.88491725921631, Train loss = 0.37286180652315065, Test loss = 0.44076472481171175; Train Acc = 0.8688833328088125, Test Acc = 0.8420999974012375\n",
      "Training at step=45, batch=0, train loss = 0.35394163479454527, train acc = 0.8500000238418579, time = 0.009225845336914062\n",
      "Training at step=45, batch=120, train loss = 0.4409348923453993, train acc = 0.8299999833106995, time = 0.008948326110839844\n",
      "Training at step=45, batch=240, train loss = 0.3389934360671363, train acc = 0.8899999856948853, time = 0.00891733169555664\n",
      "Training at step=45, batch=360, train loss = 0.5568980720821497, train acc = 0.8500000238418579, time = 0.008957862854003906\n",
      "Training at step=45, batch=480, train loss = 0.4071643997630542, train acc = 0.8299999833106995, time = 0.008867740631103516\n",
      "Testing at step=45, batch=0, test loss = 0.36001811257279653, test acc = 0.8700000047683716, time = 0.0019266605377197266\n",
      "Testing at step=45, batch=20, test loss = 0.4751315190631054, test acc = 0.8600000143051147, time = 0.002409219741821289\n",
      "Testing at step=45, batch=40, test loss = 0.5452602527487848, test acc = 0.8199999928474426, time = 0.00189971923828125\n",
      "Testing at step=45, batch=60, test loss = 0.38858630926061316, test acc = 0.8799999952316284, time = 0.0018870830535888672\n",
      "Testing at step=45, batch=80, test loss = 0.3428703333154741, test acc = 0.8700000047683716, time = 0.001924753189086914\n",
      "Step 45 finished in 22.863500833511353, Train loss = 0.3722577047651607, Test loss = 0.4431547412096776; Train Acc = 0.8694499992330869, Test Acc = 0.8403999984264374\n",
      "Training at step=46, batch=0, train loss = 0.3899759682121482, train acc = 0.8700000047683716, time = 0.009101390838623047\n",
      "Training at step=46, batch=120, train loss = 0.3515554715188831, train acc = 0.8799999952316284, time = 0.009131908416748047\n",
      "Training at step=46, batch=240, train loss = 0.3171691293514983, train acc = 0.8799999952316284, time = 0.008897066116333008\n",
      "Training at step=46, batch=360, train loss = 0.34504369429449205, train acc = 0.8999999761581421, time = 0.009004354476928711\n",
      "Training at step=46, batch=480, train loss = 0.4486869869986514, train acc = 0.8299999833106995, time = 0.008925437927246094\n",
      "Testing at step=46, batch=0, test loss = 0.5118134747945099, test acc = 0.8100000023841858, time = 0.0019083023071289062\n",
      "Testing at step=46, batch=20, test loss = 0.32784167129081615, test acc = 0.8899999856948853, time = 0.0019011497497558594\n",
      "Testing at step=46, batch=40, test loss = 0.33678343230285185, test acc = 0.8999999761581421, time = 0.001886129379272461\n",
      "Testing at step=46, batch=60, test loss = 0.40937593605013084, test acc = 0.8399999737739563, time = 0.0018954277038574219\n",
      "Testing at step=46, batch=80, test loss = 0.3826155664206618, test acc = 0.8899999856948853, time = 0.001943826675415039\n",
      "Step 46 finished in 22.86520218849182, Train loss = 0.3720130643562256, Test loss = 0.44193560232221346; Train Acc = 0.8692333334684372, Test Acc = 0.843299994468689\n",
      "Training at step=47, batch=0, train loss = 0.2614520105347369, train acc = 0.9100000262260437, time = 0.009049415588378906\n",
      "Training at step=47, batch=120, train loss = 0.2712146060905682, train acc = 0.8700000047683716, time = 0.008993148803710938\n",
      "Training at step=47, batch=240, train loss = 0.38532895235790293, train acc = 0.8500000238418579, time = 0.00912165641784668\n",
      "Training at step=47, batch=360, train loss = 0.44522884644430527, train acc = 0.8500000238418579, time = 0.008939981460571289\n",
      "Training at step=47, batch=480, train loss = 0.5396556071214796, train acc = 0.7799999713897705, time = 0.00899815559387207\n",
      "Testing at step=47, batch=0, test loss = 0.7233109122373851, test acc = 0.7699999809265137, time = 0.0019288063049316406\n",
      "Testing at step=47, batch=20, test loss = 0.5253042180287004, test acc = 0.800000011920929, time = 0.0018951892852783203\n",
      "Testing at step=47, batch=40, test loss = 0.4114950435755744, test acc = 0.8199999928474426, time = 0.0018966197967529297\n",
      "Testing at step=47, batch=60, test loss = 0.2908459467536168, test acc = 0.9200000166893005, time = 0.0020127296447753906\n",
      "Testing at step=47, batch=80, test loss = 0.5822227725571821, test acc = 0.800000011920929, time = 0.001895904541015625\n",
      "Step 47 finished in 22.855118989944458, Train loss = 0.37196440453727025, Test loss = 0.4425722902504565; Train Acc = 0.8688666661580403, Test Acc = 0.8407999992370605\n",
      "Training at step=48, batch=0, train loss = 0.32509901320120294, train acc = 0.8799999952316284, time = 0.009046316146850586\n",
      "Training at step=48, batch=120, train loss = 0.29279456438517615, train acc = 0.8799999952316284, time = 0.008959293365478516\n",
      "Training at step=48, batch=240, train loss = 0.4235522542736973, train acc = 0.8299999833106995, time = 0.008934974670410156\n",
      "Training at step=48, batch=360, train loss = 0.2847774374892993, train acc = 0.8899999856948853, time = 0.009107112884521484\n",
      "Training at step=48, batch=480, train loss = 0.29208090368772827, train acc = 0.8899999856948853, time = 0.008971214294433594\n",
      "Testing at step=48, batch=0, test loss = 0.39225101431394754, test acc = 0.8299999833106995, time = 0.0019469261169433594\n",
      "Testing at step=48, batch=20, test loss = 0.3356691967496625, test acc = 0.8899999856948853, time = 0.0019137859344482422\n",
      "Testing at step=48, batch=40, test loss = 0.3711818232196365, test acc = 0.8500000238418579, time = 0.0018985271453857422\n",
      "Testing at step=48, batch=60, test loss = 0.38285628772030766, test acc = 0.8500000238418579, time = 0.0019237995147705078\n",
      "Testing at step=48, batch=80, test loss = 0.457554673227161, test acc = 0.8700000047683716, time = 0.0019011497497558594\n",
      "Step 48 finished in 23.00134563446045, Train loss = 0.37113999641702433, Test loss = 0.44499338978624037; Train Acc = 0.8701833320657412, Test Acc = 0.8402999997138977\n",
      "Training at step=49, batch=0, train loss = 0.40667803924813356, train acc = 0.8399999737739563, time = 0.009005546569824219\n",
      "Training at step=49, batch=120, train loss = 0.23947255777632429, train acc = 0.9399999976158142, time = 0.009175777435302734\n",
      "Training at step=49, batch=240, train loss = 0.37896762278402013, train acc = 0.8399999737739563, time = 0.008916139602661133\n",
      "Training at step=49, batch=360, train loss = 0.479378367412656, train acc = 0.8500000238418579, time = 0.008879899978637695\n",
      "Training at step=49, batch=480, train loss = 0.268802074316428, train acc = 0.8999999761581421, time = 0.009029626846313477\n",
      "Testing at step=49, batch=0, test loss = 0.4236962616567592, test acc = 0.8600000143051147, time = 0.0019254684448242188\n",
      "Testing at step=49, batch=20, test loss = 0.48858484425707205, test acc = 0.9200000166893005, time = 0.0019078254699707031\n",
      "Testing at step=49, batch=40, test loss = 0.4980667930033328, test acc = 0.800000011920929, time = 0.0018973350524902344\n",
      "Testing at step=49, batch=60, test loss = 0.41617379747653893, test acc = 0.8399999737739563, time = 0.0019316673278808594\n",
      "Testing at step=49, batch=80, test loss = 0.3647167234468972, test acc = 0.8799999952316284, time = 0.0018901824951171875\n",
      "Step 49 finished in 22.990453004837036, Train loss = 0.3702063590262085, Test loss = 0.44307134853297186; Train Acc = 0.8707666679223378, Test Acc = 0.841599999666214\n",
      "Training at step=50, batch=0, train loss = 0.3048057606641555, train acc = 0.8899999856948853, time = 0.009024858474731445\n",
      "Training at step=50, batch=120, train loss = 0.25278350579475456, train acc = 0.9300000071525574, time = 0.008914947509765625\n",
      "Training at step=50, batch=240, train loss = 0.449889871857767, train acc = 0.8500000238418579, time = 0.008977651596069336\n",
      "Training at step=50, batch=360, train loss = 0.363709870235801, train acc = 0.8500000238418579, time = 0.00885462760925293\n",
      "Training at step=50, batch=480, train loss = 0.3070087478191688, train acc = 0.8999999761581421, time = 0.008925914764404297\n",
      "Testing at step=50, batch=0, test loss = 0.4713565136286404, test acc = 0.800000011920929, time = 0.001967191696166992\n",
      "Testing at step=50, batch=20, test loss = 0.38692788898391994, test acc = 0.8399999737739563, time = 0.0019114017486572266\n",
      "Testing at step=50, batch=40, test loss = 0.38402496096310934, test acc = 0.8500000238418579, time = 0.0019044876098632812\n",
      "Testing at step=50, batch=60, test loss = 0.2671035322963522, test acc = 0.9200000166893005, time = 0.0019042491912841797\n",
      "Testing at step=50, batch=80, test loss = 0.4713715826696116, test acc = 0.8500000238418579, time = 0.0018880367279052734\n",
      "Step 50 finished in 22.824352979660034, Train loss = 0.3706414732305488, Test loss = 0.4460180253198011; Train Acc = 0.8692999982833862, Test Acc = 0.8407999986410141\n",
      "Training at step=51, batch=0, train loss = 0.22537355834442604, train acc = 0.9599999785423279, time = 0.009009361267089844\n",
      "Training at step=51, batch=120, train loss = 0.45482235211583344, train acc = 0.8199999928474426, time = 0.008932352066040039\n",
      "Training at step=51, batch=240, train loss = 0.49841217171633134, train acc = 0.8399999737739563, time = 0.008907079696655273\n",
      "Training at step=51, batch=360, train loss = 0.3408213550642333, train acc = 0.8500000238418579, time = 0.008938789367675781\n",
      "Training at step=51, batch=480, train loss = 0.2717247259815718, train acc = 0.9100000262260437, time = 0.008934974670410156\n",
      "Testing at step=51, batch=0, test loss = 0.32364344307729653, test acc = 0.8899999856948853, time = 0.0019288063049316406\n",
      "Testing at step=51, batch=20, test loss = 0.44463331309060833, test acc = 0.8600000143051147, time = 0.001905679702758789\n",
      "Testing at step=51, batch=40, test loss = 0.5974789686037715, test acc = 0.7799999713897705, time = 0.0019235610961914062\n",
      "Testing at step=51, batch=60, test loss = 0.33463610437122976, test acc = 0.8600000143051147, time = 0.0019211769104003906\n",
      "Testing at step=51, batch=80, test loss = 0.5040388651646375, test acc = 0.8199999928474426, time = 0.0018951892852783203\n",
      "Step 51 finished in 23.03158473968506, Train loss = 0.3698875488010253, Test loss = 0.45011608375792994; Train Acc = 0.8699499999483427, Test Acc = 0.8407999962568283\n",
      "Training at step=52, batch=0, train loss = 0.23528604708528977, train acc = 0.9200000166893005, time = 0.009050369262695312\n",
      "Training at step=52, batch=120, train loss = 0.5018977955684809, train acc = 0.8799999952316284, time = 0.008967161178588867\n",
      "Training at step=52, batch=240, train loss = 0.42979598423269016, train acc = 0.8799999952316284, time = 0.008942842483520508\n",
      "Training at step=52, batch=360, train loss = 0.3778627082923344, train acc = 0.8700000047683716, time = 0.009155750274658203\n",
      "Training at step=52, batch=480, train loss = 0.3671161351766059, train acc = 0.8700000047683716, time = 0.008905410766601562\n",
      "Testing at step=52, batch=0, test loss = 0.4481640322854151, test acc = 0.8399999737739563, time = 0.0019540786743164062\n",
      "Testing at step=52, batch=20, test loss = 0.5624180129911097, test acc = 0.7799999713897705, time = 0.0018987655639648438\n",
      "Testing at step=52, batch=40, test loss = 0.5238741490534948, test acc = 0.8500000238418579, time = 0.0019054412841796875\n",
      "Testing at step=52, batch=60, test loss = 0.3943178188914409, test acc = 0.8399999737739563, time = 0.001903533935546875\n",
      "Testing at step=52, batch=80, test loss = 0.4429333074878697, test acc = 0.8500000238418579, time = 0.0019032955169677734\n",
      "Step 52 finished in 23.023593187332153, Train loss = 0.3698156540749428, Test loss = 0.4461151586586039; Train Acc = 0.870066666205724, Test Acc = 0.8419999974966049\n",
      "Training at step=53, batch=0, train loss = 0.4745546803953117, train acc = 0.8500000238418579, time = 0.009011507034301758\n",
      "Training at step=53, batch=120, train loss = 0.4088129765610667, train acc = 0.8299999833106995, time = 0.008920431137084961\n",
      "Training at step=53, batch=240, train loss = 0.3924897058581947, train acc = 0.8500000238418579, time = 0.008995771408081055\n",
      "Training at step=53, batch=360, train loss = 0.34458973170381474, train acc = 0.8500000238418579, time = 0.008899450302124023\n",
      "Training at step=53, batch=480, train loss = 0.4418262408923568, train acc = 0.8700000047683716, time = 0.008822441101074219\n",
      "Testing at step=53, batch=0, test loss = 0.36480226408914995, test acc = 0.8600000143051147, time = 0.001903533935546875\n",
      "Testing at step=53, batch=20, test loss = 0.5042338355801012, test acc = 0.8600000143051147, time = 0.001956462860107422\n",
      "Testing at step=53, batch=40, test loss = 0.4155746918559993, test acc = 0.8600000143051147, time = 0.0019066333770751953\n",
      "Testing at step=53, batch=60, test loss = 0.4084642166865883, test acc = 0.8899999856948853, time = 0.0019061565399169922\n",
      "Testing at step=53, batch=80, test loss = 0.5747738811206161, test acc = 0.8100000023841858, time = 0.0019011497497558594\n",
      "Step 53 finished in 22.951738119125366, Train loss = 0.3685687385193311, Test loss = 0.4430431083210141; Train Acc = 0.8705333336194356, Test Acc = 0.8421999984979629\n",
      "Training at step=54, batch=0, train loss = 0.32622700635939245, train acc = 0.8999999761581421, time = 0.00903773307800293\n",
      "Training at step=54, batch=120, train loss = 0.27587158565870906, train acc = 0.8999999761581421, time = 0.008969783782958984\n",
      "Training at step=54, batch=240, train loss = 0.4663001449329952, train acc = 0.8399999737739563, time = 0.00890970230102539\n",
      "Training at step=54, batch=360, train loss = 0.38133523630692123, train acc = 0.8600000143051147, time = 0.008956432342529297\n",
      "Training at step=54, batch=480, train loss = 0.31326133480537444, train acc = 0.8799999952316284, time = 0.008921623229980469\n",
      "Testing at step=54, batch=0, test loss = 0.37752513238734564, test acc = 0.8600000143051147, time = 0.0019223690032958984\n",
      "Testing at step=54, batch=20, test loss = 0.3262587976060151, test acc = 0.8799999952316284, time = 0.001936197280883789\n",
      "Testing at step=54, batch=40, test loss = 0.6276905592238262, test acc = 0.7599999904632568, time = 0.0019040107727050781\n",
      "Testing at step=54, batch=60, test loss = 0.3606762330510442, test acc = 0.8700000047683716, time = 0.0020339488983154297\n",
      "Testing at step=54, batch=80, test loss = 0.7014318932576016, test acc = 0.800000011920929, time = 0.0019643306732177734\n",
      "Step 54 finished in 22.889333486557007, Train loss = 0.3692027265070494, Test loss = 0.44817668394171384; Train Acc = 0.8709499987959862, Test Acc = 0.8407999962568283\n",
      "Training at step=55, batch=0, train loss = 0.41083272815711497, train acc = 0.8600000143051147, time = 0.009040594100952148\n",
      "Training at step=55, batch=120, train loss = 0.3865801602760409, train acc = 0.8600000143051147, time = 0.008916854858398438\n",
      "Training at step=55, batch=240, train loss = 0.3511534955852075, train acc = 0.8500000238418579, time = 0.008936882019042969\n",
      "Training at step=55, batch=360, train loss = 0.32920879260866104, train acc = 0.8799999952316284, time = 0.00893259048461914\n",
      "Training at step=55, batch=480, train loss = 0.3403338264458082, train acc = 0.9100000262260437, time = 0.009081363677978516\n",
      "Testing at step=55, batch=0, test loss = 0.431265970949988, test acc = 0.8500000238418579, time = 0.0019338130950927734\n",
      "Testing at step=55, batch=20, test loss = 0.42749533991948807, test acc = 0.8600000143051147, time = 0.0018923282623291016\n",
      "Testing at step=55, batch=40, test loss = 0.33225637030677746, test acc = 0.8700000047683716, time = 0.0019307136535644531\n",
      "Testing at step=55, batch=60, test loss = 0.49954524529718464, test acc = 0.8100000023841858, time = 0.0018985271453857422\n",
      "Testing at step=55, batch=80, test loss = 0.5952179216708403, test acc = 0.8299999833106995, time = 0.0018987655639648438\n",
      "Step 55 finished in 22.913615226745605, Train loss = 0.3684418321254751, Test loss = 0.4471587999956025; Train Acc = 0.8704333338141441, Test Acc = 0.8427999997138977\n",
      "Training at step=56, batch=0, train loss = 0.24627778968005018, train acc = 0.8899999856948853, time = 0.009066343307495117\n",
      "Training at step=56, batch=120, train loss = 0.35371805778408033, train acc = 0.8799999952316284, time = 0.010149002075195312\n",
      "Training at step=56, batch=240, train loss = 0.38370494078416795, train acc = 0.8600000143051147, time = 0.008879899978637695\n",
      "Training at step=56, batch=360, train loss = 0.30553374179147696, train acc = 0.8600000143051147, time = 0.008989572525024414\n",
      "Training at step=56, batch=480, train loss = 0.2889856283834514, train acc = 0.9100000262260437, time = 0.00893855094909668\n",
      "Testing at step=56, batch=0, test loss = 0.5730808099419266, test acc = 0.800000011920929, time = 0.0019245147705078125\n",
      "Testing at step=56, batch=20, test loss = 0.4401632450791768, test acc = 0.8500000238418579, time = 0.0018949508666992188\n",
      "Testing at step=56, batch=40, test loss = 0.2737611156792161, test acc = 0.8999999761581421, time = 0.002224445343017578\n",
      "Testing at step=56, batch=60, test loss = 0.2985496350294097, test acc = 0.8999999761581421, time = 0.0019066333770751953\n",
      "Testing at step=56, batch=80, test loss = 0.29108428798497704, test acc = 0.9100000262260437, time = 0.0019180774688720703\n",
      "Step 56 finished in 22.890827417373657, Train loss = 0.3675972017742772, Test loss = 0.4460429072850198; Train Acc = 0.8705666669209798, Test Acc = 0.8430000007152557\n",
      "Training at step=57, batch=0, train loss = 0.3203110704101466, train acc = 0.8799999952316284, time = 0.009029388427734375\n",
      "Training at step=57, batch=120, train loss = 0.4450452730199285, train acc = 0.8799999952316284, time = 0.008858442306518555\n",
      "Training at step=57, batch=240, train loss = 0.45733120051671533, train acc = 0.8700000047683716, time = 0.008927345275878906\n",
      "Training at step=57, batch=360, train loss = 0.4116689937333085, train acc = 0.8700000047683716, time = 0.008959054946899414\n",
      "Training at step=57, batch=480, train loss = 0.24164728212369113, train acc = 0.8999999761581421, time = 0.008983135223388672\n",
      "Testing at step=57, batch=0, test loss = 0.3165033814580811, test acc = 0.8700000047683716, time = 0.001909017562866211\n",
      "Testing at step=57, batch=20, test loss = 0.4526743699232931, test acc = 0.8600000143051147, time = 0.0019006729125976562\n",
      "Testing at step=57, batch=40, test loss = 0.5447161551357727, test acc = 0.800000011920929, time = 0.0019040107727050781\n",
      "Testing at step=57, batch=60, test loss = 0.4327895861202959, test acc = 0.8600000143051147, time = 0.0019211769104003906\n",
      "Testing at step=57, batch=80, test loss = 0.3356809491396049, test acc = 0.8799999952316284, time = 0.0018999576568603516\n",
      "Step 57 finished in 22.948009729385376, Train loss = 0.36750784227029837, Test loss = 0.4463157997380609; Train Acc = 0.8709333344300588, Test Acc = 0.8408999991416931\n",
      "Training at step=58, batch=0, train loss = 0.30328942858467267, train acc = 0.9100000262260437, time = 0.009032964706420898\n",
      "Training at step=58, batch=120, train loss = 0.4175100515342166, train acc = 0.8700000047683716, time = 0.008891820907592773\n",
      "Training at step=58, batch=240, train loss = 0.46134050460074455, train acc = 0.8399999737739563, time = 0.008979558944702148\n",
      "Training at step=58, batch=360, train loss = 0.4000962915364143, train acc = 0.8500000238418579, time = 0.008928537368774414\n",
      "Training at step=58, batch=480, train loss = 0.49975723862877586, train acc = 0.8299999833106995, time = 0.01025247573852539\n",
      "Testing at step=58, batch=0, test loss = 0.5763981053425263, test acc = 0.800000011920929, time = 0.00193023681640625\n",
      "Testing at step=58, batch=20, test loss = 0.47365947191383617, test acc = 0.8299999833106995, time = 0.001928567886352539\n",
      "Testing at step=58, batch=40, test loss = 0.44850833447204386, test acc = 0.800000011920929, time = 0.0019378662109375\n",
      "Testing at step=58, batch=60, test loss = 0.6176954154531593, test acc = 0.7900000214576721, time = 0.0019352436065673828\n",
      "Testing at step=58, batch=80, test loss = 0.4063527126159345, test acc = 0.8500000238418579, time = 0.0018906593322753906\n",
      "Step 58 finished in 22.8374342918396, Train loss = 0.36694221496587287, Test loss = 0.4458979387438194; Train Acc = 0.8711666655540466, Test Acc = 0.843299999833107\n",
      "Training at step=59, batch=0, train loss = 0.4204621103195936, train acc = 0.8199999928474426, time = 0.009019851684570312\n",
      "Training at step=59, batch=120, train loss = 0.37841694850152474, train acc = 0.8399999737739563, time = 0.008942842483520508\n",
      "Training at step=59, batch=240, train loss = 0.3901330272704355, train acc = 0.8100000023841858, time = 0.008922815322875977\n",
      "Training at step=59, batch=360, train loss = 0.3549865193194765, train acc = 0.8399999737739563, time = 0.008954286575317383\n",
      "Training at step=59, batch=480, train loss = 0.4324697560915139, train acc = 0.8399999737739563, time = 0.009010076522827148\n",
      "Testing at step=59, batch=0, test loss = 0.3032227101480802, test acc = 0.8799999952316284, time = 0.0019626617431640625\n",
      "Testing at step=59, batch=20, test loss = 0.5599161906743126, test acc = 0.7799999713897705, time = 0.0019116401672363281\n",
      "Testing at step=59, batch=40, test loss = 0.5701369014193243, test acc = 0.8100000023841858, time = 0.00194549560546875\n",
      "Testing at step=59, batch=60, test loss = 0.3539353431848518, test acc = 0.8600000143051147, time = 0.00191497802734375\n",
      "Testing at step=59, batch=80, test loss = 0.6465380564644062, test acc = 0.8199999928474426, time = 0.0019137859344482422\n",
      "Step 59 finished in 22.806220531463623, Train loss = 0.36620530233261034, Test loss = 0.4442043239995258; Train Acc = 0.8708999994397163, Test Acc = 0.8414000004529953\n",
      "Training at step=60, batch=0, train loss = 0.42228670777368926, train acc = 0.8100000023841858, time = 0.009079694747924805\n",
      "Training at step=60, batch=120, train loss = 0.41867695988585896, train acc = 0.8399999737739563, time = 0.008852005004882812\n",
      "Training at step=60, batch=240, train loss = 0.37829728305920485, train acc = 0.8500000238418579, time = 0.008943557739257812\n",
      "Training at step=60, batch=360, train loss = 0.5393781902786118, train acc = 0.8199999928474426, time = 0.00893259048461914\n",
      "Training at step=60, batch=480, train loss = 0.36376236015677554, train acc = 0.8700000047683716, time = 0.008952617645263672\n",
      "Testing at step=60, batch=0, test loss = 0.36546898037579295, test acc = 0.8399999737739563, time = 0.001905679702758789\n",
      "Testing at step=60, batch=20, test loss = 0.3194696080041624, test acc = 0.8600000143051147, time = 0.0019121170043945312\n",
      "Testing at step=60, batch=40, test loss = 0.31700174898597683, test acc = 0.8999999761581421, time = 0.0019137859344482422\n",
      "Testing at step=60, batch=60, test loss = 0.4881429102039073, test acc = 0.8100000023841858, time = 0.0018987655639648438\n",
      "Testing at step=60, batch=80, test loss = 0.43872844694538576, test acc = 0.7900000214576721, time = 0.001954793930053711\n",
      "Step 60 finished in 22.891458988189697, Train loss = 0.36674700440713937, Test loss = 0.44714379587375364; Train Acc = 0.8713833344976107, Test Acc = 0.8408000010251999\n",
      "Training at step=61, batch=0, train loss = 0.3474761768382002, train acc = 0.8600000143051147, time = 0.009007692337036133\n",
      "Training at step=61, batch=120, train loss = 0.42145065181370917, train acc = 0.8799999952316284, time = 0.008917808532714844\n",
      "Training at step=61, batch=240, train loss = 0.41042178365172455, train acc = 0.8399999737739563, time = 0.008959531784057617\n",
      "Training at step=61, batch=360, train loss = 0.2796951570216297, train acc = 0.8799999952316284, time = 0.008930206298828125\n",
      "Training at step=61, batch=480, train loss = 0.40769664174510045, train acc = 0.8700000047683716, time = 0.009030342102050781\n",
      "Testing at step=61, batch=0, test loss = 0.4367915170292945, test acc = 0.8700000047683716, time = 0.0019278526306152344\n",
      "Testing at step=61, batch=20, test loss = 0.40078455921136397, test acc = 0.8399999737739563, time = 0.0019240379333496094\n",
      "Testing at step=61, batch=40, test loss = 0.5330308984510038, test acc = 0.8199999928474426, time = 0.0019297599792480469\n",
      "Testing at step=61, batch=60, test loss = 0.5186795710324331, test acc = 0.7400000095367432, time = 0.0020220279693603516\n",
      "Testing at step=61, batch=80, test loss = 0.6038093894203902, test acc = 0.7799999713897705, time = 0.0018994808197021484\n",
      "Step 61 finished in 22.934985876083374, Train loss = 0.3657762624026784, Test loss = 0.44664857389387; Train Acc = 0.8713500003019968, Test Acc = 0.8411999982595444\n",
      "Training at step=62, batch=0, train loss = 0.44832861374370964, train acc = 0.8399999737739563, time = 0.009184837341308594\n",
      "Training at step=62, batch=120, train loss = 0.44509842193643384, train acc = 0.8600000143051147, time = 0.00895071029663086\n",
      "Training at step=62, batch=240, train loss = 0.4757702812553417, train acc = 0.8299999833106995, time = 0.009213924407958984\n",
      "Training at step=62, batch=360, train loss = 0.3392356828922546, train acc = 0.9200000166893005, time = 0.008931398391723633\n",
      "Training at step=62, batch=480, train loss = 0.49293541972463173, train acc = 0.800000011920929, time = 0.008893489837646484\n",
      "Testing at step=62, batch=0, test loss = 0.47374183291844746, test acc = 0.8700000047683716, time = 0.0019099712371826172\n",
      "Testing at step=62, batch=20, test loss = 0.4893594682674365, test acc = 0.8399999737739563, time = 0.0019042491912841797\n",
      "Testing at step=62, batch=40, test loss = 0.4383419663889462, test acc = 0.8299999833106995, time = 0.0019447803497314453\n",
      "Testing at step=62, batch=60, test loss = 0.43661338160987256, test acc = 0.8399999737739563, time = 0.0019075870513916016\n",
      "Testing at step=62, batch=80, test loss = 0.36526251493117534, test acc = 0.8799999952316284, time = 0.001893758773803711\n",
      "Step 62 finished in 22.854653120040894, Train loss = 0.36548416218459795, Test loss = 0.44433916521675615; Train Acc = 0.8718166677157084, Test Acc = 0.8428999966382981\n",
      "Training at step=63, batch=0, train loss = 0.35790568595493705, train acc = 0.8700000047683716, time = 0.009196758270263672\n",
      "Training at step=63, batch=120, train loss = 0.4028382741778118, train acc = 0.8700000047683716, time = 0.00889277458190918\n",
      "Training at step=63, batch=240, train loss = 0.3976866781158364, train acc = 0.8600000143051147, time = 0.008936405181884766\n",
      "Training at step=63, batch=360, train loss = 0.35191648869406117, train acc = 0.8600000143051147, time = 0.009016752243041992\n",
      "Training at step=63, batch=480, train loss = 0.41420608360338823, train acc = 0.8500000238418579, time = 0.008917808532714844\n",
      "Testing at step=63, batch=0, test loss = 0.4043847922474509, test acc = 0.8899999856948853, time = 0.0019268989562988281\n",
      "Testing at step=63, batch=20, test loss = 0.45855427617777905, test acc = 0.8399999737739563, time = 0.0019311904907226562\n",
      "Testing at step=63, batch=40, test loss = 0.43872141344148013, test acc = 0.8199999928474426, time = 0.0019295215606689453\n",
      "Testing at step=63, batch=60, test loss = 0.4258890261169021, test acc = 0.8700000047683716, time = 0.0019085407257080078\n",
      "Testing at step=63, batch=80, test loss = 0.5976658551900482, test acc = 0.7900000214576721, time = 0.0018966197967529297\n",
      "Step 63 finished in 22.838805437088013, Train loss = 0.36560676325569363, Test loss = 0.4477410317046986; Train Acc = 0.8716666678587596, Test Acc = 0.8403999990224839\n",
      "Training at step=64, batch=0, train loss = 0.4482395873973323, train acc = 0.8899999856948853, time = 0.009045600891113281\n",
      "Training at step=64, batch=120, train loss = 0.29000641091054985, train acc = 0.8700000047683716, time = 0.008981466293334961\n",
      "Training at step=64, batch=240, train loss = 0.29729337021763447, train acc = 0.8799999952316284, time = 0.008916854858398438\n",
      "Training at step=64, batch=360, train loss = 0.34178504099793455, train acc = 0.8700000047683716, time = 0.008939981460571289\n",
      "Training at step=64, batch=480, train loss = 0.2965986608964124, train acc = 0.8799999952316284, time = 0.008932113647460938\n",
      "Testing at step=64, batch=0, test loss = 0.5563591725375818, test acc = 0.8399999737739563, time = 0.0019273757934570312\n",
      "Testing at step=64, batch=20, test loss = 0.4650745279073058, test acc = 0.8100000023841858, time = 0.0019352436065673828\n",
      "Testing at step=64, batch=40, test loss = 0.6950334322341977, test acc = 0.75, time = 0.001971006393432617\n",
      "Testing at step=64, batch=60, test loss = 0.4579990608228768, test acc = 0.8600000143051147, time = 0.0020513534545898438\n",
      "Testing at step=64, batch=80, test loss = 0.30035490923609537, test acc = 0.8700000047683716, time = 0.0019257068634033203\n",
      "Step 64 finished in 22.89844059944153, Train loss = 0.364763840645611, Test loss = 0.44777040994773976; Train Acc = 0.8721833351254463, Test Acc = 0.8418999993801117\n",
      "Training at step=65, batch=0, train loss = 0.40770966345130644, train acc = 0.8299999833106995, time = 0.009024858474731445\n",
      "Training at step=65, batch=120, train loss = 0.3590909261314984, train acc = 0.8600000143051147, time = 0.00906062126159668\n",
      "Training at step=65, batch=240, train loss = 0.3106309101624421, train acc = 0.8799999952316284, time = 0.008937597274780273\n",
      "Training at step=65, batch=360, train loss = 0.32699045628464574, train acc = 0.8799999952316284, time = 0.008925199508666992\n",
      "Training at step=65, batch=480, train loss = 0.4612344832144036, train acc = 0.8199999928474426, time = 0.00977468490600586\n",
      "Testing at step=65, batch=0, test loss = 0.682650076299902, test acc = 0.7599999904632568, time = 0.0018913745880126953\n",
      "Testing at step=65, batch=20, test loss = 0.35311271838990843, test acc = 0.8899999856948853, time = 0.0018949508666992188\n",
      "Testing at step=65, batch=40, test loss = 0.40349650875878645, test acc = 0.8600000143051147, time = 0.002007722854614258\n",
      "Testing at step=65, batch=60, test loss = 0.33166399473571945, test acc = 0.8899999856948853, time = 0.0019044876098632812\n",
      "Testing at step=65, batch=80, test loss = 0.49014007383053737, test acc = 0.8399999737739563, time = 0.0018930435180664062\n",
      "Step 65 finished in 22.821767807006836, Train loss = 0.36370048572451624, Test loss = 0.44669817018448754; Train Acc = 0.8722333329916, Test Acc = 0.841599999666214\n",
      "Training at step=66, batch=0, train loss = 0.31755508185851933, train acc = 0.8299999833106995, time = 0.00907754898071289\n",
      "Training at step=66, batch=120, train loss = 0.4594633334132816, train acc = 0.8299999833106995, time = 0.00895833969116211\n",
      "Training at step=66, batch=240, train loss = 0.35396327268578753, train acc = 0.8399999737739563, time = 0.00892019271850586\n",
      "Training at step=66, batch=360, train loss = 0.45659846715244135, train acc = 0.8600000143051147, time = 0.008975505828857422\n",
      "Training at step=66, batch=480, train loss = 0.31320544789488164, train acc = 0.8799999952316284, time = 0.00898432731628418\n",
      "Testing at step=66, batch=0, test loss = 0.39557635416760095, test acc = 0.8500000238418579, time = 0.002001047134399414\n",
      "Testing at step=66, batch=20, test loss = 0.46954167473450353, test acc = 0.8199999928474426, time = 0.0019085407257080078\n",
      "Testing at step=66, batch=40, test loss = 0.40927246607710166, test acc = 0.8700000047683716, time = 0.0019643306732177734\n",
      "Testing at step=66, batch=60, test loss = 0.4672019586437722, test acc = 0.8100000023841858, time = 0.0019075870513916016\n",
      "Testing at step=66, batch=80, test loss = 0.390911929178842, test acc = 0.8500000238418579, time = 0.0019116401672363281\n",
      "Step 66 finished in 22.83497142791748, Train loss = 0.36469751312690896, Test loss = 0.4477221237890805; Train Acc = 0.8720666664838791, Test Acc = 0.8416000014543533\n",
      "Training at step=67, batch=0, train loss = 0.41858007395705693, train acc = 0.8600000143051147, time = 0.009018659591674805\n",
      "Training at step=67, batch=120, train loss = 0.34795494524756576, train acc = 0.8799999952316284, time = 0.008985042572021484\n",
      "Training at step=67, batch=240, train loss = 0.3092063512570018, train acc = 0.9100000262260437, time = 0.009049415588378906\n",
      "Training at step=67, batch=360, train loss = 0.4240696393687765, train acc = 0.8100000023841858, time = 0.008988142013549805\n",
      "Training at step=67, batch=480, train loss = 0.4631458989355888, train acc = 0.8999999761581421, time = 0.008977413177490234\n",
      "Testing at step=67, batch=0, test loss = 0.5251873342188268, test acc = 0.800000011920929, time = 0.001959562301635742\n",
      "Testing at step=67, batch=20, test loss = 0.3655676206863746, test acc = 0.8600000143051147, time = 0.0019757747650146484\n",
      "Testing at step=67, batch=40, test loss = 0.396604990673964, test acc = 0.8500000238418579, time = 0.0018947124481201172\n",
      "Testing at step=67, batch=60, test loss = 0.46517802233249145, test acc = 0.8500000238418579, time = 0.002043008804321289\n",
      "Testing at step=67, batch=80, test loss = 0.36835227929986986, test acc = 0.8399999737739563, time = 0.0019342899322509766\n",
      "Step 67 finished in 22.938928604125977, Train loss = 0.36361254046461566, Test loss = 0.44630593698580007; Train Acc = 0.8727333337068558, Test Acc = 0.8417000013589859\n",
      "Training at step=68, batch=0, train loss = 0.20885393250029982, train acc = 0.9200000166893005, time = 0.009024858474731445\n",
      "Training at step=68, batch=120, train loss = 0.2803428368273203, train acc = 0.9100000262260437, time = 0.009022951126098633\n",
      "Training at step=68, batch=240, train loss = 0.2942226178244543, train acc = 0.9200000166893005, time = 0.00898599624633789\n",
      "Training at step=68, batch=360, train loss = 0.45373257804741324, train acc = 0.8600000143051147, time = 0.00892019271850586\n",
      "Training at step=68, batch=480, train loss = 0.3771222438348415, train acc = 0.8500000238418579, time = 0.008954763412475586\n",
      "Testing at step=68, batch=0, test loss = 0.38358236780592025, test acc = 0.8799999952316284, time = 0.0019221305847167969\n",
      "Testing at step=68, batch=20, test loss = 0.4086430841958568, test acc = 0.8500000238418579, time = 0.001909494400024414\n",
      "Testing at step=68, batch=40, test loss = 0.47751206402586704, test acc = 0.800000011920929, time = 0.001909494400024414\n",
      "Testing at step=68, batch=60, test loss = 0.3357587687912937, test acc = 0.8799999952316284, time = 0.0018992424011230469\n",
      "Testing at step=68, batch=80, test loss = 0.39721548926571315, test acc = 0.8799999952316284, time = 0.001903533935546875\n",
      "Step 68 finished in 22.840211391448975, Train loss = 0.36312624155507733, Test loss = 0.45058819499615543; Train Acc = 0.872466665605704, Test Acc = 0.8429000002145767\n",
      "Training at step=69, batch=0, train loss = 0.3664517430197441, train acc = 0.8899999856948853, time = 0.009051799774169922\n",
      "Training at step=69, batch=120, train loss = 0.23706954234912975, train acc = 0.9200000166893005, time = 0.008906126022338867\n",
      "Training at step=69, batch=240, train loss = 0.49438872980500087, train acc = 0.8100000023841858, time = 0.00898885726928711\n",
      "Training at step=69, batch=360, train loss = 0.2167121114914986, train acc = 0.9100000262260437, time = 0.008925914764404297\n",
      "Training at step=69, batch=480, train loss = 0.3464599495522179, train acc = 0.8700000047683716, time = 0.008915901184082031\n",
      "Testing at step=69, batch=0, test loss = 0.34721260794367304, test acc = 0.8600000143051147, time = 0.001985788345336914\n",
      "Testing at step=69, batch=20, test loss = 0.38978181481558466, test acc = 0.8600000143051147, time = 0.0019221305847167969\n",
      "Testing at step=69, batch=40, test loss = 0.4770251923673112, test acc = 0.8500000238418579, time = 0.0020143985748291016\n",
      "Testing at step=69, batch=60, test loss = 0.5613738117471166, test acc = 0.8100000023841858, time = 0.0019087791442871094\n",
      "Testing at step=69, batch=80, test loss = 0.5434408049125957, test acc = 0.8199999928474426, time = 0.002057790756225586\n",
      "Step 69 finished in 22.878815412521362, Train loss = 0.36293659115780896, Test loss = 0.4567997851229367; Train Acc = 0.8725666667024294, Test Acc = 0.8373000001907349\n",
      "Training at step=70, batch=0, train loss = 0.2851507086140172, train acc = 0.8899999856948853, time = 0.00906682014465332\n",
      "Training at step=70, batch=120, train loss = 0.41003006947697573, train acc = 0.8199999928474426, time = 0.008926153182983398\n",
      "Training at step=70, batch=240, train loss = 0.5205215038524483, train acc = 0.8899999856948853, time = 0.008936882019042969\n",
      "Training at step=70, batch=360, train loss = 0.3199543021511457, train acc = 0.8799999952316284, time = 0.008946895599365234\n",
      "Training at step=70, batch=480, train loss = 0.43308164172517744, train acc = 0.8299999833106995, time = 0.008897542953491211\n",
      "Testing at step=70, batch=0, test loss = 0.3697861034781269, test acc = 0.8500000238418579, time = 0.0019221305847167969\n",
      "Testing at step=70, batch=20, test loss = 0.5128645533743746, test acc = 0.8500000238418579, time = 0.0018966197967529297\n",
      "Testing at step=70, batch=40, test loss = 0.5673647999210057, test acc = 0.8399999737739563, time = 0.001903533935546875\n",
      "Testing at step=70, batch=60, test loss = 0.4567996205667778, test acc = 0.8399999737739563, time = 0.0019028186798095703\n",
      "Testing at step=70, batch=80, test loss = 0.5552087371207688, test acc = 0.7900000214576721, time = 0.001913309097290039\n",
      "Step 70 finished in 22.91956615447998, Train loss = 0.3632048186695782, Test loss = 0.4490810826567905; Train Acc = 0.873083332280318, Test Acc = 0.841000000834465\n",
      "Training at step=71, batch=0, train loss = 0.3823433928895728, train acc = 0.8799999952316284, time = 0.00907588005065918\n",
      "Training at step=71, batch=120, train loss = 0.38225693173340436, train acc = 0.8500000238418579, time = 0.00897979736328125\n",
      "Training at step=71, batch=240, train loss = 0.409934755964724, train acc = 0.8999999761581421, time = 0.008985280990600586\n",
      "Training at step=71, batch=360, train loss = 0.24531443450141022, train acc = 0.9100000262260437, time = 0.008920907974243164\n",
      "Training at step=71, batch=480, train loss = 0.374923367473783, train acc = 0.8799999952316284, time = 0.008879899978637695\n",
      "Testing at step=71, batch=0, test loss = 0.24511674503885167, test acc = 0.9100000262260437, time = 0.0019199848175048828\n",
      "Testing at step=71, batch=20, test loss = 0.38170842192269605, test acc = 0.8899999856948853, time = 0.0019023418426513672\n",
      "Testing at step=71, batch=40, test loss = 0.5162279625166996, test acc = 0.8799999952316284, time = 0.0018641948699951172\n",
      "Testing at step=71, batch=60, test loss = 0.442495422842222, test acc = 0.8299999833106995, time = 0.002007722854614258\n",
      "Testing at step=71, batch=80, test loss = 0.3752813626441954, test acc = 0.8500000238418579, time = 0.0019030570983886719\n",
      "Step 71 finished in 22.84154200553894, Train loss = 0.3627129646235563, Test loss = 0.44794136760320635; Train Acc = 0.8713000005483628, Test Acc = 0.8418999987840653\n",
      "Training at step=72, batch=0, train loss = 0.39763701786965194, train acc = 0.8799999952316284, time = 0.009113311767578125\n",
      "Training at step=72, batch=120, train loss = 0.45433347051549866, train acc = 0.8500000238418579, time = 0.008994340896606445\n",
      "Training at step=72, batch=240, train loss = 0.35969328798090444, train acc = 0.8600000143051147, time = 0.008989810943603516\n",
      "Training at step=72, batch=360, train loss = 0.2750495523317351, train acc = 0.9100000262260437, time = 0.008954048156738281\n",
      "Training at step=72, batch=480, train loss = 0.46206122547471645, train acc = 0.8299999833106995, time = 0.00894784927368164\n",
      "Testing at step=72, batch=0, test loss = 0.429770088438276, test acc = 0.8199999928474426, time = 0.001898050308227539\n",
      "Testing at step=72, batch=20, test loss = 0.5716043458658949, test acc = 0.8100000023841858, time = 0.0019001960754394531\n",
      "Testing at step=72, batch=40, test loss = 0.42178062481479095, test acc = 0.8199999928474426, time = 0.0018925666809082031\n",
      "Testing at step=72, batch=60, test loss = 0.4202924032677673, test acc = 0.8500000238418579, time = 0.0019135475158691406\n",
      "Testing at step=72, batch=80, test loss = 0.41658790555585734, test acc = 0.8500000238418579, time = 0.0018935203552246094\n",
      "Step 72 finished in 22.858281135559082, Train loss = 0.3623701610116893, Test loss = 0.44847369509307305; Train Acc = 0.8717333335677783, Test Acc = 0.8405999976396561\n",
      "Training at step=73, batch=0, train loss = 0.427709964319781, train acc = 0.8799999952316284, time = 0.009093761444091797\n",
      "Training at step=73, batch=120, train loss = 0.344524534054032, train acc = 0.8500000238418579, time = 0.009979963302612305\n",
      "Training at step=73, batch=240, train loss = 0.3956440215940994, train acc = 0.8299999833106995, time = 0.008982419967651367\n",
      "Training at step=73, batch=360, train loss = 0.5332326032686394, train acc = 0.7799999713897705, time = 0.008877277374267578\n",
      "Training at step=73, batch=480, train loss = 0.29085852051894057, train acc = 0.8700000047683716, time = 0.008938312530517578\n",
      "Testing at step=73, batch=0, test loss = 0.5102985619183311, test acc = 0.800000011920929, time = 0.001936197280883789\n",
      "Testing at step=73, batch=20, test loss = 0.38961639537021275, test acc = 0.8399999737739563, time = 0.002032041549682617\n",
      "Testing at step=73, batch=40, test loss = 0.3415613580147966, test acc = 0.8600000143051147, time = 0.0019414424896240234\n",
      "Testing at step=73, batch=60, test loss = 0.42519724302849654, test acc = 0.8100000023841858, time = 0.0019383430480957031\n",
      "Testing at step=73, batch=80, test loss = 0.5740868742424117, test acc = 0.8100000023841858, time = 0.0019156932830810547\n",
      "Step 73 finished in 22.860920667648315, Train loss = 0.36221254334618147, Test loss = 0.46139682425909995; Train Acc = 0.8724333330988884, Test Acc = 0.8351999992132186\n",
      "Training at step=74, batch=0, train loss = 0.4023461341383954, train acc = 0.8700000047683716, time = 0.009004354476928711\n",
      "Training at step=74, batch=120, train loss = 0.2905007083517573, train acc = 0.8999999761581421, time = 0.008987665176391602\n",
      "Training at step=74, batch=240, train loss = 0.4773303294233742, train acc = 0.8500000238418579, time = 0.008898019790649414\n",
      "Training at step=74, batch=360, train loss = 0.4431921535913559, train acc = 0.8700000047683716, time = 0.00886845588684082\n",
      "Training at step=74, batch=480, train loss = 0.40190818067174633, train acc = 0.8600000143051147, time = 0.009705543518066406\n",
      "Testing at step=74, batch=0, test loss = 0.3991715996464962, test acc = 0.8500000238418579, time = 0.0019156932830810547\n",
      "Testing at step=74, batch=20, test loss = 0.5461816024091553, test acc = 0.7699999809265137, time = 0.0019218921661376953\n",
      "Testing at step=74, batch=40, test loss = 0.32650121321904213, test acc = 0.8700000047683716, time = 0.001947641372680664\n",
      "Testing at step=74, batch=60, test loss = 0.4555455454597005, test acc = 0.8399999737739563, time = 0.0019054412841796875\n",
      "Testing at step=74, batch=80, test loss = 0.32429787214130784, test acc = 0.8999999761581421, time = 0.0018956661224365234\n",
      "Step 74 finished in 22.897708415985107, Train loss = 0.36201472663092715, Test loss = 0.44893532201345415; Train Acc = 0.8720666665832202, Test Acc = 0.8397999978065491\n",
      "Training at step=75, batch=0, train loss = 0.4171774118389181, train acc = 0.8299999833106995, time = 0.009103059768676758\n",
      "Training at step=75, batch=120, train loss = 0.4094238583214181, train acc = 0.8500000238418579, time = 0.008948802947998047\n",
      "Training at step=75, batch=240, train loss = 0.3259975866728101, train acc = 0.8500000238418579, time = 0.008919954299926758\n",
      "Training at step=75, batch=360, train loss = 0.30056781305626773, train acc = 0.9100000262260437, time = 0.008986949920654297\n",
      "Training at step=75, batch=480, train loss = 0.3194155335163448, train acc = 0.8799999952316284, time = 0.008970499038696289\n",
      "Testing at step=75, batch=0, test loss = 0.5696095562313661, test acc = 0.7699999809265137, time = 0.0019185543060302734\n",
      "Testing at step=75, batch=20, test loss = 0.3273401893572337, test acc = 0.8899999856948853, time = 0.0018963813781738281\n",
      "Testing at step=75, batch=40, test loss = 0.4085250429609308, test acc = 0.8799999952316284, time = 0.0019333362579345703\n",
      "Testing at step=75, batch=60, test loss = 0.4199559809021558, test acc = 0.8700000047683716, time = 0.0019123554229736328\n",
      "Testing at step=75, batch=80, test loss = 0.41472845756741206, test acc = 0.8500000238418579, time = 0.001920461654663086\n",
      "Step 75 finished in 22.914011001586914, Train loss = 0.36123952000950055, Test loss = 0.45081401828685375; Train Acc = 0.873466666440169, Test Acc = 0.841299996972084\n",
      "Training at step=76, batch=0, train loss = 0.2914887608153282, train acc = 0.8999999761581421, time = 0.008990764617919922\n",
      "Training at step=76, batch=120, train loss = 0.3748947123673236, train acc = 0.8399999737739563, time = 0.00902104377746582\n",
      "Training at step=76, batch=240, train loss = 0.3120638468100551, train acc = 0.8799999952316284, time = 0.00901341438293457\n",
      "Training at step=76, batch=360, train loss = 0.4892896914596314, train acc = 0.8199999928474426, time = 0.008983850479125977\n",
      "Training at step=76, batch=480, train loss = 0.3661421390339181, train acc = 0.9200000166893005, time = 0.008938789367675781\n",
      "Testing at step=76, batch=0, test loss = 0.42352692108796547, test acc = 0.8399999737739563, time = 0.0019252300262451172\n",
      "Testing at step=76, batch=20, test loss = 0.6969231672692899, test acc = 0.800000011920929, time = 0.0019085407257080078\n",
      "Testing at step=76, batch=40, test loss = 0.37933856688351125, test acc = 0.8799999952316284, time = 0.0018770694732666016\n",
      "Testing at step=76, batch=60, test loss = 0.42968265332250494, test acc = 0.8199999928474426, time = 0.0019142627716064453\n",
      "Testing at step=76, batch=80, test loss = 0.23355549041196924, test acc = 0.8899999856948853, time = 0.0019032955169677734\n",
      "Step 76 finished in 22.93791389465332, Train loss = 0.3614797657886582, Test loss = 0.4495005238125461; Train Acc = 0.8734666658441226, Test Acc = 0.8414999973773957\n",
      "Training at step=77, batch=0, train loss = 0.427385962654655, train acc = 0.8399999737739563, time = 0.009148120880126953\n",
      "Training at step=77, batch=120, train loss = 0.46188522306259217, train acc = 0.800000011920929, time = 0.008907794952392578\n",
      "Training at step=77, batch=240, train loss = 0.3643360322550305, train acc = 0.8700000047683716, time = 0.008947372436523438\n",
      "Training at step=77, batch=360, train loss = 0.4454559320075132, train acc = 0.8600000143051147, time = 0.008935689926147461\n",
      "Training at step=77, batch=480, train loss = 0.41154078780534364, train acc = 0.9300000071525574, time = 0.009128093719482422\n",
      "Testing at step=77, batch=0, test loss = 0.3163979666545707, test acc = 0.8700000047683716, time = 0.0021097660064697266\n",
      "Testing at step=77, batch=20, test loss = 0.361951022466824, test acc = 0.8600000143051147, time = 0.0019080638885498047\n",
      "Testing at step=77, batch=40, test loss = 0.4359119344529768, test acc = 0.8500000238418579, time = 0.0019071102142333984\n",
      "Testing at step=77, batch=60, test loss = 0.29845143185819006, test acc = 0.8899999856948853, time = 0.0018804073333740234\n",
      "Testing at step=77, batch=80, test loss = 0.4826891727023382, test acc = 0.8500000238418579, time = 0.001979827880859375\n",
      "Step 77 finished in 22.961114645004272, Train loss = 0.36131739552717657, Test loss = 0.44961136867285156; Train Acc = 0.8724166669448217, Test Acc = 0.8416999965906143\n",
      "Training at step=78, batch=0, train loss = 0.3092923162601513, train acc = 0.9200000166893005, time = 0.009099960327148438\n",
      "Training at step=78, batch=120, train loss = 0.23416837919528202, train acc = 0.9300000071525574, time = 0.008917808532714844\n",
      "Training at step=78, batch=240, train loss = 0.43792913361858077, train acc = 0.8299999833106995, time = 0.008942365646362305\n",
      "Training at step=78, batch=360, train loss = 0.44845265907426046, train acc = 0.8500000238418579, time = 0.00895380973815918\n",
      "Training at step=78, batch=480, train loss = 0.39486373024534016, train acc = 0.8799999952316284, time = 0.008965730667114258\n",
      "Testing at step=78, batch=0, test loss = 0.5151296959035965, test acc = 0.800000011920929, time = 0.0019202232360839844\n",
      "Testing at step=78, batch=20, test loss = 0.3582734083850779, test acc = 0.8999999761581421, time = 0.0019161701202392578\n",
      "Testing at step=78, batch=40, test loss = 0.2920873249373783, test acc = 0.8500000238418579, time = 0.0019304752349853516\n",
      "Testing at step=78, batch=60, test loss = 0.5024249064586973, test acc = 0.8500000238418579, time = 0.0019037723541259766\n",
      "Testing at step=78, batch=80, test loss = 0.378683877167074, test acc = 0.8899999856948853, time = 0.0019063949584960938\n",
      "Step 78 finished in 22.94047474861145, Train loss = 0.3617254718440012, Test loss = 0.452324695144462; Train Acc = 0.8733166660865148, Test Acc = 0.8402999967336655\n",
      "Training at step=79, batch=0, train loss = 0.2382522097521585, train acc = 0.9200000166893005, time = 0.008979082107543945\n",
      "Training at step=79, batch=120, train loss = 0.3426346543754806, train acc = 0.8600000143051147, time = 0.009362459182739258\n",
      "Training at step=79, batch=240, train loss = 0.2556411661416637, train acc = 0.8999999761581421, time = 0.008893251419067383\n",
      "Training at step=79, batch=360, train loss = 0.24445452604078788, train acc = 0.8899999856948853, time = 0.008939027786254883\n",
      "Training at step=79, batch=480, train loss = 0.30364222519434, train acc = 0.8899999856948853, time = 0.009042501449584961\n",
      "Testing at step=79, batch=0, test loss = 0.3425119341857792, test acc = 0.8999999761581421, time = 0.001943349838256836\n",
      "Testing at step=79, batch=20, test loss = 0.2653871401496862, test acc = 0.8899999856948853, time = 0.0019822120666503906\n",
      "Testing at step=79, batch=40, test loss = 0.5475438249639445, test acc = 0.7799999713897705, time = 0.0019352436065673828\n",
      "Testing at step=79, batch=60, test loss = 0.5245235128622817, test acc = 0.8500000238418579, time = 0.0019266605377197266\n",
      "Testing at step=79, batch=80, test loss = 0.6164730671084502, test acc = 0.7599999904632568, time = 0.0018973350524902344\n",
      "Step 79 finished in 22.919702529907227, Train loss = 0.36019280823374544, Test loss = 0.448960812640516; Train Acc = 0.873149999777476, Test Acc = 0.8406999987363816\n",
      "Training at step=80, batch=0, train loss = 0.3939110720498039, train acc = 0.8500000238418579, time = 0.009029626846313477\n",
      "Training at step=80, batch=120, train loss = 0.4309389512556718, train acc = 0.8199999928474426, time = 0.00897216796875\n",
      "Training at step=80, batch=240, train loss = 0.23868568584528796, train acc = 0.8999999761581421, time = 0.008902311325073242\n",
      "Training at step=80, batch=360, train loss = 0.35167056869581714, train acc = 0.8700000047683716, time = 0.008949041366577148\n",
      "Training at step=80, batch=480, train loss = 0.2367530067422048, train acc = 0.8999999761581421, time = 0.009221315383911133\n",
      "Testing at step=80, batch=0, test loss = 0.47816886934366154, test acc = 0.8500000238418579, time = 0.001924276351928711\n",
      "Testing at step=80, batch=20, test loss = 0.49659573219258896, test acc = 0.8399999737739563, time = 0.00189971923828125\n",
      "Testing at step=80, batch=40, test loss = 0.5178723567737147, test acc = 0.8600000143051147, time = 0.001920461654663086\n",
      "Testing at step=80, batch=60, test loss = 0.3938070366633707, test acc = 0.8700000047683716, time = 0.0019030570983886719\n",
      "Testing at step=80, batch=80, test loss = 0.40894433911890987, test acc = 0.8799999952316284, time = 0.0018999576568603516\n",
      "Step 80 finished in 22.89659881591797, Train loss = 0.3599218268905857, Test loss = 0.4503938489706764; Train Acc = 0.87323333243529, Test Acc = 0.8398999989032745\n",
      "Training at step=81, batch=0, train loss = 0.2822574350542935, train acc = 0.8899999856948853, time = 0.009117603302001953\n",
      "Training at step=81, batch=120, train loss = 0.33961764140403744, train acc = 0.8999999761581421, time = 0.008894681930541992\n",
      "Training at step=81, batch=240, train loss = 0.2666363881455911, train acc = 0.9300000071525574, time = 0.00890207290649414\n",
      "Training at step=81, batch=360, train loss = 0.33057731503827475, train acc = 0.8600000143051147, time = 0.00888824462890625\n",
      "Training at step=81, batch=480, train loss = 0.31227721415672605, train acc = 0.8899999856948853, time = 0.00926351547241211\n",
      "Testing at step=81, batch=0, test loss = 0.5584473633528111, test acc = 0.8100000023841858, time = 0.0019648075103759766\n",
      "Testing at step=81, batch=20, test loss = 0.3457954609470498, test acc = 0.8700000047683716, time = 0.0019299983978271484\n",
      "Testing at step=81, batch=40, test loss = 0.43305481536646406, test acc = 0.8199999928474426, time = 0.0019202232360839844\n",
      "Testing at step=81, batch=60, test loss = 0.4171230684054252, test acc = 0.8500000238418579, time = 0.0019729137420654297\n",
      "Testing at step=81, batch=80, test loss = 0.32267125435005745, test acc = 0.8899999856948853, time = 0.0019040107727050781\n",
      "Step 81 finished in 22.78793501853943, Train loss = 0.3598761029452354, Test loss = 0.45609418764797455; Train Acc = 0.872833333214124, Test Acc = 0.838699996471405\n",
      "Training at step=82, batch=0, train loss = 0.4055104879025294, train acc = 0.8600000143051147, time = 0.008936405181884766\n",
      "Training at step=82, batch=120, train loss = 0.49046867092194973, train acc = 0.8500000238418579, time = 0.009115219116210938\n",
      "Training at step=82, batch=240, train loss = 0.33630914354882213, train acc = 0.8600000143051147, time = 0.008895397186279297\n",
      "Training at step=82, batch=360, train loss = 0.26483291963556005, train acc = 0.8999999761581421, time = 0.010662317276000977\n",
      "Training at step=82, batch=480, train loss = 0.2257959888655078, train acc = 0.8899999856948853, time = 0.008894920349121094\n",
      "Testing at step=82, batch=0, test loss = 0.4085601990844557, test acc = 0.8500000238418579, time = 0.0019235610961914062\n",
      "Testing at step=82, batch=20, test loss = 0.5106351054962883, test acc = 0.8100000023841858, time = 0.0019266605377197266\n",
      "Testing at step=82, batch=40, test loss = 0.4441583968229601, test acc = 0.8199999928474426, time = 0.001905202865600586\n",
      "Testing at step=82, batch=60, test loss = 0.42799107172457435, test acc = 0.8199999928474426, time = 0.001886606216430664\n",
      "Testing at step=82, batch=80, test loss = 0.5214204395261283, test acc = 0.8299999833106995, time = 0.001924276351928711\n",
      "Step 82 finished in 22.865556240081787, Train loss = 0.3604498353409741, Test loss = 0.4513148754722978; Train Acc = 0.8735666670401891, Test Acc = 0.8410000002384186\n",
      "Training at step=83, batch=0, train loss = 0.4261976431905941, train acc = 0.8899999856948853, time = 0.009055376052856445\n",
      "Training at step=83, batch=120, train loss = 0.30895996942640525, train acc = 0.9100000262260437, time = 0.008943319320678711\n",
      "Training at step=83, batch=240, train loss = 0.3336744272436023, train acc = 0.8899999856948853, time = 0.008988142013549805\n",
      "Training at step=83, batch=360, train loss = 0.37839605644860774, train acc = 0.8799999952316284, time = 0.009041786193847656\n",
      "Training at step=83, batch=480, train loss = 0.4536626194718702, train acc = 0.8399999737739563, time = 0.008964300155639648\n",
      "Testing at step=83, batch=0, test loss = 0.5391939798809572, test acc = 0.8100000023841858, time = 0.0019485950469970703\n",
      "Testing at step=83, batch=20, test loss = 0.4137214435700925, test acc = 0.8600000143051147, time = 0.001934051513671875\n",
      "Testing at step=83, batch=40, test loss = 0.423345827479353, test acc = 0.8399999737739563, time = 0.0019047260284423828\n",
      "Testing at step=83, batch=60, test loss = 0.7589386128293453, test acc = 0.8199999928474426, time = 0.0019152164459228516\n",
      "Testing at step=83, batch=80, test loss = 0.40622024496687487, test acc = 0.8299999833106995, time = 0.0019097328186035156\n",
      "Step 83 finished in 22.920756816864014, Train loss = 0.35947222321623085, Test loss = 0.4490424315467655; Train Acc = 0.8736833327015241, Test Acc = 0.8422999989986419\n",
      "Training at step=84, batch=0, train loss = 0.3654795129733708, train acc = 0.8700000047683716, time = 0.009089231491088867\n",
      "Training at step=84, batch=120, train loss = 0.3894264415682322, train acc = 0.8999999761581421, time = 0.008892297744750977\n",
      "Training at step=84, batch=240, train loss = 0.4506645757107329, train acc = 0.8700000047683716, time = 0.008904218673706055\n",
      "Training at step=84, batch=360, train loss = 0.37131771057304425, train acc = 0.8799999952316284, time = 0.00897073745727539\n",
      "Training at step=84, batch=480, train loss = 0.35048017486710753, train acc = 0.8500000238418579, time = 0.008984565734863281\n",
      "Testing at step=84, batch=0, test loss = 0.4718476017679953, test acc = 0.8299999833106995, time = 0.00193023681640625\n",
      "Testing at step=84, batch=20, test loss = 0.4398751379321741, test acc = 0.8500000238418579, time = 0.0019044876098632812\n",
      "Testing at step=84, batch=40, test loss = 0.4347154877507653, test acc = 0.8899999856948853, time = 0.001917123794555664\n",
      "Testing at step=84, batch=60, test loss = 0.3581063447305858, test acc = 0.8700000047683716, time = 0.0018887519836425781\n",
      "Testing at step=84, batch=80, test loss = 0.3938803026352379, test acc = 0.8600000143051147, time = 0.0020704269409179688\n",
      "Step 84 finished in 22.931376695632935, Train loss = 0.3595587197505558, Test loss = 0.45360892587293367; Train Acc = 0.874049998819828, Test Acc = 0.841800000667572\n",
      "Training at step=85, batch=0, train loss = 0.31063447213216105, train acc = 0.8999999761581421, time = 0.009026765823364258\n",
      "Training at step=85, batch=120, train loss = 0.34480091701484006, train acc = 0.8600000143051147, time = 0.008932352066040039\n",
      "Training at step=85, batch=240, train loss = 0.2009859521638563, train acc = 0.9399999976158142, time = 0.009026288986206055\n",
      "Training at step=85, batch=360, train loss = 0.3013723503242435, train acc = 0.8899999856948853, time = 0.00894618034362793\n",
      "Training at step=85, batch=480, train loss = 0.3842543860379961, train acc = 0.8600000143051147, time = 0.008998870849609375\n",
      "Testing at step=85, batch=0, test loss = 0.5723758982118751, test acc = 0.8100000023841858, time = 0.001995086669921875\n",
      "Testing at step=85, batch=20, test loss = 0.47761746250925946, test acc = 0.8199999928474426, time = 0.0019309520721435547\n",
      "Testing at step=85, batch=40, test loss = 0.5037707695743611, test acc = 0.8500000238418579, time = 0.0019669532775878906\n",
      "Testing at step=85, batch=60, test loss = 0.5161465382768982, test acc = 0.7900000214576721, time = 0.0019211769104003906\n",
      "Testing at step=85, batch=80, test loss = 0.2970941746842942, test acc = 0.8899999856948853, time = 0.0019404888153076172\n",
      "Step 85 finished in 22.87074089050293, Train loss = 0.35926854185911433, Test loss = 0.45309802076201455; Train Acc = 0.8741999992728233, Test Acc = 0.8416999989748001\n",
      "Training at step=86, batch=0, train loss = 0.275266399516177, train acc = 0.9300000071525574, time = 0.009124517440795898\n",
      "Training at step=86, batch=120, train loss = 0.35307047428620675, train acc = 0.9200000166893005, time = 0.008949518203735352\n",
      "Training at step=86, batch=240, train loss = 0.39818485363565465, train acc = 0.8500000238418579, time = 0.008921623229980469\n",
      "Training at step=86, batch=360, train loss = 0.3425386142952814, train acc = 0.8600000143051147, time = 0.008901119232177734\n",
      "Training at step=86, batch=480, train loss = 0.40573737140039495, train acc = 0.8600000143051147, time = 0.008997440338134766\n",
      "Testing at step=86, batch=0, test loss = 0.48657174550901305, test acc = 0.8500000238418579, time = 0.0018949508666992188\n",
      "Testing at step=86, batch=20, test loss = 0.4216171050899936, test acc = 0.8299999833106995, time = 0.0019032955169677734\n",
      "Testing at step=86, batch=40, test loss = 0.4502891798873376, test acc = 0.8500000238418579, time = 0.0018889904022216797\n",
      "Testing at step=86, batch=60, test loss = 0.5283117012713534, test acc = 0.8299999833106995, time = 0.0018925666809082031\n",
      "Testing at step=86, batch=80, test loss = 0.2893356241606389, test acc = 0.8799999952316284, time = 0.0019278526306152344\n",
      "Step 86 finished in 22.842231273651123, Train loss = 0.3587777554211789, Test loss = 0.45889272633769773; Train Acc = 0.8735666665434837, Test Acc = 0.837700001001358\n",
      "Training at step=87, batch=0, train loss = 0.3345149896614869, train acc = 0.8600000143051147, time = 0.009127140045166016\n",
      "Training at step=87, batch=120, train loss = 0.42533404607358877, train acc = 0.8299999833106995, time = 0.009101390838623047\n",
      "Training at step=87, batch=240, train loss = 0.26946312260525873, train acc = 0.9200000166893005, time = 0.009093761444091797\n",
      "Training at step=87, batch=360, train loss = 0.2842151837325008, train acc = 0.9100000262260437, time = 0.009082555770874023\n",
      "Training at step=87, batch=480, train loss = 0.3001001665329216, train acc = 0.8899999856948853, time = 0.008912324905395508\n",
      "Testing at step=87, batch=0, test loss = 0.3627557348484794, test acc = 0.8700000047683716, time = 0.0019359588623046875\n",
      "Testing at step=87, batch=20, test loss = 0.4441398107088451, test acc = 0.8299999833106995, time = 0.0019152164459228516\n",
      "Testing at step=87, batch=40, test loss = 0.4837104222750652, test acc = 0.8600000143051147, time = 0.001947641372680664\n",
      "Testing at step=87, batch=60, test loss = 0.37719931205431345, test acc = 0.9100000262260437, time = 0.0018868446350097656\n",
      "Testing at step=87, batch=80, test loss = 0.377196890041789, test acc = 0.9100000262260437, time = 0.0018885135650634766\n",
      "Step 87 finished in 22.93679904937744, Train loss = 0.35911754573968846, Test loss = 0.4552346369213209; Train Acc = 0.8738833332061767, Test Acc = 0.840099995136261\n",
      "Training at step=88, batch=0, train loss = 0.3195528552011617, train acc = 0.8600000143051147, time = 0.009102821350097656\n",
      "Training at step=88, batch=120, train loss = 0.3192644608694533, train acc = 0.8799999952316284, time = 0.008918523788452148\n",
      "Training at step=88, batch=240, train loss = 0.44036427370853437, train acc = 0.8100000023841858, time = 0.008951425552368164\n",
      "Training at step=88, batch=360, train loss = 0.24413228115734933, train acc = 0.949999988079071, time = 0.009006500244140625\n",
      "Training at step=88, batch=480, train loss = 0.3642617416300713, train acc = 0.8999999761581421, time = 0.008997917175292969\n",
      "Testing at step=88, batch=0, test loss = 0.5616341172867604, test acc = 0.8399999737739563, time = 0.0019414424896240234\n",
      "Testing at step=88, batch=20, test loss = 0.48019261470077107, test acc = 0.8199999928474426, time = 0.0018954277038574219\n",
      "Testing at step=88, batch=40, test loss = 0.41161193341314883, test acc = 0.8299999833106995, time = 0.0020301342010498047\n",
      "Testing at step=88, batch=60, test loss = 0.4795566281081285, test acc = 0.8399999737739563, time = 0.00193023681640625\n",
      "Testing at step=88, batch=80, test loss = 0.4230271702510892, test acc = 0.8100000023841858, time = 0.0018930435180664062\n",
      "Step 88 finished in 22.921276330947876, Train loss = 0.35837290449861314, Test loss = 0.45133526194077705; Train Acc = 0.8740666663646698, Test Acc = 0.8408999979496002\n",
      "Training at step=89, batch=0, train loss = 0.41793607740481115, train acc = 0.8899999856948853, time = 0.009001731872558594\n",
      "Training at step=89, batch=120, train loss = 0.29596992031019737, train acc = 0.8899999856948853, time = 0.00889444351196289\n",
      "Training at step=89, batch=240, train loss = 0.3026269243731954, train acc = 0.8799999952316284, time = 0.008948087692260742\n",
      "Training at step=89, batch=360, train loss = 0.3996619186041573, train acc = 0.8399999737739563, time = 0.008992671966552734\n",
      "Training at step=89, batch=480, train loss = 0.37405717176863995, train acc = 0.8500000238418579, time = 0.008953094482421875\n",
      "Testing at step=89, batch=0, test loss = 0.48049033389615564, test acc = 0.8799999952316284, time = 0.0019741058349609375\n",
      "Testing at step=89, batch=20, test loss = 0.3173748024539126, test acc = 0.8700000047683716, time = 0.001920938491821289\n",
      "Testing at step=89, batch=40, test loss = 0.315907687828407, test acc = 0.8600000143051147, time = 0.001924276351928711\n",
      "Testing at step=89, batch=60, test loss = 0.4194533980637398, test acc = 0.8600000143051147, time = 0.0019309520721435547\n",
      "Testing at step=89, batch=80, test loss = 0.3903094115909005, test acc = 0.8600000143051147, time = 0.001968860626220703\n",
      "Step 89 finished in 22.909112215042114, Train loss = 0.35911593912841994, Test loss = 0.4527272260578695; Train Acc = 0.8742166660229365, Test Acc = 0.8423999977111817\n",
      "Training at step=90, batch=0, train loss = 0.4429369357791208, train acc = 0.8600000143051147, time = 0.008979558944702148\n",
      "Training at step=90, batch=120, train loss = 0.3175922896083995, train acc = 0.8600000143051147, time = 0.008876800537109375\n",
      "Training at step=90, batch=240, train loss = 0.35372652395199217, train acc = 0.8799999952316284, time = 0.008953094482421875\n",
      "Training at step=90, batch=360, train loss = 0.3698744127624859, train acc = 0.8899999856948853, time = 0.008972644805908203\n",
      "Training at step=90, batch=480, train loss = 0.22106596850707821, train acc = 0.9200000166893005, time = 0.008908748626708984\n",
      "Testing at step=90, batch=0, test loss = 0.39551927750926746, test acc = 0.8700000047683716, time = 0.001935720443725586\n",
      "Testing at step=90, batch=20, test loss = 0.33353713154171905, test acc = 0.9100000262260437, time = 0.001922607421875\n",
      "Testing at step=90, batch=40, test loss = 0.3187095911678247, test acc = 0.8600000143051147, time = 0.0018787384033203125\n",
      "Testing at step=90, batch=60, test loss = 0.43148019078624544, test acc = 0.8399999737739563, time = 0.0019376277923583984\n",
      "Testing at step=90, batch=80, test loss = 0.24256179756057544, test acc = 0.8999999761581421, time = 0.0019021034240722656\n",
      "Step 90 finished in 22.92402935028076, Train loss = 0.35882107835837485, Test loss = 0.45110127518591703; Train Acc = 0.8746166665355365, Test Acc = 0.8411999970674515\n",
      "Training at step=91, batch=0, train loss = 0.45004325505478887, train acc = 0.8500000238418579, time = 0.009158134460449219\n",
      "Training at step=91, batch=120, train loss = 0.35164914282386384, train acc = 0.8500000238418579, time = 0.009212017059326172\n",
      "Training at step=91, batch=240, train loss = 0.20197508276337864, train acc = 0.9399999976158142, time = 0.009141683578491211\n",
      "Training at step=91, batch=360, train loss = 0.48715553453283983, train acc = 0.8799999952316284, time = 0.009036779403686523\n",
      "Training at step=91, batch=480, train loss = 0.4083375328334658, train acc = 0.8600000143051147, time = 0.008954286575317383\n",
      "Testing at step=91, batch=0, test loss = 0.41957632424985114, test acc = 0.8799999952316284, time = 0.0019237995147705078\n",
      "Testing at step=91, batch=20, test loss = 0.6358094527665203, test acc = 0.800000011920929, time = 0.0019240379333496094\n",
      "Testing at step=91, batch=40, test loss = 0.4270594831086287, test acc = 0.8100000023841858, time = 0.0019214153289794922\n",
      "Testing at step=91, batch=60, test loss = 0.38939417606457805, test acc = 0.8500000238418579, time = 0.0019109249114990234\n",
      "Testing at step=91, batch=80, test loss = 0.32759474890443074, test acc = 0.8799999952316284, time = 0.001973390579223633\n",
      "Step 91 finished in 22.914180517196655, Train loss = 0.3577764956556779, Test loss = 0.4545304350327163; Train Acc = 0.874733332892259, Test Acc = 0.838899998664856\n",
      "Training at step=92, batch=0, train loss = 0.356399498169632, train acc = 0.8700000047683716, time = 0.009071111679077148\n",
      "Training at step=92, batch=120, train loss = 0.45125390815774197, train acc = 0.8299999833106995, time = 0.008920669555664062\n",
      "Training at step=92, batch=240, train loss = 0.2553567680716158, train acc = 0.8999999761581421, time = 0.008962869644165039\n",
      "Training at step=92, batch=360, train loss = 0.25902555600738564, train acc = 0.9300000071525574, time = 0.008994102478027344\n",
      "Training at step=92, batch=480, train loss = 0.35461922356802866, train acc = 0.8899999856948853, time = 0.00890803337097168\n",
      "Testing at step=92, batch=0, test loss = 0.3152775568085262, test acc = 0.8999999761581421, time = 0.0019371509552001953\n",
      "Testing at step=92, batch=20, test loss = 0.39947086149549954, test acc = 0.8999999761581421, time = 0.0018970966339111328\n",
      "Testing at step=92, batch=40, test loss = 0.37292458555239627, test acc = 0.8600000143051147, time = 0.0019245147705078125\n",
      "Testing at step=92, batch=60, test loss = 0.33960123880380877, test acc = 0.8899999856948853, time = 0.0019078254699707031\n",
      "Testing at step=92, batch=80, test loss = 0.4737299999825927, test acc = 0.8600000143051147, time = 0.0019478797912597656\n",
      "Step 92 finished in 22.876124143600464, Train loss = 0.35745229997311656, Test loss = 0.45340170089226106; Train Acc = 0.8743333329757055, Test Acc = 0.8397999984025956\n",
      "Training at step=93, batch=0, train loss = 0.36181527063184177, train acc = 0.8500000238418579, time = 0.009065389633178711\n",
      "Training at step=93, batch=120, train loss = 0.32044389135574214, train acc = 0.8700000047683716, time = 0.00894784927368164\n",
      "Training at step=93, batch=240, train loss = 0.36293425363552023, train acc = 0.8600000143051147, time = 0.008941411972045898\n",
      "Training at step=93, batch=360, train loss = 0.20606941324487962, train acc = 0.9300000071525574, time = 0.00900721549987793\n",
      "Training at step=93, batch=480, train loss = 0.3615018943743831, train acc = 0.8799999952316284, time = 0.008986473083496094\n",
      "Testing at step=93, batch=0, test loss = 0.5877770240774166, test acc = 0.7900000214576721, time = 0.0019347667694091797\n",
      "Testing at step=93, batch=20, test loss = 0.46000287584244165, test acc = 0.8399999737739563, time = 0.001878976821899414\n",
      "Testing at step=93, batch=40, test loss = 0.36695412565260993, test acc = 0.8799999952316284, time = 0.0018966197967529297\n",
      "Testing at step=93, batch=60, test loss = 0.36242072503635187, test acc = 0.8500000238418579, time = 0.0019011497497558594\n",
      "Testing at step=93, batch=80, test loss = 0.541859115769571, test acc = 0.800000011920929, time = 0.0019061565399169922\n",
      "Step 93 finished in 22.941341876983643, Train loss = 0.3573544797435161, Test loss = 0.4508640189964208; Train Acc = 0.8743166668216388, Test Acc = 0.8393999999761581\n",
      "Training at step=94, batch=0, train loss = 0.43303535070407234, train acc = 0.8999999761581421, time = 0.009061098098754883\n",
      "Training at step=94, batch=120, train loss = 0.2657370599321467, train acc = 0.9200000166893005, time = 0.008818864822387695\n",
      "Training at step=94, batch=240, train loss = 0.2838079707913266, train acc = 0.8799999952316284, time = 0.00891733169555664\n",
      "Training at step=94, batch=360, train loss = 0.3571990875495269, train acc = 0.8700000047683716, time = 0.008952856063842773\n",
      "Training at step=94, batch=480, train loss = 0.3402007644084093, train acc = 0.8700000047683716, time = 0.009200334548950195\n",
      "Testing at step=94, batch=0, test loss = 0.3997564683146084, test acc = 0.8600000143051147, time = 0.0027663707733154297\n",
      "Testing at step=94, batch=20, test loss = 0.6020225681397215, test acc = 0.8199999928474426, time = 0.0019011497497558594\n",
      "Testing at step=94, batch=40, test loss = 0.6433046411056118, test acc = 0.8199999928474426, time = 0.0019617080688476562\n",
      "Testing at step=94, batch=60, test loss = 0.3970861581658632, test acc = 0.9100000262260437, time = 0.0018994808197021484\n",
      "Testing at step=94, batch=80, test loss = 0.3730559667998275, test acc = 0.8600000143051147, time = 0.002023935317993164\n",
      "Step 94 finished in 22.844453811645508, Train loss = 0.35745329242142293, Test loss = 0.4521423831477949; Train Acc = 0.8749166665474574, Test Acc = 0.8419000005722046\n",
      "Training at step=95, batch=0, train loss = 0.36427719563353345, train acc = 0.8600000143051147, time = 0.009068012237548828\n",
      "Training at step=95, batch=120, train loss = 0.29354216013011497, train acc = 0.8899999856948853, time = 0.008856773376464844\n",
      "Training at step=95, batch=240, train loss = 0.23925369182638861, train acc = 0.8999999761581421, time = 0.00893712043762207\n",
      "Training at step=95, batch=360, train loss = 0.2789489285137536, train acc = 0.8999999761581421, time = 0.009002447128295898\n",
      "Training at step=95, batch=480, train loss = 0.30575739153550424, train acc = 0.8600000143051147, time = 0.00889730453491211\n",
      "Testing at step=95, batch=0, test loss = 0.32635189689562716, test acc = 0.8999999761581421, time = 0.0019116401672363281\n",
      "Testing at step=95, batch=20, test loss = 0.3942012278800477, test acc = 0.8399999737739563, time = 0.0018842220306396484\n",
      "Testing at step=95, batch=40, test loss = 0.5523702747569871, test acc = 0.8399999737739563, time = 0.0019063949584960938\n",
      "Testing at step=95, batch=60, test loss = 0.43359510541459073, test acc = 0.8100000023841858, time = 0.0019545555114746094\n",
      "Testing at step=95, batch=80, test loss = 0.3362101267344837, test acc = 0.8500000238418579, time = 0.0018973350524902344\n",
      "Step 95 finished in 22.84236216545105, Train loss = 0.3569399476995738, Test loss = 0.452128708554975; Train Acc = 0.8742999988794327, Test Acc = 0.8420999974012375\n",
      "Training at step=96, batch=0, train loss = 0.3208923701880568, train acc = 0.8700000047683716, time = 0.009076833724975586\n",
      "Training at step=96, batch=120, train loss = 0.3434233418412033, train acc = 0.8799999952316284, time = 0.00888824462890625\n",
      "Training at step=96, batch=240, train loss = 0.3296561598999033, train acc = 0.8600000143051147, time = 0.009003877639770508\n",
      "Training at step=96, batch=360, train loss = 0.5128990505006544, train acc = 0.8299999833106995, time = 0.009298563003540039\n",
      "Training at step=96, batch=480, train loss = 0.4473763421304289, train acc = 0.8500000238418579, time = 0.008949995040893555\n",
      "Testing at step=96, batch=0, test loss = 0.37566547230110947, test acc = 0.8500000238418579, time = 0.001962900161743164\n",
      "Testing at step=96, batch=20, test loss = 0.4123385171878706, test acc = 0.8700000047683716, time = 0.0019559860229492188\n",
      "Testing at step=96, batch=40, test loss = 0.4071351903752531, test acc = 0.8500000238418579, time = 0.002036571502685547\n",
      "Testing at step=96, batch=60, test loss = 0.3703950288276896, test acc = 0.8700000047683716, time = 0.0018932819366455078\n",
      "Testing at step=96, batch=80, test loss = 0.5257126767353613, test acc = 0.8100000023841858, time = 0.0019559860229492188\n",
      "Step 96 finished in 23.038874626159668, Train loss = 0.3564704577132007, Test loss = 0.4508075645837665; Train Acc = 0.8740499981244405, Test Acc = 0.8418999993801117\n",
      "Training at step=97, batch=0, train loss = 0.27954258416108974, train acc = 0.9200000166893005, time = 0.009081602096557617\n",
      "Training at step=97, batch=120, train loss = 0.5458441965466371, train acc = 0.8700000047683716, time = 0.008920669555664062\n",
      "Training at step=97, batch=240, train loss = 0.2937266614529785, train acc = 0.8700000047683716, time = 0.009100914001464844\n",
      "Training at step=97, batch=360, train loss = 0.4008088326666817, train acc = 0.8500000238418579, time = 0.008901357650756836\n",
      "Training at step=97, batch=480, train loss = 0.4254009218067619, train acc = 0.8399999737739563, time = 0.010272979736328125\n",
      "Testing at step=97, batch=0, test loss = 0.3975483424087565, test acc = 0.8100000023841858, time = 0.0019533634185791016\n",
      "Testing at step=97, batch=20, test loss = 0.5613828804341251, test acc = 0.8199999928474426, time = 0.0022361278533935547\n",
      "Testing at step=97, batch=40, test loss = 0.3978800186615848, test acc = 0.8700000047683716, time = 0.0019202232360839844\n",
      "Testing at step=97, batch=60, test loss = 0.374607731594661, test acc = 0.8500000238418579, time = 0.0019121170043945312\n",
      "Testing at step=97, batch=80, test loss = 0.5040523224472611, test acc = 0.800000011920929, time = 0.0019080638885498047\n",
      "Step 97 finished in 23.080517053604126, Train loss = 0.3567380806623697, Test loss = 0.45632367144975744; Train Acc = 0.8750000002980233, Test Acc = 0.8385999983549118\n",
      "Training at step=98, batch=0, train loss = 0.5555741478381452, train acc = 0.8399999737739563, time = 0.009071826934814453\n",
      "Training at step=98, batch=120, train loss = 0.4554219827839101, train acc = 0.7900000214576721, time = 0.008880853652954102\n",
      "Training at step=98, batch=240, train loss = 0.47623948968553154, train acc = 0.8600000143051147, time = 0.009139537811279297\n",
      "Training at step=98, batch=360, train loss = 0.27831980994975913, train acc = 0.8700000047683716, time = 0.008952856063842773\n",
      "Training at step=98, batch=480, train loss = 0.42137291970513635, train acc = 0.8600000143051147, time = 0.008929014205932617\n",
      "Testing at step=98, batch=0, test loss = 0.5500956014657122, test acc = 0.8299999833106995, time = 0.0019969940185546875\n",
      "Testing at step=98, batch=20, test loss = 0.5743037751419524, test acc = 0.8299999833106995, time = 0.0018994808197021484\n",
      "Testing at step=98, batch=40, test loss = 0.3572133141371692, test acc = 0.8799999952316284, time = 0.001947164535522461\n",
      "Testing at step=98, batch=60, test loss = 0.28920586152890254, test acc = 0.8899999856948853, time = 0.0019114017486572266\n",
      "Testing at step=98, batch=80, test loss = 0.4435366535374344, test acc = 0.8500000238418579, time = 0.0020401477813720703\n",
      "Step 98 finished in 22.923402070999146, Train loss = 0.3569151278173665, Test loss = 0.45320449728247886; Train Acc = 0.8737666682402293, Test Acc = 0.8398999989032745\n",
      "Training at step=99, batch=0, train loss = 0.37081156231158224, train acc = 0.8899999856948853, time = 0.00905609130859375\n",
      "Training at step=99, batch=120, train loss = 0.37945800499175664, train acc = 0.8299999833106995, time = 0.008903264999389648\n",
      "Training at step=99, batch=240, train loss = 0.3685083907798844, train acc = 0.8600000143051147, time = 0.008853435516357422\n",
      "Training at step=99, batch=360, train loss = 0.5102636475388098, train acc = 0.8299999833106995, time = 0.008875131607055664\n",
      "Training at step=99, batch=480, train loss = 0.24731112565143643, train acc = 0.8899999856948853, time = 0.009038209915161133\n",
      "Testing at step=99, batch=0, test loss = 0.35550875023184864, test acc = 0.8999999761581421, time = 0.0019707679748535156\n",
      "Testing at step=99, batch=20, test loss = 0.3907221496141766, test acc = 0.8700000047683716, time = 0.0019040107727050781\n",
      "Testing at step=99, batch=40, test loss = 0.3814963437670143, test acc = 0.8799999952316284, time = 0.0019485950469970703\n",
      "Testing at step=99, batch=60, test loss = 0.3821515442664981, test acc = 0.8700000047683716, time = 0.0019083023071289062\n",
      "Testing at step=99, batch=80, test loss = 0.5728818917130527, test acc = 0.8100000023841858, time = 0.0019099712371826172\n",
      "Step 99 finished in 22.913893699645996, Train loss = 0.3563778870679723, Test loss = 0.45400162282466927; Train Acc = 0.8747333329916, Test Acc = 0.8421999990940094\n"
     ]
    }
   ],
   "source": [
    "import torchmetrics\n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "#accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 3e-4\n",
    "STEPS = 100\n",
    "PRINT_EVERY_PERCENT = 0.2\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    optim=torch.optim.SGD,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    accuracy = torchmetrics.Accuracy,\n",
    "    steps = 100,\n",
    "    print_every_percent=0.1,\n",
    "    batchsize = 100,\n",
    "    lr = 0.001,\n",
    "    device=torch.device(\"cpu\")\n",
    "):\n",
    "  trainloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "  testloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batchsize, shuffle=True\n",
    "  )\n",
    "\n",
    "  n_train_batches = len(trainloader)\n",
    "  n_test_batches = len(testloader)\n",
    "  print_every_train_batch = int(n_train_batches*print_every_percent)\n",
    "  print_every_test_batch = int(n_test_batches*print_every_percent)\n",
    "\n",
    "  print(f\"Number of train batches = {n_train_batches}, Number of test batches = {n_test_batches}\")\n",
    "  print(f\"Print every train batch = {print_every_train_batch}, Print every test batch = {print_every_test_batch}\")\n",
    "\n",
    "  model.to(device)\n",
    "  optimizer = optim(model.parameters(), lr=lr, momentum=0.9)\n",
    "  loss = criterion()\n",
    "  acc_func = accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "  step_train_losses = []\n",
    "  step_test_losses = []\n",
    "  step_train_accs = []\n",
    "  step_test_accs = []\n",
    "  for i in range(steps):\n",
    "    step_start = time.time()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    # train\n",
    "    model.train()\n",
    "    for batchid, (images, labels) in enumerate(trainloader):\n",
    "      batch_start = time.time()\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)\n",
    "      train_loss = loss(outputs, labels)\n",
    "      train_loss.backward()\n",
    "      optimizer.step()\n",
    "      train_acc = acc_func(outputs, labels)\n",
    "      batch_train_loss.append(train_loss.item())\n",
    "      batch_train_acc.append(train_acc.item())\n",
    "      batch_finish = time.time()\n",
    "\n",
    "      if (batchid) % print_every_train_batch == 0:\n",
    "        print(f\"Training at step={i}, batch={batchid}, train loss = {train_loss.item()}, train acc = {train_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    # eval\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      for batchid, (images, labels) in enumerate(testloader):\n",
    "        batch_start = time.time()\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        test_loss = loss(outputs, labels)\n",
    "        test_acc = acc_func(outputs, labels)\n",
    "        batch_test_loss.append(test_loss.item())\n",
    "        batch_test_acc.append(test_acc.item())\n",
    "        batch_finish = time.time()\n",
    "        if (batchid) % print_every_test_batch == 0:\n",
    "          print(f\"Testing at step={i}, batch={batchid}, test loss = {test_loss.item()}, test acc = {test_acc.item()}, time = {batch_finish-batch_start}\")\n",
    "\n",
    "    step_train_losses.append(np.mean(batch_train_loss))\n",
    "    step_test_losses.append(np.mean(batch_test_loss))\n",
    "    step_train_accs.append(np.mean(batch_train_acc))\n",
    "    step_test_accs.append(np.mean(batch_test_acc))\n",
    "    step_finish = time.time()\n",
    "    print(f\"Step {i} finished in {step_finish-step_start}, Train loss = {step_train_losses[-1]}, Test loss = {step_test_losses[-1]}; Train Acc = {step_train_accs[-1]}, Test Acc = {step_test_accs[-1]}\")\n",
    "\n",
    "  return step_train_losses, step_test_losses, step_train_accs, step_test_accs\n",
    "\n",
    "train_losses, test_losses, train_accs, test_accs = train(net,\n",
    "                                                        optim=torch.optim.SGD,\n",
    "                                                        criterion=torch.nn.CrossEntropyLoss,\n",
    "                                                        accuracy = torchmetrics.Accuracy,\n",
    "                                                        steps = STEPS,\n",
    "                                                        print_every_percent=PRINT_EVERY_PERCENT,\n",
    "                                                        batchsize = BATCH_SIZE,\n",
    "                                                        lr = LEARNING_RATE,\n",
    "                                                        device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V7YYHs33saIx",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1712093845925,
     "user_tz": -660,
     "elapsed": 953,
     "user": {
      "displayName": "P. W.",
      "userId": "06457912707533471190"
     }
    },
    "outputId": "928859bd-acd3-46ea-be13-39efadae9e50"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# plot\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot the losses\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(test_losses, label='Test Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Loss vs. Epoch')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot the accuracies\n",
    "ax2.plot(train_accs, label='Train Accuracy')\n",
    "ax2.plot(test_accs, label='Test Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs. Epoch')\n",
    "ax2.legend()\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
